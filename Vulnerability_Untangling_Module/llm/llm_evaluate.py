from pathlib import Path
import json
import os
import time
from openai import OpenAI
import csv
import csv

#Counters for result categories
count = 0
yes = 0
uncertain = 0
no = 0

#Initialize OpenAI client using Hugging Face
API_KEY = os.environ["HUGGING_FACE_TOKEN"]
client = OpenAI(base_url="https://router.huggingface.co/v1", api_key=API_KEY)

count = 0

def chat_single(question_str, short=True):
    """
    Sends a single query to the Hugging Face LLM API and returns its response.

    Parameters:
        question_str (str): The question or prompt to send to the model.
        short (bool): If True, uses a smaller token limit and shorter delay between requests.

    Returns:
        str or None: The model's textual response, or None if an error occurs.
    """
    global count
    count += 1

    if not short:
        time.sleep(6)  #Add delay for longer requests to avoid rate limits

    try:
        #Send message to the model
        completion = client.chat.completions.create(
            model="Qwen/Qwen3-14B:nebius",
            messages=[{"role": "user", "content": question_str}],
            max_tokens=256 if short else 512,
        )
        answer = completion.choices[0].message.content
        return answer

    except Exception as e:
        print(f"[Error] {e}")
        return None


def get_answer(answer):
    """
    Standardizes a model response into 'YES', 'NO', or 'UNCERTAIN'.
    """
    if isinstance(answer, str):
        answer = answer.upper()
    if 'YES' in answer:
        return 'YES'
    elif 'NO' in answer:
        return 'NO'
    else:
        return 'UNCERTAIN'

def question_answer(language, file_name, write_name):
    """
    Processes patch data for a given programming language and evaluates each
    using an LLM to determine if the code modifications are related to a CVE fix.

    Parameters:
        language (list[str]): List of languages (e.g., ['c', 'h']) to evaluate.
        file_name (str | Path): Path to the JSONL input file containing patch data.
        write_name (str | Path): Path to write the LLM-evaluated output file.

    Input files:
        Raw_Data_Crawling/github/merge_result_new/language/merge_<language>.jsonl

    Output files:
        Vulnerability_Untangling_Module/output/llm/merge_<language>.jsonl
    """
    Max_patch = 2000 * 5  #Reserved variable for future patch size control

    #Read JSONL file line-by-line
    with open(file_name, "r", encoding="utf-8") as r:
        content = r.readlines()

        for i in range(len(content)):
            record_dict = json.loads(content[i])
            CWEs = record_dict['cwe_id']  #List of CWE IDs
            CWE_info = ''

            #Load CWE descriptions and mitigations
            if len(CWEs) > 0:
                for i, CWE in enumerate(CWEs):
                    with open('CWE.csv', 'r', encoding='utf-8') as csvfile:
                        csv_reader = csv.reader(csvfile)
                        cnt = 0
                        desription = ''
                        mitigations = ''
                        for row in csv_reader:
                            if cnt != 0:
                                CWE_id = CWE.split('-')[1]
                                if row[0] == CWE_id:
                                    desription = row[4]
                                    mitigations = row[16]
                                    break
                            cnt += 1

                    CWE_info += f'[CWE {i+1} start]: ' + CWE + ' ' + desription + ' ' + mitigations + f' [CWE {i+1} end]\n'

            ori_message = record_dict['commit_message']
            details = record_dict['details']

            #Define base system message for the LLM
            system_message = (
                "You are now an expert in code vulnerability and patch fixes. "
                "You will be provided with a CVE (Common Vulnerabilities and Exposures) "
                "and CWE (Common Weakness Enumeration) information. "
                "You will also receive a patch, including commit messages and code modifications. "
                "Each patch may involve multiple files, and your task is to determine whether "
                "the given file modifications are related to the CVE fix."
            )

            prompt_OSL = ''

            for idx, detail in enumerate(details):
                #Construct prompt with CWE info, commit message, and file modifications
                prompt = prompt_OSL + system_message + ' [CWE information]: ' + CWE_info + ' [Commit Message]: ' + ori_message + ' '
                prompt_long = prompt
                prompt_short = prompt

                functions_patchs = detail.get('functions_patchs', '')
                functions_patchs_remain = detail.get('functions_patchs_remain', '')
                answer_prompt = "\nYou should check whether modifications in the modified file are related to the CVE and answer 'YES', 'NO', or 'UNCERTAIN'."

                #Add modifications and context to prompt
                for idx1, function_patch in enumerate(functions_patchs):
                    patch_prompt = f'[Modification {idx1} Start]: ' + function_patch['patch'] + f'[Modification {idx1} End]\n'
                    function_prompt = f'[Function or Structure {idx1} Start]: ' + function_patch['function'] + f'[Function or Structure {idx1} End]\n'
                    prompt_short += patch_prompt
                    prompt_long += patch_prompt + function_prompt

                temp = len(functions_patchs)
                for idx2, function_patch_remain in enumerate(functions_patchs_remain):
                    patch_prompt = f'[Modification {temp + idx2} Start]: ' + function_patch_remain + f'[Modification {temp + idx2} End]\n'
                    prompt_short += patch_prompt
                    prompt_long += patch_prompt

                prompt_long += answer_prompt
                prompt_short += answer_prompt

                #Try multiple attempts to get a valid LLM answer
                Max_try = 10
                tried = 0
                got_ans = False
                answer = ''
                global count

                try:
                    while tried < Max_try and not got_ans:
                        response = chat_single(prompt_long)
                        if response:
                            got_ans = True
                            answer = response
                        else:
                            tried += 1
                        count += 1

                    while tried < Max_try and not got_ans:
                        count += 1
                        response = chat_single(prompt_long, short=False)
                        if response:
                            got_ans = True
                            answer = response
                        else:
                            tried += 1

                    while tried < Max_try and not got_ans:
                        count += 1
                        response = chat_single(prompt_short, short=False)
                        if response:
                            got_ans = True
                            answer = response
                        else:
                            tried += 1

                except Exception as e:
                    print(e)

                #Standardize and record the answer
                if answer == '':
                    answer = 'UNCERTAIN'
                if answer != '':
                    answer = get_answer(answer)
                    record_dict['details'][idx]['llm_check'] = answer

            #Write evaluated record to output file
            with open(write_name, 'a', encoding='utf-8') as f2:
                jsonobj = json.dumps(record_dict, ensure_ascii=False)
                f2.write(jsonobj + '\n')

    return 1


def question_answer_last(language, file_name, write_name):
    """
    Performs a final LLM evaluation pass for entries that have not yet been validated
    (agree_check == -1), adding decisions to each record.

    Parameters:
        language (list[str]): List of target file extensions/languages.
        file_name (str | Path): Path to JSONL file containing raw CVE patch data.
        write_name (str | Path): Path to save the updated results with LLM checks.

    Input files:
        Raw_Data_Crawling/github/merge_result_new/language/merge_<language>.jsonl

    Output files:
        Vulnerability_Untangling_Module/output/llm/merge_<language>.jsonl
    """
    Max_patch = 2000 * 5

    with open(file_name, "r", encoding="utf-8") as r:
        content = r.readlines()
        for i in range(len(content)):
            record_dict = json.loads(content[i])
            CWEs = record_dict['cwe_id']
            CWE_info = ''

            #Load CWE information
            if len(CWEs) > 0:
                for i, CWE in enumerate(CWEs):
                    with open('CWE.csv', 'r', encoding='utf-8') as csvfile:
                        csv_reader = csv.reader(csvfile)
                        cnt = 0
                        desription = ''
                        mitigations = ''
                        for row in csv_reader:
                            if cnt != 0:
                                CWE_id = CWE.split('-')[1]
                                if row[0] == CWE_id:
                                    desription = row[4]
                                    mitigations = row[16]
                                    break
                            cnt += 1
                    CWE_info += f'[CWE {i+1} start]: ' + CWE + ' ' + desription + ' ' + mitigations + f' [CWE {i+1} end]\n'

            ori_message = record_dict['commit_message']
            details = record_dict['details']

            system_message = (
                "You are now an expert in code vulnerability and patch fixes. "
                "You will be provided with a CVE and CWE description, "
                "commit message, and patch details. "
                "Determine if the file modifications are related to the CVE fix."
            )

            prompt_OSL = ''

            for idx, detail in enumerate(details):
                #Only process entries still unverified
                if detail['agree_check'] == -1 and detail['file_language'] in language:
                    prompt = prompt_OSL + system_message + ' [CWE information]: ' + CWE_info + ' [Commit Message]: ' + ori_message + ' '
                    prompt_long = prompt
                    prompt_short = prompt
                    functions_patchs = detail['functions_patchs']
                    functions_patchs_remain = detail['functions_patchs_remain']
                    answer_prompt = "\nYou should check whether modifications are related to the CVE and answer 'YES' or 'NO'."

                    for idx1, function_patch in enumerate(functions_patchs):
                        patch_prompt = f'[Modification {idx1} Start]: ' + function_patch['patch'] + f'[Modification {idx1} End]\n'
                        function_prompt = f'[Function or Structure {idx1} Start]: ' + function_patch['function'] + f'[Function or Structure {idx1} End]\n'
                        prompt_short += patch_prompt
                        prompt_long += patch_prompt + function_prompt

                    temp = len(functions_patchs)
                    for idx2, function_patch_remain in enumerate(functions_patchs_remain):
                        patch_prompt = f'[Modification {temp + idx2} Start]: ' + function_patch_remain + f'[Modification {temp + idx2} End]\n'
                        prompt_short += patch_prompt
                        prompt_long += patch_prompt

                    prompt_long += answer_prompt
                    prompt_short += answer_prompt

                    #Multiple attempts to ensure a valid LLM response
                    Max_try = 20
                    tried = 0
                    got_ans = False
                    answer = ''
                    global count

                    try:
                        while tried < Max_try and not got_ans:
                            response = chat_single(prompt_long)
                            if response:
                                got_ans = True
                                answer = response
                            else:
                                tried += 1
                            count += 1

                        while tried < Max_try and not got_ans:
                            count += 1
                            response = chat_single(prompt_long, short=False)
                            if response:
                                got_ans = True
                                answer = response
                            else:
                                tried += 1

                        while tried < Max_try and not got_ans:
                            count += 1
                            response = chat_single(prompt_short, short=False)
                            if response:
                                got_ans = True
                                answer = response
                            else:
                                tried += 1

                    except Exception as e:
                        print(e)

                    if answer == '':
                        answer = 'UNCERTAIN'
                    if answer != '':
                        answer = get_answer(answer)
                        record_dict['details'][idx]['agree_check'] = answer

                #Save updated record
                with open(write_name, 'a', encoding='utf-8') as f2:
                    jsonobj = json.dumps(record_dict, ensure_ascii=False)
                    f2.write(jsonobj + '\n')

    return 1

def main():
    """
    Entry point for executing the LLM-based vulnerability verification.
    Runs evaluation for both C and C++ source code patches.
    """
    project_root = Path(__file__).resolve().parents[2]
    base_dir = project_root / "Vulnerability_Untangling_Module"

    #Evaluate C files
    language = ["c", "h"]
    file_name = project_root / "Raw_Data_Crawling" / "github" / "merge_result_new" / "language" / "merge_C.jsonl"
    write_name = base_dir / "output" / "llm" / "merge_C.jsonl"
    question_answer(language, file_name, write_name)
    print("Finished LLM evaluation for C files")

    #Evaluate C++ files
    language = ["cpp", "cc", "h"]
    file_name = project_root / "Raw_Data_Crawling" / "github" / "merge_result_new" / "language" / "merge_cpp.jsonl"
    write_name = base_dir / "output" / "llm" / "merge_cpp.jsonl"
    question_answer(language, file_name, write_name)
    print("Finished LLM evaluation for C++ files")

if __name__ == "__main__":
    main()
