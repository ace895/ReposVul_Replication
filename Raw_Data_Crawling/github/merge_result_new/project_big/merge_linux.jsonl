{"index": 33, "cve_id": "CVE-2016-3841", "cwe_id": ["CWE-264", "CWE-416"], "cve_language": "C", "cve_description": "The IPv6 stack in the Linux kernel before 4.3.3 mishandles options data, which allows local users to gain privileges or cause a denial of service (use-after-free and system crash) via a crafted sendmsg system call.", "cvss": "7.3", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "REQUIRED", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "45f6fad84cc305103b28d73482b344d7f5b76f39", "commit_message": "ipv6: add complete rcu protection around np->opt\n\nThis patch addresses multiple problems :\n\nUDP/RAW sendmsg() need to get a stable struct ipv6_txoptions\nwhile socket is not locked : Other threads can change np->opt\nconcurrently. Dmitry posted a syzkaller\n(http://github.com/google/syzkaller) program desmonstrating\nuse-after-free.\n\nStarting with TCP/DCCP lockless listeners, tcp_v6_syn_recv_sock()\nand dccp_v6_request_recv_sock() also need to use RCU protection\nto dereference np->opt once (before calling ipv6_dup_options())\n\nThis patch adds full RCU protection to np->opt\n\nReported-by: Dmitry Vyukov <dvyukov@google.com>\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nAcked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "commit_date": "2015-12-03T04:37:16Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/45f6fad84cc305103b28d73482b344d7f5b76f39", "html_url": "https://github.com/torvalds/linux/commit/45f6fad84cc305103b28d73482b344d7f5b76f39", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "01b3f52157ff5a47d6d8d796f396a4b34a53c61d", "url_before": "https://api.github.com/repos/torvalds/linux/commits/01b3f52157ff5a47d6d8d796f396a4b34a53c61d", "html_url_before": "https://github.com/torvalds/linux/commit/01b3f52157ff5a47d6d8d796f396a4b34a53c61d"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/include/linux/ipv6.h", "code": "#ifndef _IPV6_H\n#define _IPV6_H\n\n#include <uapi/linux/ipv6.h>\n\n#define ipv6_optlen(p)  (((p)->hdrlen+1) << 3)\n#define ipv6_authlen(p) (((p)->hdrlen+2) << 2)\n/*\n * This structure contains configuration options per IPv6 link.\n */\nstruct ipv6_devconf {\n\t__s32\t\tforwarding;\n\t__s32\t\thop_limit;\n\t__s32\t\tmtu6;\n\t__s32\t\taccept_ra;\n\t__s32\t\taccept_redirects;\n\t__s32\t\tautoconf;\n\t__s32\t\tdad_transmits;\n\t__s32\t\trtr_solicits;\n\t__s32\t\trtr_solicit_interval;\n\t__s32\t\trtr_solicit_delay;\n\t__s32\t\tforce_mld_version;\n\t__s32\t\tmldv1_unsolicited_report_interval;\n\t__s32\t\tmldv2_unsolicited_report_interval;\n\t__s32\t\tuse_tempaddr;\n\t__s32\t\ttemp_valid_lft;\n\t__s32\t\ttemp_prefered_lft;\n\t__s32\t\tregen_max_retry;\n\t__s32\t\tmax_desync_factor;\n\t__s32\t\tmax_addresses;\n\t__s32\t\taccept_ra_defrtr;\n\t__s32\t\taccept_ra_min_hop_limit;\n\t__s32\t\taccept_ra_pinfo;\n\t__s32\t\tignore_routes_with_linkdown;\n#ifdef CONFIG_IPV6_ROUTER_PREF\n\t__s32\t\taccept_ra_rtr_pref;\n\t__s32\t\trtr_probe_interval;\n#ifdef CONFIG_IPV6_ROUTE_INFO\n\t__s32\t\taccept_ra_rt_info_max_plen;\n#endif\n#endif\n\t__s32\t\tproxy_ndp;\n\t__s32\t\taccept_source_route;\n\t__s32\t\taccept_ra_from_local;\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\t__s32\t\toptimistic_dad;\n\t__s32\t\tuse_optimistic;\n#endif\n#ifdef CONFIG_IPV6_MROUTE\n\t__s32\t\tmc_forwarding;\n#endif\n\t__s32\t\tdisable_ipv6;\n\t__s32\t\taccept_dad;\n\t__s32\t\tforce_tllao;\n\t__s32           ndisc_notify;\n\t__s32\t\tsuppress_frag_ndisc;\n\t__s32\t\taccept_ra_mtu;\n\tstruct ipv6_stable_secret {\n\t\tbool initialized;\n\t\tstruct in6_addr secret;\n\t} stable_secret;\n\t__s32\t\tuse_oif_addrs_only;\n\tvoid\t\t*sysctl;\n};\n\nstruct ipv6_params {\n\t__s32 disable_ipv6;\n\t__s32 autoconf;\n};\nextern struct ipv6_params ipv6_defaults;\n#include <linux/icmpv6.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n\n#include <net/inet_sock.h>\n\nstatic inline struct ipv6hdr *ipv6_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ipv6hdr *)skb_network_header(skb);\n}\n\nstatic inline struct ipv6hdr *inner_ipv6_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ipv6hdr *)skb_inner_network_header(skb);\n}\n\nstatic inline struct ipv6hdr *ipipv6_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ipv6hdr *)skb_transport_header(skb);\n}\n\n/* \n   This structure contains results of exthdrs parsing\n   as offsets from skb->nh.\n */\n\nstruct inet6_skb_parm {\n\tint\t\t\tiif;\n\t__be16\t\t\tra;\n\t__u16\t\t\tdst0;\n\t__u16\t\t\tsrcrt;\n\t__u16\t\t\tdst1;\n\t__u16\t\t\tlastopt;\n\t__u16\t\t\tnhoff;\n\t__u16\t\t\tflags;\n#if defined(CONFIG_IPV6_MIP6) || defined(CONFIG_IPV6_MIP6_MODULE)\n\t__u16\t\t\tdsthao;\n#endif\n\t__u16\t\t\tfrag_max_size;\n\n#define IP6SKB_XFRM_TRANSFORMED\t1\n#define IP6SKB_FORWARDED\t2\n#define IP6SKB_REROUTED\t\t4\n#define IP6SKB_ROUTERALERT\t8\n#define IP6SKB_FRAGMENTED      16\n#define IP6SKB_HOPBYHOP        32\n};\n\n#define IP6CB(skb)\t((struct inet6_skb_parm*)((skb)->cb))\n#define IP6CBMTU(skb)\t((struct ip6_mtuinfo *)((skb)->cb))\n\nstatic inline int inet6_iif(const struct sk_buff *skb)\n{\n\treturn IP6CB(skb)->iif;\n}\n\nstruct tcp6_request_sock {\n\tstruct tcp_request_sock\t  tcp6rsk_tcp;\n};\n\nstruct ipv6_mc_socklist;\nstruct ipv6_ac_socklist;\nstruct ipv6_fl_socklist;\n\nstruct inet6_cork {\n\tstruct ipv6_txoptions *opt;\n\tu8 hop_limit;\n\tu8 tclass;\n};\n\n/**\n * struct ipv6_pinfo - ipv6 private area\n *\n * In the struct sock hierarchy (tcp6_sock, upd6_sock, etc)\n * this _must_ be the last member, so that inet6_sk_generic\n * is able to calculate its offset from the base struct sock\n * by using the struct proto->slab_obj_size member. -acme\n */\nstruct ipv6_pinfo {\n\tstruct in6_addr \tsaddr;\n\tstruct in6_pktinfo\tsticky_pktinfo;\n\tconst struct in6_addr\t\t*daddr_cache;\n#ifdef CONFIG_IPV6_SUBTREES\n\tconst struct in6_addr\t\t*saddr_cache;\n#endif\n\n\t__be32\t\t\tflow_label;\n\t__u32\t\t\tfrag_size;\n\n\t/*\n\t * Packed in 16bits.\n\t * Omit one shift by by putting the signed field at MSB.\n\t */\n#if defined(__BIG_ENDIAN_BITFIELD)\n\t__s16\t\t\thop_limit:9;\n\t__u16\t\t\t__unused_1:7;\n#else\n\t__u16\t\t\t__unused_1:7;\n\t__s16\t\t\thop_limit:9;\n#endif\n\n#if defined(__BIG_ENDIAN_BITFIELD)\n\t/* Packed in 16bits. */\n\t__s16\t\t\tmcast_hops:9;\n\t__u16\t\t\t__unused_2:6,\n\t\t\t\tmc_loop:1;\n#else\n\t__u16\t\t\tmc_loop:1,\n\t\t\t\t__unused_2:6;\n\t__s16\t\t\tmcast_hops:9;\n#endif\n\tint\t\t\tucast_oif;\n\tint\t\t\tmcast_oif;\n\n\t/* pktoption flags */\n\tunion {\n\t\tstruct {\n\t\t\t__u16\tsrcrt:1,\n\t\t\t\tosrcrt:1,\n\t\t\t        rxinfo:1,\n\t\t\t        rxoinfo:1,\n\t\t\t\trxhlim:1,\n\t\t\t\trxohlim:1,\n\t\t\t\thopopts:1,\n\t\t\t\tohopopts:1,\n\t\t\t\tdstopts:1,\n\t\t\t\todstopts:1,\n                                rxflow:1,\n\t\t\t\trxtclass:1,\n\t\t\t\trxpmtu:1,\n\t\t\t\trxorigdstaddr:1;\n\t\t\t\t/* 2 bits hole */\n\t\t} bits;\n\t\t__u16\t\tall;\n\t} rxopt;\n\n\t/* sockopt flags */\n\t__u16\t\t\trecverr:1,\n\t                        sndflow:1,\n\t\t\t\trepflow:1,\n\t\t\t\tpmtudisc:3,\n\t\t\t\tpadding:1,\t/* 1 bit hole */\n\t\t\t\tsrcprefs:3,\t/* 001: prefer temporary address\n\t\t\t\t\t\t * 010: prefer public address\n\t\t\t\t\t\t * 100: prefer care-of address\n\t\t\t\t\t\t */\n\t\t\t\tdontfrag:1,\n\t\t\t\tautoflowlabel:1;\n\t__u8\t\t\tmin_hopcount;\n\t__u8\t\t\ttclass;\n\t__be32\t\t\trcv_flowinfo;\n\n\t__u32\t\t\tdst_cookie;\n\t__u32\t\t\trx_dst_cookie;\n\n\tstruct ipv6_mc_socklist\t__rcu *ipv6_mc_list;\n\tstruct ipv6_ac_socklist\t*ipv6_ac_list;\n\tstruct ipv6_fl_socklist __rcu *ipv6_fl_list;\n\n\tstruct ipv6_txoptions __rcu\t*opt;\n\tstruct sk_buff\t\t*pktoptions;\n\tstruct sk_buff\t\t*rxpmtu;\n\tstruct inet6_cork\tcork;\n};\n\n/* WARNING: don't change the layout of the members in {raw,udp,tcp}6_sock! */\nstruct raw6_sock {\n\t/* inet_sock has to be the first member of raw6_sock */\n\tstruct inet_sock\tinet;\n\t__u32\t\t\tchecksum;\t/* perform checksum */\n\t__u32\t\t\toffset;\t\t/* checksum offset  */\n\tstruct icmp6_filter\tfilter;\n\t__u32\t\t\tip6mr_table;\n\t/* ipv6_pinfo has to be the last member of raw6_sock, see inet6_sk_generic */\n\tstruct ipv6_pinfo\tinet6;\n};\n\nstruct udp6_sock {\n\tstruct udp_sock\t  udp;\n\t/* ipv6_pinfo has to be the last member of udp6_sock, see inet6_sk_generic */\n\tstruct ipv6_pinfo inet6;\n};\n\nstruct tcp6_sock {\n\tstruct tcp_sock\t  tcp;\n\t/* ipv6_pinfo has to be the last member of tcp6_sock, see inet6_sk_generic */\n\tstruct ipv6_pinfo inet6;\n};\n\nextern int inet6_sk_rebuild_header(struct sock *sk);\n\nstruct tcp6_timewait_sock {\n\tstruct tcp_timewait_sock   tcp6tw_tcp;\n};\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic inline struct ipv6_pinfo *inet6_sk(const struct sock *__sk)\n{\n\treturn sk_fullsock(__sk) ? inet_sk(__sk)->pinet6 : NULL;\n}\n\nstatic inline struct raw6_sock *raw6_sk(const struct sock *sk)\n{\n\treturn (struct raw6_sock *)sk;\n}\n\nstatic inline void inet_sk_copy_descendant(struct sock *sk_to,\n\t\t\t\t\t   const struct sock *sk_from)\n{\n\tint ancestor_size = sizeof(struct inet_sock);\n\n\tif (sk_from->sk_family == PF_INET6)\n\t\tancestor_size += sizeof(struct ipv6_pinfo);\n\n\t__inet_sk_copy_descendant(sk_to, sk_from, ancestor_size);\n}\n\n#define __ipv6_only_sock(sk)\t(sk->sk_ipv6only)\n#define ipv6_only_sock(sk)\t(__ipv6_only_sock(sk))\n#define ipv6_sk_rxinfo(sk)\t((sk)->sk_family == PF_INET6 && \\\n\t\t\t\t inet6_sk(sk)->rxopt.bits.rxinfo)\n\nstatic inline const struct in6_addr *inet6_rcv_saddr(const struct sock *sk)\n{\n\tif (sk->sk_family == AF_INET6)\n\t\treturn &sk->sk_v6_rcv_saddr;\n\treturn NULL;\n}\n\nstatic inline int inet_v6_ipv6only(const struct sock *sk)\n{\n\t/* ipv6only field is at same position for timewait and other sockets */\n\treturn ipv6_only_sock(sk);\n}\n#else\n#define __ipv6_only_sock(sk)\t0\n#define ipv6_only_sock(sk)\t0\n#define ipv6_sk_rxinfo(sk)\t0\n\nstatic inline struct ipv6_pinfo * inet6_sk(const struct sock *__sk)\n{\n\treturn NULL;\n}\n\nstatic inline struct inet6_request_sock *\n\t\t\tinet6_rsk(const struct request_sock *rsk)\n{\n\treturn NULL;\n}\n\nstatic inline struct raw6_sock *raw6_sk(const struct sock *sk)\n{\n\treturn NULL;\n}\n\n#define inet6_rcv_saddr(__sk)\tNULL\n#define tcp_twsk_ipv6only(__sk)\t\t0\n#define inet_v6_ipv6only(__sk)\t\t0\n#endif /* IS_ENABLED(CONFIG_IPV6) */\n#endif /* _IPV6_H */\n", "code_before": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _IPV6_H\n#define _IPV6_H\n\n#include <uapi/linux/ipv6.h>\n#include <linux/cache.h>\n\n#define ipv6_optlen(p)  (((p)->hdrlen+1) << 3)\n#define ipv6_authlen(p) (((p)->hdrlen+2) << 2)\n/*\n * This structure contains configuration options per IPv6 link.\n */\nstruct ipv6_devconf {\n\t/* RX & TX fastpath fields. */\n\t__cacheline_group_begin(ipv6_devconf_read_txrx);\n\t__s32\t\tdisable_ipv6;\n\t__s32\t\thop_limit;\n\t__s32\t\tmtu6;\n\t__s32\t\tforwarding;\n\t__s32\t\tforce_forwarding;\n\t__s32\t\tdisable_policy;\n\t__s32\t\tproxy_ndp;\n\t__cacheline_group_end(ipv6_devconf_read_txrx);\n\n\t__s32\t\taccept_ra;\n\t__s32\t\taccept_redirects;\n\t__s32\t\tautoconf;\n\t__s32\t\tdad_transmits;\n\t__s32\t\trtr_solicits;\n\t__s32\t\trtr_solicit_interval;\n\t__s32\t\trtr_solicit_max_interval;\n\t__s32\t\trtr_solicit_delay;\n\t__s32\t\tforce_mld_version;\n\t__s32\t\tmldv1_unsolicited_report_interval;\n\t__s32\t\tmldv2_unsolicited_report_interval;\n\t__s32\t\tuse_tempaddr;\n\t__s32\t\ttemp_valid_lft;\n\t__s32\t\ttemp_prefered_lft;\n\t__s32\t\tregen_min_advance;\n\t__s32\t\tregen_max_retry;\n\t__s32\t\tmax_desync_factor;\n\t__s32\t\tmax_addresses;\n\t__s32\t\taccept_ra_defrtr;\n\t__u32\t\tra_defrtr_metric;\n\t__s32\t\taccept_ra_min_hop_limit;\n\t__s32\t\taccept_ra_min_lft;\n\t__s32\t\taccept_ra_pinfo;\n\t__s32\t\tignore_routes_with_linkdown;\n#ifdef CONFIG_IPV6_ROUTER_PREF\n\t__s32\t\taccept_ra_rtr_pref;\n\t__s32\t\trtr_probe_interval;\n#ifdef CONFIG_IPV6_ROUTE_INFO\n\t__s32\t\taccept_ra_rt_info_min_plen;\n\t__s32\t\taccept_ra_rt_info_max_plen;\n#endif\n#endif\n\t__s32\t\taccept_source_route;\n\t__s32\t\taccept_ra_from_local;\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\t__s32\t\toptimistic_dad;\n\t__s32\t\tuse_optimistic;\n#endif\n#ifdef CONFIG_IPV6_MROUTE\n\tatomic_t\tmc_forwarding;\n#endif\n\t__s32\t\tdrop_unicast_in_l2_multicast;\n\t__s32\t\taccept_dad;\n\t__s32\t\tforce_tllao;\n\t__s32           ndisc_notify;\n\t__s32\t\tsuppress_frag_ndisc;\n\t__s32\t\taccept_ra_mtu;\n\t__s32\t\tdrop_unsolicited_na;\n\t__s32\t\taccept_untracked_na;\n\tstruct ipv6_stable_secret {\n\t\tbool initialized;\n\t\tstruct in6_addr secret;\n\t} stable_secret;\n\t__s32\t\tuse_oif_addrs_only;\n\t__s32\t\tkeep_addr_on_down;\n\t__s32\t\tseg6_enabled;\n#ifdef CONFIG_IPV6_SEG6_HMAC\n\t__s32\t\tseg6_require_hmac;\n#endif\n\t__u32\t\tenhanced_dad;\n\t__u32\t\taddr_gen_mode;\n\t__s32           ndisc_tclass;\n\t__s32\t\trpl_seg_enabled;\n\t__u32\t\tioam6_id;\n\t__u32\t\tioam6_id_wide;\n\t__u8\t\tioam6_enabled;\n\t__u8\t\tndisc_evict_nocarrier;\n\t__u8\t\tra_honor_pio_life;\n\t__u8\t\tra_honor_pio_pflag;\n\n\tstruct ctl_table_header *sysctl_header;\n};\n\nstruct ipv6_params {\n\t__s32 disable_ipv6;\n\t__s32 autoconf;\n};\nextern struct ipv6_params ipv6_defaults;\n#include <linux/tcp.h>\n#include <linux/udp.h>\n\n#include <net/inet_sock.h>\n\nstatic inline struct ipv6hdr *ipv6_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ipv6hdr *)skb_network_header(skb);\n}\n\nstatic inline struct ipv6hdr *inner_ipv6_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ipv6hdr *)skb_inner_network_header(skb);\n}\n\nstatic inline struct ipv6hdr *ipipv6_hdr(const struct sk_buff *skb)\n{\n\treturn (struct ipv6hdr *)skb_transport_header(skb);\n}\n\nstatic inline unsigned int ipv6_transport_len(const struct sk_buff *skb)\n{\n\treturn ntohs(ipv6_hdr(skb)->payload_len) + sizeof(struct ipv6hdr) -\n\t       skb_network_header_len(skb);\n}\n\n/* \n   This structure contains results of exthdrs parsing\n   as offsets from skb->nh.\n */\n\nstruct inet6_skb_parm {\n\tint\t\t\tiif;\n\t__be16\t\t\tra;\n\t__u16\t\t\tdst0;\n\t__u16\t\t\tsrcrt;\n\t__u16\t\t\tdst1;\n\t__u16\t\t\tlastopt;\n\t__u16\t\t\tnhoff;\n\t__u16\t\t\tflags;\n#if defined(CONFIG_IPV6_MIP6) || defined(CONFIG_IPV6_MIP6_MODULE)\n\t__u16\t\t\tdsthao;\n#endif\n\t__u16\t\t\tfrag_max_size;\n\t__u16\t\t\tsrhoff;\n\n#define IP6SKB_XFRM_TRANSFORMED\t1\n#define IP6SKB_FORWARDED\t2\n#define IP6SKB_REROUTED\t\t4\n#define IP6SKB_ROUTERALERT\t8\n#define IP6SKB_FRAGMENTED      16\n#define IP6SKB_HOPBYHOP        32\n#define IP6SKB_L3SLAVE         64\n#define IP6SKB_JUMBOGRAM      128\n#define IP6SKB_SEG6\t      256\n#define IP6SKB_FAKEJUMBO      512\n#define IP6SKB_MULTIPATH      1024\n#define IP6SKB_MCROUTE        2048\n};\n\n#if defined(CONFIG_NET_L3_MASTER_DEV)\nstatic inline bool ipv6_l3mdev_skb(__u16 flags)\n{\n\treturn flags & IP6SKB_L3SLAVE;\n}\n#else\nstatic inline bool ipv6_l3mdev_skb(__u16 flags)\n{\n\treturn false;\n}\n#endif\n\n#define IP6CB(skb)\t((struct inet6_skb_parm*)((skb)->cb))\n#define IP6CBMTU(skb)\t((struct ip6_mtuinfo *)((skb)->cb))\n\nstatic inline int inet6_iif(const struct sk_buff *skb)\n{\n\tbool l3_slave = ipv6_l3mdev_skb(IP6CB(skb)->flags);\n\n\treturn l3_slave ? skb->skb_iif : IP6CB(skb)->iif;\n}\n\nstatic inline bool inet6_is_jumbogram(const struct sk_buff *skb)\n{\n\treturn !!(IP6CB(skb)->flags & IP6SKB_JUMBOGRAM);\n}\n\n/* can not be used in TCP layer after tcp_v6_fill_cb */\nstatic inline int inet6_sdif(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)\n\tif (skb && ipv6_l3mdev_skb(IP6CB(skb)->flags))\n\t\treturn IP6CB(skb)->iif;\n#endif\n\treturn 0;\n}\n\nstruct tcp6_request_sock {\n\tstruct tcp_request_sock\t  tcp6rsk_tcp;\n};\n\nstruct ipv6_mc_socklist;\nstruct ipv6_ac_socklist;\nstruct ipv6_fl_socklist;\n\nstruct inet6_cork {\n\tstruct ipv6_txoptions *opt;\n\tu8 hop_limit;\n\tu8 tclass;\n\tu8 dontfrag:1;\n};\n\n/* struct ipv6_pinfo - ipv6 private area */\nstruct ipv6_pinfo {\n\tstruct in6_addr \tsaddr;\n\tstruct in6_pktinfo\tsticky_pktinfo;\n#ifdef CONFIG_IPV6_SUBTREES\n\tbool\t\t\tsaddr_cache;\n#endif\n\tbool\t\t\tdaddr_cache;\n\n\t__be32\t\t\tflow_label;\n\t__u32\t\t\tfrag_size;\n\n\ts16\t\t\thop_limit;\n\tu8\t\t\tmcast_hops;\n\n\tint\t\t\tucast_oif;\n\tint\t\t\tmcast_oif;\n\n\t/* pktoption flags */\n\tunion {\n\t\tstruct {\n\t\t\t__u16\tsrcrt:1,\n\t\t\t\tosrcrt:1,\n\t\t\t        rxinfo:1,\n\t\t\t        rxoinfo:1,\n\t\t\t\trxhlim:1,\n\t\t\t\trxohlim:1,\n\t\t\t\thopopts:1,\n\t\t\t\tohopopts:1,\n\t\t\t\tdstopts:1,\n\t\t\t\todstopts:1,\n                                rxflow:1,\n\t\t\t\trxtclass:1,\n\t\t\t\trxpmtu:1,\n\t\t\t\trxorigdstaddr:1,\n\t\t\t\trecvfragsize:1;\n\t\t\t\t/* 1 bits hole */\n\t\t} bits;\n\t\t__u16\t\tall;\n\t} rxopt;\n\n\t/* sockopt flags */\n\t__u8\t\t\tsrcprefs;\t/* 001: prefer temporary address\n\t\t\t\t\t\t * 010: prefer public address\n\t\t\t\t\t\t * 100: prefer care-of address\n\t\t\t\t\t\t */\n\t__u8\t\t\tpmtudisc;\n\t__u8\t\t\tmin_hopcount;\n\t__u8\t\t\ttclass;\n\t__be32\t\t\trcv_flowinfo;\n\n\t__u32\t\t\tdst_cookie;\n\n\tstruct ipv6_mc_socklist\t__rcu *ipv6_mc_list;\n\tstruct ipv6_ac_socklist\t*ipv6_ac_list;\n\tstruct ipv6_fl_socklist __rcu *ipv6_fl_list;\n\n\tstruct ipv6_txoptions __rcu\t*opt;\n\tstruct sk_buff\t\t*pktoptions;\n\tstruct sk_buff\t\t*rxpmtu;\n\tstruct inet6_cork\tcork;\n};\n\n/* We currently use available bits from inet_sk(sk)->inet_flags,\n * this could change in the future.\n */\n#define inet6_test_bit(nr, sk)\t\t\t\\\n\ttest_bit(INET_FLAGS_##nr, &inet_sk(sk)->inet_flags)\n#define inet6_set_bit(nr, sk)\t\t\t\\\n\tset_bit(INET_FLAGS_##nr, &inet_sk(sk)->inet_flags)\n#define inet6_clear_bit(nr, sk)\t\t\t\\\n\tclear_bit(INET_FLAGS_##nr, &inet_sk(sk)->inet_flags)\n#define inet6_assign_bit(nr, sk, val)\t\t\\\n\tassign_bit(INET_FLAGS_##nr, &inet_sk(sk)->inet_flags, val)\n\n/* WARNING: don't change the layout of the members in {raw,udp,tcp}6_sock! */\nstruct raw6_sock {\n\t/* inet_sock has to be the first member of raw6_sock */\n\tstruct inet_sock\tinet;\n\t__u32\t\t\tchecksum;\t/* perform checksum */\n\t__u32\t\t\toffset;\t\t/* checksum offset  */\n\tstruct icmp6_filter\tfilter;\n\t__u32\t\t\tip6mr_table;\n\tstruct numa_drop_counters drop_counters;\n\tstruct ipv6_pinfo\tinet6;\n};\n\nstruct udp6_sock {\n\tstruct udp_sock\t  udp;\n\n\tstruct ipv6_pinfo inet6;\n};\n\nstruct tcp6_sock {\n\tstruct tcp_sock\t  tcp;\n\n\tstruct ipv6_pinfo inet6;\n};\n\nextern int inet6_sk_rebuild_header(struct sock *sk);\n\nstruct tcp6_timewait_sock {\n\tstruct tcp_timewait_sock   tcp6tw_tcp;\n};\n\n#if IS_ENABLED(CONFIG_IPV6)\nbool ipv6_mod_enabled(void);\n\nstatic inline struct ipv6_pinfo *inet6_sk(const struct sock *__sk)\n{\n\treturn sk_fullsock(__sk) ? inet_sk(__sk)->pinet6 : NULL;\n}\n\n#define raw6_sk(ptr) container_of_const(ptr, struct raw6_sock, inet.sk)\n\n#define ipv6_only_sock(sk)\t(sk->sk_ipv6only)\n#define ipv6_sk_rxinfo(sk)\t((sk)->sk_family == PF_INET6 && \\\n\t\t\t\t inet6_sk(sk)->rxopt.bits.rxinfo)\n\nstatic inline const struct in6_addr *inet6_rcv_saddr(const struct sock *sk)\n{\n\tif (sk->sk_family == AF_INET6)\n\t\treturn &sk->sk_v6_rcv_saddr;\n\treturn NULL;\n}\n\nstatic inline int inet_v6_ipv6only(const struct sock *sk)\n{\n\t/* ipv6only field is at same position for timewait and other sockets */\n\treturn ipv6_only_sock(sk);\n}\n#else\n#define ipv6_only_sock(sk)\t0\n#define ipv6_sk_rxinfo(sk)\t0\n\nstatic inline bool ipv6_mod_enabled(void)\n{\n\treturn false;\n}\n\nstatic inline struct ipv6_pinfo * inet6_sk(const struct sock *__sk)\n{\n\treturn NULL;\n}\n\nstatic inline struct raw6_sock *raw6_sk(const struct sock *sk)\n{\n\treturn NULL;\n}\n\n#define inet6_rcv_saddr(__sk)\tNULL\n#define inet_v6_ipv6only(__sk)\t\t0\n#endif /* IS_ENABLED(CONFIG_IPV6) */\n#endif /* _IPV6_H */\n", "patch": "@@ -227,7 +227,7 @@ struct ipv6_pinfo {\n \tstruct ipv6_ac_socklist\t*ipv6_ac_list;\n \tstruct ipv6_fl_socklist __rcu *ipv6_fl_list;\n \n-\tstruct ipv6_txoptions\t*opt;\n+\tstruct ipv6_txoptions __rcu\t*opt;\n \tstruct sk_buff\t\t*pktoptions;\n \tstruct sk_buff\t\t*rxpmtu;\n \tstruct inet6_cork\tcork;", "file_path": "files/2016_8\\82", "file_language": "h", "file_name": "include/linux/ipv6.h", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/include/net/ipv6.h", "code": "/*\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#ifndef _NET_IPV6_H\n#define _NET_IPV6_H\n\n#include <linux/ipv6.h>\n#include <linux/hardirq.h>\n#include <linux/jhash.h>\n#include <net/if_inet6.h>\n#include <net/ndisc.h>\n#include <net/flow.h>\n#include <net/flow_dissector.h>\n#include <net/snmp.h>\n\n#define SIN6_LEN_RFC2133\t24\n\n#define IPV6_MAXPLEN\t\t65535\n\n/*\n *\tNextHeader field of IPv6 header\n */\n\n#define NEXTHDR_HOP\t\t0\t/* Hop-by-hop option header. */\n#define NEXTHDR_TCP\t\t6\t/* TCP segment. */\n#define NEXTHDR_UDP\t\t17\t/* UDP message. */\n#define NEXTHDR_IPV6\t\t41\t/* IPv6 in IPv6 */\n#define NEXTHDR_ROUTING\t\t43\t/* Routing header. */\n#define NEXTHDR_FRAGMENT\t44\t/* Fragmentation/reassembly header. */\n#define NEXTHDR_GRE\t\t47\t/* GRE header. */\n#define NEXTHDR_ESP\t\t50\t/* Encapsulating security payload. */\n#define NEXTHDR_AUTH\t\t51\t/* Authentication header. */\n#define NEXTHDR_ICMP\t\t58\t/* ICMP for IPv6. */\n#define NEXTHDR_NONE\t\t59\t/* No next header */\n#define NEXTHDR_DEST\t\t60\t/* Destination options header. */\n#define NEXTHDR_SCTP\t\t132\t/* SCTP message. */\n#define NEXTHDR_MOBILITY\t135\t/* Mobility header. */\n\n#define NEXTHDR_MAX\t\t255\n\n#define IPV6_DEFAULT_HOPLIMIT   64\n#define IPV6_DEFAULT_MCASTHOPS\t1\n\n/*\n *\tAddr type\n *\t\n *\ttype\t-\tunicast | multicast\n *\tscope\t-\tlocal\t| site\t    | global\n *\tv4\t-\tcompat\n *\tv4mapped\n *\tany\n *\tloopback\n */\n\n#define IPV6_ADDR_ANY\t\t0x0000U\n\n#define IPV6_ADDR_UNICAST      \t0x0001U\t\n#define IPV6_ADDR_MULTICAST    \t0x0002U\t\n\n#define IPV6_ADDR_LOOPBACK\t0x0010U\n#define IPV6_ADDR_LINKLOCAL\t0x0020U\n#define IPV6_ADDR_SITELOCAL\t0x0040U\n\n#define IPV6_ADDR_COMPATv4\t0x0080U\n\n#define IPV6_ADDR_SCOPE_MASK\t0x00f0U\n\n#define IPV6_ADDR_MAPPED\t0x1000U\n\n/*\n *\tAddr scopes\n */\n#define IPV6_ADDR_MC_SCOPE(a)\t\\\n\t((a)->s6_addr[1] & 0x0f)\t/* nonstandard */\n#define __IPV6_ADDR_SCOPE_INVALID\t-1\n#define IPV6_ADDR_SCOPE_NODELOCAL\t0x01\n#define IPV6_ADDR_SCOPE_LINKLOCAL\t0x02\n#define IPV6_ADDR_SCOPE_SITELOCAL\t0x05\n#define IPV6_ADDR_SCOPE_ORGLOCAL\t0x08\n#define IPV6_ADDR_SCOPE_GLOBAL\t\t0x0e\n\n/*\n *\tAddr flags\n */\n#define IPV6_ADDR_MC_FLAG_TRANSIENT(a)\t\\\n\t((a)->s6_addr[1] & 0x10)\n#define IPV6_ADDR_MC_FLAG_PREFIX(a)\t\\\n\t((a)->s6_addr[1] & 0x20)\n#define IPV6_ADDR_MC_FLAG_RENDEZVOUS(a)\t\\\n\t((a)->s6_addr[1] & 0x40)\n\n/*\n *\tfragmentation header\n */\n\nstruct frag_hdr {\n\t__u8\tnexthdr;\n\t__u8\treserved;\n\t__be16\tfrag_off;\n\t__be32\tidentification;\n};\n\n#define\tIP6_MF\t\t0x0001\n#define\tIP6_OFFSET\t0xFFF8\n\n#define IP6_REPLY_MARK(net, mark) \\\n\t((net)->ipv6.sysctl.fwmark_reflect ? (mark) : 0)\n\n#include <net/sock.h>\n\n/* sysctls */\nextern int sysctl_mld_max_msf;\nextern int sysctl_mld_qrv;\n\n#define _DEVINC(net, statname, modifier, idev, field)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS##modifier((_idev)->stats.statname, (field)); \\\n\tSNMP_INC_STATS##modifier((net)->mib.statname##_statistics, (field));\\\n})\n\n/* per device counters are atomic_long_t */\n#define _DEVINCATOMIC(net, statname, modifier, idev, field)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS_ATOMIC_LONG((_idev)->stats.statname##dev, (field)); \\\n\tSNMP_INC_STATS##modifier((net)->mib.statname##_statistics, (field));\\\n})\n\n/* per device and per net counters are atomic_long_t */\n#define _DEVINC_ATOMIC_ATOMIC(net, statname, idev, field)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS_ATOMIC_LONG((_idev)->stats.statname##dev, (field)); \\\n\tSNMP_INC_STATS_ATOMIC_LONG((net)->mib.statname##_statistics, (field));\\\n})\n\n#define _DEVADD(net, statname, modifier, idev, field, val)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_ADD_STATS##modifier((_idev)->stats.statname, (field), (val)); \\\n\tSNMP_ADD_STATS##modifier((net)->mib.statname##_statistics, (field), (val));\\\n})\n\n#define _DEVUPD(net, statname, modifier, idev, field, val)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_UPD_PO_STATS##modifier((_idev)->stats.statname, field, (val)); \\\n\tSNMP_UPD_PO_STATS##modifier((net)->mib.statname##_statistics, field, (val));\\\n})\n\n/* MIBs */\n\n#define IP6_INC_STATS(net, idev,field)\t\t\\\n\t\t_DEVINC(net, ipv6, 64, idev, field)\n#define IP6_INC_STATS_BH(net, idev,field)\t\\\n\t\t_DEVINC(net, ipv6, 64_BH, idev, field)\n#define IP6_ADD_STATS(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, 64, idev, field, val)\n#define IP6_ADD_STATS_BH(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, 64_BH, idev, field, val)\n#define IP6_UPD_PO_STATS(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, 64, idev, field, val)\n#define IP6_UPD_PO_STATS_BH(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, 64_BH, idev, field, val)\n#define ICMP6_INC_STATS(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, , idev, field)\n#define ICMP6_INC_STATS_BH(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, _BH, idev, field)\n\n#define ICMP6MSGOUT_INC_STATS(net, idev, field)\t\t\\\n\t_DEVINC_ATOMIC_ATOMIC(net, icmpv6msg, idev, field +256)\n#define ICMP6MSGOUT_INC_STATS_BH(net, idev, field)\t\\\n\t_DEVINC_ATOMIC_ATOMIC(net, icmpv6msg, idev, field +256)\n#define ICMP6MSGIN_INC_STATS_BH(net, idev, field)\t\\\n\t_DEVINC_ATOMIC_ATOMIC(net, icmpv6msg, idev, field)\n\nstruct ip6_ra_chain {\n\tstruct ip6_ra_chain\t*next;\n\tstruct sock\t\t*sk;\n\tint\t\t\tsel;\n\tvoid\t\t\t(*destructor)(struct sock *);\n};\n\nextern struct ip6_ra_chain\t*ip6_ra_chain;\nextern rwlock_t ip6_ra_lock;\n\n/*\n   This structure is prepared by protocol, when parsing\n   ancillary data and passed to IPv6.\n */\n\nstruct ipv6_txoptions {\n\tatomic_t\t\trefcnt;\n\t/* Length of this structure */\n\tint\t\t\ttot_len;\n\n\t/* length of extension headers   */\n\n\t__u16\t\t\topt_flen;\t/* after fragment hdr */\n\t__u16\t\t\topt_nflen;\t/* before fragment hdr */\n\n\tstruct ipv6_opt_hdr\t*hopopt;\n\tstruct ipv6_opt_hdr\t*dst0opt;\n\tstruct ipv6_rt_hdr\t*srcrt;\t/* Routing Header */\n\tstruct ipv6_opt_hdr\t*dst1opt;\n\tstruct rcu_head\t\trcu;\n\t/* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */\n};\n\nstruct ip6_flowlabel {\n\tstruct ip6_flowlabel __rcu *next;\n\t__be32\t\t\tlabel;\n\tatomic_t\t\tusers;\n\tstruct in6_addr\t\tdst;\n\tstruct ipv6_txoptions\t*opt;\n\tunsigned long\t\tlinger;\n\tstruct rcu_head\t\trcu;\n\tu8\t\t\tshare;\n\tunion {\n\t\tstruct pid *pid;\n\t\tkuid_t uid;\n\t} owner;\n\tunsigned long\t\tlastuse;\n\tunsigned long\t\texpires;\n\tstruct net\t\t*fl_net;\n};\n\n#define IPV6_FLOWINFO_MASK\t\tcpu_to_be32(0x0FFFFFFF)\n#define IPV6_FLOWLABEL_MASK\t\tcpu_to_be32(0x000FFFFF)\n#define IPV6_FLOWLABEL_STATELESS_FLAG\tcpu_to_be32(0x00080000)\n\n#define IPV6_TCLASS_MASK (IPV6_FLOWINFO_MASK & ~IPV6_FLOWLABEL_MASK)\n#define IPV6_TCLASS_SHIFT\t20\n\nstruct ipv6_fl_socklist {\n\tstruct ipv6_fl_socklist\t__rcu\t*next;\n\tstruct ip6_flowlabel\t\t*fl;\n\tstruct rcu_head\t\t\trcu;\n};\n\nstatic inline struct ipv6_txoptions *txopt_get(const struct ipv6_pinfo *np)\n{\n\tstruct ipv6_txoptions *opt;\n\n\trcu_read_lock();\n\topt = rcu_dereference(np->opt);\n\tif (opt && !atomic_inc_not_zero(&opt->refcnt))\n\t\topt = NULL;\n\trcu_read_unlock();\n\treturn opt;\n}\n\nstatic inline void txopt_put(struct ipv6_txoptions *opt)\n{\n\tif (opt && atomic_dec_and_test(&opt->refcnt))\n\t\tkfree_rcu(opt, rcu);\n}\n\nstruct ip6_flowlabel *fl6_sock_lookup(struct sock *sk, __be32 label);\nstruct ipv6_txoptions *fl6_merge_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t struct ip6_flowlabel *fl,\n\t\t\t\t\t struct ipv6_txoptions *fopt);\nvoid fl6_free_socklist(struct sock *sk);\nint ipv6_flowlabel_opt(struct sock *sk, char __user *optval, int optlen);\nint ipv6_flowlabel_opt_get(struct sock *sk, struct in6_flowlabel_req *freq,\n\t\t\t   int flags);\nint ip6_flowlabel_init(void);\nvoid ip6_flowlabel_cleanup(void);\n\nstatic inline void fl6_sock_release(struct ip6_flowlabel *fl)\n{\n\tif (fl)\n\t\tatomic_dec(&fl->users);\n}\n\nvoid icmpv6_notify(struct sk_buff *skb, u8 type, u8 code, __be32 info);\n\nint icmpv6_push_pending_frames(struct sock *sk, struct flowi6 *fl6,\n\t\t\t       struct icmp6hdr *thdr, int len);\n\nint ip6_ra_control(struct sock *sk, int sel);\n\nint ipv6_parse_hopopts(struct sk_buff *skb);\n\nstruct ipv6_txoptions *ipv6_dup_options(struct sock *sk,\n\t\t\t\t\tstruct ipv6_txoptions *opt);\nstruct ipv6_txoptions *ipv6_renew_options(struct sock *sk,\n\t\t\t\t\t  struct ipv6_txoptions *opt,\n\t\t\t\t\t  int newtype,\n\t\t\t\t\t  struct ipv6_opt_hdr __user *newopt,\n\t\t\t\t\t  int newoptlen);\nstruct ipv6_txoptions *ipv6_fixup_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t  struct ipv6_txoptions *opt);\n\nbool ipv6_opt_accepted(const struct sock *sk, const struct sk_buff *skb,\n\t\t       const struct inet6_skb_parm *opt);\n\nstatic inline bool ipv6_accept_ra(struct inet6_dev *idev)\n{\n\t/* If forwarding is enabled, RA are not accepted unless the special\n\t * hybrid mode (accept_ra=2) is enabled.\n\t */\n\treturn idev->cnf.forwarding ? idev->cnf.accept_ra == 2 :\n\t    idev->cnf.accept_ra;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic inline int ip6_frag_mem(struct net *net)\n{\n\treturn sum_frag_mem_limit(&net->ipv6.frags);\n}\n#endif\n\n#define IPV6_FRAG_HIGH_THRESH\t(4 * 1024*1024)\t/* 4194304 */\n#define IPV6_FRAG_LOW_THRESH\t(3 * 1024*1024)\t/* 3145728 */\n#define IPV6_FRAG_TIMEOUT\t(60 * HZ)\t/* 60 seconds */\n\nint __ipv6_addr_type(const struct in6_addr *addr);\nstatic inline int ipv6_addr_type(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & 0xffff;\n}\n\nstatic inline int ipv6_addr_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & IPV6_ADDR_SCOPE_MASK;\n}\n\nstatic inline int __ipv6_addr_src_scope(int type)\n{\n\treturn (type == IPV6_ADDR_ANY) ? __IPV6_ADDR_SCOPE_INVALID : (type >> 16);\n}\n\nstatic inline int ipv6_addr_src_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_src_scope(__ipv6_addr_type(addr));\n}\n\nstatic inline bool __ipv6_addr_needs_scope_id(int type)\n{\n\treturn type & IPV6_ADDR_LINKLOCAL ||\n\t       (type & IPV6_ADDR_MULTICAST &&\n\t\t(type & (IPV6_ADDR_LOOPBACK|IPV6_ADDR_LINKLOCAL)));\n}\n\nstatic inline __u32 ipv6_iface_scope_id(const struct in6_addr *addr, int iface)\n{\n\treturn __ipv6_addr_needs_scope_id(__ipv6_addr_type(addr)) ? iface : 0;\n}\n\nstatic inline int ipv6_addr_cmp(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn memcmp(a1, a2, sizeof(struct in6_addr));\n}\n\nstatic inline bool\nipv6_masked_addr_cmp(const struct in6_addr *a1, const struct in6_addr *m,\n\t\t     const struct in6_addr *a2)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul1 = (const unsigned long *)a1;\n\tconst unsigned long *ulm = (const unsigned long *)m;\n\tconst unsigned long *ul2 = (const unsigned long *)a2;\n\n\treturn !!(((ul1[0] ^ ul2[0]) & ulm[0]) |\n\t\t  ((ul1[1] ^ ul2[1]) & ulm[1]));\n#else\n\treturn !!(((a1->s6_addr32[0] ^ a2->s6_addr32[0]) & m->s6_addr32[0]) |\n\t\t  ((a1->s6_addr32[1] ^ a2->s6_addr32[1]) & m->s6_addr32[1]) |\n\t\t  ((a1->s6_addr32[2] ^ a2->s6_addr32[2]) & m->s6_addr32[2]) |\n\t\t  ((a1->s6_addr32[3] ^ a2->s6_addr32[3]) & m->s6_addr32[3]));\n#endif\n}\n\nstatic inline void ipv6_addr_prefix(struct in6_addr *pfx, \n\t\t\t\t    const struct in6_addr *addr,\n\t\t\t\t    int plen)\n{\n\t/* caller must guarantee 0 <= plen <= 128 */\n\tint o = plen >> 3,\n\t    b = plen & 0x7;\n\n\tmemset(pfx->s6_addr, 0, sizeof(pfx->s6_addr));\n\tmemcpy(pfx->s6_addr, addr, o);\n\tif (b != 0)\n\t\tpfx->s6_addr[o] = addr->s6_addr[o] & (0xff00 >> b);\n}\n\nstatic inline void __ipv6_addr_set_half(__be32 *addr,\n\t\t\t\t\t__be32 wh, __be32 wl)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n#if defined(__BIG_ENDIAN)\n\tif (__builtin_constant_p(wh) && __builtin_constant_p(wl)) {\n\t\t*(__force u64 *)addr = ((__force u64)(wh) << 32 | (__force u64)(wl));\n\t\treturn;\n\t}\n#elif defined(__LITTLE_ENDIAN)\n\tif (__builtin_constant_p(wl) && __builtin_constant_p(wh)) {\n\t\t*(__force u64 *)addr = ((__force u64)(wl) << 32 | (__force u64)(wh));\n\t\treturn;\n\t}\n#endif\n#endif\n\taddr[0] = wh;\n\taddr[1] = wl;\n}\n\nstatic inline void ipv6_addr_set(struct in6_addr *addr, \n\t\t\t\t     __be32 w1, __be32 w2,\n\t\t\t\t     __be32 w3, __be32 w4)\n{\n\t__ipv6_addr_set_half(&addr->s6_addr32[0], w1, w2);\n\t__ipv6_addr_set_half(&addr->s6_addr32[2], w3, w4);\n}\n\nstatic inline bool ipv6_addr_equal(const struct in6_addr *a1,\n\t\t\t\t   const struct in6_addr *a2)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul1 = (const unsigned long *)a1;\n\tconst unsigned long *ul2 = (const unsigned long *)a2;\n\n\treturn ((ul1[0] ^ ul2[0]) | (ul1[1] ^ ul2[1])) == 0UL;\n#else\n\treturn ((a1->s6_addr32[0] ^ a2->s6_addr32[0]) |\n\t\t(a1->s6_addr32[1] ^ a2->s6_addr32[1]) |\n\t\t(a1->s6_addr32[2] ^ a2->s6_addr32[2]) |\n\t\t(a1->s6_addr32[3] ^ a2->s6_addr32[3])) == 0;\n#endif\n}\n\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\nstatic inline bool __ipv6_prefix_equal64_half(const __be64 *a1,\n\t\t\t\t\t      const __be64 *a2,\n\t\t\t\t\t      unsigned int len)\n{\n\tif (len && ((*a1 ^ *a2) & cpu_to_be64((~0UL) << (64 - len))))\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool ipv6_prefix_equal(const struct in6_addr *addr1,\n\t\t\t\t     const struct in6_addr *addr2,\n\t\t\t\t     unsigned int prefixlen)\n{\n\tconst __be64 *a1 = (const __be64 *)addr1;\n\tconst __be64 *a2 = (const __be64 *)addr2;\n\n\tif (prefixlen >= 64) {\n\t\tif (a1[0] ^ a2[0])\n\t\t\treturn false;\n\t\treturn __ipv6_prefix_equal64_half(a1 + 1, a2 + 1, prefixlen - 64);\n\t}\n\treturn __ipv6_prefix_equal64_half(a1, a2, prefixlen);\n}\n#else\nstatic inline bool ipv6_prefix_equal(const struct in6_addr *addr1,\n\t\t\t\t     const struct in6_addr *addr2,\n\t\t\t\t     unsigned int prefixlen)\n{\n\tconst __be32 *a1 = addr1->s6_addr32;\n\tconst __be32 *a2 = addr2->s6_addr32;\n\tunsigned int pdw, pbi;\n\n\t/* check complete u32 in prefix */\n\tpdw = prefixlen >> 5;\n\tif (pdw && memcmp(a1, a2, pdw << 2))\n\t\treturn false;\n\n\t/* check incomplete u32 in prefix */\n\tpbi = prefixlen & 0x1f;\n\tif (pbi && ((a1[pdw] ^ a2[pdw]) & htonl((0xffffffff) << (32 - pbi))))\n\t\treturn false;\n\n\treturn true;\n}\n#endif\n\nstruct inet_frag_queue;\n\nenum ip6_defrag_users {\n\tIP6_DEFRAG_LOCAL_DELIVER,\n\tIP6_DEFRAG_CONNTRACK_IN,\n\t__IP6_DEFRAG_CONNTRACK_IN\t= IP6_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP6_DEFRAG_CONNTRACK_OUT,\n\t__IP6_DEFRAG_CONNTRACK_OUT\t= IP6_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP6_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP6_DEFRAG_CONNTRACK_BRIDGE_IN = IP6_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n};\n\nstruct ip6_create_arg {\n\t__be32 id;\n\tu32 user;\n\tconst struct in6_addr *src;\n\tconst struct in6_addr *dst;\n\tint iif;\n\tu8 ecn;\n};\n\nvoid ip6_frag_init(struct inet_frag_queue *q, const void *a);\nbool ip6_frag_match(const struct inet_frag_queue *q, const void *a);\n\n/*\n *\tEquivalent of ipv4 struct ip\n */\nstruct frag_queue {\n\tstruct inet_frag_queue\tq;\n\n\t__be32\t\t\tid;\t\t/* fragment id\t\t*/\n\tu32\t\t\tuser;\n\tstruct in6_addr\t\tsaddr;\n\tstruct in6_addr\t\tdaddr;\n\n\tint\t\t\tiif;\n\tunsigned int\t\tcsum;\n\t__u16\t\t\tnhoffset;\n\tu8\t\t\tecn;\n};\n\nvoid ip6_expire_frag_queue(struct net *net, struct frag_queue *fq,\n\t\t\t   struct inet_frags *frags);\n\nstatic inline bool ipv6_addr_any(const struct in6_addr *a)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul = (const unsigned long *)a;\n\n\treturn (ul[0] | ul[1]) == 0UL;\n#else\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | a->s6_addr32[3]) == 0;\n#endif\n}\n\nstatic inline u32 ipv6_addr_hash(const struct in6_addr *a)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul = (const unsigned long *)a;\n\tunsigned long x = ul[0] ^ ul[1];\n\n\treturn (u32)(x ^ (x >> 32));\n#else\n\treturn (__force u32)(a->s6_addr32[0] ^ a->s6_addr32[1] ^\n\t\t\t     a->s6_addr32[2] ^ a->s6_addr32[3]);\n#endif\n}\n\n/* more secured version of ipv6_addr_hash() */\nstatic inline u32 __ipv6_addr_jhash(const struct in6_addr *a, const u32 initval)\n{\n\tu32 v = (__force u32)a->s6_addr32[0] ^ (__force u32)a->s6_addr32[1];\n\n\treturn jhash_3words(v,\n\t\t\t    (__force u32)a->s6_addr32[2],\n\t\t\t    (__force u32)a->s6_addr32[3],\n\t\t\t    initval);\n}\n\nstatic inline bool ipv6_addr_loopback(const struct in6_addr *a)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst __be64 *be = (const __be64 *)a;\n\n\treturn (be[0] | (be[1] ^ cpu_to_be64(1))) == 0UL;\n#else\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | (a->s6_addr32[3] ^ cpu_to_be32(1))) == 0;\n#endif\n}\n\n/*\n * Note that we must __force cast these to unsigned long to make sparse happy,\n * since all of the endian-annotated types are fixed size regardless of arch.\n */\nstatic inline bool ipv6_addr_v4mapped(const struct in6_addr *a)\n{\n\treturn (\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\t\t*(unsigned long *)a |\n#else\n\t\t(__force unsigned long)(a->s6_addr32[0] | a->s6_addr32[1]) |\n#endif\n\t\t(__force unsigned long)(a->s6_addr32[2] ^\n\t\t\t\t\tcpu_to_be32(0x0000ffff))) == 0UL;\n}\n\n/*\n * Check for a RFC 4843 ORCHID address\n * (Overlay Routable Cryptographic Hash Identifiers)\n */\nstatic inline bool ipv6_addr_orchid(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] & htonl(0xfffffff0)) == htonl(0x20010010);\n}\n\nstatic inline bool ipv6_addr_is_multicast(const struct in6_addr *addr)\n{\n\treturn (addr->s6_addr32[0] & htonl(0xFF000000)) == htonl(0xFF000000);\n}\n\nstatic inline void ipv6_addr_set_v4mapped(const __be32 addr,\n\t\t\t\t\t  struct in6_addr *v4mapped)\n{\n\tipv6_addr_set(v4mapped,\n\t\t\t0, 0,\n\t\t\thtonl(0x0000FFFF),\n\t\t\taddr);\n}\n\n/*\n * find the first different bit between two addresses\n * length of address must be a multiple of 32bits\n */\nstatic inline int __ipv6_addr_diff32(const void *token1, const void *token2, int addrlen)\n{\n\tconst __be32 *a1 = token1, *a2 = token2;\n\tint i;\n\n\taddrlen >>= 2;\n\n\tfor (i = 0; i < addrlen; i++) {\n\t\t__be32 xb = a1[i] ^ a2[i];\n\t\tif (xb)\n\t\t\treturn i * 32 + 31 - __fls(ntohl(xb));\n\t}\n\n\t/*\n\t *\twe should *never* get to this point since that \n\t *\twould mean the addrs are equal\n\t *\n\t *\tHowever, we do get to it 8) And exacly, when\n\t *\taddresses are equal 8)\n\t *\n\t *\tip route add 1111::/128 via ...\n\t *\tip route add 1111::/64 via ...\n\t *\tand we are here.\n\t *\n\t *\tIdeally, this function should stop comparison\n\t *\tat prefix length. It does not, but it is still OK,\n\t *\tif returned value is greater than prefix length.\n\t *\t\t\t\t\t--ANK (980803)\n\t */\n\treturn addrlen << 5;\n}\n\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\nstatic inline int __ipv6_addr_diff64(const void *token1, const void *token2, int addrlen)\n{\n\tconst __be64 *a1 = token1, *a2 = token2;\n\tint i;\n\n\taddrlen >>= 3;\n\n\tfor (i = 0; i < addrlen; i++) {\n\t\t__be64 xb = a1[i] ^ a2[i];\n\t\tif (xb)\n\t\t\treturn i * 64 + 63 - __fls(be64_to_cpu(xb));\n\t}\n\n\treturn addrlen << 6;\n}\n#endif\n\nstatic inline int __ipv6_addr_diff(const void *token1, const void *token2, int addrlen)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tif (__builtin_constant_p(addrlen) && !(addrlen & 7))\n\t\treturn __ipv6_addr_diff64(token1, token2, addrlen);\n#endif\n\treturn __ipv6_addr_diff32(token1, token2, addrlen);\n}\n\nstatic inline int ipv6_addr_diff(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn __ipv6_addr_diff(a1, a2, sizeof(struct in6_addr));\n}\n\n__be32 ipv6_select_ident(struct net *net,\n\t\t\t const struct in6_addr *daddr,\n\t\t\t const struct in6_addr *saddr);\nvoid ipv6_proxy_select_ident(struct net *net, struct sk_buff *skb);\n\nint ip6_dst_hoplimit(struct dst_entry *dst);\n\nstatic inline int ip6_sk_dst_hoplimit(struct ipv6_pinfo *np, struct flowi6 *fl6,\n\t\t\t\t      struct dst_entry *dst)\n{\n\tint hlimit;\n\n\tif (ipv6_addr_is_multicast(&fl6->daddr))\n\t\thlimit = np->mcast_hops;\n\telse\n\t\thlimit = np->hop_limit;\n\tif (hlimit < 0)\n\t\thlimit = ip6_dst_hoplimit(dst);\n\treturn hlimit;\n}\n\n/* copy IPv6 saddr & daddr to flow_keys, possibly using 64bit load/store\n * Equivalent to :\tflow->v6addrs.src = iph->saddr;\n *\t\t\tflow->v6addrs.dst = iph->daddr;\n */\nstatic inline void iph_to_flow_copy_v6addrs(struct flow_keys *flow,\n\t\t\t\t\t    const struct ipv6hdr *iph)\n{\n\tBUILD_BUG_ON(offsetof(typeof(flow->addrs), v6addrs.dst) !=\n\t\t     offsetof(typeof(flow->addrs), v6addrs.src) +\n\t\t     sizeof(flow->addrs.v6addrs.src));\n\tmemcpy(&flow->addrs.v6addrs, &iph->saddr, sizeof(flow->addrs.v6addrs));\n\tflow->control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n\n/* Sysctl settings for net ipv6.auto_flowlabels */\n#define IP6_AUTO_FLOW_LABEL_OFF\t\t0\n#define IP6_AUTO_FLOW_LABEL_OPTOUT\t1\n#define IP6_AUTO_FLOW_LABEL_OPTIN\t2\n#define IP6_AUTO_FLOW_LABEL_FORCED\t3\n\n#define IP6_AUTO_FLOW_LABEL_MAX\t\tIP6_AUTO_FLOW_LABEL_FORCED\n\n#define IP6_DEFAULT_AUTO_FLOW_LABELS\tIP6_AUTO_FLOW_LABEL_OPTOUT\n\nstatic inline __be32 ip6_make_flowlabel(struct net *net, struct sk_buff *skb,\n\t\t\t\t\t__be32 flowlabel, bool autolabel,\n\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tu32 hash;\n\n\tif (flowlabel ||\n\t    net->ipv6.sysctl.auto_flowlabels == IP6_AUTO_FLOW_LABEL_OFF ||\n\t    (!autolabel &&\n\t     net->ipv6.sysctl.auto_flowlabels != IP6_AUTO_FLOW_LABEL_FORCED))\n\t\treturn flowlabel;\n\n\thash = skb_get_hash_flowi6(skb, fl6);\n\n\t/* Since this is being sent on the wire obfuscate hash a bit\n\t * to minimize possbility that any useful information to an\n\t * attacker is leaked. Only lower 20 bits are relevant.\n\t */\n\trol32(hash, 16);\n\n\tflowlabel = (__force __be32)hash & IPV6_FLOWLABEL_MASK;\n\n\tif (net->ipv6.sysctl.flowlabel_state_ranges)\n\t\tflowlabel |= IPV6_FLOWLABEL_STATELESS_FLAG;\n\n\treturn flowlabel;\n}\n\nstatic inline int ip6_default_np_autolabel(struct net *net)\n{\n\tswitch (net->ipv6.sysctl.auto_flowlabels) {\n\tcase IP6_AUTO_FLOW_LABEL_OFF:\n\tcase IP6_AUTO_FLOW_LABEL_OPTIN:\n\tdefault:\n\t\treturn 0;\n\tcase IP6_AUTO_FLOW_LABEL_OPTOUT:\n\tcase IP6_AUTO_FLOW_LABEL_FORCED:\n\t\treturn 1;\n\t}\n}\n#else\nstatic inline void ip6_set_txhash(struct sock *sk) { }\nstatic inline __be32 ip6_make_flowlabel(struct net *net, struct sk_buff *skb,\n\t\t\t\t\t__be32 flowlabel, bool autolabel,\n\t\t\t\t\tstruct flowi6 *fl6)\n{\n\treturn flowlabel;\n}\nstatic inline int ip6_default_np_autolabel(struct net *net)\n{\n\treturn 0;\n}\n#endif\n\n\n/*\n *\tHeader manipulation\n */\nstatic inline void ip6_flow_hdr(struct ipv6hdr *hdr, unsigned int tclass,\n\t\t\t\t__be32 flowlabel)\n{\n\t*(__be32 *)hdr = htonl(0x60000000 | (tclass << 20)) | flowlabel;\n}\n\nstatic inline __be32 ip6_flowinfo(const struct ipv6hdr *hdr)\n{\n\treturn *(__be32 *)hdr & IPV6_FLOWINFO_MASK;\n}\n\nstatic inline __be32 ip6_flowlabel(const struct ipv6hdr *hdr)\n{\n\treturn *(__be32 *)hdr & IPV6_FLOWLABEL_MASK;\n}\n\nstatic inline u8 ip6_tclass(__be32 flowinfo)\n{\n\treturn ntohl(flowinfo & IPV6_TCLASS_MASK) >> IPV6_TCLASS_SHIFT;\n}\n/*\n *\tPrototypes exported by ipv6\n */\n\n/*\n *\trcv function (called from netdevice level)\n */\n\nint ipv6_rcv(struct sk_buff *skb, struct net_device *dev,\n\t     struct packet_type *pt, struct net_device *orig_dev);\n\nint ip6_rcv_finish(struct net *net, struct sock *sk, struct sk_buff *skb);\n\n/*\n *\tupper-layer output functions\n */\nint ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,\n\t     struct ipv6_txoptions *opt, int tclass);\n\nint ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr);\n\nint ip6_append_data(struct sock *sk,\n\t\t    int getfrag(void *from, char *to, int offset, int len,\n\t\t\t\tint odd, struct sk_buff *skb),\n\t\t    void *from, int length, int transhdrlen, int hlimit,\n\t\t    int tclass, struct ipv6_txoptions *opt, struct flowi6 *fl6,\n\t\t    struct rt6_info *rt, unsigned int flags, int dontfrag);\n\nint ip6_push_pending_frames(struct sock *sk);\n\nvoid ip6_flush_pending_frames(struct sock *sk);\n\nint ip6_send_skb(struct sk_buff *skb);\n\nstruct sk_buff *__ip6_make_skb(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t       struct inet_cork_full *cork,\n\t\t\t       struct inet6_cork *v6_cork);\nstruct sk_buff *ip6_make_skb(struct sock *sk,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, int length, int transhdrlen,\n\t\t\t     int hlimit, int tclass, struct ipv6_txoptions *opt,\n\t\t\t     struct flowi6 *fl6, struct rt6_info *rt,\n\t\t\t     unsigned int flags, int dontfrag);\n\nstatic inline struct sk_buff *ip6_finish_skb(struct sock *sk)\n{\n\treturn __ip6_make_skb(sk, &sk->sk_write_queue, &inet_sk(sk)->cork,\n\t\t\t      &inet6_sk(sk)->cork);\n}\n\nint ip6_dst_lookup(struct net *net, struct sock *sk, struct dst_entry **dst,\n\t\t   struct flowi6 *fl6);\nstruct dst_entry *ip6_dst_lookup_flow(const struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t      const struct in6_addr *final_dst);\nstruct dst_entry *ip6_sk_dst_lookup_flow(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t\t const struct in6_addr *final_dst);\nstruct dst_entry *ip6_blackhole_route(struct net *net,\n\t\t\t\t      struct dst_entry *orig_dst);\n\n/*\n *\tskb processing functions\n */\n\nint ip6_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip6_forward(struct sk_buff *skb);\nint ip6_input(struct sk_buff *skb);\nint ip6_mc_input(struct sk_buff *skb);\n\nint __ip6_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip6_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\n\n/*\n *\tExtension header (options) processing\n */\n\nvoid ipv6_push_nfrag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt,\n\t\t\t  u8 *proto, struct in6_addr **daddr_p);\nvoid ipv6_push_frag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt,\n\t\t\t u8 *proto);\n\nint ipv6_skip_exthdr(const struct sk_buff *, int start, u8 *nexthdrp,\n\t\t     __be16 *frag_offp);\n\nbool ipv6_ext_hdr(u8 nexthdr);\n\nenum {\n\tIP6_FH_F_FRAG\t\t= (1 << 0),\n\tIP6_FH_F_AUTH\t\t= (1 << 1),\n\tIP6_FH_F_SKIP_RH\t= (1 << 2),\n};\n\n/* find specified header and get offset to it */\nint ipv6_find_hdr(const struct sk_buff *skb, unsigned int *offset, int target,\n\t\t  unsigned short *fragoff, int *fragflg);\n\nint ipv6_find_tlv(struct sk_buff *skb, int offset, int type);\n\nstruct in6_addr *fl6_update_dst(struct flowi6 *fl6,\n\t\t\t\tconst struct ipv6_txoptions *opt,\n\t\t\t\tstruct in6_addr *orig);\n\n/*\n *\tsocket options (ipv6_sockglue.c)\n */\n\nint ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen);\nint ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen);\nint compat_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t   char __user *optval, unsigned int optlen);\nint compat_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen);\n\nint ip6_datagram_connect(struct sock *sk, struct sockaddr *addr, int addr_len);\nint ip6_datagram_connect_v6_only(struct sock *sk, struct sockaddr *addr,\n\t\t\t\t int addr_len);\n\nint ipv6_recv_error(struct sock *sk, struct msghdr *msg, int len,\n\t\t    int *addr_len);\nint ipv6_recv_rxpmtu(struct sock *sk, struct msghdr *msg, int len,\n\t\t     int *addr_len);\nvoid ipv6_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,\n\t\t     u32 info, u8 *payload);\nvoid ipv6_local_error(struct sock *sk, int err, struct flowi6 *fl6, u32 info);\nvoid ipv6_local_rxpmtu(struct sock *sk, struct flowi6 *fl6, u32 mtu);\n\nint inet6_release(struct socket *sock);\nint inet6_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len);\nint inet6_getname(struct socket *sock, struct sockaddr *uaddr, int *uaddr_len,\n\t\t  int peer);\nint inet6_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg);\n\nint inet6_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t\t      struct sock *sk);\n\n/*\n * reassembly.c\n */\nextern const struct proto_ops inet6_stream_ops;\nextern const struct proto_ops inet6_dgram_ops;\n\nstruct group_source_req;\nstruct group_filter;\n\nint ip6_mc_source(int add, int omode, struct sock *sk,\n\t\t  struct group_source_req *pgsr);\nint ip6_mc_msfilter(struct sock *sk, struct group_filter *gsf);\nint ip6_mc_msfget(struct sock *sk, struct group_filter *gsf,\n\t\t  struct group_filter __user *optval, int __user *optlen);\n\n#ifdef CONFIG_PROC_FS\nint ac6_proc_init(struct net *net);\nvoid ac6_proc_exit(struct net *net);\nint raw6_proc_init(void);\nvoid raw6_proc_exit(void);\nint tcp6_proc_init(struct net *net);\nvoid tcp6_proc_exit(struct net *net);\nint udp6_proc_init(struct net *net);\nvoid udp6_proc_exit(struct net *net);\nint udplite6_proc_init(void);\nvoid udplite6_proc_exit(void);\nint ipv6_misc_proc_init(void);\nvoid ipv6_misc_proc_exit(void);\nint snmp6_register_dev(struct inet6_dev *idev);\nint snmp6_unregister_dev(struct inet6_dev *idev);\n\n#else\nstatic inline int ac6_proc_init(struct net *net) { return 0; }\nstatic inline void ac6_proc_exit(struct net *net) { }\nstatic inline int snmp6_register_dev(struct inet6_dev *idev) { return 0; }\nstatic inline int snmp6_unregister_dev(struct inet6_dev *idev) { return 0; }\n#endif\n\n#ifdef CONFIG_SYSCTL\nextern struct ctl_table ipv6_route_table_template[];\n\nstruct ctl_table *ipv6_icmp_sysctl_init(struct net *net);\nstruct ctl_table *ipv6_route_sysctl_init(struct net *net);\nint ipv6_sysctl_register(void);\nvoid ipv6_sysctl_unregister(void);\n#endif\n\nint ipv6_sock_mc_join(struct sock *sk, int ifindex,\n\t\t      const struct in6_addr *addr);\nint ipv6_sock_mc_drop(struct sock *sk, int ifindex,\n\t\t      const struct in6_addr *addr);\n#endif /* _NET_IPV6_H */\n", "code_before": "/* SPDX-License-Identifier: GPL-2.0-or-later */\n/*\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n */\n\n#ifndef _NET_IPV6_H\n#define _NET_IPV6_H\n\n#include <linux/ipv6.h>\n#include <linux/hardirq.h>\n#include <linux/jhash.h>\n#include <linux/refcount.h>\n#include <linux/jump_label_ratelimit.h>\n#include <net/if_inet6.h>\n#include <net/flow.h>\n#include <net/flow_dissector.h>\n#include <net/inet_dscp.h>\n#include <net/snmp.h>\n#include <net/netns/hash.h>\n\nstruct ip_tunnel_info;\n\n#define SIN6_LEN_RFC2133\t24\n\n#define IPV6_MAXPLEN\t\t65535\n\n/*\n *\tNextHeader field of IPv6 header\n */\n\n#define NEXTHDR_HOP\t\t0\t/* Hop-by-hop option header. */\n#define NEXTHDR_IPV4\t\t4\t/* IPv4 in IPv6 */\n#define NEXTHDR_TCP\t\t6\t/* TCP segment. */\n#define NEXTHDR_UDP\t\t17\t/* UDP message. */\n#define NEXTHDR_IPV6\t\t41\t/* IPv6 in IPv6 */\n#define NEXTHDR_ROUTING\t\t43\t/* Routing header. */\n#define NEXTHDR_FRAGMENT\t44\t/* Fragmentation/reassembly header. */\n#define NEXTHDR_GRE\t\t47\t/* GRE header. */\n#define NEXTHDR_ESP\t\t50\t/* Encapsulating security payload. */\n#define NEXTHDR_AUTH\t\t51\t/* Authentication header. */\n#define NEXTHDR_ICMP\t\t58\t/* ICMP for IPv6. */\n#define NEXTHDR_NONE\t\t59\t/* No next header */\n#define NEXTHDR_DEST\t\t60\t/* Destination options header. */\n#define NEXTHDR_SCTP\t\t132\t/* SCTP message. */\n#define NEXTHDR_MOBILITY\t135\t/* Mobility header. */\n\n#define NEXTHDR_MAX\t\t255\n\n#define IPV6_DEFAULT_HOPLIMIT   64\n#define IPV6_DEFAULT_MCASTHOPS\t1\n\n/* Limits on Hop-by-Hop and Destination options.\n *\n * Per RFC8200 there is no limit on the maximum number or lengths of options in\n * Hop-by-Hop or Destination options other then the packet must fit in an MTU.\n * We allow configurable limits in order to mitigate potential denial of\n * service attacks.\n *\n * There are three limits that may be set:\n *   - Limit the number of options in a Hop-by-Hop or Destination options\n *     extension header\n *   - Limit the byte length of a Hop-by-Hop or Destination options extension\n *     header\n *   - Disallow unknown options\n *\n * The limits are expressed in corresponding sysctls:\n *\n * ipv6.sysctl.max_dst_opts_cnt\n * ipv6.sysctl.max_hbh_opts_cnt\n * ipv6.sysctl.max_dst_opts_len\n * ipv6.sysctl.max_hbh_opts_len\n *\n * max_*_opts_cnt is the number of TLVs that are allowed for Destination\n * options or Hop-by-Hop options. If the number is less than zero then unknown\n * TLVs are disallowed and the number of known options that are allowed is the\n * absolute value. Setting the value to INT_MAX indicates no limit.\n *\n * max_*_opts_len is the length limit in bytes of a Destination or\n * Hop-by-Hop options extension header. Setting the value to INT_MAX\n * indicates no length limit.\n *\n * If a limit is exceeded when processing an extension header the packet is\n * silently discarded.\n */\n\n/* Default limits for Hop-by-Hop and Destination options */\n#define IP6_DEFAULT_MAX_DST_OPTS_CNT\t 8\n#define IP6_DEFAULT_MAX_HBH_OPTS_CNT\t 8\n#define IP6_DEFAULT_MAX_DST_OPTS_LEN\t INT_MAX /* No limit */\n#define IP6_DEFAULT_MAX_HBH_OPTS_LEN\t INT_MAX /* No limit */\n\n/*\n *\tAddr type\n *\t\n *\ttype\t-\tunicast | multicast\n *\tscope\t-\tlocal\t| site\t    | global\n *\tv4\t-\tcompat\n *\tv4mapped\n *\tany\n *\tloopback\n */\n\n#define IPV6_ADDR_ANY\t\t0x0000U\n\n#define IPV6_ADDR_UNICAST\t0x0001U\n#define IPV6_ADDR_MULTICAST\t0x0002U\n\n#define IPV6_ADDR_LOOPBACK\t0x0010U\n#define IPV6_ADDR_LINKLOCAL\t0x0020U\n#define IPV6_ADDR_SITELOCAL\t0x0040U\n\n#define IPV6_ADDR_COMPATv4\t0x0080U\n\n#define IPV6_ADDR_SCOPE_MASK\t0x00f0U\n\n#define IPV6_ADDR_MAPPED\t0x1000U\n\n/*\n *\tAddr scopes\n */\n#define IPV6_ADDR_MC_SCOPE(a)\t\\\n\t((a)->s6_addr[1] & 0x0f)\t/* nonstandard */\n#define __IPV6_ADDR_SCOPE_INVALID\t-1\n#define IPV6_ADDR_SCOPE_NODELOCAL\t0x01\n#define IPV6_ADDR_SCOPE_LINKLOCAL\t0x02\n#define IPV6_ADDR_SCOPE_SITELOCAL\t0x05\n#define IPV6_ADDR_SCOPE_ORGLOCAL\t0x08\n#define IPV6_ADDR_SCOPE_GLOBAL\t\t0x0e\n\n/*\n *\tAddr flags\n */\n#define IPV6_ADDR_MC_FLAG_TRANSIENT(a)\t\\\n\t((a)->s6_addr[1] & 0x10)\n#define IPV6_ADDR_MC_FLAG_PREFIX(a)\t\\\n\t((a)->s6_addr[1] & 0x20)\n#define IPV6_ADDR_MC_FLAG_RENDEZVOUS(a)\t\\\n\t((a)->s6_addr[1] & 0x40)\n\n/*\n *\tfragmentation header\n */\n\nstruct frag_hdr {\n\t__u8\tnexthdr;\n\t__u8\treserved;\n\t__be16\tfrag_off;\n\t__be32\tidentification;\n};\n\n/*\n * Jumbo payload option, as described in RFC 2675 2.\n */\nstruct hop_jumbo_hdr {\n\tu8\tnexthdr;\n\tu8\thdrlen;\n\tu8\ttlv_type;\t/* IPV6_TLV_JUMBO, 0xC2 */\n\tu8\ttlv_len;\t/* 4 */\n\t__be32\tjumbo_payload_len;\n};\n\n#define\tIP6_MF\t\t0x0001\n#define\tIP6_OFFSET\t0xFFF8\n\nstruct ip6_fraglist_iter {\n\tstruct ipv6hdr\t*tmp_hdr;\n\tstruct sk_buff\t*frag;\n\tint\t\toffset;\n\tunsigned int\thlen;\n\t__be32\t\tfrag_id;\n\tu8\t\tnexthdr;\n};\n\nint ip6_fraglist_init(struct sk_buff *skb, unsigned int hlen, u8 *prevhdr,\n\t\t      u8 nexthdr, __be32 frag_id,\n\t\t      struct ip6_fraglist_iter *iter);\nvoid ip6_fraglist_prepare(struct sk_buff *skb, struct ip6_fraglist_iter *iter);\n\nstatic inline struct sk_buff *ip6_fraglist_next(struct ip6_fraglist_iter *iter)\n{\n\tstruct sk_buff *skb = iter->frag;\n\n\titer->frag = skb->next;\n\tskb_mark_not_on_list(skb);\n\n\treturn skb;\n}\n\nstruct ip6_frag_state {\n\tu8\t\t*prevhdr;\n\tunsigned int\thlen;\n\tunsigned int\tmtu;\n\tunsigned int\tleft;\n\tint\t\toffset;\n\tint\t\tptr;\n\tint\t\throom;\n\tint\t\ttroom;\n\t__be32\t\tfrag_id;\n\tu8\t\tnexthdr;\n};\n\nvoid ip6_frag_init(struct sk_buff *skb, unsigned int hlen, unsigned int mtu,\n\t\t   unsigned short needed_tailroom, int hdr_room, u8 *prevhdr,\n\t\t   u8 nexthdr, __be32 frag_id, struct ip6_frag_state *state);\nstruct sk_buff *ip6_frag_next(struct sk_buff *skb,\n\t\t\t      struct ip6_frag_state *state);\n\n#define IP6_REPLY_MARK(net, mark) \\\n\t((net)->ipv6.sysctl.fwmark_reflect ? (mark) : 0)\n\n#include <net/sock.h>\n\n/* sysctls */\nextern int sysctl_mld_max_msf;\nextern int sysctl_mld_qrv;\n\n#define _DEVINC(net, statname, mod, idev, field)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tmod##SNMP_INC_STATS64((_idev)->stats.statname, (field));\\\n\tmod##SNMP_INC_STATS64((net)->mib.statname##_statistics, (field));\\\n})\n\n/* per device counters are atomic_long_t */\n#define _DEVINCATOMIC(net, statname, mod, idev, field)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS_ATOMIC_LONG((_idev)->stats.statname##dev, (field)); \\\n\tmod##SNMP_INC_STATS((net)->mib.statname##_statistics, (field));\\\n})\n\n/* per device and per net counters are atomic_long_t */\n#define _DEVINC_ATOMIC_ATOMIC(net, statname, idev, field)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS_ATOMIC_LONG((_idev)->stats.statname##dev, (field)); \\\n\tSNMP_INC_STATS_ATOMIC_LONG((net)->mib.statname##_statistics, (field));\\\n})\n\n#define _DEVADD(net, statname, mod, idev, field, val)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tmod##SNMP_ADD_STATS((_idev)->stats.statname, (field), (val)); \\\n\tmod##SNMP_ADD_STATS((net)->mib.statname##_statistics, (field), (val));\\\n})\n\n#define _DEVUPD(net, statname, mod, idev, field, val)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tmod##SNMP_UPD_PO_STATS((_idev)->stats.statname, field, (val)); \\\n\tmod##SNMP_UPD_PO_STATS((net)->mib.statname##_statistics, field, (val));\\\n})\n\n/* MIBs */\n\n#define IP6_INC_STATS(net, idev,field)\t\t\\\n\t\t_DEVINC(net, ipv6, , idev, field)\n#define __IP6_INC_STATS(net, idev,field)\t\\\n\t\t_DEVINC(net, ipv6, __, idev, field)\n#define IP6_ADD_STATS(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, , idev, field, val)\n#define __IP6_ADD_STATS(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, __, idev, field, val)\n#define IP6_UPD_PO_STATS(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, , idev, field, val)\n#define __IP6_UPD_PO_STATS(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, __, idev, field, val)\n#define ICMP6_INC_STATS(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, , idev, field)\n#define __ICMP6_INC_STATS(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, __, idev, field)\n\n#define ICMP6MSGOUT_INC_STATS(net, idev, field)\t\t\\\n\t_DEVINC_ATOMIC_ATOMIC(net, icmpv6msg, idev, field +256)\n#define ICMP6MSGIN_INC_STATS(net, idev, field)\t\\\n\t_DEVINC_ATOMIC_ATOMIC(net, icmpv6msg, idev, field)\n\nstruct ip6_ra_chain {\n\tstruct ip6_ra_chain\t*next;\n\tstruct sock\t\t*sk;\n\tint\t\t\tsel;\n\tvoid\t\t\t(*destructor)(struct sock *);\n};\n\nextern struct ip6_ra_chain\t*ip6_ra_chain;\nextern rwlock_t ip6_ra_lock;\n\n/*\n   This structure is prepared by protocol, when parsing\n   ancillary data and passed to IPv6.\n */\n\nstruct ipv6_txoptions {\n\trefcount_t\t\trefcnt;\n\t/* Length of this structure */\n\tint\t\t\ttot_len;\n\n\t/* length of extension headers   */\n\n\t__u16\t\t\topt_flen;\t/* after fragment hdr */\n\t__u16\t\t\topt_nflen;\t/* before fragment hdr */\n\n\tstruct ipv6_opt_hdr\t*hopopt;\n\tstruct ipv6_opt_hdr\t*dst0opt;\n\tstruct ipv6_rt_hdr\t*srcrt;\t/* Routing Header */\n\tstruct ipv6_opt_hdr\t*dst1opt;\n\tstruct rcu_head\t\trcu;\n\t/* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */\n};\n\n/* flowlabel_reflect sysctl values */\nenum flowlabel_reflect {\n\tFLOWLABEL_REFLECT_ESTABLISHED\t\t= 1,\n\tFLOWLABEL_REFLECT_TCP_RESET\t\t= 2,\n\tFLOWLABEL_REFLECT_ICMPV6_ECHO_REPLIES\t= 4,\n};\n\nstruct ip6_flowlabel {\n\tstruct ip6_flowlabel __rcu *next;\n\t__be32\t\t\tlabel;\n\tatomic_t\t\tusers;\n\tstruct in6_addr\t\tdst;\n\tstruct ipv6_txoptions\t*opt;\n\tunsigned long\t\tlinger;\n\tstruct rcu_head\t\trcu;\n\tu8\t\t\tshare;\n\tunion {\n\t\tstruct pid *pid;\n\t\tkuid_t uid;\n\t} owner;\n\tunsigned long\t\tlastuse;\n\tunsigned long\t\texpires;\n\tstruct net\t\t*fl_net;\n};\n\n#define IPV6_FLOWINFO_MASK\t\tcpu_to_be32(0x0FFFFFFF)\n#define IPV6_FLOWLABEL_MASK\t\tcpu_to_be32(0x000FFFFF)\n#define IPV6_FLOWLABEL_STATELESS_FLAG\tcpu_to_be32(0x00080000)\n\n#define IPV6_TCLASS_MASK (IPV6_FLOWINFO_MASK & ~IPV6_FLOWLABEL_MASK)\n#define IPV6_TCLASS_SHIFT\t20\n\nstruct ipv6_fl_socklist {\n\tstruct ipv6_fl_socklist\t__rcu\t*next;\n\tstruct ip6_flowlabel\t\t*fl;\n\tstruct rcu_head\t\t\trcu;\n};\n\nstruct ipcm6_cookie {\n\tstruct sockcm_cookie sockc;\n\t__s16 hlimit;\n\t__s16 tclass;\n\t__u16 gso_size;\n\t__s8  dontfrag;\n\tstruct ipv6_txoptions *opt;\n};\n\nstatic inline void ipcm6_init_sk(struct ipcm6_cookie *ipc6,\n\t\t\t\t const struct sock *sk)\n{\n\t*ipc6 = (struct ipcm6_cookie) {\n\t\t.hlimit = -1,\n\t\t.tclass = inet6_sk(sk)->tclass,\n\t\t.dontfrag = inet6_test_bit(DONTFRAG, sk),\n\t};\n\n\tsockcm_init(&ipc6->sockc, sk);\n}\n\nstatic inline struct ipv6_txoptions *txopt_get(const struct ipv6_pinfo *np)\n{\n\tstruct ipv6_txoptions *opt;\n\n\trcu_read_lock();\n\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\tif (!refcount_inc_not_zero(&opt->refcnt))\n\t\t\topt = NULL;\n\t\telse\n\t\t\topt = rcu_pointer_handoff(opt);\n\t}\n\trcu_read_unlock();\n\treturn opt;\n}\n\nstatic inline void txopt_put(struct ipv6_txoptions *opt)\n{\n\tif (opt && refcount_dec_and_test(&opt->refcnt))\n\t\tkfree_rcu(opt, rcu);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstruct ip6_flowlabel *__fl6_sock_lookup(struct sock *sk, __be32 label);\n\nextern struct static_key_false_deferred ipv6_flowlabel_exclusive;\nstatic inline struct ip6_flowlabel *fl6_sock_lookup(struct sock *sk,\n\t\t\t\t\t\t    __be32 label)\n{\n\tif (static_branch_unlikely(&ipv6_flowlabel_exclusive.key) &&\n\t    READ_ONCE(sock_net(sk)->ipv6.flowlabel_has_excl))\n\t\treturn __fl6_sock_lookup(sk, label) ? : ERR_PTR(-ENOENT);\n\n\treturn NULL;\n}\n#endif\n\nstruct ipv6_txoptions *fl6_merge_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t struct ip6_flowlabel *fl,\n\t\t\t\t\t struct ipv6_txoptions *fopt);\nvoid fl6_free_socklist(struct sock *sk);\nint ipv6_flowlabel_opt(struct sock *sk, sockptr_t optval, int optlen);\nint ipv6_flowlabel_opt_get(struct sock *sk, struct in6_flowlabel_req *freq,\n\t\t\t   int flags);\nint ip6_flowlabel_init(void);\nvoid ip6_flowlabel_cleanup(void);\nbool ip6_autoflowlabel(struct net *net, const struct sock *sk);\n\nstatic inline void fl6_sock_release(struct ip6_flowlabel *fl)\n{\n\tif (fl)\n\t\tatomic_dec(&fl->users);\n}\n\nenum skb_drop_reason icmpv6_notify(struct sk_buff *skb, u8 type,\n\t\t\t\t   u8 code, __be32 info);\n\nvoid icmpv6_push_pending_frames(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\tstruct icmp6hdr *thdr, int len);\n\nint ip6_ra_control(struct sock *sk, int sel);\n\nint ipv6_parse_hopopts(struct sk_buff *skb);\n\nstruct ipv6_txoptions *ipv6_dup_options(struct sock *sk,\n\t\t\t\t\tstruct ipv6_txoptions *opt);\nstruct ipv6_txoptions *ipv6_renew_options(struct sock *sk,\n\t\t\t\t\t  struct ipv6_txoptions *opt,\n\t\t\t\t\t  int newtype,\n\t\t\t\t\t  struct ipv6_opt_hdr *newopt);\nstruct ipv6_txoptions *__ipv6_fixup_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t    struct ipv6_txoptions *opt);\n\nstatic inline struct ipv6_txoptions *\nipv6_fixup_options(struct ipv6_txoptions *opt_space, struct ipv6_txoptions *opt)\n{\n\tif (!opt)\n\t\treturn NULL;\n\treturn __ipv6_fixup_options(opt_space, opt);\n}\n\nbool ipv6_opt_accepted(const struct sock *sk, const struct sk_buff *skb,\n\t\t       const struct inet6_skb_parm *opt);\nstruct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt);\n\n/* This helper is specialized for BIG TCP needs.\n * It assumes the hop_jumbo_hdr will immediately follow the IPV6 header.\n * It assumes headers are already in skb->head.\n * Returns: 0, or IPPROTO_TCP if a BIG TCP packet is there.\n */\nstatic inline int ipv6_has_hopopt_jumbo(const struct sk_buff *skb)\n{\n\tconst struct hop_jumbo_hdr *jhdr;\n\tconst struct ipv6hdr *nhdr;\n\n\tif (likely(skb->len <= GRO_LEGACY_MAX_SIZE))\n\t\treturn 0;\n\n\tif (skb->protocol != htons(ETH_P_IPV6))\n\t\treturn 0;\n\n\tif (skb_network_offset(skb) +\n\t    sizeof(struct ipv6hdr) +\n\t    sizeof(struct hop_jumbo_hdr) > skb_headlen(skb))\n\t\treturn 0;\n\n\tnhdr = ipv6_hdr(skb);\n\n\tif (nhdr->nexthdr != NEXTHDR_HOP)\n\t\treturn 0;\n\n\tjhdr = (const struct hop_jumbo_hdr *) (nhdr + 1);\n\tif (jhdr->tlv_type != IPV6_TLV_JUMBO || jhdr->hdrlen != 0 ||\n\t    jhdr->nexthdr != IPPROTO_TCP)\n\t\treturn 0;\n\treturn jhdr->nexthdr;\n}\n\n/* Return 0 if HBH header is successfully removed\n * Or if HBH removal is unnecessary (packet is not big TCP)\n * Return error to indicate dropping the packet\n */\nstatic inline int ipv6_hopopt_jumbo_remove(struct sk_buff *skb)\n{\n\tconst int hophdr_len = sizeof(struct hop_jumbo_hdr);\n\tint nexthdr = ipv6_has_hopopt_jumbo(skb);\n\tstruct ipv6hdr *h6;\n\n\tif (!nexthdr)\n\t\treturn 0;\n\n\tif (skb_cow_head(skb, 0))\n\t\treturn -1;\n\n\t/* Remove the HBH header.\n\t * Layout: [Ethernet header][IPv6 header][HBH][L4 Header]\n\t */\n\tmemmove(skb_mac_header(skb) + hophdr_len, skb_mac_header(skb),\n\t\tskb_network_header(skb) - skb_mac_header(skb) +\n\t\tsizeof(struct ipv6hdr));\n\n\t__skb_pull(skb, hophdr_len);\n\tskb->network_header += hophdr_len;\n\tskb->mac_header += hophdr_len;\n\n\th6 = ipv6_hdr(skb);\n\th6->nexthdr = nexthdr;\n\n\treturn 0;\n}\n\nstatic inline bool ipv6_accept_ra(const struct inet6_dev *idev)\n{\n\ts32 accept_ra = READ_ONCE(idev->cnf.accept_ra);\n\n\t/* If forwarding is enabled, RA are not accepted unless the special\n\t * hybrid mode (accept_ra=2) is enabled.\n\t */\n\treturn READ_ONCE(idev->cnf.forwarding) ? accept_ra == 2 :\n\t\taccept_ra;\n}\n\n#define IPV6_FRAG_HIGH_THRESH\t(4 * 1024*1024)\t/* 4194304 */\n#define IPV6_FRAG_LOW_THRESH\t(3 * 1024*1024)\t/* 3145728 */\n#define IPV6_FRAG_TIMEOUT\t(60 * HZ)\t/* 60 seconds */\n\nint __ipv6_addr_type(const struct in6_addr *addr);\nstatic inline int ipv6_addr_type(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & 0xffff;\n}\n\nstatic inline int ipv6_addr_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & IPV6_ADDR_SCOPE_MASK;\n}\n\nstatic inline int __ipv6_addr_src_scope(int type)\n{\n\treturn (type == IPV6_ADDR_ANY) ? __IPV6_ADDR_SCOPE_INVALID : (type >> 16);\n}\n\nstatic inline int ipv6_addr_src_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_src_scope(__ipv6_addr_type(addr));\n}\n\nstatic inline bool __ipv6_addr_needs_scope_id(int type)\n{\n\treturn type & IPV6_ADDR_LINKLOCAL ||\n\t       (type & IPV6_ADDR_MULTICAST &&\n\t\t(type & (IPV6_ADDR_LOOPBACK|IPV6_ADDR_LINKLOCAL)));\n}\n\nstatic inline __u32 ipv6_iface_scope_id(const struct in6_addr *addr, int iface)\n{\n\treturn __ipv6_addr_needs_scope_id(__ipv6_addr_type(addr)) ? iface : 0;\n}\n\nstatic inline int ipv6_addr_cmp(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn memcmp(a1, a2, sizeof(struct in6_addr));\n}\n\nstatic inline bool\nipv6_masked_addr_cmp(const struct in6_addr *a1, const struct in6_addr *m,\n\t\t     const struct in6_addr *a2)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul1 = (const unsigned long *)a1;\n\tconst unsigned long *ulm = (const unsigned long *)m;\n\tconst unsigned long *ul2 = (const unsigned long *)a2;\n\n\treturn !!(((ul1[0] ^ ul2[0]) & ulm[0]) |\n\t\t  ((ul1[1] ^ ul2[1]) & ulm[1]));\n#else\n\treturn !!(((a1->s6_addr32[0] ^ a2->s6_addr32[0]) & m->s6_addr32[0]) |\n\t\t  ((a1->s6_addr32[1] ^ a2->s6_addr32[1]) & m->s6_addr32[1]) |\n\t\t  ((a1->s6_addr32[2] ^ a2->s6_addr32[2]) & m->s6_addr32[2]) |\n\t\t  ((a1->s6_addr32[3] ^ a2->s6_addr32[3]) & m->s6_addr32[3]));\n#endif\n}\n\nstatic inline void ipv6_addr_prefix(struct in6_addr *pfx,\n\t\t\t\t    const struct in6_addr *addr,\n\t\t\t\t    int plen)\n{\n\t/* caller must guarantee 0 <= plen <= 128 */\n\tint o = plen >> 3,\n\t    b = plen & 0x7;\n\n\tmemset(pfx->s6_addr, 0, sizeof(pfx->s6_addr));\n\tmemcpy(pfx->s6_addr, addr, o);\n\tif (b != 0)\n\t\tpfx->s6_addr[o] = addr->s6_addr[o] & (0xff00 >> b);\n}\n\nstatic inline void ipv6_addr_prefix_copy(struct in6_addr *addr,\n\t\t\t\t\t const struct in6_addr *pfx,\n\t\t\t\t\t int plen)\n{\n\t/* caller must guarantee 0 <= plen <= 128 */\n\tint o = plen >> 3,\n\t    b = plen & 0x7;\n\n\tmemcpy(addr->s6_addr, pfx, o);\n\tif (b != 0) {\n\t\taddr->s6_addr[o] &= ~(0xff00 >> b);\n\t\taddr->s6_addr[o] |= (pfx->s6_addr[o] & (0xff00 >> b));\n\t}\n}\n\nstatic inline void __ipv6_addr_set_half(__be32 *addr,\n\t\t\t\t\t__be32 wh, __be32 wl)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n#if defined(__BIG_ENDIAN)\n\tif (__builtin_constant_p(wh) && __builtin_constant_p(wl)) {\n\t\t*(__force u64 *)addr = ((__force u64)(wh) << 32 | (__force u64)(wl));\n\t\treturn;\n\t}\n#elif defined(__LITTLE_ENDIAN)\n\tif (__builtin_constant_p(wl) && __builtin_constant_p(wh)) {\n\t\t*(__force u64 *)addr = ((__force u64)(wl) << 32 | (__force u64)(wh));\n\t\treturn;\n\t}\n#endif\n#endif\n\taddr[0] = wh;\n\taddr[1] = wl;\n}\n\nstatic inline void ipv6_addr_set(struct in6_addr *addr,\n\t\t\t\t     __be32 w1, __be32 w2,\n\t\t\t\t     __be32 w3, __be32 w4)\n{\n\t__ipv6_addr_set_half(&addr->s6_addr32[0], w1, w2);\n\t__ipv6_addr_set_half(&addr->s6_addr32[2], w3, w4);\n}\n\nstatic inline bool ipv6_addr_equal(const struct in6_addr *a1,\n\t\t\t\t   const struct in6_addr *a2)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul1 = (const unsigned long *)a1;\n\tconst unsigned long *ul2 = (const unsigned long *)a2;\n\n\treturn ((ul1[0] ^ ul2[0]) | (ul1[1] ^ ul2[1])) == 0UL;\n#else\n\treturn ((a1->s6_addr32[0] ^ a2->s6_addr32[0]) |\n\t\t(a1->s6_addr32[1] ^ a2->s6_addr32[1]) |\n\t\t(a1->s6_addr32[2] ^ a2->s6_addr32[2]) |\n\t\t(a1->s6_addr32[3] ^ a2->s6_addr32[3])) == 0;\n#endif\n}\n\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\nstatic inline bool __ipv6_prefix_equal64_half(const __be64 *a1,\n\t\t\t\t\t      const __be64 *a2,\n\t\t\t\t\t      unsigned int len)\n{\n\tif (len && ((*a1 ^ *a2) & cpu_to_be64((~0UL) << (64 - len))))\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool ipv6_prefix_equal(const struct in6_addr *addr1,\n\t\t\t\t     const struct in6_addr *addr2,\n\t\t\t\t     unsigned int prefixlen)\n{\n\tconst __be64 *a1 = (const __be64 *)addr1;\n\tconst __be64 *a2 = (const __be64 *)addr2;\n\n\tif (prefixlen >= 64) {\n\t\tif (a1[0] ^ a2[0])\n\t\t\treturn false;\n\t\treturn __ipv6_prefix_equal64_half(a1 + 1, a2 + 1, prefixlen - 64);\n\t}\n\treturn __ipv6_prefix_equal64_half(a1, a2, prefixlen);\n}\n#else\nstatic inline bool ipv6_prefix_equal(const struct in6_addr *addr1,\n\t\t\t\t     const struct in6_addr *addr2,\n\t\t\t\t     unsigned int prefixlen)\n{\n\tconst __be32 *a1 = addr1->s6_addr32;\n\tconst __be32 *a2 = addr2->s6_addr32;\n\tunsigned int pdw, pbi;\n\n\t/* check complete u32 in prefix */\n\tpdw = prefixlen >> 5;\n\tif (pdw && memcmp(a1, a2, pdw << 2))\n\t\treturn false;\n\n\t/* check incomplete u32 in prefix */\n\tpbi = prefixlen & 0x1f;\n\tif (pbi && ((a1[pdw] ^ a2[pdw]) & htonl((0xffffffff) << (32 - pbi))))\n\t\treturn false;\n\n\treturn true;\n}\n#endif\n\nstatic inline bool ipv6_addr_any(const struct in6_addr *a)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul = (const unsigned long *)a;\n\n\treturn (ul[0] | ul[1]) == 0UL;\n#else\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | a->s6_addr32[3]) == 0;\n#endif\n}\n\nstatic inline u32 ipv6_addr_hash(const struct in6_addr *a)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst unsigned long *ul = (const unsigned long *)a;\n\tunsigned long x = ul[0] ^ ul[1];\n\n\treturn (u32)(x ^ (x >> 32));\n#else\n\treturn (__force u32)(a->s6_addr32[0] ^ a->s6_addr32[1] ^\n\t\t\t     a->s6_addr32[2] ^ a->s6_addr32[3]);\n#endif\n}\n\n/* more secured version of ipv6_addr_hash() */\nstatic inline u32 __ipv6_addr_jhash(const struct in6_addr *a, const u32 initval)\n{\n\treturn jhash2((__force const u32 *)a->s6_addr32,\n\t\t      ARRAY_SIZE(a->s6_addr32), initval);\n}\n\nstatic inline bool ipv6_addr_loopback(const struct in6_addr *a)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tconst __be64 *be = (const __be64 *)a;\n\n\treturn (be[0] | (be[1] ^ cpu_to_be64(1))) == 0UL;\n#else\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | (a->s6_addr32[3] ^ cpu_to_be32(1))) == 0;\n#endif\n}\n\n/*\n * Note that we must __force cast these to unsigned long to make sparse happy,\n * since all of the endian-annotated types are fixed size regardless of arch.\n */\nstatic inline bool ipv6_addr_v4mapped(const struct in6_addr *a)\n{\n\treturn (\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\t\t*(unsigned long *)a |\n#else\n\t\t(__force unsigned long)(a->s6_addr32[0] | a->s6_addr32[1]) |\n#endif\n\t\t(__force unsigned long)(a->s6_addr32[2] ^\n\t\t\t\t\tcpu_to_be32(0x0000ffff))) == 0UL;\n}\n\nstatic inline bool ipv6_addr_v4mapped_loopback(const struct in6_addr *a)\n{\n\treturn ipv6_addr_v4mapped(a) && ipv4_is_loopback(a->s6_addr32[3]);\n}\n\nstatic inline u32 ipv6_portaddr_hash(const struct net *net,\n\t\t\t\t     const struct in6_addr *addr6,\n\t\t\t\t     unsigned int port)\n{\n\tunsigned int hash, mix = net_hash_mix(net);\n\n\tif (ipv6_addr_any(addr6))\n\t\thash = jhash_1word(0, mix);\n\telse if (ipv6_addr_v4mapped(addr6))\n\t\thash = jhash_1word((__force u32)addr6->s6_addr32[3], mix);\n\telse\n\t\thash = jhash2((__force u32 *)addr6->s6_addr32, 4, mix);\n\n\treturn hash ^ port;\n}\n\n/*\n * Check for a RFC 4843 ORCHID address\n * (Overlay Routable Cryptographic Hash Identifiers)\n */\nstatic inline bool ipv6_addr_orchid(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] & htonl(0xfffffff0)) == htonl(0x20010010);\n}\n\nstatic inline bool ipv6_addr_is_multicast(const struct in6_addr *addr)\n{\n\treturn (addr->s6_addr32[0] & htonl(0xFF000000)) == htonl(0xFF000000);\n}\n\nstatic inline void ipv6_addr_set_v4mapped(const __be32 addr,\n\t\t\t\t\t  struct in6_addr *v4mapped)\n{\n\tipv6_addr_set(v4mapped,\n\t\t\t0, 0,\n\t\t\thtonl(0x0000FFFF),\n\t\t\taddr);\n}\n\n/*\n * find the first different bit between two addresses\n * length of address must be a multiple of 32bits\n */\nstatic inline int __ipv6_addr_diff32(const void *token1, const void *token2, int addrlen)\n{\n\tconst __be32 *a1 = token1, *a2 = token2;\n\tint i;\n\n\taddrlen >>= 2;\n\n\tfor (i = 0; i < addrlen; i++) {\n\t\t__be32 xb = a1[i] ^ a2[i];\n\t\tif (xb)\n\t\t\treturn i * 32 + 31 - __fls(ntohl(xb));\n\t}\n\n\t/*\n\t *\twe should *never* get to this point since that\n\t *\twould mean the addrs are equal\n\t *\n\t *\tHowever, we do get to it 8) And exactly, when\n\t *\taddresses are equal 8)\n\t *\n\t *\tip route add 1111::/128 via ...\n\t *\tip route add 1111::/64 via ...\n\t *\tand we are here.\n\t *\n\t *\tIdeally, this function should stop comparison\n\t *\tat prefix length. It does not, but it is still OK,\n\t *\tif returned value is greater than prefix length.\n\t *\t\t\t\t\t--ANK (980803)\n\t */\n\treturn addrlen << 5;\n}\n\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\nstatic inline int __ipv6_addr_diff64(const void *token1, const void *token2, int addrlen)\n{\n\tconst __be64 *a1 = token1, *a2 = token2;\n\tint i;\n\n\taddrlen >>= 3;\n\n\tfor (i = 0; i < addrlen; i++) {\n\t\t__be64 xb = a1[i] ^ a2[i];\n\t\tif (xb)\n\t\t\treturn i * 64 + 63 - __fls(be64_to_cpu(xb));\n\t}\n\n\treturn addrlen << 6;\n}\n#endif\n\nstatic inline int __ipv6_addr_diff(const void *token1, const void *token2, int addrlen)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tif (__builtin_constant_p(addrlen) && !(addrlen & 7))\n\t\treturn __ipv6_addr_diff64(token1, token2, addrlen);\n#endif\n\treturn __ipv6_addr_diff32(token1, token2, addrlen);\n}\n\nstatic inline int ipv6_addr_diff(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn __ipv6_addr_diff(a1, a2, sizeof(struct in6_addr));\n}\n\n__be32 ipv6_select_ident(struct net *net,\n\t\t\t const struct in6_addr *daddr,\n\t\t\t const struct in6_addr *saddr);\n__be32 ipv6_proxy_select_ident(struct net *net, struct sk_buff *skb);\n\nint ip6_dst_hoplimit(struct dst_entry *dst);\n\nstatic inline int ip6_sk_dst_hoplimit(struct ipv6_pinfo *np, struct flowi6 *fl6,\n\t\t\t\t      struct dst_entry *dst)\n{\n\tint hlimit;\n\n\tif (ipv6_addr_is_multicast(&fl6->daddr))\n\t\thlimit = READ_ONCE(np->mcast_hops);\n\telse\n\t\thlimit = READ_ONCE(np->hop_limit);\n\tif (hlimit < 0)\n\t\thlimit = ip6_dst_hoplimit(dst);\n\treturn hlimit;\n}\n\n/* copy IPv6 saddr & daddr to flow_keys, possibly using 64bit load/store\n * Equivalent to :\tflow->v6addrs.src = iph->saddr;\n *\t\t\tflow->v6addrs.dst = iph->daddr;\n */\nstatic inline void iph_to_flow_copy_v6addrs(struct flow_keys *flow,\n\t\t\t\t\t    const struct ipv6hdr *iph)\n{\n\tBUILD_BUG_ON(offsetof(typeof(flow->addrs), v6addrs.dst) !=\n\t\t     offsetof(typeof(flow->addrs), v6addrs.src) +\n\t\t     sizeof(flow->addrs.v6addrs.src));\n\tmemcpy(&flow->addrs.v6addrs, &iph->addrs, sizeof(flow->addrs.v6addrs));\n\tflow->control.addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n\nstatic inline bool ipv6_can_nonlocal_bind(struct net *net,\n\t\t\t\t\t  struct inet_sock *inet)\n{\n\treturn net->ipv6.sysctl.ip_nonlocal_bind ||\n\t\ttest_bit(INET_FLAGS_FREEBIND, &inet->inet_flags) ||\n\t\ttest_bit(INET_FLAGS_TRANSPARENT, &inet->inet_flags);\n}\n\n/* Sysctl settings for net ipv6.auto_flowlabels */\n#define IP6_AUTO_FLOW_LABEL_OFF\t\t0\n#define IP6_AUTO_FLOW_LABEL_OPTOUT\t1\n#define IP6_AUTO_FLOW_LABEL_OPTIN\t2\n#define IP6_AUTO_FLOW_LABEL_FORCED\t3\n\n#define IP6_AUTO_FLOW_LABEL_MAX\t\tIP6_AUTO_FLOW_LABEL_FORCED\n\n#define IP6_DEFAULT_AUTO_FLOW_LABELS\tIP6_AUTO_FLOW_LABEL_OPTOUT\n\nstatic inline __be32 ip6_make_flowlabel(struct net *net, struct sk_buff *skb,\n\t\t\t\t\t__be32 flowlabel, bool autolabel,\n\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tu32 hash;\n\n\t/* @flowlabel may include more than a flow label, eg, the traffic class.\n\t * Here we want only the flow label value.\n\t */\n\tflowlabel &= IPV6_FLOWLABEL_MASK;\n\n\tif (flowlabel ||\n\t    net->ipv6.sysctl.auto_flowlabels == IP6_AUTO_FLOW_LABEL_OFF ||\n\t    (!autolabel &&\n\t     net->ipv6.sysctl.auto_flowlabels != IP6_AUTO_FLOW_LABEL_FORCED))\n\t\treturn flowlabel;\n\n\thash = skb_get_hash_flowi6(skb, fl6);\n\n\t/* Since this is being sent on the wire obfuscate hash a bit\n\t * to minimize possibility that any useful information to an\n\t * attacker is leaked. Only lower 20 bits are relevant.\n\t */\n\thash = rol32(hash, 16);\n\n\tflowlabel = (__force __be32)hash & IPV6_FLOWLABEL_MASK;\n\n\tif (net->ipv6.sysctl.flowlabel_state_ranges)\n\t\tflowlabel |= IPV6_FLOWLABEL_STATELESS_FLAG;\n\n\treturn flowlabel;\n}\n\nstatic inline int ip6_default_np_autolabel(struct net *net)\n{\n\tswitch (net->ipv6.sysctl.auto_flowlabels) {\n\tcase IP6_AUTO_FLOW_LABEL_OFF:\n\tcase IP6_AUTO_FLOW_LABEL_OPTIN:\n\tdefault:\n\t\treturn 0;\n\tcase IP6_AUTO_FLOW_LABEL_OPTOUT:\n\tcase IP6_AUTO_FLOW_LABEL_FORCED:\n\t\treturn 1;\n\t}\n}\n#else\nstatic inline __be32 ip6_make_flowlabel(struct net *net, struct sk_buff *skb,\n\t\t\t\t\t__be32 flowlabel, bool autolabel,\n\t\t\t\t\tstruct flowi6 *fl6)\n{\n\treturn flowlabel;\n}\nstatic inline int ip6_default_np_autolabel(struct net *net)\n{\n\treturn 0;\n}\n#endif\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic inline int ip6_multipath_hash_policy(const struct net *net)\n{\n\treturn net->ipv6.sysctl.multipath_hash_policy;\n}\nstatic inline u32 ip6_multipath_hash_fields(const struct net *net)\n{\n\treturn net->ipv6.sysctl.multipath_hash_fields;\n}\n#else\nstatic inline int ip6_multipath_hash_policy(const struct net *net)\n{\n\treturn 0;\n}\nstatic inline u32 ip6_multipath_hash_fields(const struct net *net)\n{\n\treturn 0;\n}\n#endif\n\n/*\n *\tHeader manipulation\n */\nstatic inline void ip6_flow_hdr(struct ipv6hdr *hdr, unsigned int tclass,\n\t\t\t\t__be32 flowlabel)\n{\n\t*(__be32 *)hdr = htonl(0x60000000 | (tclass << 20)) | flowlabel;\n}\n\nstatic inline __be32 ip6_flowinfo(const struct ipv6hdr *hdr)\n{\n\treturn *(__be32 *)hdr & IPV6_FLOWINFO_MASK;\n}\n\nstatic inline __be32 ip6_flowlabel(const struct ipv6hdr *hdr)\n{\n\treturn *(__be32 *)hdr & IPV6_FLOWLABEL_MASK;\n}\n\nstatic inline u8 ip6_tclass(__be32 flowinfo)\n{\n\treturn ntohl(flowinfo & IPV6_TCLASS_MASK) >> IPV6_TCLASS_SHIFT;\n}\n\nstatic inline dscp_t ip6_dscp(__be32 flowinfo)\n{\n\treturn inet_dsfield_to_dscp(ip6_tclass(flowinfo));\n}\n\nstatic inline __be32 ip6_make_flowinfo(unsigned int tclass, __be32 flowlabel)\n{\n\treturn htonl(tclass << IPV6_TCLASS_SHIFT) | flowlabel;\n}\n\nstatic inline __be32 flowi6_get_flowlabel(const struct flowi6 *fl6)\n{\n\treturn fl6->flowlabel & IPV6_FLOWLABEL_MASK;\n}\n\n/*\n *\tPrototypes exported by ipv6\n */\n\n/*\n *\trcv function (called from netdevice level)\n */\n\nint ipv6_rcv(struct sk_buff *skb, struct net_device *dev,\n\t     struct packet_type *pt, struct net_device *orig_dev);\nvoid ipv6_list_rcv(struct list_head *head, struct packet_type *pt,\n\t\t   struct net_device *orig_dev);\n\nint ip6_rcv_finish(struct net *net, struct sock *sk, struct sk_buff *skb);\n\n/*\n *\tupper-layer output functions\n */\nint ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,\n\t     __u32 mark, struct ipv6_txoptions *opt, int tclass, u32 priority);\n\nint ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr);\n\nint ip6_append_data(struct sock *sk,\n\t\t    int getfrag(void *from, char *to, int offset, int len,\n\t\t\t\tint odd, struct sk_buff *skb),\n\t\t    void *from, size_t length, int transhdrlen,\n\t\t    struct ipcm6_cookie *ipc6, struct flowi6 *fl6,\n\t\t    struct rt6_info *rt, unsigned int flags);\n\nint ip6_push_pending_frames(struct sock *sk);\n\nvoid ip6_flush_pending_frames(struct sock *sk);\n\nint ip6_send_skb(struct sk_buff *skb);\n\nstruct sk_buff *__ip6_make_skb(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t       struct inet_cork_full *cork,\n\t\t\t       struct inet6_cork *v6_cork);\nstruct sk_buff *ip6_make_skb(struct sock *sk,\n\t\t\t     int getfrag(void *from, char *to, int offset,\n\t\t\t\t\t int len, int odd, struct sk_buff *skb),\n\t\t\t     void *from, size_t length, int transhdrlen,\n\t\t\t     struct ipcm6_cookie *ipc6,\n\t\t\t     struct rt6_info *rt, unsigned int flags,\n\t\t\t     struct inet_cork_full *cork);\n\nstatic inline struct sk_buff *ip6_finish_skb(struct sock *sk)\n{\n\treturn __ip6_make_skb(sk, &sk->sk_write_queue, &inet_sk(sk)->cork,\n\t\t\t      &inet6_sk(sk)->cork);\n}\n\nint ip6_dst_lookup(struct net *net, struct sock *sk, struct dst_entry **dst,\n\t\t   struct flowi6 *fl6);\nstruct dst_entry *ip6_dst_lookup_flow(struct net *net, const struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t      const struct in6_addr *final_dst);\nstruct dst_entry *ip6_sk_dst_lookup_flow(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t\t const struct in6_addr *final_dst,\n\t\t\t\t\t bool connected);\nstruct dst_entry *ip6_blackhole_route(struct net *net,\n\t\t\t\t      struct dst_entry *orig_dst);\n\n/*\n *\tskb processing functions\n */\n\nint ip6_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip6_forward(struct sk_buff *skb);\nint ip6_input(struct sk_buff *skb);\nint ip6_mc_input(struct sk_buff *skb);\nvoid ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,\n\t\t\t      bool have_final);\n\nint __ip6_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip6_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\n\n/*\n *\tExtension header (options) processing\n */\n\nvoid ipv6_push_nfrag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt,\n\t\t\t  u8 *proto, struct in6_addr **daddr_p,\n\t\t\t  struct in6_addr *saddr);\nvoid ipv6_push_frag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt,\n\t\t\t u8 *proto);\n\nint ipv6_skip_exthdr(const struct sk_buff *, int start, u8 *nexthdrp,\n\t\t     __be16 *frag_offp);\n\nbool ipv6_ext_hdr(u8 nexthdr);\n\nenum {\n\tIP6_FH_F_FRAG\t\t= (1 << 0),\n\tIP6_FH_F_AUTH\t\t= (1 << 1),\n\tIP6_FH_F_SKIP_RH\t= (1 << 2),\n};\n\n/* find specified header and get offset to it */\nint ipv6_find_hdr(const struct sk_buff *skb, unsigned int *offset, int target,\n\t\t  unsigned short *fragoff, int *fragflg);\n\nint ipv6_find_tlv(const struct sk_buff *skb, int offset, int type);\n\nstruct in6_addr *fl6_update_dst(struct flowi6 *fl6,\n\t\t\t\tconst struct ipv6_txoptions *opt,\n\t\t\t\tstruct in6_addr *orig);\n\n/*\n *\tsocket options (ipv6_sockglue.c)\n */\nDECLARE_STATIC_KEY_FALSE(ip6_min_hopcount);\n\nint do_ipv6_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t       unsigned int optlen);\nint ipv6_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t    unsigned int optlen);\nint do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, sockptr_t optlen);\nint ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen);\n\nint __ip6_datagram_connect(struct sock *sk, struct sockaddr *addr,\n\t\t\t   int addr_len);\nint ip6_datagram_connect(struct sock *sk, struct sockaddr *addr, int addr_len);\nint ip6_datagram_connect_v6_only(struct sock *sk, struct sockaddr *addr,\n\t\t\t\t int addr_len);\nint ip6_datagram_dst_update(struct sock *sk, bool fix_sk_saddr);\nvoid ip6_datagram_release_cb(struct sock *sk);\n\nint ipv6_recv_error(struct sock *sk, struct msghdr *msg, int len,\n\t\t    int *addr_len);\nint ipv6_recv_rxpmtu(struct sock *sk, struct msghdr *msg, int len,\n\t\t     int *addr_len);\nvoid ipv6_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,\n\t\t     u32 info, u8 *payload);\nvoid ipv6_local_error(struct sock *sk, int err, struct flowi6 *fl6, u32 info);\nvoid ipv6_local_rxpmtu(struct sock *sk, struct flowi6 *fl6, u32 mtu);\n\nvoid inet6_cleanup_sock(struct sock *sk);\nvoid inet6_sock_destruct(struct sock *sk);\nint inet6_release(struct socket *sock);\nint inet6_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len);\nint inet6_bind_sk(struct sock *sk, struct sockaddr *uaddr, int addr_len);\nint inet6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t  int peer);\nint inet6_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg);\nint inet6_compat_ioctl(struct socket *sock, unsigned int cmd,\n\t\tunsigned long arg);\n\nint inet6_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t\t      struct sock *sk);\nint inet6_sendmsg(struct socket *sock, struct msghdr *msg, size_t size);\nint inet6_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t  int flags);\n\n/*\n * reassembly.c\n */\nextern const struct proto_ops inet6_stream_ops;\nextern const struct proto_ops inet6_dgram_ops;\nextern const struct proto_ops inet6_sockraw_ops;\n\nstruct group_source_req;\nstruct group_filter;\n\nint ip6_mc_source(int add, int omode, struct sock *sk,\n\t\t  struct group_source_req *pgsr);\nint ip6_mc_msfilter(struct sock *sk, struct group_filter *gsf,\n\t\t  struct sockaddr_storage *list);\nint ip6_mc_msfget(struct sock *sk, struct group_filter *gsf,\n\t\t  sockptr_t optval, size_t ss_offset);\n\n#ifdef CONFIG_PROC_FS\nint ac6_proc_init(struct net *net);\nvoid ac6_proc_exit(struct net *net);\nint raw6_proc_init(void);\nvoid raw6_proc_exit(void);\nint tcp6_proc_init(struct net *net);\nvoid tcp6_proc_exit(struct net *net);\nint udp6_proc_init(struct net *net);\nvoid udp6_proc_exit(struct net *net);\nint udplite6_proc_init(void);\nvoid udplite6_proc_exit(void);\nint ipv6_misc_proc_init(void);\nvoid ipv6_misc_proc_exit(void);\nint snmp6_register_dev(struct inet6_dev *idev);\nint snmp6_unregister_dev(struct inet6_dev *idev);\n\n#else\nstatic inline int ac6_proc_init(struct net *net) { return 0; }\nstatic inline void ac6_proc_exit(struct net *net) { }\nstatic inline int snmp6_register_dev(struct inet6_dev *idev) { return 0; }\nstatic inline int snmp6_unregister_dev(struct inet6_dev *idev) { return 0; }\n#endif\n\n#ifdef CONFIG_SYSCTL\nstruct ctl_table *ipv6_icmp_sysctl_init(struct net *net);\nsize_t ipv6_icmp_sysctl_table_size(void);\nstruct ctl_table *ipv6_route_sysctl_init(struct net *net);\nsize_t ipv6_route_sysctl_table_size(struct net *net);\nint ipv6_sysctl_register(void);\nvoid ipv6_sysctl_unregister(void);\n#endif\n\nint ipv6_sock_mc_join(struct sock *sk, int ifindex,\n\t\t      const struct in6_addr *addr);\nint ipv6_sock_mc_join_ssm(struct sock *sk, int ifindex,\n\t\t\t  const struct in6_addr *addr, unsigned int mode);\nint ipv6_sock_mc_drop(struct sock *sk, int ifindex,\n\t\t      const struct in6_addr *addr);\n\nstatic inline int ip6_sock_set_v6only(struct sock *sk)\n{\n\tif (inet_sk(sk)->inet_num)\n\t\treturn -EINVAL;\n\tlock_sock(sk);\n\tsk->sk_ipv6only = true;\n\trelease_sock(sk);\n\treturn 0;\n}\n\nstatic inline void ip6_sock_set_recverr(struct sock *sk)\n{\n\tinet6_set_bit(RECVERR6, sk);\n}\n\n#define IPV6_PREFER_SRC_MASK (IPV6_PREFER_SRC_TMP | IPV6_PREFER_SRC_PUBLIC | \\\n\t\t\t      IPV6_PREFER_SRC_COA)\n\nstatic inline int ip6_sock_set_addr_preferences(struct sock *sk, int val)\n{\n\tunsigned int prefmask = ~IPV6_PREFER_SRC_MASK;\n\tunsigned int pref = 0;\n\n\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\tswitch (val & (IPV6_PREFER_SRC_PUBLIC |\n\t\t       IPV6_PREFER_SRC_TMP |\n\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC |\n\t\t\t      IPV6_PREFER_SRC_TMP);\n\t\tbreak;\n\tcase IPV6_PREFER_SRC_TMP:\n\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC |\n\t\t\t      IPV6_PREFER_SRC_TMP);\n\t\tbreak;\n\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC |\n\t\t\t      IPV6_PREFER_SRC_TMP);\n\t\tbreak;\n\tcase 0:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* check HOME/COA conflicts */\n\tswitch (val & (IPV6_PREFER_SRC_HOME | IPV6_PREFER_SRC_COA)) {\n\tcase IPV6_PREFER_SRC_HOME:\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\n\t\tbreak;\n\tcase IPV6_PREFER_SRC_COA:\n\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tbreak;\n\tcase 0:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* check CGA/NONCGA conflicts */\n\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\tcase IPV6_PREFER_SRC_CGA:\n\tcase IPV6_PREFER_SRC_NONCGA:\n\tcase 0:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tWRITE_ONCE(inet6_sk(sk)->srcprefs,\n\t\t   (READ_ONCE(inet6_sk(sk)->srcprefs) & prefmask) | pref);\n\treturn 0;\n}\n\nstatic inline void ip6_sock_set_recvpktinfo(struct sock *sk)\n{\n\tlock_sock(sk);\n\tinet6_sk(sk)->rxopt.bits.rxinfo = true;\n\trelease_sock(sk);\n}\n\n#define IPV6_ADDR_WORDS 4\n\nstatic inline void ipv6_addr_cpu_to_be32(__be32 *dst, const u32 *src)\n{\n\tcpu_to_be32_array(dst, src, IPV6_ADDR_WORDS);\n}\n\nstatic inline void ipv6_addr_be32_to_cpu(u32 *dst, const __be32 *src)\n{\n\tbe32_to_cpu_array(dst, src, IPV6_ADDR_WORDS);\n}\n\n#endif /* _NET_IPV6_H */\n", "patch": "@@ -205,6 +205,7 @@ extern rwlock_t ip6_ra_lock;\n  */\n \n struct ipv6_txoptions {\n+\tatomic_t\t\trefcnt;\n \t/* Length of this structure */\n \tint\t\t\ttot_len;\n \n@@ -217,7 +218,7 @@ struct ipv6_txoptions {\n \tstruct ipv6_opt_hdr\t*dst0opt;\n \tstruct ipv6_rt_hdr\t*srcrt;\t/* Routing Header */\n \tstruct ipv6_opt_hdr\t*dst1opt;\n-\n+\tstruct rcu_head\t\trcu;\n \t/* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */\n };\n \n@@ -252,6 +253,24 @@ struct ipv6_fl_socklist {\n \tstruct rcu_head\t\t\trcu;\n };\n \n+static inline struct ipv6_txoptions *txopt_get(const struct ipv6_pinfo *np)\n+{\n+\tstruct ipv6_txoptions *opt;\n+\n+\trcu_read_lock();\n+\topt = rcu_dereference(np->opt);\n+\tif (opt && !atomic_inc_not_zero(&opt->refcnt))\n+\t\topt = NULL;\n+\trcu_read_unlock();\n+\treturn opt;\n+}\n+\n+static inline void txopt_put(struct ipv6_txoptions *opt)\n+{\n+\tif (opt && atomic_dec_and_test(&opt->refcnt))\n+\t\tkfree_rcu(opt, rcu);\n+}\n+\n struct ip6_flowlabel *fl6_sock_lookup(struct sock *sk, __be32 label);\n struct ipv6_txoptions *fl6_merge_options(struct ipv6_txoptions *opt_space,\n \t\t\t\t\t struct ip6_flowlabel *fl,", "file_path": "files/2016_8\\83", "file_language": "h", "file_name": "include/net/ipv6.h", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/dccp/ipv6.c", "code": "/*\n *\tDCCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tBased on net/dccp6/ipv6.c\n *\n *\tArnaldo Carvalho de Melo <acme@ghostprotocols.net>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/xfrm.h>\n\n#include <net/addrconf.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_sock.h>\n#include <net/inet6_connection_sock.h>\n#include <net/inet6_hashtables.h>\n#include <net/ip6_route.h>\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n#include <net/secure_seq.h>\n\n#include \"dccp.h\"\n#include \"ipv6.h\"\n#include \"feat.h\"\n\n/* The per-net dccp.v6_ctl_sk is used for sending RSTs and ACKs */\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped;\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops;\n\n/* add pseudo-header to DCCP checksum stored in skb->csum */\nstatic inline __sum16 dccp_v6_csum_finish(struct sk_buff *skb,\n\t\t\t\t      const struct in6_addr *saddr,\n\t\t\t\t      const struct in6_addr *daddr)\n{\n\treturn csum_ipv6_magic(saddr, daddr, skb->len, IPPROTO_DCCP, skb->csum);\n}\n\nstatic inline void dccp_v6_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tdccp_csum_outgoing(skb);\n\tdh->dccph_checksum = dccp_v6_csum_finish(skb, &np->saddr, &sk->sk_v6_daddr);\n}\n\nstatic inline __u64 dccp_v6_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_dccpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t     ipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_dport,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_sport     );\n\n}\n\nstatic void dccp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct dccp_hdr *dh = (struct dccp_hdr *)(skb->data + offset);\n\tstruct dccp_sock *dp;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\t__u64 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->len < offset + sizeof(*dh) ||\n\t    skb->len < offset + __dccp_basic_hdr_len(dh)) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tsk = __inet6_lookup_established(net, &dccp_hashinfo,\n\t\t\t\t\t&hdr->daddr, dh->dccph_dport,\n\t\t\t\t\t&hdr->saddr, ntohs(dh->dccph_sport),\n\t\t\t\t\tinet6_iif(skb));\n\n\tif (!sk) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = dccp_hdr_seq(dh);\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV)\n\t\treturn dccp_req_err(sk, seq);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\tif (dst)\n\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst = NULL;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (DCCPF_LISTEN | DCCPF_CLOSED))\n\t\t\tgoto out;\n\n\t\tdst = inet6_csk_update_pmtu(sk, ntohl(info));\n\t\tif (!dst)\n\t\t\tgoto out;\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst))\n\t\t\tdccp_sync_mss(sk, dst_mtu(dst));\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\t\t\t/*\n\t\t\t * Wake people up to see the error\n\t\t\t * (see connect in sock.c)\n\t\t\t */\n\t\t\tsk->sk_error_report(sk);\n\t\t\tdccp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\n\nstatic int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic void dccp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tdccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);\n\tkfree_skb(inet_rsk(req)->pktopts);\n}\n\nstatic void dccp_v6_ctl_send_reset(const struct sock *sk, struct sk_buff *rxskb)\n{\n\tconst struct ipv6hdr *rxip6h;\n\tstruct sk_buff *skb;\n\tstruct flowi6 fl6;\n\tstruct net *net = dev_net(skb_dst(rxskb)->dev);\n\tstruct sock *ctl_sk = net->dccp.v6_ctl_sk;\n\tstruct dst_entry *dst;\n\n\tif (dccp_hdr(rxskb)->dccph_type == DCCP_PKT_RESET)\n\t\treturn;\n\n\tif (!ipv6_unicast_destination(rxskb))\n\t\treturn;\n\n\tskb = dccp_ctl_make_reset(ctl_sk, rxskb);\n\tif (skb == NULL)\n\t\treturn;\n\n\trxip6h = ipv6_hdr(rxskb);\n\tdccp_hdr(skb)->dccph_checksum = dccp_v6_csum_finish(skb, &rxip6h->saddr,\n\t\t\t\t\t\t\t    &rxip6h->daddr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.daddr = rxip6h->saddr;\n\tfl6.saddr = rxip6h->daddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.flowi6_oif = inet6_iif(rxskb);\n\tfl6.fl6_dport = dccp_hdr(skb)->dccph_dport;\n\tfl6.fl6_sport = dccp_hdr(skb)->dccph_sport;\n\tsecurity_skb_classify_flow(rxskb, flowi6_to_flowi(&fl6));\n\n\t/* sk = NULL, but it is safe for now. RST socket required. */\n\tdst = ip6_dst_lookup_flow(ctl_sk, &fl6, NULL);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(skb, dst);\n\t\tip6_xmit(ctl_sk, skb, &fl6, NULL, 0);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTSEGS);\n\t\tDCCP_INC_STATS_BH(DCCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(skb);\n}\n\nstatic struct request_sock_ops dccp6_request_sock_ops = {\n\t.family\t\t= AF_INET6,\n\t.obj_size\t= sizeof(struct dccp6_request_sock),\n\t.rtx_syn_ack\t= dccp_v6_send_response,\n\t.send_ack\t= dccp_reqsk_send_ack,\n\t.destructor\t= dccp_v6_reqsk_destructor,\n\t.send_reset\t= dccp_v6_ctl_send_reset,\n\t.syn_ack_timeout = dccp_syn_ack_timeout,\n};\n\nstatic int dccp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct request_sock *req;\n\tstruct dccp_request_sock *dreq;\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tconst __be32 service = dccp_hdr_request(skb)->dccph_req_service;\n\tstruct dccp_skb_cb *dcb = DCCP_SKB_CB(skb);\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\treturn 0;\t/* discard, don't send a reset here */\n\n\tif (dccp_bad_service_code(sk, service)) {\n\t\tdcb->dccpd_reset_code = DCCP_RESET_CODE_BAD_SERVICE_CODE;\n\t\tgoto drop;\n\t}\n\t/*\n\t * There are no SYN attacks on IPv6, yet...\n\t */\n\tdcb->dccpd_reset_code = DCCP_RESET_CODE_TOO_BUSY;\n\tif (inet_csk_reqsk_queue_is_full(sk))\n\t\tgoto drop;\n\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)\n\t\tgoto drop;\n\n\treq = inet_reqsk_alloc(&dccp6_request_sock_ops, sk, true);\n\tif (req == NULL)\n\t\tgoto drop;\n\n\tif (dccp_reqsk_init(req, dccp_sk(sk), skb))\n\t\tgoto drop_and_free;\n\n\tdreq = dccp_rsk(req);\n\tif (dccp_parse_options(sk, dreq, skb))\n\t\tgoto drop_and_free;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tireq = inet_rsk(req);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tireq->ireq_family = AF_INET6;\n\n\tif (ipv6_opt_accepted(sk, skb, IP6CB(skb)) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = inet6_iif(skb);\n\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t *   Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookie\n\t *\n\t * Setting S.SWL/S.SWH to is deferred to dccp_create_openreq_child().\n\t */\n\tdreq->dreq_isr\t   = dcb->dccpd_seq;\n\tdreq->dreq_gsr     = dreq->dreq_isr;\n\tdreq->dreq_iss\t   = dccp_v6_init_sequence(skb);\n\tdreq->dreq_gss     = dreq->dreq_iss;\n\tdreq->dreq_service = service;\n\n\tif (dccp_v6_send_response(sk, req))\n\t\tgoto drop_and_free;\n\n\tinet_csk_reqsk_queue_hash_add(sk, req, DCCP_TIMEOUT_INIT);\n\treturn 0;\n\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\tDCCP_INC_STATS_BH(DCCP_MIB_ATTEMPTFAILS);\n\treturn -1;\n}\n\nstatic struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst,\n\t\t\t\t\t      struct request_sock *req_unhash,\n\t\t\t\t\t      bool *own_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t\t  req_unhash, own_req);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tstruct flowi6 fl6;\n\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_DCCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr\t= ireq->ir_v6_rmt_addr;\n\tnewnp->saddr\t\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr\t= ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if\t= ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\tdccp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\t/* Clone pktoptions received with SYN, if we own the req */\n\tif (*own_req && ireq->pktopts) {\n\t\tnewnp->pktoptions = skb_clone(ireq->pktopts, GFP_ATOMIC);\n\t\tconsume_skb(ireq->pktopts);\n\t\tireq->pktopts = NULL;\n\t\tif (newnp->pktoptions)\n\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nstatic int dccp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *opt_skb = NULL;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, dccp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   dccp_v6_hnd_req and dccp_v6_ctl_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_do_rcv(sk, skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard;\n\n\t/*\n\t * socket locking is here for SMP purposes as backlog rcv is currently\n\t * called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all)\n\t/*\n\t * FIXME: Add handling of IPV6_PKTOPTIONS skb. See the comments below\n\t *        (wrt ipv6_pktopions) and net/ipv6/tcp_ipv6.c for an example.\n\t */\n\t\topt_skb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (sk->sk_state == DCCP_OPEN) { /* Fast path */\n\t\tif (dccp_rcv_established(sk, skb, dccp_hdr(skb), skb->len))\n\t\t\tgoto reset;\n\t\tif (opt_skb) {\n\t\t\t/* XXX This is where we would goto ipv6_pktoptions. */\n\t\t\t__kfree_skb(opt_skb);\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/*\n\t *  Step 3: Process LISTEN state\n\t *     If S.state == LISTEN,\n\t *\t If P.type == Request or P contains a valid Init Cookie option,\n\t *\t      (* Must scan the packet's options to check for Init\n\t *\t\t Cookies.  Only Init Cookies are processed here,\n\t *\t\t however; other options are processed in Step 8.  This\n\t *\t\t scan need only be performed if the endpoint uses Init\n\t *\t\t Cookies *)\n\t *\t      (* Generate a new socket and switch to that socket *)\n\t *\t      Set S := new socket for this port pair\n\t *\t      S.state = RESPOND\n\t *\t      Choose S.ISS (initial seqno) or set from Init Cookies\n\t *\t      Initialize S.GAR := S.ISS\n\t *\t      Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookies\n\t *\t      Continue with S.state == RESPOND\n\t *\t      (* A Response packet will be generated in Step 11 *)\n\t *\t Otherwise,\n\t *\t      Generate Reset(No Connection) unless P.type == Reset\n\t *\t      Drop packet and return\n\t *\n\t * NOTE: the check for the packet types is done in\n\t *\t dccp_rcv_state_process\n\t */\n\n\tif (dccp_rcv_state_process(sk, skb, dccp_hdr(skb), skb->len))\n\t\tgoto reset;\n\tif (opt_skb) {\n\t\t/* XXX This is where we would goto ipv6_pktoptions. */\n\t\t__kfree_skb(opt_skb);\n\t}\n\treturn 0;\n\nreset:\n\tdccp_v6_ctl_send_reset(sk, skb);\ndiscard:\n\tif (opt_skb != NULL)\n\t\t__kfree_skb(opt_skb);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int dccp_v6_rcv(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tstruct sock *sk;\n\tint min_cov;\n\n\t/* Step 1: Check header basics */\n\n\tif (dccp_invalid_packet(skb))\n\t\tgoto discard_it;\n\n\t/* Step 1: If header checksum is incorrect, drop packet and return. */\n\tif (dccp_v6_csum_finish(skb, &ipv6_hdr(skb)->saddr,\n\t\t\t\t     &ipv6_hdr(skb)->daddr)) {\n\t\tDCCP_WARN(\"dropped packet with invalid checksum\\n\");\n\t\tgoto discard_it;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\tDCCP_SKB_CB(skb)->dccpd_seq  = dccp_hdr_seq(dh);\n\tDCCP_SKB_CB(skb)->dccpd_type = dh->dccph_type;\n\n\tif (dccp_packet_without_ack(skb))\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = DCCP_PKT_WITHOUT_ACK_SEQ;\n\telse\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = dccp_hdr_ack_seq(skb);\n\nlookup:\n\tsk = __inet6_lookup_skb(&dccp_hashinfo, skb,\n\t\t\t        dh->dccph_sport, dh->dccph_dport,\n\t\t\t\tinet6_iif(skb));\n\tif (!sk) {\n\t\tdccp_pr_debug(\"failed to look up flow ID in table and \"\n\t\t\t      \"get corresponding socket\\n\");\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * Step 2:\n\t *\t... or S.state == TIMEWAIT,\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tdccp_pr_debug(\"sk->sk_state == DCCP_TIME_WAIT: do_time_wait\\n\");\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto no_dccp_socket;\n\t}\n\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\t\tstruct sock *nsk = NULL;\n\n\t\tsk = req->rsk_listener;\n\t\tif (likely(sk->sk_state == DCCP_LISTEN)) {\n\t\t\tnsk = dccp_check_req(sk, skb, req);\n\t\t} else {\n\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\n\t\t\tgoto lookup;\n\t\t}\n\t\tif (!nsk) {\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_it;\n\t\t}\n\t\tif (nsk == sk) {\n\t\t\tsock_hold(sk);\n\t\t\treqsk_put(req);\n\t\t} else if (dccp_child_process(sk, nsk, skb)) {\n\t\t\tdccp_v6_ctl_send_reset(sk, skb);\n\t\t\tgoto discard_it;\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t}\n\t/*\n\t * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage\n\t *\to if MinCsCov = 0, only packets with CsCov = 0 are accepted\n\t *\to if MinCsCov > 0, also accept packets with CsCov >= MinCsCov\n\t */\n\tmin_cov = dccp_sk(sk)->dccps_pcrlen;\n\tif (dh->dccph_cscov  &&  (min_cov == 0 || dh->dccph_cscov < min_cov))  {\n\t\tdccp_pr_debug(\"Packet CsCov %d does not satisfy MinCsCov %d\\n\",\n\t\t\t      dh->dccph_cscov, min_cov);\n\t\t/* FIXME: send Data Dropped option (see also dccp_v4_rcv) */\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\treturn sk_receive_skb(sk, skb, 1) ? -1 : 0;\n\nno_dccp_socket:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (dh->dccph_type != DCCP_PKT_RESET) {\n\t\tDCCP_SKB_CB(skb)->dccpd_reset_code =\n\t\t\t\t\tDCCP_RESET_CODE_NO_CONNECTION;\n\t\tdccp_v6_ctl_send_reset(sk, skb);\n\t}\n\ndiscard_it:\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n}\n\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = dccp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n/*\n *\tDCCP over IPv4 via INET6 API\n */\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = dccp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int dccp_v6_init_sock(struct sock *sk)\n{\n\tstatic __u8 dccp_v6_ctl_sock_initialized;\n\tint err = dccp_init_sock(sk, dccp_v6_ctl_sock_initialized);\n\n\tif (err == 0) {\n\t\tif (unlikely(!dccp_v6_ctl_sock_initialized))\n\t\t\tdccp_v6_ctl_sock_initialized = 1;\n\t\tinet_csk(sk)->icsk_af_ops = &dccp_ipv6_af_ops;\n\t}\n\n\treturn err;\n}\n\nstatic void dccp_v6_destroy_sock(struct sock *sk)\n{\n\tdccp_destroy_sock(sk);\n\tinet6_destroy_sock(sk);\n}\n\nstatic struct timewait_sock_ops dccp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct dccp6_timewait_sock),\n};\n\nstatic struct proto dccp_v6_prot = {\n\t.name\t\t   = \"DCCPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = dccp_close,\n\t.connect\t   = dccp_v6_connect,\n\t.disconnect\t   = dccp_disconnect,\n\t.ioctl\t\t   = dccp_ioctl,\n\t.init\t\t   = dccp_v6_init_sock,\n\t.setsockopt\t   = dccp_setsockopt,\n\t.getsockopt\t   = dccp_getsockopt,\n\t.sendmsg\t   = dccp_sendmsg,\n\t.recvmsg\t   = dccp_recvmsg,\n\t.backlog_rcv\t   = dccp_v6_do_rcv,\n\t.hash\t\t   = inet_hash,\n\t.unhash\t\t   = inet_unhash,\n\t.accept\t\t   = inet_csk_accept,\n\t.get_port\t   = inet_csk_get_port,\n\t.shutdown\t   = dccp_shutdown,\n\t.destroy\t   = dccp_v6_destroy_sock,\n\t.orphan_count\t   = &dccp_orphan_count,\n\t.max_header\t   = MAX_DCCP_HEADER,\n\t.obj_size\t   = sizeof(struct dccp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.rsk_prot\t   = &dccp6_request_sock_ops,\n\t.twsk_prot\t   = &dccp6_timewait_sock_ops,\n\t.h.hashinfo\t   = &dccp_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_dccp_setsockopt,\n\t.compat_getsockopt = compat_dccp_getsockopt,\n#endif\n};\n\nstatic const struct inet6_protocol dccp_v6_protocol = {\n\t.handler\t= dccp_v6_rcv,\n\t.err_handler\t= dccp_v6_err,\n\t.flags\t\t= INET6_PROTO_NOPOLICY | INET6_PROTO_FINAL,\n};\n\nstatic const struct proto_ops inet6_dccp_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = dccp_poll,\n\t.ioctl\t\t   = inet6_ioctl,\n\t.listen\t\t   = inet_dccp_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw dccp_v6_protosw = {\n\t.type\t\t= SOCK_DCCP,\n\t.protocol\t= IPPROTO_DCCP,\n\t.prot\t\t= &dccp_v6_prot,\n\t.ops\t\t= &inet6_dccp_ops,\n\t.flags\t\t= INET_PROTOSW_ICSK,\n};\n\nstatic int __net_init dccp_v6_init_net(struct net *net)\n{\n\tif (dccp_hashinfo.bhash == NULL)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\treturn inet_ctl_sock_create(&net->dccp.v6_ctl_sk, PF_INET6,\n\t\t\t\t    SOCK_DCCP, IPPROTO_DCCP, net);\n}\n\nstatic void __net_exit dccp_v6_exit_net(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->dccp.v6_ctl_sk);\n}\n\nstatic struct pernet_operations dccp_v6_ops = {\n\t.init   = dccp_v6_init_net,\n\t.exit   = dccp_v6_exit_net,\n};\n\nstatic int __init dccp_v6_init(void)\n{\n\tint err = proto_register(&dccp_v6_prot, 1);\n\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet6_add_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tif (err != 0)\n\t\tgoto out_unregister_proto;\n\n\tinet6_register_protosw(&dccp_v6_protosw);\n\n\terr = register_pernet_subsys(&dccp_v6_ops);\n\tif (err != 0)\n\t\tgoto out_destroy_ctl_sock;\nout:\n\treturn err;\n\nout_destroy_ctl_sock:\n\tinet6_del_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tinet6_unregister_protosw(&dccp_v6_protosw);\nout_unregister_proto:\n\tproto_unregister(&dccp_v6_prot);\n\tgoto out;\n}\n\nstatic void __exit dccp_v6_exit(void)\n{\n\tunregister_pernet_subsys(&dccp_v6_ops);\n\tinet6_del_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tinet6_unregister_protosw(&dccp_v6_protosw);\n\tproto_unregister(&dccp_v6_prot);\n}\n\nmodule_init(dccp_v6_init);\nmodule_exit(dccp_v6_exit);\n\n/*\n * __stringify doesn't likes enums, so use SOCK_DCCP (6) and IPPROTO_DCCP (33)\n * values directly, Also cover the case where the protocol is not specified,\n * i.e. net-pf-PF_INET6-proto-0-type-SOCK_DCCP\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 33, 6);\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 0, 6);\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Arnaldo Carvalho de Melo <acme@mandriva.com>\");\nMODULE_DESCRIPTION(\"DCCPv6 - Datagram Congestion Controlled Protocol\");\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tDCCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tBased on net/dccp6/ipv6.c\n *\n *\tArnaldo Carvalho de Melo <acme@ghostprotocols.net>\n */\n\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/xfrm.h>\n#include <linux/string.h>\n\n#include <net/addrconf.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/inet_sock.h>\n#include <net/inet6_connection_sock.h>\n#include <net/inet6_hashtables.h>\n#include <net/ip6_route.h>\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n#include <net/secure_seq.h>\n#include <net/netns/generic.h>\n#include <net/sock.h>\n#include <net/rstreason.h>\n\n#include \"dccp.h\"\n#include \"ipv6.h\"\n#include \"feat.h\"\n\nstruct dccp_v6_pernet {\n\tstruct sock *v6_ctl_sk;\n};\n\nstatic unsigned int dccp_v6_pernet_id __read_mostly;\n\n/* The per-net v6_ctl_sk is used for sending RSTs and ACKs */\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped;\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops;\n\n/* add pseudo-header to DCCP checksum stored in skb->csum */\nstatic inline __sum16 dccp_v6_csum_finish(struct sk_buff *skb,\n\t\t\t\t      const struct in6_addr *saddr,\n\t\t\t\t      const struct in6_addr *daddr)\n{\n\treturn csum_ipv6_magic(saddr, daddr, skb->len, IPPROTO_DCCP, skb->csum);\n}\n\nstatic inline void dccp_v6_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\n\tdccp_csum_outgoing(skb);\n\tdh->dccph_checksum = dccp_v6_csum_finish(skb, &np->saddr, &sk->sk_v6_daddr);\n}\n\nstatic inline __u64 dccp_v6_init_sequence(struct sk_buff *skb)\n{\n\treturn secure_dccpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t     ipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_dport,\n\t\t\t\t\t     dccp_hdr(skb)->dccph_sport     );\n\n}\n\nstatic int dccp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr;\n\tconst struct dccp_hdr *dh;\n\tstruct dccp_sock *dp;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tint err;\n\t__u64 seq;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (!pskb_may_pull(skb, offset + sizeof(*dh)))\n\t\treturn -EINVAL;\n\tdh = (struct dccp_hdr *)(skb->data + offset);\n\tif (!pskb_may_pull(skb, offset + __dccp_basic_hdr_len(dh)))\n\t\treturn -EINVAL;\n\thdr = (const struct ipv6hdr *)skb->data;\n\tdh = (struct dccp_hdr *)(skb->data + offset);\n\n\tsk = __inet6_lookup_established(net, &dccp_hashinfo,\n\t\t\t\t\t&hdr->daddr, dh->dccph_dport,\n\t\t\t\t\t&hdr->saddr, ntohs(dh->dccph_sport),\n\t\t\t\t\tinet6_iif(skb), 0);\n\n\tif (!sk) {\n\t\t__ICMP6_INC_STATS(net, __in6_dev_get(skb->dev),\n\t\t\t\t  ICMP6_MIB_INERRORS);\n\t\treturn -ENOENT;\n\t}\n\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn 0;\n\t}\n\tseq = dccp_hdr_seq(dh);\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV) {\n\t\tdccp_req_err(sk, seq);\n\t\treturn 0;\n\t}\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk))\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == DCCP_CLOSED)\n\t\tgoto out;\n\n\tdp = dccp_sk(sk);\n\tif ((1 << sk->sk_state) & ~(DCCPF_REQUESTING | DCCPF_LISTEN) &&\n\t    !between48(seq, dp->dccps_awl, dp->dccps_awh)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\t\tif (dst)\n\t\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tstruct dst_entry *dst = NULL;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\tif (sock_owned_by_user(sk))\n\t\t\tgoto out;\n\t\tif ((1 << sk->sk_state) & (DCCPF_LISTEN | DCCPF_CLOSED))\n\t\t\tgoto out;\n\n\t\tdst = inet6_csk_update_pmtu(sk, ntohl(info));\n\t\tif (!dst)\n\t\t\tgoto out;\n\n\t\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst))\n\t\t\tdccp_sync_mss(sk, dst_mtu(dst));\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase DCCP_REQUESTING:\n\tcase DCCP_RESPOND:  /* Cannot happen.\n\t\t\t       It can, it SYNs are crossed. --ANK */\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\t__DCCP_INC_STATS(DCCP_MIB_ATTEMPTFAILS);\n\t\t\tsk->sk_err = err;\n\t\t\t/*\n\t\t\t * Wake people up to see the error\n\t\t\t * (see connect in sock.c)\n\t\t\t */\n\t\t\tsk_error_report(sk);\n\t\t\tdccp_done(sk);\n\t\t} else {\n\t\t\tWRITE_ONCE(sk->sk_err_soft, err);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && inet6_test_bit(RECVERR6, sk)) {\n\t\tsk->sk_err = err;\n\t\tsk_error_report(sk);\n\t} else {\n\t\tWRITE_ONCE(sk->sk_err_soft, err);\n\t}\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\treturn 0;\n}\n\n\nstatic int dccp_v6_send_response(const struct sock *sk, struct request_sock *req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct in6_addr *final_p, final;\n\tstruct flowi6 fl6;\n\tint err = -1;\n\tstruct dst_entry *dst;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\tfl6.saddr = ireq->ir_v6_loc_addr;\n\tfl6.flowlabel = 0;\n\tfl6.flowi6_oif = ireq->ir_iif;\n\tfl6.fl6_dport = ireq->ir_rmt_port;\n\tfl6.fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi_common(&fl6));\n\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto done;\n\t}\n\n\tskb = dccp_make_response(sk, dst, req);\n\tif (skb != NULL) {\n\t\tstruct dccp_hdr *dh = dccp_hdr(skb);\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tdh->dccph_checksum = dccp_v6_csum_finish(skb,\n\t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n\t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\trcu_read_lock();\n\t\topt = ireq->ipv6_opt;\n\t\tif (!opt)\n\t\t\topt = rcu_dereference(np->opt);\n\t\terr = ip6_xmit(sk, skb, &fl6, READ_ONCE(sk->sk_mark), opt,\n\t\t\t       np->tclass, READ_ONCE(sk->sk_priority));\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\tdst_release(dst);\n\treturn err;\n}\n\nstatic void dccp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tdccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);\n\tkfree(inet_rsk(req)->ipv6_opt);\n\tkfree_skb(inet_rsk(req)->pktopts);\n}\n\nstatic void dccp_v6_ctl_send_reset(const struct sock *sk, struct sk_buff *rxskb,\n\t\t\t\t   enum sk_rst_reason reason)\n{\n\tconst struct ipv6hdr *rxip6h;\n\tstruct sk_buff *skb;\n\tstruct flowi6 fl6;\n\tstruct net *net = dev_net(skb_dst(rxskb)->dev);\n\tstruct dccp_v6_pernet *pn;\n\tstruct sock *ctl_sk;\n\tstruct dst_entry *dst;\n\n\tif (dccp_hdr(rxskb)->dccph_type == DCCP_PKT_RESET)\n\t\treturn;\n\n\tif (!ipv6_unicast_destination(rxskb))\n\t\treturn;\n\n\tpn = net_generic(net, dccp_v6_pernet_id);\n\tctl_sk = pn->v6_ctl_sk;\n\tskb = dccp_ctl_make_reset(ctl_sk, rxskb);\n\tif (skb == NULL)\n\t\treturn;\n\n\trxip6h = ipv6_hdr(rxskb);\n\tdccp_hdr(skb)->dccph_checksum = dccp_v6_csum_finish(skb, &rxip6h->saddr,\n\t\t\t\t\t\t\t    &rxip6h->daddr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.daddr = rxip6h->saddr;\n\tfl6.saddr = rxip6h->daddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.flowi6_oif = inet6_iif(rxskb);\n\tfl6.fl6_dport = dccp_hdr(skb)->dccph_dport;\n\tfl6.fl6_sport = dccp_hdr(skb)->dccph_sport;\n\tsecurity_skb_classify_flow(rxskb, flowi6_to_flowi_common(&fl6));\n\n\t/* sk = NULL, but it is safe for now. RST socket required. */\n\tdst = ip6_dst_lookup_flow(sock_net(ctl_sk), ctl_sk, &fl6, NULL);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(skb, dst);\n\t\tip6_xmit(ctl_sk, skb, &fl6, 0, NULL, 0, 0);\n\t\tDCCP_INC_STATS(DCCP_MIB_OUTSEGS);\n\t\tDCCP_INC_STATS(DCCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(skb);\n}\n\nstatic struct request_sock_ops dccp6_request_sock_ops = {\n\t.family\t\t= AF_INET6,\n\t.obj_size\t= sizeof(struct dccp6_request_sock),\n\t.rtx_syn_ack\t= dccp_v6_send_response,\n\t.send_ack\t= dccp_reqsk_send_ack,\n\t.destructor\t= dccp_v6_reqsk_destructor,\n\t.send_reset\t= dccp_v6_ctl_send_reset,\n\t.syn_ack_timeout = dccp_syn_ack_timeout,\n};\n\nstatic int dccp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct request_sock *req;\n\tstruct dccp_request_sock *dreq;\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tconst __be32 service = dccp_hdr_request(skb)->dccph_req_service;\n\tstruct dccp_skb_cb *dcb = DCCP_SKB_CB(skb);\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\treturn 0;\t/* discard, don't send a reset here */\n\n\tif (ipv6_addr_v4mapped(&ipv6_hdr(skb)->saddr)) {\n\t\t__IP6_INC_STATS(sock_net(sk), NULL, IPSTATS_MIB_INHDRERRORS);\n\t\treturn 0;\n\t}\n\n\tif (dccp_bad_service_code(sk, service)) {\n\t\tdcb->dccpd_reset_code = DCCP_RESET_CODE_BAD_SERVICE_CODE;\n\t\tgoto drop;\n\t}\n\t/*\n\t * There are no SYN attacks on IPv6, yet...\n\t */\n\tdcb->dccpd_reset_code = DCCP_RESET_CODE_TOO_BUSY;\n\tif (inet_csk_reqsk_queue_is_full(sk))\n\t\tgoto drop;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto drop;\n\n\treq = inet_reqsk_alloc(&dccp6_request_sock_ops, sk, true);\n\tif (req == NULL)\n\t\tgoto drop;\n\n\tif (dccp_reqsk_init(req, dccp_sk(sk), skb))\n\t\tgoto drop_and_free;\n\n\tdreq = dccp_rsk(req);\n\tif (dccp_parse_options(sk, dreq, skb))\n\t\tgoto drop_and_free;\n\n\tireq = inet_rsk(req);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tireq->ir_rmt_addr = LOOPBACK4_IPV6;\n\tireq->ir_loc_addr = LOOPBACK4_IPV6;\n\n\tireq->ireq_family = AF_INET6;\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tif (ipv6_opt_accepted(sk, skb, IP6CB(skb)) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\trefcount_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\tireq->ir_iif = READ_ONCE(sk->sk_bound_dev_if);\n\n\t/* So that link locals have meaning */\n\tif (!ireq->ir_iif &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = inet6_iif(skb);\n\n\t/*\n\t * Step 3: Process LISTEN state\n\t *\n\t *   Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookie\n\t *\n\t * Setting S.SWL/S.SWH to is deferred to dccp_create_openreq_child().\n\t */\n\tdreq->dreq_isr\t   = dcb->dccpd_seq;\n\tdreq->dreq_gsr     = dreq->dreq_isr;\n\tdreq->dreq_iss\t   = dccp_v6_init_sequence(skb);\n\tdreq->dreq_gss     = dreq->dreq_iss;\n\tdreq->dreq_service = service;\n\n\tif (dccp_v6_send_response(sk, req))\n\t\tgoto drop_and_free;\n\n\tif (unlikely(!inet_csk_reqsk_queue_hash_add(sk, req, DCCP_TIMEOUT_INIT)))\n\t\treqsk_free(req);\n\telse\n\t\treqsk_put(req);\n\n\treturn 0;\n\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\t__DCCP_INC_STATS(DCCP_MIB_ATTEMPTFAILS);\n\treturn -1;\n}\n\nstatic struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct request_sock *req,\n\t\t\t\t\t      struct dst_entry *dst,\n\t\t\t\t\t      struct request_sock *req_unhash,\n\t\t\t\t\t      bool *own_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct inet_sock *newinet;\n\tstruct dccp6_sock *newdp6;\n\tstruct sock *newsk;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\t\tnewsk = dccp_v4_request_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t\t  req_unhash, own_req);\n\t\tif (newsk == NULL)\n\t\t\treturn NULL;\n\n\t\tnewdp6 = (struct dccp6_sock *)newsk;\n\t\tnewinet = inet_sk(newsk);\n\t\tnewinet->pinet6 = &newdp6->inet6;\n\t\tnewnp = inet6_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->ipv6_mc_list = NULL;\n\t\tnewnp->ipv6_ac_list = NULL;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->mcast_oif   = inet_iif(skb);\n\t\tnewnp->mcast_hops  = ip_hdr(skb)->ttl;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, dccp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\tdccp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tstruct flowi6 fl6;\n\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_DCCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = dccp_create_openreq_child(sk, req, skb);\n\tif (newsk == NULL)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, dccp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tip6_dst_store(newsk, dst, NULL, NULL);\n\tnewsk->sk_route_caps = dst->dev->features & ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t\t      NETIF_F_TSO);\n\tnewdp6 = (struct dccp6_sock *)newsk;\n\tnewinet = inet_sk(newsk);\n\tnewinet->pinet6 = &newdp6->inet6;\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewnp->saddr\t\t= ireq->ir_v6_loc_addr;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->ipv6_mc_list = NULL;\n\tnewnp->ipv6_ac_list = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = inet6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\n\t/*\n\t * Clone native IPv6 options from listening socket (if any)\n\t *\n\t * Yes, keeping reference count would be much more clever, but we make\n\t * one more one thing there: reattach optmem to newsk.\n\t */\n\topt = ireq->ipv6_opt;\n\tif (!opt)\n\t\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\tdccp_sync_mss(newsk, dst_mtu(dst));\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\tdccp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash), NULL);\n\t/* Clone pktoptions received with SYN, if we own the req */\n\tif (*own_req && ireq->pktopts) {\n\t\tnewnp->pktoptions = skb_clone_and_charge_r(ireq->pktopts, newsk);\n\t\tconsume_skb(ireq->pktopts);\n\t\tireq->pktopts = NULL;\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\n\n/* The socket must have it's spinlock held when we get\n * here.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nstatic int dccp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *opt_skb = NULL;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, dccp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   dccp_v6_hnd_req and dccp_v6_ctl_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn dccp_v4_do_rcv(sk, skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard;\n\n\t/*\n\t * socket locking is here for SMP purposes as backlog rcv is currently\n\t * called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all && sk->sk_state != DCCP_LISTEN)\n\t\topt_skb = skb_clone_and_charge_r(skb, sk);\n\n\tif (sk->sk_state == DCCP_OPEN) { /* Fast path */\n\t\tif (dccp_rcv_established(sk, skb, dccp_hdr(skb), skb->len))\n\t\t\tgoto reset;\n\t\tif (opt_skb)\n\t\t\tgoto ipv6_pktoptions;\n\t\treturn 0;\n\t}\n\n\t/*\n\t *  Step 3: Process LISTEN state\n\t *     If S.state == LISTEN,\n\t *\t If P.type == Request or P contains a valid Init Cookie option,\n\t *\t      (* Must scan the packet's options to check for Init\n\t *\t\t Cookies.  Only Init Cookies are processed here,\n\t *\t\t however; other options are processed in Step 8.  This\n\t *\t\t scan need only be performed if the endpoint uses Init\n\t *\t\t Cookies *)\n\t *\t      (* Generate a new socket and switch to that socket *)\n\t *\t      Set S := new socket for this port pair\n\t *\t      S.state = RESPOND\n\t *\t      Choose S.ISS (initial seqno) or set from Init Cookies\n\t *\t      Initialize S.GAR := S.ISS\n\t *\t      Set S.ISR, S.GSR, S.SWL, S.SWH from packet or Init Cookies\n\t *\t      Continue with S.state == RESPOND\n\t *\t      (* A Response packet will be generated in Step 11 *)\n\t *\t Otherwise,\n\t *\t      Generate Reset(No Connection) unless P.type == Reset\n\t *\t      Drop packet and return\n\t *\n\t * NOTE: the check for the packet types is done in\n\t *\t dccp_rcv_state_process\n\t */\n\n\tif (dccp_rcv_state_process(sk, skb, dccp_hdr(skb), skb->len))\n\t\tgoto reset;\n\tif (opt_skb)\n\t\tgoto ipv6_pktoptions;\n\treturn 0;\n\nreset:\n\tdccp_v6_ctl_send_reset(sk, skb, SK_RST_REASON_NOT_SPECIFIED);\ndiscard:\n\tif (opt_skb != NULL)\n\t\t__kfree_skb(opt_skb);\n\tkfree_skb(skb);\n\treturn 0;\n\n/* Handling IPV6_PKTOPTIONS skb the similar\n * way it's done for net/ipv6/tcp_ipv6.c\n */\nipv6_pktoptions:\n\tif (!((1 << sk->sk_state) & (DCCPF_CLOSED | DCCPF_LISTEN))) {\n\t\tif (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo)\n\t\t\tWRITE_ONCE(np->mcast_oif, inet6_iif(opt_skb));\n\t\tif (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim)\n\t\t\tWRITE_ONCE(np->mcast_hops, ipv6_hdr(opt_skb)->hop_limit);\n\t\tif (np->rxopt.bits.rxflow || np->rxopt.bits.rxtclass)\n\t\t\tnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(opt_skb));\n\t\tif (inet6_test_bit(REPFLOW, sk))\n\t\t\tnp->flow_label = ip6_flowlabel(ipv6_hdr(opt_skb));\n\t\tif (ipv6_opt_accepted(sk, opt_skb,\n\t\t\t\t      &DCCP_SKB_CB(opt_skb)->header.h6)) {\n\t\t\tmemmove(IP6CB(opt_skb),\n\t\t\t\t&DCCP_SKB_CB(opt_skb)->header.h6,\n\t\t\t\tsizeof(struct inet6_skb_parm));\n\t\t\topt_skb = xchg(&np->pktoptions, opt_skb);\n\t\t} else {\n\t\t\t__kfree_skb(opt_skb);\n\t\t\topt_skb = xchg(&np->pktoptions, NULL);\n\t\t}\n\t}\n\n\tkfree_skb(opt_skb);\n\treturn 0;\n}\n\nstatic int dccp_v6_rcv(struct sk_buff *skb)\n{\n\tconst struct dccp_hdr *dh;\n\tbool refcounted;\n\tstruct sock *sk;\n\tint min_cov;\n\n\t/* Step 1: Check header basics */\n\n\tif (dccp_invalid_packet(skb))\n\t\tgoto discard_it;\n\n\t/* Step 1: If header checksum is incorrect, drop packet and return. */\n\tif (dccp_v6_csum_finish(skb, &ipv6_hdr(skb)->saddr,\n\t\t\t\t     &ipv6_hdr(skb)->daddr)) {\n\t\tDCCP_WARN(\"dropped packet with invalid checksum\\n\");\n\t\tgoto discard_it;\n\t}\n\n\tdh = dccp_hdr(skb);\n\n\tDCCP_SKB_CB(skb)->dccpd_seq  = dccp_hdr_seq(dh);\n\tDCCP_SKB_CB(skb)->dccpd_type = dh->dccph_type;\n\n\tif (dccp_packet_without_ack(skb))\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = DCCP_PKT_WITHOUT_ACK_SEQ;\n\telse\n\t\tDCCP_SKB_CB(skb)->dccpd_ack_seq = dccp_hdr_ack_seq(skb);\n\nlookup:\n\tsk = __inet6_lookup_skb(&dccp_hashinfo, skb, __dccp_hdr_len(dh),\n\t\t\t        dh->dccph_sport, dh->dccph_dport,\n\t\t\t\tinet6_iif(skb), 0, &refcounted);\n\tif (!sk) {\n\t\tdccp_pr_debug(\"failed to look up flow ID in table and \"\n\t\t\t      \"get corresponding socket\\n\");\n\t\tgoto no_dccp_socket;\n\t}\n\n\t/*\n\t * Step 2:\n\t *\t... or S.state == TIMEWAIT,\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (sk->sk_state == DCCP_TIME_WAIT) {\n\t\tdccp_pr_debug(\"sk->sk_state == DCCP_TIME_WAIT: do_time_wait\\n\");\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto no_dccp_socket;\n\t}\n\n\tif (sk->sk_state == DCCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\t\tstruct sock *nsk;\n\n\t\tsk = req->rsk_listener;\n\t\tif (unlikely(sk->sk_state != DCCP_LISTEN)) {\n\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\n\t\t\tgoto lookup;\n\t\t}\n\t\tsock_hold(sk);\n\t\trefcounted = true;\n\t\tnsk = dccp_check_req(sk, skb, req);\n\t\tif (!nsk) {\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_and_relse;\n\t\t}\n\t\tif (nsk == sk) {\n\t\t\treqsk_put(req);\n\t\t} else if (dccp_child_process(sk, nsk, skb)) {\n\t\t\tdccp_v6_ctl_send_reset(sk, skb, SK_RST_REASON_NOT_SPECIFIED);\n\t\t\tgoto discard_and_relse;\n\t\t} else {\n\t\t\tsock_put(sk);\n\t\t\treturn 0;\n\t\t}\n\t}\n\t/*\n\t * RFC 4340, sec. 9.2.1: Minimum Checksum Coverage\n\t *\to if MinCsCov = 0, only packets with CsCov = 0 are accepted\n\t *\to if MinCsCov > 0, also accept packets with CsCov >= MinCsCov\n\t */\n\tmin_cov = dccp_sk(sk)->dccps_pcrlen;\n\tif (dh->dccph_cscov  &&  (min_cov == 0 || dh->dccph_cscov < min_cov))  {\n\t\tdccp_pr_debug(\"Packet CsCov %d does not satisfy MinCsCov %d\\n\",\n\t\t\t      dh->dccph_cscov, min_cov);\n\t\t/* FIXME: send Data Dropped option (see also dccp_v4_rcv) */\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\tnf_reset_ct(skb);\n\n\treturn __sk_receive_skb(sk, skb, 1, dh->dccph_doff * 4,\n\t\t\t\trefcounted) ? -1 : 0;\n\nno_dccp_socket:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\t/*\n\t * Step 2:\n\t *\tIf no socket ...\n\t *\t\tGenerate Reset(No Connection) unless P.type == Reset\n\t *\t\tDrop packet and return\n\t */\n\tif (dh->dccph_type != DCCP_PKT_RESET) {\n\t\tDCCP_SKB_CB(skb)->dccpd_reset_code =\n\t\t\t\t\tDCCP_RESET_CODE_NO_CONNECTION;\n\t\tdccp_v6_ctl_send_reset(sk, skb, SK_RST_REASON_NOT_SPECIFIED);\n\t}\n\ndiscard_it:\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tif (refcounted)\n\t\tsock_put(sk);\n\tgoto discard_it;\n}\n\nstatic int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dccp_sock *dp = dccp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tdp->dccps_role = DCCP_ROLE_CLIENT;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (inet6_test_bit(SNDFLOW, sk)) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\t/*\n\t * connect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t * DCCP over IPv4\n\t */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tnet_dbg_ratelimited(\"connect: ipv4 mapped\\n\");\n\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &dccp_ipv6_mapped;\n\t\tsk->sk_backlog_rcv = dccp_v4_do_rcv;\n\n\t\terr = dccp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\t\tsk->sk_backlog_rcv = dccp_v6_do_rcv;\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_DCCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\topt = rcu_dereference_protected(np->opt, lockdep_sock_is_held(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (saddr == NULL) {\n\t\tsaddr = &fl6.saddr;\n\n\t\terr = inet_bhash2_update_saddr(sk, saddr, AF_INET6);\n\t\tif (err)\n\t\t\tgoto failure;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tip6_dst_store(sk, dst, NULL, NULL);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\tdccp_set_state(sk, DCCP_REQUESTING);\n\terr = inet6_hash_connect(&dccp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tdp->dccps_iss = secure_dccpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t      sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t      inet->inet_sport,\n\t\t\t\t\t\t      inet->inet_dport);\n\terr = dccp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\tdccp_set_state(sk, DCCP_CLOSED);\n\tinet_bhash2_reset_saddr(sk);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_af_ops = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = dccp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n};\n\n/*\n *\tDCCP over IPv4 via INET6 API\n */\nstatic const struct inet_connection_sock_af_ops dccp_ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = dccp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.conn_request\t   = dccp_v6_conn_request,\n\t.syn_recv_sock\t   = dccp_v6_request_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n};\n\nstatic void dccp_v6_sk_destruct(struct sock *sk)\n{\n\tdccp_destruct_common(sk);\n\tinet6_sock_destruct(sk);\n}\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int dccp_v6_init_sock(struct sock *sk)\n{\n\tstatic __u8 dccp_v6_ctl_sock_initialized;\n\tint err = dccp_init_sock(sk, dccp_v6_ctl_sock_initialized);\n\n\tif (err == 0) {\n\t\tif (unlikely(!dccp_v6_ctl_sock_initialized))\n\t\t\tdccp_v6_ctl_sock_initialized = 1;\n\t\tinet_csk(sk)->icsk_af_ops = &dccp_ipv6_af_ops;\n\t\tsk->sk_destruct = dccp_v6_sk_destruct;\n\t}\n\n\treturn err;\n}\n\nstatic struct timewait_sock_ops dccp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct dccp6_timewait_sock),\n};\n\nstatic struct proto dccp_v6_prot = {\n\t.name\t\t   = \"DCCPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = dccp_close,\n\t.connect\t   = dccp_v6_connect,\n\t.disconnect\t   = dccp_disconnect,\n\t.ioctl\t\t   = dccp_ioctl,\n\t.init\t\t   = dccp_v6_init_sock,\n\t.setsockopt\t   = dccp_setsockopt,\n\t.getsockopt\t   = dccp_getsockopt,\n\t.sendmsg\t   = dccp_sendmsg,\n\t.recvmsg\t   = dccp_recvmsg,\n\t.backlog_rcv\t   = dccp_v6_do_rcv,\n\t.hash\t\t   = inet6_hash,\n\t.unhash\t\t   = inet_unhash,\n\t.accept\t\t   = inet_csk_accept,\n\t.get_port\t   = inet_csk_get_port,\n\t.shutdown\t   = dccp_shutdown,\n\t.destroy\t   = dccp_destroy_sock,\n\t.orphan_count\t   = &dccp_orphan_count,\n\t.max_header\t   = MAX_DCCP_HEADER,\n\t.obj_size\t   = sizeof(struct dccp6_sock),\n\t.ipv6_pinfo_offset = offsetof(struct dccp6_sock, inet6),\n\t.slab_flags\t   = SLAB_TYPESAFE_BY_RCU,\n\t.rsk_prot\t   = &dccp6_request_sock_ops,\n\t.twsk_prot\t   = &dccp6_timewait_sock_ops,\n\t.h.hashinfo\t   = &dccp_hashinfo,\n};\n\nstatic const struct inet6_protocol dccp_v6_protocol = {\n\t.handler\t= dccp_v6_rcv,\n\t.err_handler\t= dccp_v6_err,\n\t.flags\t\t= INET6_PROTO_NOPOLICY | INET6_PROTO_FINAL,\n};\n\nstatic const struct proto_ops inet6_dccp_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_stream_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = inet_accept,\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = dccp_poll,\n\t.ioctl\t\t   = inet6_ioctl,\n\t.gettstamp\t   = sock_gettstamp,\n\t.listen\t\t   = inet_dccp_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = inet6_compat_ioctl,\n#endif\n};\n\nstatic struct inet_protosw dccp_v6_protosw = {\n\t.type\t\t= SOCK_DCCP,\n\t.protocol\t= IPPROTO_DCCP,\n\t.prot\t\t= &dccp_v6_prot,\n\t.ops\t\t= &inet6_dccp_ops,\n\t.flags\t\t= INET_PROTOSW_ICSK,\n};\n\nstatic int __net_init dccp_v6_init_net(struct net *net)\n{\n\tstruct dccp_v6_pernet *pn = net_generic(net, dccp_v6_pernet_id);\n\n\tif (dccp_hashinfo.bhash == NULL)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\treturn inet_ctl_sock_create(&pn->v6_ctl_sk, PF_INET6,\n\t\t\t\t    SOCK_DCCP, IPPROTO_DCCP, net);\n}\n\nstatic void __net_exit dccp_v6_exit_net(struct net *net)\n{\n\tstruct dccp_v6_pernet *pn = net_generic(net, dccp_v6_pernet_id);\n\n\tinet_ctl_sock_destroy(pn->v6_ctl_sk);\n}\n\nstatic struct pernet_operations dccp_v6_ops = {\n\t.init   = dccp_v6_init_net,\n\t.exit   = dccp_v6_exit_net,\n\t.id\t= &dccp_v6_pernet_id,\n\t.size   = sizeof(struct dccp_v6_pernet),\n};\n\nstatic int __init dccp_v6_init(void)\n{\n\tint err = proto_register(&dccp_v6_prot, 1);\n\n\tif (err)\n\t\tgoto out;\n\n\tinet6_register_protosw(&dccp_v6_protosw);\n\n\terr = register_pernet_subsys(&dccp_v6_ops);\n\tif (err)\n\t\tgoto out_destroy_ctl_sock;\n\n\terr = inet6_add_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tif (err)\n\t\tgoto out_unregister_proto;\n\nout:\n\treturn err;\nout_unregister_proto:\n\tunregister_pernet_subsys(&dccp_v6_ops);\nout_destroy_ctl_sock:\n\tinet6_unregister_protosw(&dccp_v6_protosw);\n\tproto_unregister(&dccp_v6_prot);\n\tgoto out;\n}\n\nstatic void __exit dccp_v6_exit(void)\n{\n\tinet6_del_protocol(&dccp_v6_protocol, IPPROTO_DCCP);\n\tunregister_pernet_subsys(&dccp_v6_ops);\n\tinet6_unregister_protosw(&dccp_v6_protosw);\n\tproto_unregister(&dccp_v6_prot);\n}\n\nmodule_init(dccp_v6_init);\nmodule_exit(dccp_v6_exit);\n\n/*\n * __stringify doesn't likes enums, so use SOCK_DCCP (6) and IPPROTO_DCCP (33)\n * values directly, Also cover the case where the protocol is not specified,\n * i.e. net-pf-PF_INET6-proto-0-type-SOCK_DCCP\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 33, 6);\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 0, 6);\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Arnaldo Carvalho de Melo <acme@mandriva.com>\");\nMODULE_DESCRIPTION(\"DCCPv6 - Datagram Congestion Controlled Protocol\");\n", "patch": "@@ -202,7 +202,9 @@ static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req\n \tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n \n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -219,7 +221,10 @@ static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req\n \t\t\t\t\t\t\t &ireq->ir_v6_loc_addr,\n \t\t\t\t\t\t\t &ireq->ir_v6_rmt_addr);\n \t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n-\t\terr = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n+\t\trcu_read_lock();\n+\t\terr = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n+\t\t\t       np->tclass);\n+\t\trcu_read_unlock();\n \t\terr = net_xmit_eval(err);\n \t}\n \n@@ -387,6 +392,7 @@ static struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n \tstruct inet_request_sock *ireq = inet_rsk(req);\n \tstruct ipv6_pinfo *newnp;\n \tconst struct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt;\n \tstruct inet_sock *newinet;\n \tstruct dccp6_sock *newdp6;\n \tstruct sock *newsk;\n@@ -488,13 +494,15 @@ static struct sock *dccp_v6_request_recv_sock(const struct sock *sk,\n \t * Yes, keeping reference count would be much more clever, but we make\n \t * one more one thing there: reattach optmem to newsk.\n \t */\n-\tif (np->opt != NULL)\n-\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n-\n+\topt = rcu_dereference(np->opt);\n+\tif (opt) {\n+\t\topt = ipv6_dup_options(newsk, opt);\n+\t\tRCU_INIT_POINTER(newnp->opt, opt);\n+\t}\n \tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n-\tif (newnp->opt != NULL)\n-\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n-\t\t\t\t\t\t     newnp->opt->opt_flen);\n+\tif (opt)\n+\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n+\t\t\t\t\t\t    opt->opt_flen;\n \n \tdccp_sync_mss(newsk, dst_mtu(dst));\n \n@@ -757,6 +765,7 @@ static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct dccp_sock *dp = dccp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -856,7 +865,8 @@ static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n \tfl6.fl6_sport = inet->inet_sport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \tif (IS_ERR(dst)) {\n@@ -876,9 +886,8 @@ static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n \t__ip6_dst_store(sk, dst, NULL, NULL);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt != NULL)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n \n \tinet->inet_dport = usin->sin6_port;\n ", "file_path": "files/2016_8\\84", "file_language": "c", "file_name": "net/dccp/ipv6.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/af_inet6.c", "code": "/*\n *\tPF_INET6 socket protocol family\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tAdapted from linux/net/ipv4/af_inet.c\n *\n *\tFixes:\n *\tpiggy, Karl Knutson\t:\tSocket protocol table\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tArnaldo Melo\t\t:\tcheck proc_net_create return, cleanups\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) \"IPv6: \" fmt\n\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/fcntl.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/proc_fs.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/icmpv6.h>\n#include <linux/netfilter_ipv6.h>\n\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <net/tcp.h>\n#include <net/ping.h>\n#include <net/protocol.h>\n#include <net/inet_common.h>\n#include <net/route.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/ndisc.h>\n#ifdef CONFIG_IPV6_TUNNEL\n#include <net/ip6_tunnel.h>\n#endif\n\n#include <asm/uaccess.h>\n#include <linux/mroute6.h>\n\nMODULE_AUTHOR(\"Cast of dozens\");\nMODULE_DESCRIPTION(\"IPv6 protocol stack for Linux\");\nMODULE_LICENSE(\"GPL\");\n\n/* The inetsw6 table contains everything that inet6_create needs to\n * build a new socket.\n */\nstatic struct list_head inetsw6[SOCK_MAX];\nstatic DEFINE_SPINLOCK(inetsw6_lock);\n\nstruct ipv6_params ipv6_defaults = {\n\t.disable_ipv6 = 0,\n\t.autoconf = 1,\n};\n\nstatic int disable_ipv6_mod;\n\nmodule_param_named(disable, disable_ipv6_mod, int, 0444);\nMODULE_PARM_DESC(disable, \"Disable IPv6 module such that it is non-functional\");\n\nmodule_param_named(disable_ipv6, ipv6_defaults.disable_ipv6, int, 0444);\nMODULE_PARM_DESC(disable_ipv6, \"Disable IPv6 on all interfaces\");\n\nmodule_param_named(autoconf, ipv6_defaults.autoconf, int, 0444);\nMODULE_PARM_DESC(autoconf, \"Enable IPv6 address autoconfiguration on all interfaces\");\n\nstatic __inline__ struct ipv6_pinfo *inet6_sk_generic(struct sock *sk)\n{\n\tconst int offset = sk->sk_prot->obj_size - sizeof(struct ipv6_pinfo);\n\n\treturn (struct ipv6_pinfo *)(((u8 *)sk) + offset);\n}\n\nstatic int inet6_create(struct net *net, struct socket *sock, int protocol,\n\t\t\tint kern)\n{\n\tstruct inet_sock *inet;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tstruct inet_protosw *answer;\n\tstruct proto *answer_prot;\n\tunsigned char answer_flags;\n\tint try_loading_module = 0;\n\tint err;\n\n\t/* Look for the requested type/protocol pair. */\nlookup_protocol:\n\terr = -ESOCKTNOSUPPORT;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(answer, &inetsw6[sock->type], list) {\n\n\t\terr = 0;\n\t\t/* Check the non-wild match. */\n\t\tif (protocol == answer->protocol) {\n\t\t\tif (protocol != IPPROTO_IP)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* Check for the two wild cases. */\n\t\t\tif (IPPROTO_IP == protocol) {\n\t\t\t\tprotocol = answer->protocol;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IPPROTO_IP == answer->protocol)\n\t\t\t\tbreak;\n\t\t}\n\t\terr = -EPROTONOSUPPORT;\n\t}\n\n\tif (err) {\n\t\tif (try_loading_module < 2) {\n\t\t\trcu_read_unlock();\n\t\t\t/*\n\t\t\t * Be more specific, e.g. net-pf-10-proto-132-type-1\n\t\t\t * (net-pf-PF_INET6-proto-IPPROTO_SCTP-type-SOCK_STREAM)\n\t\t\t */\n\t\t\tif (++try_loading_module == 1)\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d-type-%d\",\n\t\t\t\t\t\tPF_INET6, protocol, sock->type);\n\t\t\t/*\n\t\t\t * Fall back to generic, e.g. net-pf-10-proto-132\n\t\t\t * (net-pf-PF_INET6-proto-IPPROTO_SCTP)\n\t\t\t */\n\t\t\telse\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d\",\n\t\t\t\t\t\tPF_INET6, protocol);\n\t\t\tgoto lookup_protocol;\n\t\t} else\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\terr = -EPERM;\n\tif (sock->type == SOCK_RAW && !kern &&\n\t    !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\tgoto out_rcu_unlock;\n\n\tsock->ops = answer->ops;\n\tanswer_prot = answer->prot;\n\tanswer_flags = answer->flags;\n\trcu_read_unlock();\n\n\tWARN_ON(!answer_prot->slab);\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_INET6, GFP_KERNEL, answer_prot, kern);\n\tif (!sk)\n\t\tgoto out;\n\n\tsock_init_data(sock, sk);\n\n\terr = 0;\n\tif (INET_PROTOSW_REUSE & answer_flags)\n\t\tsk->sk_reuse = SK_CAN_REUSE;\n\n\tinet = inet_sk(sk);\n\tinet->is_icsk = (INET_PROTOSW_ICSK & answer_flags) != 0;\n\n\tif (SOCK_RAW == sock->type) {\n\t\tinet->inet_num = protocol;\n\t\tif (IPPROTO_RAW == protocol)\n\t\t\tinet->hdrincl = 1;\n\t}\n\n\tsk->sk_destruct\t\t= inet_sock_destruct;\n\tsk->sk_family\t\t= PF_INET6;\n\tsk->sk_protocol\t\t= protocol;\n\n\tsk->sk_backlog_rcv\t= answer->prot->backlog_rcv;\n\n\tinet_sk(sk)->pinet6 = np = inet6_sk_generic(sk);\n\tnp->hop_limit\t= -1;\n\tnp->mcast_hops\t= IPV6_DEFAULT_MCASTHOPS;\n\tnp->mc_loop\t= 1;\n\tnp->pmtudisc\t= IPV6_PMTUDISC_WANT;\n\tnp->autoflowlabel = ip6_default_np_autolabel(sock_net(sk));\n\tsk->sk_ipv6only\t= net->ipv6.sysctl.bindv6only;\n\n\t/* Init the ipv4 part of the socket since we can have sockets\n\t * using v6 API for ipv4.\n\t */\n\tinet->uc_ttl\t= -1;\n\n\tinet->mc_loop\t= 1;\n\tinet->mc_ttl\t= 1;\n\tinet->mc_index\t= 0;\n\tinet->mc_list\t= NULL;\n\tinet->rcv_tos\t= 0;\n\n\tif (net->ipv4.sysctl_ip_no_pmtu_disc)\n\t\tinet->pmtudisc = IP_PMTUDISC_DONT;\n\telse\n\t\tinet->pmtudisc = IP_PMTUDISC_WANT;\n\t/*\n\t * Increment only the relevant sk_prot->socks debug field, this changes\n\t * the previous behaviour of incrementing both the equivalent to\n\t * answer->prot->socks (inet6_sock_nr) and inet_sock_nr.\n\t *\n\t * This allows better debug granularity as we'll know exactly how many\n\t * UDPv6, TCPv6, etc socks were allocated, not the sum of all IPv6\n\t * transport protocol socks. -acme\n\t */\n\tsk_refcnt_debug_inc(sk);\n\n\tif (inet->inet_num) {\n\t\t/* It assumes that any protocol which allows\n\t\t * the user to assign a number at socket\n\t\t * creation time automatically shares.\n\t\t */\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t\tsk->sk_prot->hash(sk);\n\t}\n\tif (sk->sk_prot->init) {\n\t\terr = sk->sk_prot->init(sk);\n\t\tif (err) {\n\t\t\tsk_common_release(sk);\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\treturn err;\nout_rcu_unlock:\n\trcu_read_unlock();\n\tgoto out;\n}\n\n\n/* bind for INET6 API */\nint inet6_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6 *addr = (struct sockaddr_in6 *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tunsigned short snum;\n\tint addr_type = 0;\n\tint err = 0;\n\n\t/* If the socket has its own bind function then use it. */\n\tif (sk->sk_prot->bind)\n\t\treturn sk->sk_prot->bind(sk, uaddr, addr_len);\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (addr->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\taddr_type = ipv6_addr_type(&addr->sin6_addr);\n\tif ((addr_type & IPV6_ADDR_MULTICAST) && sock->type == SOCK_STREAM)\n\t\treturn -EINVAL;\n\n\tsnum = ntohs(addr->sin6_port);\n\tif (snum && snum < PROT_SOCK && !ns_capable(net->user_ns, CAP_NET_BIND_SERVICE))\n\t\treturn -EACCES;\n\n\tlock_sock(sk);\n\n\t/* Check these errors (active socket, double bind). */\n\tif (sk->sk_state != TCP_CLOSE || inet->inet_num) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Check if the address belongs to the host. */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tint chk_addr_ret;\n\n\t\t/* Binding to v4-mapped address on a v6-only socket\n\t\t * makes no sense\n\t\t */\n\t\tif (sk->sk_ipv6only) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/* Reproduce AF_INET checks to make the bindings consistent */\n\t\tv4addr = addr->sin6_addr.s6_addr32[3];\n\t\tchk_addr_ret = inet_addr_type(net, v4addr);\n\t\tif (!net->ipv4.sysctl_ip_nonlocal_bind &&\n\t\t    !(inet->freebind || inet->transparent) &&\n\t\t    v4addr != htonl(INADDR_ANY) &&\n\t\t    chk_addr_ret != RTN_LOCAL &&\n\t\t    chk_addr_ret != RTN_MULTICAST &&\n\t\t    chk_addr_ret != RTN_BROADCAST) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (addr_type != IPV6_ADDR_ANY) {\n\t\t\tstruct net_device *dev = NULL;\n\n\t\t\trcu_read_lock();\n\t\t\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t\t    addr->sin6_scope_id) {\n\t\t\t\t\t/* Override any existing binding, if another one\n\t\t\t\t\t * is supplied by user.\n\t\t\t\t\t */\n\t\t\t\t\tsk->sk_bound_dev_if = addr->sin6_scope_id;\n\t\t\t\t}\n\n\t\t\t\t/* Binding to link-local address requires an interface */\n\t\t\t\tif (!sk->sk_bound_dev_if) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t\tdev = dev_get_by_index_rcu(net, sk->sk_bound_dev_if);\n\t\t\t\tif (!dev) {\n\t\t\t\t\terr = -ENODEV;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t\t */\n\t\t\tv4addr = LOOPBACK4_IPV6;\n\t\t\tif (!(addr_type & IPV6_ADDR_MULTICAST))\t{\n\t\t\t\tif (!net->ipv6.sysctl.ip_nonlocal_bind &&\n\t\t\t\t    !(inet->freebind || inet->transparent) &&\n\t\t\t\t    !ipv6_chk_addr(net, &addr->sin6_addr,\n\t\t\t\t\t\t   dev, 0)) {\n\t\t\t\t\terr = -EADDRNOTAVAIL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\tinet->inet_rcv_saddr = v4addr;\n\tinet->inet_saddr = v4addr;\n\n\tsk->sk_v6_rcv_saddr = addr->sin6_addr;\n\n\tif (!(addr_type & IPV6_ADDR_MULTICAST))\n\t\tnp->saddr = addr->sin6_addr;\n\n\t/* Make sure we are allowed to bind here. */\n\tif ((snum || !inet->bind_address_no_port) &&\n\t    sk->sk_prot->get_port(sk, snum)) {\n\t\tinet_reset_saddr(sk);\n\t\terr = -EADDRINUSE;\n\t\tgoto out;\n\t}\n\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tsk->sk_userlocks |= SOCK_BINDADDR_LOCK;\n\t\tif (addr_type != IPV6_ADDR_MAPPED)\n\t\t\tsk->sk_ipv6only = 1;\n\t}\n\tif (snum)\n\t\tsk->sk_userlocks |= SOCK_BINDPORT_LOCK;\n\tinet->inet_sport = htons(inet->inet_num);\n\tinet->inet_dport = 0;\n\tinet->inet_daddr = 0;\nout:\n\trelease_sock(sk);\n\treturn err;\nout_unlock:\n\trcu_read_unlock();\n\tgoto out;\n}\nEXPORT_SYMBOL(inet6_bind);\n\nint inet6_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (!sk)\n\t\treturn -EINVAL;\n\n\t/* Free mc lists */\n\tipv6_sock_mc_close(sk);\n\n\t/* Free ac lists */\n\tipv6_sock_ac_close(sk);\n\n\treturn inet_release(sock);\n}\nEXPORT_SYMBOL(inet6_release);\n\nvoid inet6_destroy_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (skb)\n\t\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}\nEXPORT_SYMBOL_GPL(inet6_destroy_sock);\n\n/*\n *\tThis does both peername and sockname.\n */\n\nint inet6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t int *uaddr_len, int peer)\n{\n\tstruct sockaddr_in6 *sin = (struct sockaddr_in6 *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\tsin->sin6_family = AF_INET6;\n\tsin->sin6_flowinfo = 0;\n\tsin->sin6_scope_id = 0;\n\tif (peer) {\n\t\tif (!inet->inet_dport)\n\t\t\treturn -ENOTCONN;\n\t\tif (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&\n\t\t    peer == 1)\n\t\t\treturn -ENOTCONN;\n\t\tsin->sin6_port = inet->inet_dport;\n\t\tsin->sin6_addr = sk->sk_v6_daddr;\n\t\tif (np->sndflow)\n\t\t\tsin->sin6_flowinfo = np->flow_label;\n\t} else {\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\t\tsin->sin6_addr = np->saddr;\n\t\telse\n\t\t\tsin->sin6_addr = sk->sk_v6_rcv_saddr;\n\n\t\tsin->sin6_port = inet->inet_sport;\n\t}\n\tsin->sin6_scope_id = ipv6_iface_scope_id(&sin->sin6_addr,\n\t\t\t\t\t\t sk->sk_bound_dev_if);\n\t*uaddr_len = sizeof(*sin);\n\treturn 0;\n}\nEXPORT_SYMBOL(inet6_getname);\n\nint inet6_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\n\tswitch (cmd) {\n\tcase SIOCGSTAMP:\n\t\treturn sock_get_timestamp(sk, (struct timeval __user *)arg);\n\n\tcase SIOCGSTAMPNS:\n\t\treturn sock_get_timestampns(sk, (struct timespec __user *)arg);\n\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\n\t\treturn ipv6_route_ioctl(net, cmd, (void __user *)arg);\n\n\tcase SIOCSIFADDR:\n\t\treturn addrconf_add_ifaddr(net, (void __user *) arg);\n\tcase SIOCDIFADDR:\n\t\treturn addrconf_del_ifaddr(net, (void __user *) arg);\n\tcase SIOCSIFDSTADDR:\n\t\treturn addrconf_set_dstaddr(net, (void __user *) arg);\n\tdefault:\n\t\tif (!sk->sk_prot->ioctl)\n\t\t\treturn -ENOIOCTLCMD;\n\t\treturn sk->sk_prot->ioctl(sk, cmd, arg);\n\t}\n\t/*NOTREACHED*/\n\treturn 0;\n}\nEXPORT_SYMBOL(inet6_ioctl);\n\nconst struct proto_ops inet6_stream_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_stream_connect,\t/* ok\t\t*/\n\t.socketpair\t   = sock_no_socketpair,\t/* a do nothing\t*/\n\t.accept\t\t   = inet_accept,\t\t/* ok\t\t*/\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = tcp_poll,\t\t\t/* ok\t\t*/\n\t.ioctl\t\t   = inet6_ioctl,\t\t/* must change  */\n\t.listen\t\t   = inet_listen,\t\t/* ok\t\t*/\n\t.shutdown\t   = inet_shutdown,\t\t/* ok\t\t*/\n\t.setsockopt\t   = sock_common_setsockopt,\t/* ok\t\t*/\n\t.getsockopt\t   = sock_common_getsockopt,\t/* ok\t\t*/\n\t.sendmsg\t   = inet_sendmsg,\t\t/* ok\t\t*/\n\t.recvmsg\t   = inet_recvmsg,\t\t/* ok\t\t*/\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = inet_sendpage,\n\t.splice_read\t   = tcp_splice_read,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nconst struct proto_ops inet6_dgram_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_dgram_connect,\t/* ok\t\t*/\n\t.socketpair\t   = sock_no_socketpair,\t/* a do nothing\t*/\n\t.accept\t\t   = sock_no_accept,\t\t/* a do nothing\t*/\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = udp_poll,\t\t\t/* ok\t\t*/\n\t.ioctl\t\t   = inet6_ioctl,\t\t/* must change  */\n\t.listen\t\t   = sock_no_listen,\t\t/* ok\t\t*/\n\t.shutdown\t   = inet_shutdown,\t\t/* ok\t\t*/\n\t.setsockopt\t   = sock_common_setsockopt,\t/* ok\t\t*/\n\t.getsockopt\t   = sock_common_getsockopt,\t/* ok\t\t*/\n\t.sendmsg\t   = inet_sendmsg,\t\t/* ok\t\t*/\n\t.recvmsg\t   = inet_recvmsg,\t\t/* ok\t\t*/\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic const struct net_proto_family inet6_family_ops = {\n\t.family = PF_INET6,\n\t.create = inet6_create,\n\t.owner\t= THIS_MODULE,\n};\n\nint inet6_register_protosw(struct inet_protosw *p)\n{\n\tstruct list_head *lh;\n\tstruct inet_protosw *answer;\n\tstruct list_head *last_perm;\n\tint protocol = p->protocol;\n\tint ret;\n\n\tspin_lock_bh(&inetsw6_lock);\n\n\tret = -EINVAL;\n\tif (p->type >= SOCK_MAX)\n\t\tgoto out_illegal;\n\n\t/* If we are trying to override a permanent protocol, bail. */\n\tanswer = NULL;\n\tret = -EPERM;\n\tlast_perm = &inetsw6[p->type];\n\tlist_for_each(lh, &inetsw6[p->type]) {\n\t\tanswer = list_entry(lh, struct inet_protosw, list);\n\n\t\t/* Check only the non-wild match. */\n\t\tif (INET_PROTOSW_PERMANENT & answer->flags) {\n\t\t\tif (protocol == answer->protocol)\n\t\t\t\tbreak;\n\t\t\tlast_perm = lh;\n\t\t}\n\n\t\tanswer = NULL;\n\t}\n\tif (answer)\n\t\tgoto out_permanent;\n\n\t/* Add the new entry after the last permanent entry if any, so that\n\t * the new entry does not override a permanent entry when matched with\n\t * a wild-card protocol. But it is allowed to override any existing\n\t * non-permanent entry.  This means that when we remove this entry, the\n\t * system automatically returns to the old behavior.\n\t */\n\tlist_add_rcu(&p->list, last_perm);\n\tret = 0;\nout:\n\tspin_unlock_bh(&inetsw6_lock);\n\treturn ret;\n\nout_permanent:\n\tpr_err(\"Attempt to override permanent protocol %d\\n\", protocol);\n\tgoto out;\n\nout_illegal:\n\tpr_err(\"Ignoring attempt to register invalid socket type %d\\n\",\n\t       p->type);\n\tgoto out;\n}\nEXPORT_SYMBOL(inet6_register_protosw);\n\nvoid\ninet6_unregister_protosw(struct inet_protosw *p)\n{\n\tif (INET_PROTOSW_PERMANENT & p->flags) {\n\t\tpr_err(\"Attempt to unregister permanent protocol %d\\n\",\n\t\t       p->protocol);\n\t} else {\n\t\tspin_lock_bh(&inetsw6_lock);\n\t\tlist_del_rcu(&p->list);\n\t\tspin_unlock_bh(&inetsw6_lock);\n\n\t\tsynchronize_net();\n\t}\n}\nEXPORT_SYMBOL(inet6_unregister_protosw);\n\nint inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\t\trcu_read_lock();\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n\t\t\t\t\t &final);\n\t\trcu_read_unlock();\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\t__ip6_dst_store(sk, dst, NULL, NULL);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(inet6_sk_rebuild_header);\n\nbool ipv6_opt_accepted(const struct sock *sk, const struct sk_buff *skb,\n\t\t       const struct inet6_skb_parm *opt)\n{\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\n\tif (np->rxopt.all) {\n\t\tif (((opt->flags & IP6SKB_HOPBYHOP) &&\n\t\t     (np->rxopt.bits.hopopts || np->rxopt.bits.ohopopts)) ||\n\t\t    (ip6_flowinfo((struct ipv6hdr *) skb_network_header(skb)) &&\n\t\t     np->rxopt.bits.rxflow) ||\n\t\t    (opt->srcrt && (np->rxopt.bits.srcrt ||\n\t\t     np->rxopt.bits.osrcrt)) ||\n\t\t    ((opt->dst1 || opt->dst0) &&\n\t\t     (np->rxopt.bits.dstopts || np->rxopt.bits.odstopts)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(ipv6_opt_accepted);\n\nstatic struct packet_type ipv6_packet_type __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IPV6),\n\t.func = ipv6_rcv,\n};\n\nstatic int __init ipv6_packet_init(void)\n{\n\tdev_add_pack(&ipv6_packet_type);\n\treturn 0;\n}\n\nstatic void ipv6_packet_cleanup(void)\n{\n\tdev_remove_pack(&ipv6_packet_type);\n}\n\nstatic int __net_init ipv6_init_mibs(struct net *net)\n{\n\tint i;\n\n\tnet->mib.udp_stats_in6 = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udp_stats_in6)\n\t\treturn -ENOMEM;\n\tnet->mib.udplite_stats_in6 = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udplite_stats_in6)\n\t\tgoto err_udplite_mib;\n\tnet->mib.ipv6_statistics = alloc_percpu(struct ipstats_mib);\n\tif (!net->mib.ipv6_statistics)\n\t\tgoto err_ip_mib;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct ipstats_mib *af_inet6_stats;\n\t\taf_inet6_stats = per_cpu_ptr(net->mib.ipv6_statistics, i);\n\t\tu64_stats_init(&af_inet6_stats->syncp);\n\t}\n\n\n\tnet->mib.icmpv6_statistics = alloc_percpu(struct icmpv6_mib);\n\tif (!net->mib.icmpv6_statistics)\n\t\tgoto err_icmp_mib;\n\tnet->mib.icmpv6msg_statistics = kzalloc(sizeof(struct icmpv6msg_mib),\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!net->mib.icmpv6msg_statistics)\n\t\tgoto err_icmpmsg_mib;\n\treturn 0;\n\nerr_icmpmsg_mib:\n\tfree_percpu(net->mib.icmpv6_statistics);\nerr_icmp_mib:\n\tfree_percpu(net->mib.ipv6_statistics);\nerr_ip_mib:\n\tfree_percpu(net->mib.udplite_stats_in6);\nerr_udplite_mib:\n\tfree_percpu(net->mib.udp_stats_in6);\n\treturn -ENOMEM;\n}\n\nstatic void ipv6_cleanup_mibs(struct net *net)\n{\n\tfree_percpu(net->mib.udp_stats_in6);\n\tfree_percpu(net->mib.udplite_stats_in6);\n\tfree_percpu(net->mib.ipv6_statistics);\n\tfree_percpu(net->mib.icmpv6_statistics);\n\tkfree(net->mib.icmpv6msg_statistics);\n}\n\nstatic int __net_init inet6_net_init(struct net *net)\n{\n\tint err = 0;\n\n\tnet->ipv6.sysctl.bindv6only = 0;\n\tnet->ipv6.sysctl.icmpv6_time = 1*HZ;\n\tnet->ipv6.sysctl.flowlabel_consistency = 1;\n\tnet->ipv6.sysctl.auto_flowlabels = IP6_DEFAULT_AUTO_FLOW_LABELS;\n\tnet->ipv6.sysctl.idgen_retries = 3;\n\tnet->ipv6.sysctl.idgen_delay = 1 * HZ;\n\tnet->ipv6.sysctl.flowlabel_state_ranges = 0;\n\tatomic_set(&net->ipv6.fib6_sernum, 1);\n\n\terr = ipv6_init_mibs(net);\n\tif (err)\n\t\treturn err;\n#ifdef CONFIG_PROC_FS\n\terr = udp6_proc_init(net);\n\tif (err)\n\t\tgoto out;\n\terr = tcp6_proc_init(net);\n\tif (err)\n\t\tgoto proc_tcp6_fail;\n\terr = ac6_proc_init(net);\n\tif (err)\n\t\tgoto proc_ac6_fail;\n#endif\n\treturn err;\n\n#ifdef CONFIG_PROC_FS\nproc_ac6_fail:\n\ttcp6_proc_exit(net);\nproc_tcp6_fail:\n\tudp6_proc_exit(net);\nout:\n\tipv6_cleanup_mibs(net);\n\treturn err;\n#endif\n}\n\nstatic void __net_exit inet6_net_exit(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tudp6_proc_exit(net);\n\ttcp6_proc_exit(net);\n\tac6_proc_exit(net);\n#endif\n\tipv6_cleanup_mibs(net);\n}\n\nstatic struct pernet_operations inet6_net_ops = {\n\t.init = inet6_net_init,\n\t.exit = inet6_net_exit,\n};\n\nstatic const struct ipv6_stub ipv6_stub_impl = {\n\t.ipv6_sock_mc_join = ipv6_sock_mc_join,\n\t.ipv6_sock_mc_drop = ipv6_sock_mc_drop,\n\t.ipv6_dst_lookup = ip6_dst_lookup,\n\t.udpv6_encap_enable = udpv6_encap_enable,\n\t.ndisc_send_na = ndisc_send_na,\n\t.nd_tbl\t= &nd_tbl,\n};\n\nstatic int __init inet6_init(void)\n{\n\tstruct list_head *r;\n\tint err = 0;\n\n\tsock_skb_cb_check_size(sizeof(struct inet6_skb_parm));\n\n\t/* Register the socket-side information for inet6_create.  */\n\tfor (r = &inetsw6[0]; r < &inetsw6[SOCK_MAX]; ++r)\n\t\tINIT_LIST_HEAD(r);\n\n\tif (disable_ipv6_mod) {\n\t\tpr_info(\"Loaded, but administratively disabled, reboot required to enable\\n\");\n\t\tgoto out;\n\t}\n\n\terr = proto_register(&tcpv6_prot, 1);\n\tif (err)\n\t\tgoto out;\n\n\terr = proto_register(&udpv6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_tcp_proto;\n\n\terr = proto_register(&udplitev6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_udp_proto;\n\n\terr = proto_register(&rawv6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_udplite_proto;\n\n\terr = proto_register(&pingv6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_ping_proto;\n\n\t/* We MUST register RAW sockets before we create the ICMP6,\n\t * IGMP6, or NDISC control sockets.\n\t */\n\terr = rawv6_init();\n\tif (err)\n\t\tgoto out_unregister_raw_proto;\n\n\t/* Register the family here so that the init calls below will\n\t * be able to create sockets. (?? is this dangerous ??)\n\t */\n\terr = sock_register(&inet6_family_ops);\n\tif (err)\n\t\tgoto out_sock_register_fail;\n\n\t/*\n\t *\tipngwg API draft makes clear that the correct semantics\n\t *\tfor TCP and UDP is to consider one TCP and UDP instance\n\t *\tin a host available by both INET and INET6 APIs and\n\t *\table to communicate via both network protocols.\n\t */\n\n\terr = register_pernet_subsys(&inet6_net_ops);\n\tif (err)\n\t\tgoto register_pernet_fail;\n\terr = icmpv6_init();\n\tif (err)\n\t\tgoto icmp_fail;\n\terr = ip6_mr_init();\n\tif (err)\n\t\tgoto ipmr_fail;\n\terr = ndisc_init();\n\tif (err)\n\t\tgoto ndisc_fail;\n\terr = igmp6_init();\n\tif (err)\n\t\tgoto igmp_fail;\n\n\tipv6_stub = &ipv6_stub_impl;\n\n\terr = ipv6_netfilter_init();\n\tif (err)\n\t\tgoto netfilter_fail;\n\t/* Create /proc/foo6 entries. */\n#ifdef CONFIG_PROC_FS\n\terr = -ENOMEM;\n\tif (raw6_proc_init())\n\t\tgoto proc_raw6_fail;\n\tif (udplite6_proc_init())\n\t\tgoto proc_udplite6_fail;\n\tif (ipv6_misc_proc_init())\n\t\tgoto proc_misc6_fail;\n\tif (if6_proc_init())\n\t\tgoto proc_if6_fail;\n#endif\n\terr = ip6_route_init();\n\tif (err)\n\t\tgoto ip6_route_fail;\n\terr = ndisc_late_init();\n\tif (err)\n\t\tgoto ndisc_late_fail;\n\terr = ip6_flowlabel_init();\n\tif (err)\n\t\tgoto ip6_flowlabel_fail;\n\terr = addrconf_init();\n\tif (err)\n\t\tgoto addrconf_fail;\n\n\t/* Init v6 extension headers. */\n\terr = ipv6_exthdrs_init();\n\tif (err)\n\t\tgoto ipv6_exthdrs_fail;\n\n\terr = ipv6_frag_init();\n\tif (err)\n\t\tgoto ipv6_frag_fail;\n\n\t/* Init v6 transport protocols. */\n\terr = udpv6_init();\n\tif (err)\n\t\tgoto udpv6_fail;\n\n\terr = udplitev6_init();\n\tif (err)\n\t\tgoto udplitev6_fail;\n\n\terr = tcpv6_init();\n\tif (err)\n\t\tgoto tcpv6_fail;\n\n\terr = ipv6_packet_init();\n\tif (err)\n\t\tgoto ipv6_packet_fail;\n\n\terr = pingv6_init();\n\tif (err)\n\t\tgoto pingv6_fail;\n\n#ifdef CONFIG_SYSCTL\n\terr = ipv6_sysctl_register();\n\tif (err)\n\t\tgoto sysctl_fail;\n#endif\nout:\n\treturn err;\n\n#ifdef CONFIG_SYSCTL\nsysctl_fail:\n\tpingv6_exit();\n#endif\npingv6_fail:\n\tipv6_packet_cleanup();\nipv6_packet_fail:\n\ttcpv6_exit();\ntcpv6_fail:\n\tudplitev6_exit();\nudplitev6_fail:\n\tudpv6_exit();\nudpv6_fail:\n\tipv6_frag_exit();\nipv6_frag_fail:\n\tipv6_exthdrs_exit();\nipv6_exthdrs_fail:\n\taddrconf_cleanup();\naddrconf_fail:\n\tip6_flowlabel_cleanup();\nip6_flowlabel_fail:\n\tndisc_late_cleanup();\nndisc_late_fail:\n\tip6_route_cleanup();\nip6_route_fail:\n#ifdef CONFIG_PROC_FS\n\tif6_proc_exit();\nproc_if6_fail:\n\tipv6_misc_proc_exit();\nproc_misc6_fail:\n\tudplite6_proc_exit();\nproc_udplite6_fail:\n\traw6_proc_exit();\nproc_raw6_fail:\n#endif\n\tipv6_netfilter_fini();\nnetfilter_fail:\n\tigmp6_cleanup();\nigmp_fail:\n\tndisc_cleanup();\nndisc_fail:\n\tip6_mr_cleanup();\nipmr_fail:\n\ticmpv6_cleanup();\nicmp_fail:\n\tunregister_pernet_subsys(&inet6_net_ops);\nregister_pernet_fail:\n\tsock_unregister(PF_INET6);\n\trtnl_unregister_all(PF_INET6);\nout_sock_register_fail:\n\trawv6_exit();\nout_unregister_ping_proto:\n\tproto_unregister(&pingv6_prot);\nout_unregister_raw_proto:\n\tproto_unregister(&rawv6_prot);\nout_unregister_udplite_proto:\n\tproto_unregister(&udplitev6_prot);\nout_unregister_udp_proto:\n\tproto_unregister(&udpv6_prot);\nout_unregister_tcp_proto:\n\tproto_unregister(&tcpv6_prot);\n\tgoto out;\n}\nmodule_init(inet6_init);\n\nMODULE_ALIAS_NETPROTO(PF_INET6);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tPF_INET6 socket protocol family\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tAdapted from linux/net/ipv4/af_inet.c\n *\n *\tFixes:\n *\tpiggy, Karl Knutson\t:\tSocket protocol table\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tArnaldo Melo\t\t:\tcheck proc_net_create return, cleanups\n */\n\n#define pr_fmt(fmt) \"IPv6: \" fmt\n\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/fcntl.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/proc_fs.h>\n#include <linux/stat.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/icmpv6.h>\n#include <linux/netfilter_ipv6.h>\n\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <net/tcp.h>\n#include <net/ping.h>\n#include <net/protocol.h>\n#include <net/inet_common.h>\n#include <net/route.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/ipv6_stubs.h>\n#include <net/ndisc.h>\n#ifdef CONFIG_IPV6_TUNNEL\n#include <net/ip6_tunnel.h>\n#endif\n#include <net/calipso.h>\n#include <net/seg6.h>\n#include <net/rpl.h>\n#include <net/compat.h>\n#include <net/xfrm.h>\n#include <net/ioam6.h>\n#include <net/rawv6.h>\n#include <net/rps.h>\n\n#include <linux/uaccess.h>\n#include <linux/mroute6.h>\n\n#include \"ip6_offload.h\"\n\nMODULE_AUTHOR(\"Cast of dozens\");\nMODULE_DESCRIPTION(\"IPv6 protocol stack for Linux\");\nMODULE_LICENSE(\"GPL\");\n\n/* The inetsw6 table contains everything that inet6_create needs to\n * build a new socket.\n */\nstatic struct list_head inetsw6[SOCK_MAX];\nstatic DEFINE_SPINLOCK(inetsw6_lock);\n\nstruct ipv6_params ipv6_defaults = {\n\t.disable_ipv6 = 0,\n\t.autoconf = 1,\n};\n\nstatic int disable_ipv6_mod;\n\nmodule_param_named(disable, disable_ipv6_mod, int, 0444);\nMODULE_PARM_DESC(disable, \"Disable IPv6 module such that it is non-functional\");\n\nmodule_param_named(disable_ipv6, ipv6_defaults.disable_ipv6, int, 0444);\nMODULE_PARM_DESC(disable_ipv6, \"Disable IPv6 on all interfaces\");\n\nmodule_param_named(autoconf, ipv6_defaults.autoconf, int, 0444);\nMODULE_PARM_DESC(autoconf, \"Enable IPv6 address autoconfiguration on all interfaces\");\n\nbool ipv6_mod_enabled(void)\n{\n\treturn disable_ipv6_mod == 0;\n}\nEXPORT_SYMBOL_GPL(ipv6_mod_enabled);\n\nstatic struct ipv6_pinfo *inet6_sk_generic(struct sock *sk)\n{\n\tconst int offset = sk->sk_prot->ipv6_pinfo_offset;\n\n\treturn (struct ipv6_pinfo *)(((u8 *)sk) + offset);\n}\n\nvoid inet6_sock_destruct(struct sock *sk)\n{\n\tinet6_cleanup_sock(sk);\n\tinet_sock_destruct(sk);\n}\nEXPORT_SYMBOL_GPL(inet6_sock_destruct);\n\nstatic int inet6_create(struct net *net, struct socket *sock, int protocol,\n\t\t\tint kern)\n{\n\tstruct inet_sock *inet;\n\tstruct ipv6_pinfo *np;\n\tstruct sock *sk;\n\tstruct inet_protosw *answer;\n\tstruct proto *answer_prot;\n\tunsigned char answer_flags;\n\tint try_loading_module = 0;\n\tint err;\n\n\tif (protocol < 0 || protocol >= IPPROTO_MAX)\n\t\treturn -EINVAL;\n\n\t/* Look for the requested type/protocol pair. */\nlookup_protocol:\n\terr = -ESOCKTNOSUPPORT;\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(answer, &inetsw6[sock->type], list) {\n\n\t\terr = 0;\n\t\t/* Check the non-wild match. */\n\t\tif (protocol == answer->protocol) {\n\t\t\tif (protocol != IPPROTO_IP)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* Check for the two wild cases. */\n\t\t\tif (IPPROTO_IP == protocol) {\n\t\t\t\tprotocol = answer->protocol;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (IPPROTO_IP == answer->protocol)\n\t\t\t\tbreak;\n\t\t}\n\t\terr = -EPROTONOSUPPORT;\n\t}\n\n\tif (err) {\n\t\tif (try_loading_module < 2) {\n\t\t\trcu_read_unlock();\n\t\t\t/*\n\t\t\t * Be more specific, e.g. net-pf-10-proto-132-type-1\n\t\t\t * (net-pf-PF_INET6-proto-IPPROTO_SCTP-type-SOCK_STREAM)\n\t\t\t */\n\t\t\tif (++try_loading_module == 1)\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d-type-%d\",\n\t\t\t\t\t\tPF_INET6, protocol, sock->type);\n\t\t\t/*\n\t\t\t * Fall back to generic, e.g. net-pf-10-proto-132\n\t\t\t * (net-pf-PF_INET6-proto-IPPROTO_SCTP)\n\t\t\t */\n\t\t\telse\n\t\t\t\trequest_module(\"net-pf-%d-proto-%d\",\n\t\t\t\t\t\tPF_INET6, protocol);\n\t\t\tgoto lookup_protocol;\n\t\t} else\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\terr = -EPERM;\n\tif (sock->type == SOCK_RAW && !kern &&\n\t    !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\tgoto out_rcu_unlock;\n\n\tsock->ops = answer->ops;\n\tanswer_prot = answer->prot;\n\tanswer_flags = answer->flags;\n\trcu_read_unlock();\n\n\tWARN_ON(!answer_prot->slab);\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_INET6, GFP_KERNEL, answer_prot, kern);\n\tif (!sk)\n\t\tgoto out;\n\n\tsock_init_data(sock, sk);\n\n\terr = 0;\n\tif (INET_PROTOSW_REUSE & answer_flags)\n\t\tsk->sk_reuse = SK_CAN_REUSE;\n\n\tif (INET_PROTOSW_ICSK & answer_flags)\n\t\tinet_init_csk_locks(sk);\n\n\tinet = inet_sk(sk);\n\tinet_assign_bit(IS_ICSK, sk, INET_PROTOSW_ICSK & answer_flags);\n\n\tif (SOCK_RAW == sock->type) {\n\t\tinet->inet_num = protocol;\n\t\tif (IPPROTO_RAW == protocol)\n\t\t\tinet_set_bit(HDRINCL, sk);\n\t}\n\n\tsk->sk_destruct\t\t= inet6_sock_destruct;\n\tsk->sk_family\t\t= PF_INET6;\n\tsk->sk_protocol\t\t= protocol;\n\n\tsk->sk_backlog_rcv\t= answer->prot->backlog_rcv;\n\n\tinet_sk(sk)->pinet6 = np = inet6_sk_generic(sk);\n\tnp->hop_limit\t= -1;\n\tnp->mcast_hops\t= IPV6_DEFAULT_MCASTHOPS;\n\tinet6_set_bit(MC6_LOOP, sk);\n\tinet6_set_bit(MC6_ALL, sk);\n\tnp->pmtudisc\t= IPV6_PMTUDISC_WANT;\n\tinet6_assign_bit(REPFLOW, sk, net->ipv6.sysctl.flowlabel_reflect &\n\t\t\t\t     FLOWLABEL_REFLECT_ESTABLISHED);\n\tsk->sk_ipv6only\t= net->ipv6.sysctl.bindv6only;\n\tsk->sk_txrehash = READ_ONCE(net->core.sysctl_txrehash);\n\n\t/* Init the ipv4 part of the socket since we can have sockets\n\t * using v6 API for ipv4.\n\t */\n\tinet->uc_ttl\t= -1;\n\n\tinet_set_bit(MC_LOOP, sk);\n\tinet->mc_ttl\t= 1;\n\tinet->mc_index\t= 0;\n\tRCU_INIT_POINTER(inet->mc_list, NULL);\n\tinet->rcv_tos\t= 0;\n\n\tif (READ_ONCE(net->ipv4.sysctl_ip_no_pmtu_disc))\n\t\tinet->pmtudisc = IP_PMTUDISC_DONT;\n\telse\n\t\tinet->pmtudisc = IP_PMTUDISC_WANT;\n\n\tif (inet->inet_num) {\n\t\t/* It assumes that any protocol which allows\n\t\t * the user to assign a number at socket\n\t\t * creation time automatically shares.\n\t\t */\n\t\tinet->inet_sport = htons(inet->inet_num);\n\t\terr = sk->sk_prot->hash(sk);\n\t\tif (err)\n\t\t\tgoto out_sk_release;\n\t}\n\tif (sk->sk_prot->init) {\n\t\terr = sk->sk_prot->init(sk);\n\t\tif (err)\n\t\t\tgoto out_sk_release;\n\t}\n\n\tif (!kern) {\n\t\terr = BPF_CGROUP_RUN_PROG_INET_SOCK(sk);\n\t\tif (err)\n\t\t\tgoto out_sk_release;\n\t}\nout:\n\treturn err;\nout_rcu_unlock:\n\trcu_read_unlock();\n\tgoto out;\nout_sk_release:\n\tsk_common_release(sk);\n\tsock->sk = NULL;\n\tgoto out;\n}\n\nstatic int __inet6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len,\n\t\t\tu32 flags)\n{\n\tstruct sockaddr_in6 *addr = (struct sockaddr_in6 *)uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\t__be32 v4addr = 0;\n\tunsigned short snum;\n\tbool saved_ipv6only;\n\tint addr_type = 0;\n\tint err = 0;\n\n\tif (addr->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\taddr_type = ipv6_addr_type(&addr->sin6_addr);\n\tif ((addr_type & IPV6_ADDR_MULTICAST) && sk->sk_type == SOCK_STREAM)\n\t\treturn -EINVAL;\n\n\tsnum = ntohs(addr->sin6_port);\n\tif (!(flags & BIND_NO_CAP_NET_BIND_SERVICE) &&\n\t    snum && inet_port_requires_bind_service(net, snum) &&\n\t    !ns_capable(net->user_ns, CAP_NET_BIND_SERVICE))\n\t\treturn -EACCES;\n\n\tif (flags & BIND_WITH_LOCK)\n\t\tlock_sock(sk);\n\n\t/* Check these errors (active socket, double bind). */\n\tif (sk->sk_state != TCP_CLOSE || inet->inet_num) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* Check if the address belongs to the host. */\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct net_device *dev = NULL;\n\t\tint chk_addr_ret;\n\n\t\t/* Binding to v4-mapped address on a v6-only socket\n\t\t * makes no sense\n\t\t */\n\t\tif (ipv6_only_sock(sk)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tdev = dev_get_by_index_rcu(net, sk->sk_bound_dev_if);\n\t\t\tif (!dev) {\n\t\t\t\terr = -ENODEV;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\t/* Reproduce AF_INET checks to make the bindings consistent */\n\t\tv4addr = addr->sin6_addr.s6_addr32[3];\n\t\tchk_addr_ret = inet_addr_type_dev_table(net, dev, v4addr);\n\t\trcu_read_unlock();\n\n\t\tif (!inet_addr_valid_or_nonlocal(net, inet, v4addr,\n\t\t\t\t\t\t chk_addr_ret)) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (addr_type != IPV6_ADDR_ANY) {\n\t\t\tstruct net_device *dev = NULL;\n\n\t\t\trcu_read_lock();\n\t\t\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t\t    addr->sin6_scope_id) {\n\t\t\t\t\t/* Override any existing binding, if another one\n\t\t\t\t\t * is supplied by user.\n\t\t\t\t\t */\n\t\t\t\t\tsk->sk_bound_dev_if = addr->sin6_scope_id;\n\t\t\t\t}\n\n\t\t\t\t/* Binding to link-local address requires an interface */\n\t\t\t\tif (!sk->sk_bound_dev_if) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (sk->sk_bound_dev_if) {\n\t\t\t\tdev = dev_get_by_index_rcu(net, sk->sk_bound_dev_if);\n\t\t\t\tif (!dev) {\n\t\t\t\t\terr = -ENODEV;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t\t */\n\t\t\tv4addr = LOOPBACK4_IPV6;\n\t\t\tif (!(addr_type & IPV6_ADDR_MULTICAST))\t{\n\t\t\t\tif (!ipv6_can_nonlocal_bind(net, inet) &&\n\t\t\t\t    !ipv6_chk_addr(net, &addr->sin6_addr,\n\t\t\t\t\t\t   dev, 0)) {\n\t\t\t\t\terr = -EADDRNOTAVAIL;\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\t}\n\t\t\t}\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\tinet->inet_rcv_saddr = v4addr;\n\tinet->inet_saddr = v4addr;\n\n\tsk->sk_v6_rcv_saddr = addr->sin6_addr;\n\n\tif (!(addr_type & IPV6_ADDR_MULTICAST))\n\t\tnp->saddr = addr->sin6_addr;\n\n\tsaved_ipv6only = sk->sk_ipv6only;\n\tif (addr_type != IPV6_ADDR_ANY && addr_type != IPV6_ADDR_MAPPED)\n\t\tsk->sk_ipv6only = 1;\n\n\t/* Make sure we are allowed to bind here. */\n\tif (snum || !(inet_test_bit(BIND_ADDRESS_NO_PORT, sk) ||\n\t\t      (flags & BIND_FORCE_ADDRESS_NO_PORT))) {\n\t\terr = sk->sk_prot->get_port(sk, snum);\n\t\tif (err) {\n\t\t\tsk->sk_ipv6only = saved_ipv6only;\n\t\t\tinet_reset_saddr(sk);\n\t\t\tgoto out;\n\t\t}\n\t\tif (!(flags & BIND_FROM_BPF)) {\n\t\t\terr = BPF_CGROUP_RUN_PROG_INET6_POST_BIND(sk);\n\t\t\tif (err) {\n\t\t\t\tsk->sk_ipv6only = saved_ipv6only;\n\t\t\t\tinet_reset_saddr(sk);\n\t\t\t\tif (sk->sk_prot->put_port)\n\t\t\t\t\tsk->sk_prot->put_port(sk);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (addr_type != IPV6_ADDR_ANY)\n\t\tsk->sk_userlocks |= SOCK_BINDADDR_LOCK;\n\tif (snum)\n\t\tsk->sk_userlocks |= SOCK_BINDPORT_LOCK;\n\tinet->inet_sport = htons(inet->inet_num);\n\tinet->inet_dport = 0;\n\tinet->inet_daddr = 0;\nout:\n\tif (flags & BIND_WITH_LOCK)\n\t\trelease_sock(sk);\n\treturn err;\nout_unlock:\n\trcu_read_unlock();\n\tgoto out;\n}\n\nint inet6_bind_sk(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tu32 flags = BIND_WITH_LOCK;\n\tconst struct proto *prot;\n\tint err = 0;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tprot = READ_ONCE(sk->sk_prot);\n\t/* If the socket has its own bind function then use it. */\n\tif (prot->bind)\n\t\treturn prot->bind(sk, uaddr, addr_len);\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\t/* BPF prog is run before any checks are done so that if the prog\n\t * changes context in a wrong way it will be caught.\n\t */\n\terr = BPF_CGROUP_RUN_PROG_INET_BIND_LOCK(sk, uaddr, &addr_len,\n\t\t\t\t\t\t CGROUP_INET6_BIND, &flags);\n\tif (err)\n\t\treturn err;\n\n\treturn __inet6_bind(sk, uaddr, addr_len, flags);\n}\n\n/* bind for INET6 API */\nint inet6_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\treturn inet6_bind_sk(sock->sk, uaddr, addr_len);\n}\nEXPORT_SYMBOL(inet6_bind);\n\nint inet6_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (!sk)\n\t\treturn -EINVAL;\n\n\t/* Free mc lists */\n\tipv6_sock_mc_close(sk);\n\n\t/* Free ac lists */\n\tipv6_sock_ac_close(sk);\n\n\treturn inet_release(sock);\n}\nEXPORT_SYMBOL(inet6_release);\n\nvoid inet6_cleanup_sock(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ipv6_txoptions *opt;\n\n\t/* Release rx options */\n\n\tskb = xchg(&np->pktoptions, NULL);\n\tkfree_skb(skb);\n\n\tskb = xchg(&np->rxpmtu, NULL);\n\tkfree_skb(skb);\n\n\t/* Free flowlabels */\n\tfl6_free_socklist(sk);\n\n\t/* Free tx options */\n\n\topt = unrcu_pointer(xchg(&np->opt, NULL));\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n}\nEXPORT_SYMBOL_GPL(inet6_cleanup_sock);\n\n/*\n *\tThis does both peername and sockname.\n */\nint inet6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t  int peer)\n{\n\tstruct sockaddr_in6 *sin = (struct sockaddr_in6 *)uaddr;\n\tint sin_addr_len = sizeof(*sin);\n\tstruct sock *sk = sock->sk;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\tsin->sin6_family = AF_INET6;\n\tsin->sin6_flowinfo = 0;\n\tsin->sin6_scope_id = 0;\n\tlock_sock(sk);\n\tif (peer) {\n\t\tif (!inet->inet_dport ||\n\t\t    (((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_SYN_SENT)) &&\n\t\t    peer == 1)) {\n\t\t\trelease_sock(sk);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\t\tsin->sin6_port = inet->inet_dport;\n\t\tsin->sin6_addr = sk->sk_v6_daddr;\n\t\tif (inet6_test_bit(SNDFLOW, sk))\n\t\t\tsin->sin6_flowinfo = np->flow_label;\n\t\tBPF_CGROUP_RUN_SA_PROG(sk, (struct sockaddr *)sin, &sin_addr_len,\n\t\t\t\t       CGROUP_INET6_GETPEERNAME);\n\t} else {\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\t\tsin->sin6_addr = np->saddr;\n\t\telse\n\t\t\tsin->sin6_addr = sk->sk_v6_rcv_saddr;\n\t\tsin->sin6_port = inet->inet_sport;\n\t\tBPF_CGROUP_RUN_SA_PROG(sk, (struct sockaddr *)sin, &sin_addr_len,\n\t\t\t\t       CGROUP_INET6_GETSOCKNAME);\n\t}\n\tsin->sin6_scope_id = ipv6_iface_scope_id(&sin->sin6_addr,\n\t\t\t\t\t\t sk->sk_bound_dev_if);\n\trelease_sock(sk);\n\treturn sin_addr_len;\n}\nEXPORT_SYMBOL(inet6_getname);\n\nint inet6_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tvoid __user *argp = (void __user *)arg;\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tconst struct proto *prot;\n\n\tswitch (cmd) {\n\tcase SIOCADDRT:\n\tcase SIOCDELRT: {\n\t\tstruct in6_rtmsg rtmsg;\n\n\t\tif (copy_from_user(&rtmsg, argp, sizeof(rtmsg)))\n\t\t\treturn -EFAULT;\n\t\treturn ipv6_route_ioctl(net, cmd, &rtmsg);\n\t}\n\tcase SIOCSIFADDR:\n\t\treturn addrconf_add_ifaddr(net, argp);\n\tcase SIOCDIFADDR:\n\t\treturn addrconf_del_ifaddr(net, argp);\n\tcase SIOCSIFDSTADDR:\n\t\treturn addrconf_set_dstaddr(net, argp);\n\tdefault:\n\t\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\t\tprot = READ_ONCE(sk->sk_prot);\n\t\tif (!prot->ioctl)\n\t\t\treturn -ENOIOCTLCMD;\n\t\treturn sk_ioctl(sk, cmd, (void __user *)arg);\n\t}\n\t/*NOTREACHED*/\n\treturn 0;\n}\nEXPORT_SYMBOL(inet6_ioctl);\n\n#ifdef CONFIG_COMPAT\nstruct compat_in6_rtmsg {\n\tstruct in6_addr\t\trtmsg_dst;\n\tstruct in6_addr\t\trtmsg_src;\n\tstruct in6_addr\t\trtmsg_gateway;\n\tu32\t\t\trtmsg_type;\n\tu16\t\t\trtmsg_dst_len;\n\tu16\t\t\trtmsg_src_len;\n\tu32\t\t\trtmsg_metric;\n\tu32\t\t\trtmsg_info;\n\tu32\t\t\trtmsg_flags;\n\ts32\t\t\trtmsg_ifindex;\n};\n\nstatic int inet6_compat_routing_ioctl(struct sock *sk, unsigned int cmd,\n\t\tstruct compat_in6_rtmsg __user *ur)\n{\n\tstruct in6_rtmsg rt;\n\n\tif (copy_from_user(&rt.rtmsg_dst, &ur->rtmsg_dst,\n\t\t\t3 * sizeof(struct in6_addr)) ||\n\t    get_user(rt.rtmsg_type, &ur->rtmsg_type) ||\n\t    get_user(rt.rtmsg_dst_len, &ur->rtmsg_dst_len) ||\n\t    get_user(rt.rtmsg_src_len, &ur->rtmsg_src_len) ||\n\t    get_user(rt.rtmsg_metric, &ur->rtmsg_metric) ||\n\t    get_user(rt.rtmsg_info, &ur->rtmsg_info) ||\n\t    get_user(rt.rtmsg_flags, &ur->rtmsg_flags) ||\n\t    get_user(rt.rtmsg_ifindex, &ur->rtmsg_ifindex))\n\t\treturn -EFAULT;\n\n\n\treturn ipv6_route_ioctl(sock_net(sk), cmd, &rt);\n}\n\nint inet6_compat_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tvoid __user *argp = compat_ptr(arg);\n\tstruct sock *sk = sock->sk;\n\n\tswitch (cmd) {\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\t\treturn inet6_compat_routing_ioctl(sk, cmd, argp);\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n}\nEXPORT_SYMBOL_GPL(inet6_compat_ioctl);\n#endif /* CONFIG_COMPAT */\n\nINDIRECT_CALLABLE_DECLARE(int udpv6_sendmsg(struct sock *, struct msghdr *,\n\t\t\t\t\t    size_t));\nint inet6_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)\n{\n\tstruct sock *sk = sock->sk;\n\tconst struct proto *prot;\n\n\tif (unlikely(inet_send_prepare(sk)))\n\t\treturn -EAGAIN;\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tprot = READ_ONCE(sk->sk_prot);\n\treturn INDIRECT_CALL_2(prot->sendmsg, tcp_sendmsg, udpv6_sendmsg,\n\t\t\t       sk, msg, size);\n}\n\nINDIRECT_CALLABLE_DECLARE(int udpv6_recvmsg(struct sock *, struct msghdr *,\n\t\t\t\t\t    size_t, int, int *));\nint inet6_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t  int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tconst struct proto *prot;\n\tint addr_len = 0;\n\tint err;\n\n\tif (likely(!(flags & MSG_ERRQUEUE)))\n\t\tsock_rps_record_flow(sk);\n\n\t/* IPV6_ADDRFORM can change sk->sk_prot under us. */\n\tprot = READ_ONCE(sk->sk_prot);\n\terr = INDIRECT_CALL_2(prot->recvmsg, tcp_recvmsg, udpv6_recvmsg,\n\t\t\t      sk, msg, size, flags, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\n\nconst struct proto_ops inet6_stream_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_stream_connect,\t/* ok\t\t*/\n\t.socketpair\t   = sock_no_socketpair,\t/* a do nothing\t*/\n\t.accept\t\t   = inet_accept,\t\t/* ok\t\t*/\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = tcp_poll,\t\t\t/* ok\t\t*/\n\t.ioctl\t\t   = inet6_ioctl,\t\t/* must change  */\n\t.gettstamp\t   = sock_gettstamp,\n\t.listen\t\t   = inet_listen,\t\t/* ok\t\t*/\n\t.shutdown\t   = inet_shutdown,\t\t/* ok\t\t*/\n\t.setsockopt\t   = sock_common_setsockopt,\t/* ok\t\t*/\n\t.getsockopt\t   = sock_common_getsockopt,\t/* ok\t\t*/\n\t.sendmsg\t   = inet6_sendmsg,\t\t/* retpoline's sake */\n\t.recvmsg\t   = inet6_recvmsg,\t\t/* retpoline's sake */\n#ifdef CONFIG_MMU\n\t.mmap\t\t   = tcp_mmap,\n#endif\n\t.splice_eof\t   = inet_splice_eof,\n\t.sendmsg_locked    = tcp_sendmsg_locked,\n\t.splice_read\t   = tcp_splice_read,\n\t.set_peek_off      = sk_set_peek_off,\n\t.read_sock\t   = tcp_read_sock,\n\t.read_skb\t   = tcp_read_skb,\n\t.peek_len\t   = tcp_peek_len,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = inet6_compat_ioctl,\n#endif\n\t.set_rcvlowat\t   = tcp_set_rcvlowat,\n};\nEXPORT_SYMBOL_GPL(inet6_stream_ops);\n\nconst struct proto_ops inet6_dgram_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_dgram_connect,\t/* ok\t\t*/\n\t.socketpair\t   = sock_no_socketpair,\t/* a do nothing\t*/\n\t.accept\t\t   = sock_no_accept,\t\t/* a do nothing\t*/\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = udp_poll,\t\t\t/* ok\t\t*/\n\t.ioctl\t\t   = inet6_ioctl,\t\t/* must change  */\n\t.gettstamp\t   = sock_gettstamp,\n\t.listen\t\t   = sock_no_listen,\t\t/* ok\t\t*/\n\t.shutdown\t   = inet_shutdown,\t\t/* ok\t\t*/\n\t.setsockopt\t   = sock_common_setsockopt,\t/* ok\t\t*/\n\t.getsockopt\t   = sock_common_getsockopt,\t/* ok\t\t*/\n\t.sendmsg\t   = inet6_sendmsg,\t\t/* retpoline's sake */\n\t.recvmsg\t   = inet6_recvmsg,\t\t/* retpoline's sake */\n\t.read_skb\t   = udp_read_skb,\n\t.mmap\t\t   = sock_no_mmap,\n\t.set_peek_off\t   = udp_set_peek_off,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = inet6_compat_ioctl,\n#endif\n};\n\nstatic const struct net_proto_family inet6_family_ops = {\n\t.family = PF_INET6,\n\t.create = inet6_create,\n\t.owner\t= THIS_MODULE,\n};\n\nint inet6_register_protosw(struct inet_protosw *p)\n{\n\tstruct list_head *lh;\n\tstruct inet_protosw *answer;\n\tstruct list_head *last_perm;\n\tint protocol = p->protocol;\n\tint ret;\n\n\tspin_lock_bh(&inetsw6_lock);\n\n\tret = -EINVAL;\n\tif (p->type >= SOCK_MAX)\n\t\tgoto out_illegal;\n\n\t/* If we are trying to override a permanent protocol, bail. */\n\tanswer = NULL;\n\tret = -EPERM;\n\tlast_perm = &inetsw6[p->type];\n\tlist_for_each(lh, &inetsw6[p->type]) {\n\t\tanswer = list_entry(lh, struct inet_protosw, list);\n\n\t\t/* Check only the non-wild match. */\n\t\tif (INET_PROTOSW_PERMANENT & answer->flags) {\n\t\t\tif (protocol == answer->protocol)\n\t\t\t\tbreak;\n\t\t\tlast_perm = lh;\n\t\t}\n\n\t\tanswer = NULL;\n\t}\n\tif (answer)\n\t\tgoto out_permanent;\n\n\t/* Add the new entry after the last permanent entry if any, so that\n\t * the new entry does not override a permanent entry when matched with\n\t * a wild-card protocol. But it is allowed to override any existing\n\t * non-permanent entry.  This means that when we remove this entry, the\n\t * system automatically returns to the old behavior.\n\t */\n\tlist_add_rcu(&p->list, last_perm);\n\tret = 0;\nout:\n\tspin_unlock_bh(&inetsw6_lock);\n\treturn ret;\n\nout_permanent:\n\tpr_err(\"Attempt to override permanent protocol %d\\n\", protocol);\n\tgoto out;\n\nout_illegal:\n\tpr_err(\"Ignoring attempt to register invalid socket type %d\\n\",\n\t       p->type);\n\tgoto out;\n}\nEXPORT_SYMBOL(inet6_register_protosw);\n\nvoid\ninet6_unregister_protosw(struct inet_protosw *p)\n{\n\tif (INET_PROTOSW_PERMANENT & p->flags) {\n\t\tpr_err(\"Attempt to unregister permanent protocol %d\\n\",\n\t\t       p->protocol);\n\t} else {\n\t\tspin_lock_bh(&inetsw6_lock);\n\t\tlist_del_rcu(&p->list);\n\t\tspin_unlock_bh(&inetsw6_lock);\n\n\t\tsynchronize_net();\n\t}\n}\nEXPORT_SYMBOL(inet6_unregister_protosw);\n\nint inet6_sk_rebuild_header(struct sock *sk)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct dst_entry *dst;\n\n\tdst = __sk_dst_check(sk, np->dst_cookie);\n\n\tif (!dst) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = sk->sk_protocol;\n\t\tfl6.daddr = sk->sk_v6_daddr;\n\t\tfl6.saddr = np->saddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tfl6.fl6_sport = inet->inet_sport;\n\t\tfl6.flowi6_uid = sk_uid(sk);\n\t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\t\trcu_read_lock();\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n\t\t\t\t\t &final);\n\t\trcu_read_unlock();\n\n\t\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tsk->sk_route_caps = 0;\n\t\t\tWRITE_ONCE(sk->sk_err_soft, -PTR_ERR(dst));\n\t\t\treturn PTR_ERR(dst);\n\t\t}\n\n\t\tip6_dst_store(sk, dst, NULL, false);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(inet6_sk_rebuild_header);\n\nbool ipv6_opt_accepted(const struct sock *sk, const struct sk_buff *skb,\n\t\t       const struct inet6_skb_parm *opt)\n{\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\n\tif (np->rxopt.all) {\n\t\tif (((opt->flags & IP6SKB_HOPBYHOP) &&\n\t\t     (np->rxopt.bits.hopopts || np->rxopt.bits.ohopopts)) ||\n\t\t    (ip6_flowinfo((struct ipv6hdr *) skb_network_header(skb)) &&\n\t\t     np->rxopt.bits.rxflow) ||\n\t\t    (opt->srcrt && (np->rxopt.bits.srcrt ||\n\t\t     np->rxopt.bits.osrcrt)) ||\n\t\t    ((opt->dst1 || opt->dst0) &&\n\t\t     (np->rxopt.bits.dstopts || np->rxopt.bits.odstopts)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct packet_type ipv6_packet_type __read_mostly = {\n\t.type = cpu_to_be16(ETH_P_IPV6),\n\t.func = ipv6_rcv,\n\t.list_func = ipv6_list_rcv,\n};\n\nstatic int __init ipv6_packet_init(void)\n{\n\tdev_add_pack(&ipv6_packet_type);\n\treturn 0;\n}\n\nstatic void ipv6_packet_cleanup(void)\n{\n\tdev_remove_pack(&ipv6_packet_type);\n}\n\nstatic int __net_init ipv6_init_mibs(struct net *net)\n{\n\tint i;\n\n\tnet->mib.udp_stats_in6 = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udp_stats_in6)\n\t\treturn -ENOMEM;\n\tnet->mib.udplite_stats_in6 = alloc_percpu(struct udp_mib);\n\tif (!net->mib.udplite_stats_in6)\n\t\tgoto err_udplite_mib;\n\tnet->mib.ipv6_statistics = alloc_percpu(struct ipstats_mib);\n\tif (!net->mib.ipv6_statistics)\n\t\tgoto err_ip_mib;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct ipstats_mib *af_inet6_stats;\n\t\taf_inet6_stats = per_cpu_ptr(net->mib.ipv6_statistics, i);\n\t\tu64_stats_init(&af_inet6_stats->syncp);\n\t}\n\n\n\tnet->mib.icmpv6_statistics = alloc_percpu(struct icmpv6_mib);\n\tif (!net->mib.icmpv6_statistics)\n\t\tgoto err_icmp_mib;\n\tnet->mib.icmpv6msg_statistics = kzalloc(sizeof(struct icmpv6msg_mib),\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!net->mib.icmpv6msg_statistics)\n\t\tgoto err_icmpmsg_mib;\n\treturn 0;\n\nerr_icmpmsg_mib:\n\tfree_percpu(net->mib.icmpv6_statistics);\nerr_icmp_mib:\n\tfree_percpu(net->mib.ipv6_statistics);\nerr_ip_mib:\n\tfree_percpu(net->mib.udplite_stats_in6);\nerr_udplite_mib:\n\tfree_percpu(net->mib.udp_stats_in6);\n\treturn -ENOMEM;\n}\n\nstatic void ipv6_cleanup_mibs(struct net *net)\n{\n\tfree_percpu(net->mib.udp_stats_in6);\n\tfree_percpu(net->mib.udplite_stats_in6);\n\tfree_percpu(net->mib.ipv6_statistics);\n\tfree_percpu(net->mib.icmpv6_statistics);\n\tkfree(net->mib.icmpv6msg_statistics);\n}\n\nstatic int __net_init inet6_net_init(struct net *net)\n{\n\tint err = 0;\n\n\tnet->ipv6.sysctl.bindv6only = 0;\n\tnet->ipv6.sysctl.icmpv6_time = 1*HZ;\n\tnet->ipv6.sysctl.icmpv6_echo_ignore_all = 0;\n\tnet->ipv6.sysctl.icmpv6_echo_ignore_multicast = 0;\n\tnet->ipv6.sysctl.icmpv6_echo_ignore_anycast = 0;\n\tnet->ipv6.sysctl.icmpv6_error_anycast_as_unicast = 0;\n\n\t/* By default, rate limit error messages.\n\t * Except for pmtu discovery, it would break it.\n\t * proc_do_large_bitmap needs pointer to the bitmap.\n\t */\n\tbitmap_set(net->ipv6.sysctl.icmpv6_ratemask, 0, ICMPV6_ERRMSG_MAX + 1);\n\tbitmap_clear(net->ipv6.sysctl.icmpv6_ratemask, ICMPV6_PKT_TOOBIG, 1);\n\tnet->ipv6.sysctl.icmpv6_ratemask_ptr = net->ipv6.sysctl.icmpv6_ratemask;\n\n\tnet->ipv6.sysctl.flowlabel_consistency = 1;\n\tnet->ipv6.sysctl.auto_flowlabels = IP6_DEFAULT_AUTO_FLOW_LABELS;\n\tnet->ipv6.sysctl.idgen_retries = 3;\n\tnet->ipv6.sysctl.idgen_delay = 1 * HZ;\n\tnet->ipv6.sysctl.flowlabel_state_ranges = 0;\n\tnet->ipv6.sysctl.max_dst_opts_cnt = IP6_DEFAULT_MAX_DST_OPTS_CNT;\n\tnet->ipv6.sysctl.max_hbh_opts_cnt = IP6_DEFAULT_MAX_HBH_OPTS_CNT;\n\tnet->ipv6.sysctl.max_dst_opts_len = IP6_DEFAULT_MAX_DST_OPTS_LEN;\n\tnet->ipv6.sysctl.max_hbh_opts_len = IP6_DEFAULT_MAX_HBH_OPTS_LEN;\n\tnet->ipv6.sysctl.fib_notify_on_flag_change = 0;\n\tatomic_set(&net->ipv6.fib6_sernum, 1);\n\n\tnet->ipv6.sysctl.ioam6_id = IOAM6_DEFAULT_ID;\n\tnet->ipv6.sysctl.ioam6_id_wide = IOAM6_DEFAULT_ID_WIDE;\n\n\terr = ipv6_init_mibs(net);\n\tif (err)\n\t\treturn err;\n#ifdef CONFIG_PROC_FS\n\terr = udp6_proc_init(net);\n\tif (err)\n\t\tgoto out;\n\terr = tcp6_proc_init(net);\n\tif (err)\n\t\tgoto proc_tcp6_fail;\n\terr = ac6_proc_init(net);\n\tif (err)\n\t\tgoto proc_ac6_fail;\n#endif\n\treturn err;\n\n#ifdef CONFIG_PROC_FS\nproc_ac6_fail:\n\ttcp6_proc_exit(net);\nproc_tcp6_fail:\n\tudp6_proc_exit(net);\nout:\n\tipv6_cleanup_mibs(net);\n\treturn err;\n#endif\n}\n\nstatic void __net_exit inet6_net_exit(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tudp6_proc_exit(net);\n\ttcp6_proc_exit(net);\n\tac6_proc_exit(net);\n#endif\n\tipv6_cleanup_mibs(net);\n}\n\nstatic struct pernet_operations inet6_net_ops = {\n\t.init = inet6_net_init,\n\t.exit = inet6_net_exit,\n};\n\nstatic int ipv6_route_input(struct sk_buff *skb)\n{\n\tip6_route_input(skb);\n\treturn skb_dst(skb)->error;\n}\n\nstatic const struct ipv6_stub ipv6_stub_impl = {\n\t.ipv6_sock_mc_join = ipv6_sock_mc_join,\n\t.ipv6_sock_mc_drop = ipv6_sock_mc_drop,\n\t.ipv6_dst_lookup_flow = ip6_dst_lookup_flow,\n\t.ipv6_route_input  = ipv6_route_input,\n\t.fib6_get_table\t   = fib6_get_table,\n\t.fib6_table_lookup = fib6_table_lookup,\n\t.fib6_lookup       = fib6_lookup,\n\t.fib6_select_path  = fib6_select_path,\n\t.ip6_mtu_from_fib6 = ip6_mtu_from_fib6,\n\t.fib6_nh_init\t   = fib6_nh_init,\n\t.fib6_nh_release   = fib6_nh_release,\n\t.fib6_nh_release_dsts = fib6_nh_release_dsts,\n\t.fib6_update_sernum = fib6_update_sernum_stub,\n\t.fib6_rt_update\t   = fib6_rt_update,\n\t.ip6_del_rt\t   = ip6_del_rt,\n\t.udpv6_encap_enable = udpv6_encap_enable,\n\t.ndisc_send_na = ndisc_send_na,\n#if IS_ENABLED(CONFIG_XFRM)\n\t.xfrm6_local_rxpmtu = xfrm6_local_rxpmtu,\n\t.xfrm6_udp_encap_rcv = xfrm6_udp_encap_rcv,\n\t.xfrm6_gro_udp_encap_rcv = xfrm6_gro_udp_encap_rcv,\n\t.xfrm6_rcv_encap = xfrm6_rcv_encap,\n#endif\n\t.nd_tbl\t= &nd_tbl,\n\t.ipv6_fragment = ip6_fragment,\n\t.ipv6_dev_find = ipv6_dev_find,\n\t.ip6_xmit = ip6_xmit,\n};\n\nstatic const struct ipv6_bpf_stub ipv6_bpf_stub_impl = {\n\t.inet6_bind = __inet6_bind,\n\t.udp6_lib_lookup = __udp6_lib_lookup,\n\t.ipv6_setsockopt = do_ipv6_setsockopt,\n\t.ipv6_getsockopt = do_ipv6_getsockopt,\n\t.ipv6_dev_get_saddr = ipv6_dev_get_saddr,\n};\n\nstatic int __init inet6_init(void)\n{\n\tstruct list_head *r;\n\tint err = 0;\n\n\tsock_skb_cb_check_size(sizeof(struct inet6_skb_parm));\n\n\t/* Register the socket-side information for inet6_create.  */\n\tfor (r = &inetsw6[0]; r < &inetsw6[SOCK_MAX]; ++r)\n\t\tINIT_LIST_HEAD(r);\n\n\traw_hashinfo_init(&raw_v6_hashinfo);\n\n\tif (disable_ipv6_mod) {\n\t\tpr_info(\"Loaded, but administratively disabled, reboot required to enable\\n\");\n\t\tgoto out;\n\t}\n\n\terr = proto_register(&tcpv6_prot, 1);\n\tif (err)\n\t\tgoto out;\n\n\terr = proto_register(&udpv6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_tcp_proto;\n\n\terr = proto_register(&udplitev6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_udp_proto;\n\n\terr = proto_register(&rawv6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_udplite_proto;\n\n\terr = proto_register(&pingv6_prot, 1);\n\tif (err)\n\t\tgoto out_unregister_raw_proto;\n\n\t/* We MUST register RAW sockets before we create the ICMP6,\n\t * IGMP6, or NDISC control sockets.\n\t */\n\terr = rawv6_init();\n\tif (err)\n\t\tgoto out_unregister_ping_proto;\n\n\t/* Register the family here so that the init calls below will\n\t * be able to create sockets. (?? is this dangerous ??)\n\t */\n\terr = sock_register(&inet6_family_ops);\n\tif (err)\n\t\tgoto out_sock_register_fail;\n\n\t/*\n\t *\tipngwg API draft makes clear that the correct semantics\n\t *\tfor TCP and UDP is to consider one TCP and UDP instance\n\t *\tin a host available by both INET and INET6 APIs and\n\t *\table to communicate via both network protocols.\n\t */\n\n\terr = register_pernet_subsys(&inet6_net_ops);\n\tif (err)\n\t\tgoto register_pernet_fail;\n\terr = ip6_mr_init();\n\tif (err)\n\t\tgoto ipmr_fail;\n\terr = icmpv6_init();\n\tif (err)\n\t\tgoto icmp_fail;\n\terr = ndisc_init();\n\tif (err)\n\t\tgoto ndisc_fail;\n\terr = igmp6_init();\n\tif (err)\n\t\tgoto igmp_fail;\n\n\terr = ipv6_netfilter_init();\n\tif (err)\n\t\tgoto netfilter_fail;\n\t/* Create /proc/foo6 entries. */\n#ifdef CONFIG_PROC_FS\n\terr = -ENOMEM;\n\tif (raw6_proc_init())\n\t\tgoto proc_raw6_fail;\n\tif (udplite6_proc_init())\n\t\tgoto proc_udplite6_fail;\n\tif (ipv6_misc_proc_init())\n\t\tgoto proc_misc6_fail;\n\tif (if6_proc_init())\n\t\tgoto proc_if6_fail;\n#endif\n\terr = ip6_route_init();\n\tif (err)\n\t\tgoto ip6_route_fail;\n\terr = ndisc_late_init();\n\tif (err)\n\t\tgoto ndisc_late_fail;\n\terr = ip6_flowlabel_init();\n\tif (err)\n\t\tgoto ip6_flowlabel_fail;\n\terr = ipv6_anycast_init();\n\tif (err)\n\t\tgoto ipv6_anycast_fail;\n\terr = addrconf_init();\n\tif (err)\n\t\tgoto addrconf_fail;\n\n\t/* Init v6 extension headers. */\n\terr = ipv6_exthdrs_init();\n\tif (err)\n\t\tgoto ipv6_exthdrs_fail;\n\n\terr = ipv6_frag_init();\n\tif (err)\n\t\tgoto ipv6_frag_fail;\n\n\t/* Init v6 transport protocols. */\n\terr = udpv6_init();\n\tif (err)\n\t\tgoto udpv6_fail;\n\n\terr = udplitev6_init();\n\tif (err)\n\t\tgoto udplitev6_fail;\n\n\terr = udpv6_offload_init();\n\tif (err)\n\t\tgoto udpv6_offload_fail;\n\n\terr = tcpv6_init();\n\tif (err)\n\t\tgoto tcpv6_fail;\n\n\terr = ipv6_packet_init();\n\tif (err)\n\t\tgoto ipv6_packet_fail;\n\n\terr = pingv6_init();\n\tif (err)\n\t\tgoto pingv6_fail;\n\n\terr = calipso_init();\n\tif (err)\n\t\tgoto calipso_fail;\n\n\terr = seg6_init();\n\tif (err)\n\t\tgoto seg6_fail;\n\n\terr = rpl_init();\n\tif (err)\n\t\tgoto rpl_fail;\n\n\terr = ioam6_init();\n\tif (err)\n\t\tgoto ioam6_fail;\n\n\terr = igmp6_late_init();\n\tif (err)\n\t\tgoto igmp6_late_err;\n\n#ifdef CONFIG_SYSCTL\n\terr = ipv6_sysctl_register();\n\tif (err)\n\t\tgoto sysctl_fail;\n#endif\n\n\t/* ensure that ipv6 stubs are visible only after ipv6 is ready */\n\twmb();\n\tipv6_stub = &ipv6_stub_impl;\n\tipv6_bpf_stub = &ipv6_bpf_stub_impl;\nout:\n\treturn err;\n\n#ifdef CONFIG_SYSCTL\nsysctl_fail:\n\tigmp6_late_cleanup();\n#endif\nigmp6_late_err:\n\tioam6_exit();\nioam6_fail:\n\trpl_exit();\nrpl_fail:\n\tseg6_exit();\nseg6_fail:\n\tcalipso_exit();\ncalipso_fail:\n\tpingv6_exit();\npingv6_fail:\n\tipv6_packet_cleanup();\nipv6_packet_fail:\n\ttcpv6_exit();\ntcpv6_fail:\n\tudpv6_offload_exit();\nudpv6_offload_fail:\n\tudplitev6_exit();\nudplitev6_fail:\n\tudpv6_exit();\nudpv6_fail:\n\tipv6_frag_exit();\nipv6_frag_fail:\n\tipv6_exthdrs_exit();\nipv6_exthdrs_fail:\n\taddrconf_cleanup();\naddrconf_fail:\n\tipv6_anycast_cleanup();\nipv6_anycast_fail:\n\tip6_flowlabel_cleanup();\nip6_flowlabel_fail:\n\tndisc_late_cleanup();\nndisc_late_fail:\n\tip6_route_cleanup();\nip6_route_fail:\n#ifdef CONFIG_PROC_FS\n\tif6_proc_exit();\nproc_if6_fail:\n\tipv6_misc_proc_exit();\nproc_misc6_fail:\n\tudplite6_proc_exit();\nproc_udplite6_fail:\n\traw6_proc_exit();\nproc_raw6_fail:\n#endif\n\tipv6_netfilter_fini();\nnetfilter_fail:\n\tigmp6_cleanup();\nigmp_fail:\n\tndisc_cleanup();\nndisc_fail:\n\ticmpv6_cleanup();\nicmp_fail:\n\tip6_mr_cleanup();\nipmr_fail:\n\tunregister_pernet_subsys(&inet6_net_ops);\nregister_pernet_fail:\n\tsock_unregister(PF_INET6);\n\trtnl_unregister_all(PF_INET6);\nout_sock_register_fail:\n\trawv6_exit();\nout_unregister_ping_proto:\n\tproto_unregister(&pingv6_prot);\nout_unregister_raw_proto:\n\tproto_unregister(&rawv6_prot);\nout_unregister_udplite_proto:\n\tproto_unregister(&udplitev6_prot);\nout_unregister_udp_proto:\n\tproto_unregister(&udpv6_prot);\nout_unregister_tcp_proto:\n\tproto_unregister(&tcpv6_prot);\n\tgoto out;\n}\nmodule_init(inet6_init);\n\nMODULE_ALIAS_NETPROTO(PF_INET6);\n", "patch": "@@ -428,9 +428,11 @@ void inet6_destroy_sock(struct sock *sk)\n \n \t/* Free tx options */\n \n-\topt = xchg(&np->opt, NULL);\n-\tif (opt)\n-\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\topt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);\n+\tif (opt) {\n+\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\ttxopt_put(opt);\n+\t}\n }\n EXPORT_SYMBOL_GPL(inet6_destroy_sock);\n \n@@ -659,7 +661,10 @@ int inet6_sk_rebuild_header(struct sock *sk)\n \t\tfl6.fl6_sport = inet->inet_sport;\n \t\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\t\trcu_read_lock();\n+\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),\n+\t\t\t\t\t &final);\n+\t\trcu_read_unlock();\n \n \t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \t\tif (IS_ERR(dst)) {", "file_path": "files/2016_8\\85", "file_language": "c", "file_name": "net/ipv6/af_inet6.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/datagram.c", "code": "/*\n *\tcommon UDP/RAW code\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/in6.h>\n#include <linux/ipv6.h>\n#include <linux/route.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/addrconf.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/tcp_states.h>\n#include <net/dsfield.h>\n\n#include <linux/errqueue.h>\n#include <asm/uaccess.h>\n\nstatic bool ipv6_mapped_addr_any(const struct in6_addr *a)\n{\n\treturn ipv6_addr_v4mapped(a) && (a->s6_addr32[3] == 0);\n}\n\nstatic int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t*daddr, *final_p, final;\n\tstruct dst_entry\t*dst;\n\tstruct flowi6\t\tfl6;\n\tstruct ip6_flowlabel\t*flowlabel = NULL;\n\tstruct ipv6_txoptions\t*opt;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type == IPV6_ADDR_ANY) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tusin->sin6_addr.s6_addr[15] = 0x01;\n\t}\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6.flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = inet->inet_dport;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tif (!fl6.flowi6_oif && (addr_type&IPV6_ADDR_MULTICAST))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\terr = 0;\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\t/* source address lookup done in ip6_dst_lookup */\n\n\tif (ipv6_addr_any(&np->saddr))\n\t\tnp->saddr = fl6.saddr;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\tif (sk->sk_prot->rehash)\n\t\t\tsk->sk_prot->rehash(sk);\n\t}\n\n\tip6_dst_store(sk, dst,\n\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t      &np->saddr :\n#endif\n\t\t      NULL);\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n\nint ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tint res;\n\n\tlock_sock(sk);\n\tres = __ip6_datagram_connect(sk, uaddr, addr_len);\n\trelease_sock(sk);\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_connect);\n\nint ip6_datagram_connect_v6_only(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t\t int addr_len)\n{\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, uaddr);\n\tif (sin6->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\treturn ip6_datagram_connect(sk, uaddr, addr_len);\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_connect_v6_only);\n\nvoid ipv6_icmp_error(struct sock *sk, struct sk_buff *skb, int err,\n\t\t     __be16 port, u32 info, u8 *payload)\n{\n\tstruct ipv6_pinfo *np  = inet6_sk(sk);\n\tstruct icmp6hdr *icmph = icmp6_hdr(skb);\n\tstruct sock_exterr_skb *serr;\n\n\tif (!np->recverr)\n\t\treturn;\n\n\tskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_ICMP6;\n\tserr->ee.ee_type = icmph->icmp6_type;\n\tserr->ee.ee_code = icmph->icmp6_code;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&(((struct ipv6hdr *)(icmph + 1))->daddr) -\n\t\t\t\t  skb_network_header(skb);\n\tserr->port = port;\n\n\t__skb_pull(skb, payload - skb->data);\n\tskb_reset_transport_header(skb);\n\n\tif (sock_queue_err_skb(sk, skb))\n\t\tkfree_skb(skb);\n}\n\nvoid ipv6_local_error(struct sock *sk, int err, struct flowi6 *fl6, u32 info)\n{\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sock_exterr_skb *serr;\n\tstruct ipv6hdr *iph;\n\tstruct sk_buff *skb;\n\n\tif (!np->recverr)\n\t\treturn;\n\n\tskb = alloc_skb(sizeof(struct ipv6hdr), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\n\tskb_put(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\tiph = ipv6_hdr(skb);\n\tiph->daddr = fl6->daddr;\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_LOCAL;\n\tserr->ee.ee_type = 0;\n\tserr->ee.ee_code = 0;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&iph->daddr - skb_network_header(skb);\n\tserr->port = fl6->fl6_dport;\n\n\t__skb_pull(skb, skb_tail_pointer(skb) - skb->data);\n\tskb_reset_transport_header(skb);\n\n\tif (sock_queue_err_skb(sk, skb))\n\t\tkfree_skb(skb);\n}\n\nvoid ipv6_local_rxpmtu(struct sock *sk, struct flowi6 *fl6, u32 mtu)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6hdr *iph;\n\tstruct sk_buff *skb;\n\tstruct ip6_mtuinfo *mtu_info;\n\n\tif (!np->rxopt.bits.rxpmtu)\n\t\treturn;\n\n\tskb = alloc_skb(sizeof(struct ipv6hdr), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb_put(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\tiph = ipv6_hdr(skb);\n\tiph->daddr = fl6->daddr;\n\n\tmtu_info = IP6CBMTU(skb);\n\n\tmtu_info->ip6m_mtu = mtu;\n\tmtu_info->ip6m_addr.sin6_family = AF_INET6;\n\tmtu_info->ip6m_addr.sin6_port = 0;\n\tmtu_info->ip6m_addr.sin6_flowinfo = 0;\n\tmtu_info->ip6m_addr.sin6_scope_id = fl6->flowi6_oif;\n\tmtu_info->ip6m_addr.sin6_addr = ipv6_hdr(skb)->daddr;\n\n\t__skb_pull(skb, skb_tail_pointer(skb) - skb->data);\n\tskb_reset_transport_header(skb);\n\n\tskb = xchg(&np->rxpmtu, skb);\n\tkfree_skb(skb);\n}\n\n/* For some errors we have valid addr_offset even with zero payload and\n * zero port. Also, addr_offset should be supported if port is set.\n */\nstatic inline bool ipv6_datagram_support_addr(struct sock_exterr_skb *serr)\n{\n\treturn serr->ee.ee_origin == SO_EE_ORIGIN_ICMP6 ||\n\t       serr->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t       serr->ee.ee_origin == SO_EE_ORIGIN_LOCAL || serr->port;\n}\n\n/* IPv6 supports cmsg on all origins aside from SO_EE_ORIGIN_LOCAL.\n *\n * At one point, excluding local errors was a quick test to identify icmp/icmp6\n * errors. This is no longer true, but the test remained, so the v6 stack,\n * unlike v4, also honors cmsg requests on all wifi and timestamp errors.\n *\n * Timestamp code paths do not initialize the fields expected by cmsg:\n * the PKTINFO fields in skb->cb[]. Fill those in here.\n */\nstatic bool ip6_datagram_support_cmsg(struct sk_buff *skb,\n\t\t\t\t      struct sock_exterr_skb *serr)\n{\n\tif (serr->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t    serr->ee.ee_origin == SO_EE_ORIGIN_ICMP6)\n\t\treturn true;\n\n\tif (serr->ee.ee_origin == SO_EE_ORIGIN_LOCAL)\n\t\treturn false;\n\n\tif (!skb->dev)\n\t\treturn false;\n\n\tif (skb->protocol == htons(ETH_P_IPV6))\n\t\tIP6CB(skb)->iif = skb->dev->ifindex;\n\telse\n\t\tPKTINFO_SKB_CB(skb)->ipi_ifindex = skb->dev->ifindex;\n\n\treturn true;\n}\n\n/*\n *\tHandle MSG_ERRQUEUE\n */\nint ipv6_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin, msg->msg_name);\n\tstruct {\n\t\tstruct sock_extended_err ee;\n\t\tstruct sockaddr_in6\t offender;\n\t} errhdr;\n\tint err;\n\tint copied;\n\n\terr = -EAGAIN;\n\tskb = sock_dequeue_err_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\n\tif (sin && ipv6_datagram_support_addr(serr)) {\n\t\tconst unsigned char *nh = skb_network_header(skb);\n\t\tsin->sin6_family = AF_INET6;\n\t\tsin->sin6_flowinfo = 0;\n\t\tsin->sin6_port = serr->port;\n\t\tif (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tconst struct ipv6hdr *ip6h = container_of((struct in6_addr *)(nh + serr->addr_offset),\n\t\t\t\t\t\t\t\t  struct ipv6hdr, daddr);\n\t\t\tsin->sin6_addr = ip6h->daddr;\n\t\t\tif (np->sndflow)\n\t\t\t\tsin->sin6_flowinfo = ip6_flowinfo(ip6h);\n\t\t\tsin->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t} else {\n\t\t\tipv6_addr_set_v4mapped(*(__be32 *)(nh + serr->addr_offset),\n\t\t\t\t\t       &sin->sin6_addr);\n\t\t\tsin->sin6_scope_id = 0;\n\t\t}\n\t\t*addr_len = sizeof(*sin);\n\t}\n\n\tmemcpy(&errhdr.ee, &serr->ee, sizeof(struct sock_extended_err));\n\tsin = &errhdr.offender;\n\tmemset(sin, 0, sizeof(*sin));\n\n\tif (ip6_datagram_support_cmsg(skb, serr)) {\n\t\tsin->sin6_family = AF_INET6;\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\t\tif (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tsin->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tif (np->rxopt.all)\n\t\t\t\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n\t\t\tsin->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t} else {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin->sin6_addr);\n\t\t\tif (inet_sk(sk)->cmsg_flags)\n\t\t\t\tip_cmsg_recv(msg, skb);\n\t\t}\n\t}\n\n\tput_cmsg(msg, SOL_IPV6, IPV6_RECVERR, sizeof(errhdr), &errhdr);\n\n\t/* Now we could try to dump offended packet options */\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ipv6_recv_error);\n\n/*\n *\tHandle IPV6_RECVPATHMTU\n */\nint ipv6_recv_rxpmtu(struct sock *sk, struct msghdr *msg, int len,\n\t\t     int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ip6_mtuinfo mtu_info;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin, msg->msg_name);\n\tint err;\n\tint copied;\n\n\terr = -EAGAIN;\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tmemcpy(&mtu_info, IP6CBMTU(skb), sizeof(mtu_info));\n\n\tif (sin) {\n\t\tsin->sin6_family = AF_INET6;\n\t\tsin->sin6_flowinfo = 0;\n\t\tsin->sin6_port = 0;\n\t\tsin->sin6_scope_id = mtu_info.ip6m_addr.sin6_scope_id;\n\t\tsin->sin6_addr = mtu_info.ip6m_addr.sin6_addr;\n\t\t*addr_len = sizeof(*sin);\n\t}\n\n\tput_cmsg(msg, SOL_IPV6, IPV6_PATHMTU, sizeof(mtu_info), &mtu_info);\n\n\terr = copied;\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\n\n\nvoid ip6_datagram_recv_common_ctl(struct sock *sk, struct msghdr *msg,\n\t\t\t\t struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tbool is_ipv6 = skb->protocol == htons(ETH_P_IPV6);\n\n\tif (np->rxopt.bits.rxinfo) {\n\t\tstruct in6_pktinfo src_info;\n\n\t\tif (is_ipv6) {\n\t\t\tsrc_info.ipi6_ifindex = IP6CB(skb)->iif;\n\t\t\tsrc_info.ipi6_addr = ipv6_hdr(skb)->daddr;\n\t\t} else {\n\t\t\tsrc_info.ipi6_ifindex =\n\t\t\t\tPKTINFO_SKB_CB(skb)->ipi_ifindex;\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->daddr,\n\t\t\t\t\t       &src_info.ipi6_addr);\n\t\t}\n\n\t\tif (src_info.ipi6_ifindex >= 0)\n\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_PKTINFO,\n\t\t\t\t sizeof(src_info), &src_info);\n\t}\n}\n\nvoid ip6_datagram_recv_specific_ctl(struct sock *sk, struct msghdr *msg,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tunsigned char *nh = skb_network_header(skb);\n\n\tif (np->rxopt.bits.rxhlim) {\n\t\tint hlim = ipv6_hdr(skb)->hop_limit;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t}\n\n\tif (np->rxopt.bits.rxtclass) {\n\t\tint tclass = ipv6_get_dsfield(ipv6_hdr(skb));\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t}\n\n\tif (np->rxopt.bits.rxflow) {\n\t\t__be32 flowinfo = ip6_flowinfo((struct ipv6hdr *)nh);\n\t\tif (flowinfo)\n\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t}\n\n\t/* HbH is allowed only once */\n\tif (np->rxopt.bits.hopopts && (opt->flags & IP6SKB_HOPBYHOP)) {\n\t\tu8 *ptr = nh + sizeof(struct ipv6hdr);\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_HOPOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\n\tif (opt->lastopt &&\n\t    (np->rxopt.bits.dstopts || np->rxopt.bits.srcrt)) {\n\t\t/*\n\t\t * Silly enough, but we need to reparse in order to\n\t\t * report extension headers (except for HbH)\n\t\t * in order.\n\t\t *\n\t\t * Also note that IPV6_RECVRTHDRDSTOPTS is NOT\n\t\t * (and WILL NOT be) defined because\n\t\t * IPV6_RECVDSTOPTS is more generic. --yoshfuji\n\t\t */\n\t\tunsigned int off = sizeof(struct ipv6hdr);\n\t\tu8 nexthdr = ipv6_hdr(skb)->nexthdr;\n\n\t\twhile (off <= opt->lastopt) {\n\t\t\tunsigned int len;\n\t\t\tu8 *ptr = nh + off;\n\n\t\t\tswitch (nexthdr) {\n\t\t\tcase IPPROTO_DSTOPTS:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 1) << 3;\n\t\t\t\tif (np->rxopt.bits.dstopts)\n\t\t\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_DSTOPTS, len, ptr);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_ROUTING:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 1) << 3;\n\t\t\t\tif (np->rxopt.bits.srcrt)\n\t\t\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_RTHDR, len, ptr);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_AH:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 2) << 2;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 1) << 3;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\toff += len;\n\t\t}\n\t}\n\n\t/* socket options in old style */\n\tif (np->rxopt.bits.rxoinfo) {\n\t\tstruct in6_pktinfo src_info;\n\n\t\tsrc_info.ipi6_ifindex = opt->iif;\n\t\tsrc_info.ipi6_addr = ipv6_hdr(skb)->daddr;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t}\n\tif (np->rxopt.bits.rxohlim) {\n\t\tint hlim = ipv6_hdr(skb)->hop_limit;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t}\n\tif (np->rxopt.bits.ohopopts && (opt->flags & IP6SKB_HOPBYHOP)) {\n\t\tu8 *ptr = nh + sizeof(struct ipv6hdr);\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292HOPOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\tif (np->rxopt.bits.odstopts && opt->dst0) {\n\t\tu8 *ptr = nh + opt->dst0;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292DSTOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\tif (np->rxopt.bits.osrcrt && opt->srcrt) {\n\t\tstruct ipv6_rt_hdr *rthdr = (struct ipv6_rt_hdr *)(nh + opt->srcrt);\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292RTHDR, (rthdr->hdrlen+1) << 3, rthdr);\n\t}\n\tif (np->rxopt.bits.odstopts && opt->dst1) {\n\t\tu8 *ptr = nh + opt->dst1;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292DSTOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\tif (np->rxopt.bits.rxorigdstaddr) {\n\t\tstruct sockaddr_in6 sin6;\n\t\t__be16 *ports = (__be16 *) skb_transport_header(skb);\n\n\t\tif (skb_transport_offset(skb) + 4 <= skb->len) {\n\t\t\t/* All current transport protocols have the port numbers in the\n\t\t\t * first four bytes of the transport header and this function is\n\t\t\t * written with this assumption in mind.\n\t\t\t */\n\n\t\t\tsin6.sin6_family = AF_INET6;\n\t\t\tsin6.sin6_addr = ipv6_hdr(skb)->daddr;\n\t\t\tsin6.sin6_port = ports[1];\n\t\t\tsin6.sin6_flowinfo = 0;\n\t\t\tsin6.sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t    opt->iif);\n\n\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_ORIGDSTADDR, sizeof(sin6), &sin6);\n\t\t}\n\t}\n}\n\nvoid ip6_datagram_recv_ctl(struct sock *sk, struct msghdr *msg,\n\t\t\t  struct sk_buff *skb)\n{\n\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_recv_ctl);\n\nint ip6_datagram_send_ctl(struct net *net, struct sock *sk,\n\t\t\t  struct msghdr *msg, struct flowi6 *fl6,\n\t\t\t  struct ipv6_txoptions *opt,\n\t\t\t  int *hlimit, int *tclass, int *dontfrag)\n{\n\tstruct in6_pktinfo *src_info;\n\tstruct cmsghdr *cmsg;\n\tstruct ipv6_rt_hdr *rthdr;\n\tstruct ipv6_opt_hdr *hdr;\n\tint len;\n\tint err = 0;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tint addr_type;\n\n\t\tif (!CMSG_OK(msg, cmsg)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto exit_f;\n\t\t}\n\n\t\tif (cmsg->cmsg_level != SOL_IPV6)\n\t\t\tcontinue;\n\n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase IPV6_PKTINFO:\n\t\tcase IPV6_2292PKTINFO:\n\t\t    {\n\t\t\tstruct net_device *dev = NULL;\n\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct in6_pktinfo))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tsrc_info = (struct in6_pktinfo *)CMSG_DATA(cmsg);\n\n\t\t\tif (src_info->ipi6_ifindex) {\n\t\t\t\tif (fl6->flowi6_oif &&\n\t\t\t\t    src_info->ipi6_ifindex != fl6->flowi6_oif)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tfl6->flowi6_oif = src_info->ipi6_ifindex;\n\t\t\t}\n\n\t\t\taddr_type = __ipv6_addr_type(&src_info->ipi6_addr);\n\n\t\t\trcu_read_lock();\n\t\t\tif (fl6->flowi6_oif) {\n\t\t\t\tdev = dev_get_by_index_rcu(net, fl6->flowi6_oif);\n\t\t\t\tif (!dev) {\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\treturn -ENODEV;\n\t\t\t\t}\n\t\t\t} else if (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (addr_type != IPV6_ADDR_ANY) {\n\t\t\t\tint strict = __ipv6_addr_src_scope(addr_type) <= IPV6_ADDR_SCOPE_LINKLOCAL;\n\t\t\t\tif (!(inet_sk(sk)->freebind || inet_sk(sk)->transparent) &&\n\t\t\t\t    !ipv6_chk_addr(net, &src_info->ipi6_addr,\n\t\t\t\t\t\t   strict ? dev : NULL, 0) &&\n\t\t\t\t    !ipv6_chk_acast_addr_src(net, dev,\n\t\t\t\t\t\t\t     &src_info->ipi6_addr))\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\telse\n\t\t\t\t\tfl6->saddr = src_info->ipi6_addr;\n\t\t\t}\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (err)\n\t\t\t\tgoto exit_f;\n\n\t\t\tbreak;\n\t\t    }\n\n\t\tcase IPV6_FLOWINFO:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(4)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tif (fl6->flowlabel&IPV6_FLOWINFO_MASK) {\n\t\t\t\tif ((fl6->flowlabel^*(__be32 *)CMSG_DATA(cmsg))&~IPV6_FLOWINFO_MASK) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto exit_f;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfl6->flowlabel = IPV6_FLOWINFO_MASK & *(__be32 *)CMSG_DATA(cmsg);\n\t\t\tbreak;\n\n\t\tcase IPV6_2292HOPOPTS:\n\t\tcase IPV6_HOPOPTS:\n\t\t\tif (opt->hopopt || cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_opt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\thdr = (struct ipv6_opt_hdr *)CMSG_DATA(cmsg);\n\t\t\tlen = ((hdr->hdrlen + 1) << 3);\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\t\terr = -EPERM;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\topt->opt_nflen += len;\n\t\t\topt->hopopt = hdr;\n\t\t\tbreak;\n\n\t\tcase IPV6_2292DSTOPTS:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_opt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\thdr = (struct ipv6_opt_hdr *)CMSG_DATA(cmsg);\n\t\t\tlen = ((hdr->hdrlen + 1) << 3);\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\t\terr = -EPERM;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (opt->dst1opt) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\topt->opt_flen += len;\n\t\t\topt->dst1opt = hdr;\n\t\t\tbreak;\n\n\t\tcase IPV6_DSTOPTS:\n\t\tcase IPV6_RTHDRDSTOPTS:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_opt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\thdr = (struct ipv6_opt_hdr *)CMSG_DATA(cmsg);\n\t\t\tlen = ((hdr->hdrlen + 1) << 3);\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\t\terr = -EPERM;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (cmsg->cmsg_type == IPV6_DSTOPTS) {\n\t\t\t\topt->opt_flen += len;\n\t\t\t\topt->dst1opt = hdr;\n\t\t\t} else {\n\t\t\t\topt->opt_nflen += len;\n\t\t\t\topt->dst0opt = hdr;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase IPV6_2292RTHDR:\n\t\tcase IPV6_RTHDR:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_rt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\trthdr = (struct ipv6_rt_hdr *)CMSG_DATA(cmsg);\n\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto exit_f;\n\t\t\t\t}\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tlen = ((rthdr->hdrlen + 1) << 3);\n\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\t/* segments left must also match */\n\t\t\tif ((rthdr->hdrlen >> 1) != rthdr->segments_left) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\topt->opt_nflen += len;\n\t\t\topt->srcrt = rthdr;\n\n\t\t\tif (cmsg->cmsg_type == IPV6_2292RTHDR && opt->dst1opt) {\n\t\t\t\tint dsthdrlen = ((opt->dst1opt->hdrlen+1)<<3);\n\n\t\t\t\topt->opt_nflen += dsthdrlen;\n\t\t\t\topt->dst0opt = opt->dst1opt;\n\t\t\t\topt->dst1opt = NULL;\n\t\t\t\topt->opt_flen -= dsthdrlen;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase IPV6_2292HOPLIMIT:\n\t\tcase IPV6_HOPLIMIT:\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(int))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\t*hlimit = *(int *)CMSG_DATA(cmsg);\n\t\t\tif (*hlimit < -1 || *hlimit > 0xff) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase IPV6_TCLASS:\n\t\t    {\n\t\t\tint tc;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(int)))\n\t\t\t\tgoto exit_f;\n\n\t\t\ttc = *(int *)CMSG_DATA(cmsg);\n\t\t\tif (tc < -1 || tc > 0xff)\n\t\t\t\tgoto exit_f;\n\n\t\t\terr = 0;\n\t\t\t*tclass = tc;\n\n\t\t\tbreak;\n\t\t    }\n\n\t\tcase IPV6_DONTFRAG:\n\t\t    {\n\t\t\tint df;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(int)))\n\t\t\t\tgoto exit_f;\n\n\t\t\tdf = *(int *)CMSG_DATA(cmsg);\n\t\t\tif (df < 0 || df > 1)\n\t\t\t\tgoto exit_f;\n\n\t\t\terr = 0;\n\t\t\t*dontfrag = df;\n\n\t\t\tbreak;\n\t\t    }\n\t\tdefault:\n\t\t\tnet_dbg_ratelimited(\"invalid cmsg type: %d\\n\",\n\t\t\t\t\t    cmsg->cmsg_type);\n\t\t\terr = -EINVAL;\n\t\t\tgoto exit_f;\n\t\t}\n\t}\n\nexit_f:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_send_ctl);\n\nvoid ip6_dgram_sock_seq_show(struct seq_file *seq, struct sock *sp,\n\t\t\t     __u16 srcp, __u16 destp, int bucket)\n{\n\tconst struct in6_addr *dest, *src;\n\n\tdest  = &sp->sk_v6_daddr;\n\tsrc   = &sp->sk_v6_rcv_saddr;\n\tseq_printf(seq,\n\t\t   \"%5d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %pK %d\\n\",\n\t\t   bucket,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   sp->sk_state,\n\t\t   sk_wmem_alloc_get(sp),\n\t\t   sk_rmem_alloc_get(sp),\n\t\t   0, 0L, 0,\n\t\t   from_kuid_munged(seq_user_ns(seq), sock_i_uid(sp)),\n\t\t   0,\n\t\t   sock_i_ino(sp),\n\t\t   atomic_read(&sp->sk_refcnt), sp,\n\t\t   atomic_read(&sp->sk_drops));\n}\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tcommon UDP/RAW code\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n */\n\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/in6.h>\n#include <linux/ipv6.h>\n#include <linux/route.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/icmp.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/addrconf.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/tcp_states.h>\n#include <net/dsfield.h>\n#include <net/sock_reuseport.h>\n\n#include <linux/errqueue.h>\n#include <linux/uaccess.h>\n\nstatic bool ipv6_mapped_addr_any(const struct in6_addr *a)\n{\n\treturn ipv6_addr_v4mapped(a) && (a->s6_addr32[3] == 0);\n}\n\nstatic void ip6_datagram_flow_key_init(struct flowi6 *fl6,\n\t\t\t\t       const struct sock *sk)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tint oif = sk->sk_bound_dev_if;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_dport = inet->inet_dport;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->flowlabel = ip6_make_flowinfo(np->tclass, np->flow_label);\n\tfl6->flowi6_uid = sk_uid(sk);\n\n\tif (!oif)\n\t\toif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tif (!oif) {\n\t\tif (ipv6_addr_is_multicast(&fl6->daddr))\n\t\t\toif = READ_ONCE(np->mcast_oif);\n\t\telse\n\t\t\toif = READ_ONCE(np->ucast_oif);\n\t}\n\n\tfl6->flowi6_oif = oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(fl6));\n}\n\nint ip6_datagram_dst_update(struct sock *sk, bool fix_sk_saddr)\n{\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct in6_addr *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct dst_entry *dst;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tint err = 0;\n\n\tif (inet6_test_bit(SNDFLOW, sk) &&\n\t    (np->flow_label & IPV6_FLOWLABEL_MASK)) {\n\t\tflowlabel = fl6_sock_lookup(sk, np->flow_label);\n\t\tif (IS_ERR(flowlabel))\n\t\t\treturn -EINVAL;\n\t}\n\tip6_datagram_flow_key_init(&fl6, sk);\n\n\trcu_read_lock();\n\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\trcu_read_unlock();\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (fix_sk_saddr) {\n\t\tif (ipv6_addr_any(&np->saddr))\n\t\t\tnp->saddr = fl6.saddr;\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tsk->sk_v6_rcv_saddr = fl6.saddr;\n\t\t\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\t}\n\n\tip6_sk_dst_store_flow(sk, dst, &fl6);\n\nout:\n\tfl6_sock_release(flowlabel);\n\treturn err;\n}\n\nvoid ip6_datagram_release_cb(struct sock *sk)\n{\n\tstruct dst_entry *dst;\n\n\tif (ipv6_addr_v4mapped(&sk->sk_v6_daddr))\n\t\treturn;\n\n\trcu_read_lock();\n\tdst = __sk_dst_get(sk);\n\tif (!dst || !READ_ONCE(dst->obsolete) ||\n\t    dst->ops->check(dst, inet6_sk(sk)->dst_cookie)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\trcu_read_unlock();\n\n\tip6_datagram_dst_update(sk, false);\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_release_cb);\n\nint __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t\t*daddr, old_daddr;\n\t__be32\t\t\tfl6_flowlabel = 0;\n\t__be32\t\t\told_fl6_flowlabel;\n\t__be16\t\t\told_dport;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tif (inet6_test_bit(SNDFLOW, sk))\n\t\tfl6_flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tWRITE_ONCE(sk->sk_bound_dev_if, usin->sin6_scope_id);\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tWRITE_ONCE(sk->sk_bound_dev_if, READ_ONCE(np->mcast_oif));\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* save the current peer information before updating it */\n\told_daddr = sk->sk_v6_daddr;\n\told_fl6_flowlabel = np->flow_label;\n\told_dport = inet->inet_dport;\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6_flowlabel;\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\terr = ip6_datagram_dst_update(sk, true);\n\tif (err) {\n\t\t/* Restore the socket peer info, to keep it consistent with\n\t\t * the old socket state\n\t\t */\n\t\tsk->sk_v6_daddr = old_daddr;\n\t\tnp->flow_label = old_fl6_flowlabel;\n\t\tinet->inet_dport = old_dport;\n\t\tgoto out;\n\t}\n\n\treuseport_has_conns_set(sk);\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(__ip6_datagram_connect);\n\nint ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tint res;\n\n\tlock_sock(sk);\n\tres = __ip6_datagram_connect(sk, uaddr, addr_len);\n\trelease_sock(sk);\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_connect);\n\nint ip6_datagram_connect_v6_only(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t\t int addr_len)\n{\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, uaddr);\n\tif (sin6->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\treturn ip6_datagram_connect(sk, uaddr, addr_len);\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_connect_v6_only);\n\nstatic void ipv6_icmp_error_rfc4884(const struct sk_buff *skb,\n\t\t\t\t    struct sock_ee_data_rfc4884 *out)\n{\n\tswitch (icmp6_hdr(skb)->icmp6_type) {\n\tcase ICMPV6_TIME_EXCEED:\n\tcase ICMPV6_DEST_UNREACH:\n\t\tip_icmp_error_rfc4884(skb, out, sizeof(struct icmp6hdr),\n\t\t\t\t      icmp6_hdr(skb)->icmp6_datagram_len * 8);\n\t}\n}\n\nvoid ipv6_icmp_error(struct sock *sk, struct sk_buff *skb, int err,\n\t\t     __be16 port, u32 info, u8 *payload)\n{\n\tstruct icmp6hdr *icmph = icmp6_hdr(skb);\n\tstruct sock_exterr_skb *serr;\n\n\tif (!inet6_test_bit(RECVERR6, sk))\n\t\treturn;\n\n\tskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_ICMP6;\n\tserr->ee.ee_type = icmph->icmp6_type;\n\tserr->ee.ee_code = icmph->icmp6_code;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&(((struct ipv6hdr *)(icmph + 1))->daddr) -\n\t\t\t\t  skb_network_header(skb);\n\tserr->port = port;\n\n\t__skb_pull(skb, payload - skb->data);\n\n\tif (inet6_test_bit(RECVERR6_RFC4884, sk))\n\t\tipv6_icmp_error_rfc4884(skb, &serr->ee.ee_rfc4884);\n\n\tskb_reset_transport_header(skb);\n\n\tif (sock_queue_err_skb(sk, skb))\n\t\tkfree_skb(skb);\n}\nEXPORT_SYMBOL_GPL(ipv6_icmp_error);\n\nvoid ipv6_local_error(struct sock *sk, int err, struct flowi6 *fl6, u32 info)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct ipv6hdr *iph;\n\tstruct sk_buff *skb;\n\n\tif (!inet6_test_bit(RECVERR6, sk))\n\t\treturn;\n\n\tskb = alloc_skb(sizeof(struct ipv6hdr), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\n\tskb_put(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\tiph = ipv6_hdr(skb);\n\tiph->daddr = fl6->daddr;\n\tip6_flow_hdr(iph, 0, 0);\n\n\tserr = SKB_EXT_ERR(skb);\n\tserr->ee.ee_errno = err;\n\tserr->ee.ee_origin = SO_EE_ORIGIN_LOCAL;\n\tserr->ee.ee_type = 0;\n\tserr->ee.ee_code = 0;\n\tserr->ee.ee_pad = 0;\n\tserr->ee.ee_info = info;\n\tserr->ee.ee_data = 0;\n\tserr->addr_offset = (u8 *)&iph->daddr - skb_network_header(skb);\n\tserr->port = fl6->fl6_dport;\n\n\t__skb_pull(skb, skb_tail_pointer(skb) - skb->data);\n\tskb_reset_transport_header(skb);\n\n\tif (sock_queue_err_skb(sk, skb))\n\t\tkfree_skb(skb);\n}\n\nvoid ipv6_local_rxpmtu(struct sock *sk, struct flowi6 *fl6, u32 mtu)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6hdr *iph;\n\tstruct sk_buff *skb;\n\tstruct ip6_mtuinfo *mtu_info;\n\n\tif (!np->rxopt.bits.rxpmtu)\n\t\treturn;\n\n\tskb = alloc_skb(sizeof(struct ipv6hdr), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tskb_put(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\tiph = ipv6_hdr(skb);\n\tiph->daddr = fl6->daddr;\n\n\tmtu_info = IP6CBMTU(skb);\n\n\tmtu_info->ip6m_mtu = mtu;\n\tmtu_info->ip6m_addr.sin6_family = AF_INET6;\n\tmtu_info->ip6m_addr.sin6_port = 0;\n\tmtu_info->ip6m_addr.sin6_flowinfo = 0;\n\tmtu_info->ip6m_addr.sin6_scope_id = fl6->flowi6_oif;\n\tmtu_info->ip6m_addr.sin6_addr = ipv6_hdr(skb)->daddr;\n\n\t__skb_pull(skb, skb_tail_pointer(skb) - skb->data);\n\tskb_reset_transport_header(skb);\n\n\tskb = xchg(&np->rxpmtu, skb);\n\tkfree_skb(skb);\n}\n\n/* For some errors we have valid addr_offset even with zero payload and\n * zero port. Also, addr_offset should be supported if port is set.\n */\nstatic inline bool ipv6_datagram_support_addr(struct sock_exterr_skb *serr)\n{\n\treturn serr->ee.ee_origin == SO_EE_ORIGIN_ICMP6 ||\n\t       serr->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t       serr->ee.ee_origin == SO_EE_ORIGIN_LOCAL || serr->port;\n}\n\n/* IPv6 supports cmsg on all origins aside from SO_EE_ORIGIN_LOCAL.\n *\n * At one point, excluding local errors was a quick test to identify icmp/icmp6\n * errors. This is no longer true, but the test remained, so the v6 stack,\n * unlike v4, also honors cmsg requests on all wifi and timestamp errors.\n */\nstatic bool ip6_datagram_support_cmsg(struct sk_buff *skb,\n\t\t\t\t      struct sock_exterr_skb *serr)\n{\n\tif (serr->ee.ee_origin == SO_EE_ORIGIN_ICMP ||\n\t    serr->ee.ee_origin == SO_EE_ORIGIN_ICMP6)\n\t\treturn true;\n\n\tif (serr->ee.ee_origin == SO_EE_ORIGIN_LOCAL)\n\t\treturn false;\n\n\tif (!IP6CB(skb)->iif)\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n *\tHandle MSG_ERRQUEUE\n */\nint ipv6_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin, msg->msg_name);\n\tstruct {\n\t\tstruct sock_extended_err ee;\n\t\tstruct sockaddr_in6\t offender;\n\t} errhdr;\n\tint err;\n\tint copied;\n\n\terr = -EAGAIN;\n\tskb = sock_dequeue_err_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (unlikely(err)) {\n\t\tkfree_skb(skb);\n\t\treturn err;\n\t}\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\n\tif (sin && ipv6_datagram_support_addr(serr)) {\n\t\tconst unsigned char *nh = skb_network_header(skb);\n\t\tsin->sin6_family = AF_INET6;\n\t\tsin->sin6_flowinfo = 0;\n\t\tsin->sin6_port = serr->port;\n\t\tif (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tconst struct ipv6hdr *ip6h = container_of((struct in6_addr *)(nh + serr->addr_offset),\n\t\t\t\t\t\t\t\t  struct ipv6hdr, daddr);\n\t\t\tsin->sin6_addr = ip6h->daddr;\n\t\t\tif (inet6_test_bit(SNDFLOW, sk))\n\t\t\t\tsin->sin6_flowinfo = ip6_flowinfo(ip6h);\n\t\t\tsin->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t} else {\n\t\t\tipv6_addr_set_v4mapped(*(__be32 *)(nh + serr->addr_offset),\n\t\t\t\t\t       &sin->sin6_addr);\n\t\t\tsin->sin6_scope_id = 0;\n\t\t}\n\t\t*addr_len = sizeof(*sin);\n\t}\n\n\tmemcpy(&errhdr.ee, &serr->ee, sizeof(struct sock_extended_err));\n\tsin = &errhdr.offender;\n\tmemset(sin, 0, sizeof(*sin));\n\n\tif (ip6_datagram_support_cmsg(skb, serr)) {\n\t\tsin->sin6_family = AF_INET6;\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\t\tif (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tsin->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tif (np->rxopt.all)\n\t\t\t\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n\t\t\tsin->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin->sin6_addr,\n\t\t\t\t\t\t    IP6CB(skb)->iif);\n\t\t} else {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin->sin6_addr);\n\t\t\tif (inet_cmsg_flags(inet_sk(sk)))\n\t\t\t\tip_cmsg_recv(msg, skb);\n\t\t}\n\t}\n\n\tput_cmsg(msg, SOL_IPV6, IPV6_RECVERR, sizeof(errhdr), &errhdr);\n\n\t/* Now we could try to dump offended packet options */\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\n\tconsume_skb(skb);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ipv6_recv_error);\n\n/*\n *\tHandle IPV6_RECVPATHMTU\n */\nint ipv6_recv_rxpmtu(struct sock *sk, struct msghdr *msg, int len,\n\t\t     int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct ip6_mtuinfo mtu_info;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin, msg->msg_name);\n\tint err;\n\tint copied;\n\n\terr = -EAGAIN;\n\tskb = xchg(&np->rxpmtu, NULL);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tmemcpy(&mtu_info, IP6CBMTU(skb), sizeof(mtu_info));\n\n\tif (sin) {\n\t\tsin->sin6_family = AF_INET6;\n\t\tsin->sin6_flowinfo = 0;\n\t\tsin->sin6_port = 0;\n\t\tsin->sin6_scope_id = mtu_info.ip6m_addr.sin6_scope_id;\n\t\tsin->sin6_addr = mtu_info.ip6m_addr.sin6_addr;\n\t\t*addr_len = sizeof(*sin);\n\t}\n\n\tput_cmsg(msg, SOL_IPV6, IPV6_PATHMTU, sizeof(mtu_info), &mtu_info);\n\n\terr = copied;\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\n\n\nvoid ip6_datagram_recv_common_ctl(struct sock *sk, struct msghdr *msg,\n\t\t\t\t struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tbool is_ipv6 = skb->protocol == htons(ETH_P_IPV6);\n\n\tif (np->rxopt.bits.rxinfo) {\n\t\tstruct in6_pktinfo src_info;\n\n\t\tif (is_ipv6) {\n\t\t\tsrc_info.ipi6_ifindex = IP6CB(skb)->iif;\n\t\t\tsrc_info.ipi6_addr = ipv6_hdr(skb)->daddr;\n\t\t} else {\n\t\t\tsrc_info.ipi6_ifindex =\n\t\t\t\tPKTINFO_SKB_CB(skb)->ipi_ifindex;\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->daddr,\n\t\t\t\t\t       &src_info.ipi6_addr);\n\t\t}\n\n\t\tif (src_info.ipi6_ifindex >= 0)\n\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_PKTINFO,\n\t\t\t\t sizeof(src_info), &src_info);\n\t}\n}\n\nvoid ip6_datagram_recv_specific_ctl(struct sock *sk, struct msghdr *msg,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tunsigned char *nh = skb_network_header(skb);\n\n\tif (np->rxopt.bits.rxhlim) {\n\t\tint hlim = ipv6_hdr(skb)->hop_limit;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t}\n\n\tif (np->rxopt.bits.rxtclass) {\n\t\tint tclass = ipv6_get_dsfield(ipv6_hdr(skb));\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t}\n\n\tif (np->rxopt.bits.rxflow) {\n\t\t__be32 flowinfo = ip6_flowinfo((struct ipv6hdr *)nh);\n\t\tif (flowinfo)\n\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t}\n\n\t/* HbH is allowed only once */\n\tif (np->rxopt.bits.hopopts && (opt->flags & IP6SKB_HOPBYHOP)) {\n\t\tu8 *ptr = nh + sizeof(struct ipv6hdr);\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_HOPOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\n\tif (opt->lastopt &&\n\t    (np->rxopt.bits.dstopts || np->rxopt.bits.srcrt)) {\n\t\t/*\n\t\t * Silly enough, but we need to reparse in order to\n\t\t * report extension headers (except for HbH)\n\t\t * in order.\n\t\t *\n\t\t * Also note that IPV6_RECVRTHDRDSTOPTS is NOT\n\t\t * (and WILL NOT be) defined because\n\t\t * IPV6_RECVDSTOPTS is more generic. --yoshfuji\n\t\t */\n\t\tunsigned int off = sizeof(struct ipv6hdr);\n\t\tu8 nexthdr = ipv6_hdr(skb)->nexthdr;\n\n\t\twhile (off <= opt->lastopt) {\n\t\t\tunsigned int len;\n\t\t\tu8 *ptr = nh + off;\n\n\t\t\tswitch (nexthdr) {\n\t\t\tcase IPPROTO_DSTOPTS:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 1) << 3;\n\t\t\t\tif (np->rxopt.bits.dstopts)\n\t\t\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_DSTOPTS, len, ptr);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_ROUTING:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 1) << 3;\n\t\t\t\tif (np->rxopt.bits.srcrt)\n\t\t\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_RTHDR, len, ptr);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_AH:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 2) << 2;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnexthdr = ptr[0];\n\t\t\t\tlen = (ptr[1] + 1) << 3;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\toff += len;\n\t\t}\n\t}\n\n\t/* socket options in old style */\n\tif (np->rxopt.bits.rxoinfo) {\n\t\tstruct in6_pktinfo src_info;\n\n\t\tsrc_info.ipi6_ifindex = opt->iif;\n\t\tsrc_info.ipi6_addr = ipv6_hdr(skb)->daddr;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t}\n\tif (np->rxopt.bits.rxohlim) {\n\t\tint hlim = ipv6_hdr(skb)->hop_limit;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t}\n\tif (np->rxopt.bits.ohopopts && (opt->flags & IP6SKB_HOPBYHOP)) {\n\t\tu8 *ptr = nh + sizeof(struct ipv6hdr);\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292HOPOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\tif (np->rxopt.bits.odstopts && opt->dst0) {\n\t\tu8 *ptr = nh + opt->dst0;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292DSTOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\tif (np->rxopt.bits.osrcrt && opt->srcrt) {\n\t\tstruct ipv6_rt_hdr *rthdr = (struct ipv6_rt_hdr *)(nh + opt->srcrt);\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292RTHDR, (rthdr->hdrlen+1) << 3, rthdr);\n\t}\n\tif (np->rxopt.bits.odstopts && opt->dst1) {\n\t\tu8 *ptr = nh + opt->dst1;\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_2292DSTOPTS, (ptr[1]+1)<<3, ptr);\n\t}\n\tif (np->rxopt.bits.rxorigdstaddr) {\n\t\tstruct sockaddr_in6 sin6;\n\t\t__be16 _ports[2], *ports;\n\n\t\tports = skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t\t   sizeof(_ports), &_ports);\n\t\tif (ports) {\n\t\t\t/* All current transport protocols have the port numbers in the\n\t\t\t * first four bytes of the transport header and this function is\n\t\t\t * written with this assumption in mind.\n\t\t\t */\n\t\t\tsin6.sin6_family = AF_INET6;\n\t\t\tsin6.sin6_addr = ipv6_hdr(skb)->daddr;\n\t\t\tsin6.sin6_port = ports[1];\n\t\t\tsin6.sin6_flowinfo = 0;\n\t\t\tsin6.sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t    opt->iif);\n\n\t\t\tput_cmsg(msg, SOL_IPV6, IPV6_ORIGDSTADDR, sizeof(sin6), &sin6);\n\t\t}\n\t}\n\tif (np->rxopt.bits.recvfragsize && opt->frag_max_size) {\n\t\tint val = opt->frag_max_size;\n\n\t\tput_cmsg(msg, SOL_IPV6, IPV6_RECVFRAGSIZE, sizeof(val), &val);\n\t}\n}\n\nvoid ip6_datagram_recv_ctl(struct sock *sk, struct msghdr *msg,\n\t\t\t  struct sk_buff *skb)\n{\n\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_recv_ctl);\n\nint ip6_datagram_send_ctl(struct net *net, struct sock *sk,\n\t\t\t  struct msghdr *msg, struct flowi6 *fl6,\n\t\t\t  struct ipcm6_cookie *ipc6)\n{\n\tstruct in6_pktinfo *src_info;\n\tstruct cmsghdr *cmsg;\n\tstruct ipv6_rt_hdr *rthdr;\n\tstruct ipv6_opt_hdr *hdr;\n\tstruct ipv6_txoptions *opt = ipc6->opt;\n\tint len;\n\tint err = 0;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tint addr_type;\n\n\t\tif (!CMSG_OK(msg, cmsg)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto exit_f;\n\t\t}\n\n\t\tif (cmsg->cmsg_level == SOL_SOCKET) {\n\t\t\terr = __sock_cmsg_send(sk, cmsg, &ipc6->sockc);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (cmsg->cmsg_level != SOL_IPV6)\n\t\t\tcontinue;\n\n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase IPV6_PKTINFO:\n\t\tcase IPV6_2292PKTINFO:\n\t\t    {\n\t\t\tstruct net_device *dev = NULL;\n\t\t\tint src_idx;\n\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct in6_pktinfo))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tsrc_info = (struct in6_pktinfo *)CMSG_DATA(cmsg);\n\t\t\tsrc_idx = src_info->ipi6_ifindex;\n\n\t\t\tif (src_idx) {\n\t\t\t\tif (fl6->flowi6_oif &&\n\t\t\t\t    src_idx != fl6->flowi6_oif &&\n\t\t\t\t    (READ_ONCE(sk->sk_bound_dev_if) != fl6->flowi6_oif ||\n\t\t\t\t     !sk_dev_equal_l3scope(sk, src_idx)))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tfl6->flowi6_oif = src_idx;\n\t\t\t}\n\n\t\t\taddr_type = __ipv6_addr_type(&src_info->ipi6_addr);\n\n\t\t\trcu_read_lock();\n\t\t\tif (fl6->flowi6_oif) {\n\t\t\t\tdev = dev_get_by_index_rcu(net, fl6->flowi6_oif);\n\t\t\t\tif (!dev) {\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\treturn -ENODEV;\n\t\t\t\t}\n\t\t\t} else if (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (addr_type != IPV6_ADDR_ANY) {\n\t\t\t\tint strict = __ipv6_addr_src_scope(addr_type) <= IPV6_ADDR_SCOPE_LINKLOCAL;\n\t\t\t\tif (!ipv6_can_nonlocal_bind(net, inet_sk(sk)) &&\n\t\t\t\t    !ipv6_chk_addr_and_flags(net, &src_info->ipi6_addr,\n\t\t\t\t\t\t\t     dev, !strict, 0,\n\t\t\t\t\t\t\t     IFA_F_TENTATIVE) &&\n\t\t\t\t    !ipv6_chk_acast_addr_src(net, dev,\n\t\t\t\t\t\t\t     &src_info->ipi6_addr))\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\telse\n\t\t\t\t\tfl6->saddr = src_info->ipi6_addr;\n\t\t\t}\n\n\t\t\trcu_read_unlock();\n\n\t\t\tif (err)\n\t\t\t\tgoto exit_f;\n\n\t\t\tbreak;\n\t\t    }\n\n\t\tcase IPV6_FLOWINFO:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(4)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tif (fl6->flowlabel&IPV6_FLOWINFO_MASK) {\n\t\t\t\tif ((fl6->flowlabel^*(__be32 *)CMSG_DATA(cmsg))&~IPV6_FLOWINFO_MASK) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto exit_f;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfl6->flowlabel = IPV6_FLOWINFO_MASK & *(__be32 *)CMSG_DATA(cmsg);\n\t\t\tbreak;\n\n\t\tcase IPV6_2292HOPOPTS:\n\t\tcase IPV6_HOPOPTS:\n\t\t\tif (opt->hopopt || cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_opt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\thdr = (struct ipv6_opt_hdr *)CMSG_DATA(cmsg);\n\t\t\tlen = ((hdr->hdrlen + 1) << 3);\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\t\terr = -EPERM;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\topt->opt_nflen += len;\n\t\t\topt->hopopt = hdr;\n\t\t\tbreak;\n\n\t\tcase IPV6_2292DSTOPTS:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_opt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\thdr = (struct ipv6_opt_hdr *)CMSG_DATA(cmsg);\n\t\t\tlen = ((hdr->hdrlen + 1) << 3);\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\t\terr = -EPERM;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (opt->dst1opt) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\topt->opt_flen += len;\n\t\t\topt->dst1opt = hdr;\n\t\t\tbreak;\n\n\t\tcase IPV6_DSTOPTS:\n\t\tcase IPV6_RTHDRDSTOPTS:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_opt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\thdr = (struct ipv6_opt_hdr *)CMSG_DATA(cmsg);\n\t\t\tlen = ((hdr->hdrlen + 1) << 3);\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (!ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\t\terr = -EPERM;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\t\t\tif (cmsg->cmsg_type == IPV6_DSTOPTS) {\n\t\t\t\topt->opt_flen += len;\n\t\t\t\topt->dst1opt = hdr;\n\t\t\t} else {\n\t\t\t\topt->opt_nflen += len;\n\t\t\t\topt->dst0opt = hdr;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase IPV6_2292RTHDR:\n\t\tcase IPV6_RTHDR:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct ipv6_rt_hdr))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\trthdr = (struct ipv6_rt_hdr *)CMSG_DATA(cmsg);\n\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto exit_f;\n\t\t\t\t}\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tlen = ((rthdr->hdrlen + 1) << 3);\n\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(len)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\t/* segments left must also match */\n\t\t\tif ((rthdr->hdrlen >> 1) != rthdr->segments_left) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\topt->opt_nflen += len;\n\t\t\topt->srcrt = rthdr;\n\n\t\t\tif (cmsg->cmsg_type == IPV6_2292RTHDR && opt->dst1opt) {\n\t\t\t\tint dsthdrlen = ((opt->dst1opt->hdrlen+1)<<3);\n\n\t\t\t\topt->opt_nflen += dsthdrlen;\n\t\t\t\topt->dst0opt = opt->dst1opt;\n\t\t\t\topt->dst1opt = NULL;\n\t\t\t\topt->opt_flen -= dsthdrlen;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase IPV6_2292HOPLIMIT:\n\t\tcase IPV6_HOPLIMIT:\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(int))) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tipc6->hlimit = *(int *)CMSG_DATA(cmsg);\n\t\t\tif (ipc6->hlimit < -1 || ipc6->hlimit > 0xff) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto exit_f;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase IPV6_TCLASS:\n\t\t    {\n\t\t\tint tc;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(int)))\n\t\t\t\tgoto exit_f;\n\n\t\t\ttc = *(int *)CMSG_DATA(cmsg);\n\t\t\tif (tc < -1 || tc > 0xff)\n\t\t\t\tgoto exit_f;\n\n\t\t\terr = 0;\n\t\t\tipc6->tclass = tc;\n\n\t\t\tbreak;\n\t\t    }\n\n\t\tcase IPV6_DONTFRAG:\n\t\t    {\n\t\t\tint df;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(int)))\n\t\t\t\tgoto exit_f;\n\n\t\t\tdf = *(int *)CMSG_DATA(cmsg);\n\t\t\tif (df < 0 || df > 1)\n\t\t\t\tgoto exit_f;\n\n\t\t\terr = 0;\n\t\t\tipc6->dontfrag = df;\n\n\t\t\tbreak;\n\t\t    }\n\t\tdefault:\n\t\t\tnet_dbg_ratelimited(\"invalid cmsg type: %d\\n\",\n\t\t\t\t\t    cmsg->cmsg_type);\n\t\t\terr = -EINVAL;\n\t\t\tgoto exit_f;\n\t\t}\n\t}\n\nexit_f:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ip6_datagram_send_ctl);\n\nvoid __ip6_dgram_sock_seq_show(struct seq_file *seq, struct sock *sp,\n\t\t\t       __u16 srcp, __u16 destp, int rqueue, int bucket)\n{\n\tconst struct in6_addr *dest, *src;\n\n\tdest  = &sp->sk_v6_daddr;\n\tsrc   = &sp->sk_v6_rcv_saddr;\n\tseq_printf(seq,\n\t\t   \"%5d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %pK %u\\n\",\n\t\t   bucket,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   sp->sk_state,\n\t\t   sk_wmem_alloc_get(sp),\n\t\t   rqueue,\n\t\t   0, 0L, 0,\n\t\t   from_kuid_munged(seq_user_ns(seq), sk_uid(sp)),\n\t\t   0,\n\t\t   sock_i_ino(sp),\n\t\t   refcount_read(&sp->sk_refcnt), sp,\n\t\t   atomic_read(&sp->sk_drops));\n}\n", "patch": "@@ -167,8 +167,10 @@ static int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int a\n \n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n-\topt = flowlabel ? flowlabel->opt : np->opt;\n+\trcu_read_lock();\n+\topt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);\n \tfinal_p = fl6_update_dst(&fl6, opt, &final);\n+\trcu_read_unlock();\n \n \tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n \terr = 0;", "file_path": "files/2016_8\\86", "file_language": "c", "file_name": "net/ipv6/datagram.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/exthdrs.c", "code": "/*\n *\tExtension Header handling for IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\tAndi Kleen\t\t<ak@muc.de>\n *\tAlexey Kuznetsov\t<kuznet@ms2.inr.ac.ru>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n/* Changes:\n *\tyoshfuji\t\t: ensure not to overrun while parsing\n *\t\t\t\t  tlv options.\n *\tMitsuru KANDA @USAGI and: Remove ipv6_parse_exthdrs().\n *\tYOSHIFUJI Hideaki @USAGI  Register inbound extension header\n *\t\t\t\t  handlers as inet6_protocol{}.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/netdevice.h>\n#include <linux/in6.h>\n#include <linux/icmpv6.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/rawv6.h>\n#include <net/ndisc.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n#include <net/xfrm.h>\n#endif\n\n#include <linux/uaccess.h>\n\n/*\n *\tParsing tlv encoded headers.\n *\n *\tParsing function \"func\" returns true, if parsing succeed\n *\tand false, if it failed.\n *\tIt MUST NOT touch skb->h.\n */\n\nstruct tlvtype_proc {\n\tint\ttype;\n\tbool\t(*func)(struct sk_buff *skb, int offset);\n};\n\n/*********************\n  Generic functions\n *********************/\n\n/* An unknown option is detected, decide what to do */\n\nstatic bool ip6_tlvopt_unknown(struct sk_buff *skb, int optoff)\n{\n\tswitch ((skb_network_header(skb)[optoff] & 0xC0) >> 6) {\n\tcase 0: /* ignore */\n\t\treturn true;\n\n\tcase 1: /* drop packet */\n\t\tbreak;\n\n\tcase 3: /* Send ICMP if not a multicast address and drop packet */\n\t\t/* Actually, it is redundant check. icmp_send\n\t\t   will recheck in any case.\n\t\t */\n\t\tif (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr))\n\t\t\tbreak;\n\tcase 2: /* send ICMP PARM PROB regardless and drop packet */\n\t\ticmpv6_param_prob(skb, ICMPV6_UNK_OPTION, optoff);\n\t\treturn false;\n\t}\n\n\tkfree_skb(skb);\n\treturn false;\n}\n\n/* Parse tlv encoded option header (hop-by-hop or destination) */\n\nstatic bool ip6_parse_tlv(const struct tlvtype_proc *procs, struct sk_buff *skb)\n{\n\tconst struct tlvtype_proc *curr;\n\tconst unsigned char *nh = skb_network_header(skb);\n\tint off = skb_network_header_len(skb);\n\tint len = (skb_transport_header(skb)[1] + 1) << 3;\n\tint padlen = 0;\n\n\tif (skb_transport_offset(skb) + len > skb_headlen(skb))\n\t\tgoto bad;\n\n\toff += 2;\n\tlen -= 2;\n\n\twhile (len > 0) {\n\t\tint optlen = nh[off + 1] + 2;\n\t\tint i;\n\n\t\tswitch (nh[off]) {\n\t\tcase IPV6_TLV_PAD1:\n\t\t\toptlen = 1;\n\t\t\tpadlen++;\n\t\t\tif (padlen > 7)\n\t\t\t\tgoto bad;\n\t\t\tbreak;\n\n\t\tcase IPV6_TLV_PADN:\n\t\t\t/* RFC 2460 states that the purpose of PadN is\n\t\t\t * to align the containing header to multiples\n\t\t\t * of 8. 7 is therefore the highest valid value.\n\t\t\t * See also RFC 4942, Section 2.1.9.5.\n\t\t\t */\n\t\t\tpadlen += optlen;\n\t\t\tif (padlen > 7)\n\t\t\t\tgoto bad;\n\t\t\t/* RFC 4942 recommends receiving hosts to\n\t\t\t * actively check PadN payload to contain\n\t\t\t * only zeroes.\n\t\t\t */\n\t\t\tfor (i = 2; i < optlen; i++) {\n\t\t\t\tif (nh[off + i] != 0)\n\t\t\t\t\tgoto bad;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault: /* Other TLV code so scan list */\n\t\t\tif (optlen > len)\n\t\t\t\tgoto bad;\n\t\t\tfor (curr = procs; curr->type >= 0; curr++) {\n\t\t\t\tif (curr->type == nh[off]) {\n\t\t\t\t\t/* type specific length/alignment\n\t\t\t\t\t   checks will be performed in the\n\t\t\t\t\t   func(). */\n\t\t\t\t\tif (curr->func(skb, off) == false)\n\t\t\t\t\t\treturn false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (curr->type < 0) {\n\t\t\t\tif (ip6_tlvopt_unknown(skb, off) == 0)\n\t\t\t\t\treturn false;\n\t\t\t}\n\t\t\tpadlen = 0;\n\t\t\tbreak;\n\t\t}\n\t\toff += optlen;\n\t\tlen -= optlen;\n\t}\n\n\tif (len == 0)\n\t\treturn true;\nbad:\n\tkfree_skb(skb);\n\treturn false;\n}\n\n/*****************************\n  Destination options header.\n *****************************/\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\nstatic bool ipv6_dest_hao(struct sk_buff *skb, int optoff)\n{\n\tstruct ipv6_destopt_hao *hao;\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct ipv6hdr *ipv6h = ipv6_hdr(skb);\n\tstruct in6_addr tmp_addr;\n\tint ret;\n\n\tif (opt->dsthao) {\n\t\tnet_dbg_ratelimited(\"hao duplicated\\n\");\n\t\tgoto discard;\n\t}\n\topt->dsthao = opt->dst1;\n\topt->dst1 = 0;\n\n\thao = (struct ipv6_destopt_hao *)(skb_network_header(skb) + optoff);\n\n\tif (hao->length != 16) {\n\t\tnet_dbg_ratelimited(\"hao invalid option length = %d\\n\",\n\t\t\t\t    hao->length);\n\t\tgoto discard;\n\t}\n\n\tif (!(ipv6_addr_type(&hao->addr) & IPV6_ADDR_UNICAST)) {\n\t\tnet_dbg_ratelimited(\"hao is not an unicast addr: %pI6\\n\",\n\t\t\t\t    &hao->addr);\n\t\tgoto discard;\n\t}\n\n\tret = xfrm6_input_addr(skb, (xfrm_address_t *)&ipv6h->daddr,\n\t\t\t       (xfrm_address_t *)&hao->addr, IPPROTO_DSTOPTS);\n\tif (unlikely(ret < 0))\n\t\tgoto discard;\n\n\tif (skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, 0, GFP_ATOMIC))\n\t\t\tgoto discard;\n\n\t\t/* update all variable using below by copied skbuff */\n\t\thao = (struct ipv6_destopt_hao *)(skb_network_header(skb) +\n\t\t\t\t\t\t  optoff);\n\t\tipv6h = ipv6_hdr(skb);\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\ttmp_addr = ipv6h->saddr;\n\tipv6h->saddr = hao->addr;\n\thao->addr = tmp_addr;\n\n\tif (skb->tstamp.tv64 == 0)\n\t\t__net_timestamp(skb);\n\n\treturn true;\n\n discard:\n\tkfree_skb(skb);\n\treturn false;\n}\n#endif\n\nstatic const struct tlvtype_proc tlvprocdestopt_lst[] = {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t{\n\t\t.type\t= IPV6_TLV_HAO,\n\t\t.func\t= ipv6_dest_hao,\n\t},\n#endif\n\t{-1,\t\t\tNULL}\n};\n\nstatic int ipv6_destopt_rcv(struct sk_buff *skb)\n{\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t__u16 dstbuf;\n#endif\n\tstruct dst_entry *dst = skb_dst(skb);\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + 8) ||\n\t    !pskb_may_pull(skb, (skb_transport_offset(skb) +\n\t\t\t\t ((skb_transport_header(skb)[1] + 1) << 3)))) {\n\t\tIP6_INC_STATS_BH(dev_net(dst->dev), ip6_dst_idev(dst),\n\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\topt->lastopt = opt->dst1 = skb_network_header_len(skb);\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\tdstbuf = opt->dst1;\n#endif\n\n\tif (ip6_parse_tlv(tlvprocdestopt_lst, skb)) {\n\t\tskb->transport_header += (skb_transport_header(skb)[1] + 1) << 3;\n\t\topt = IP6CB(skb);\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\topt->nhoff = dstbuf;\n#else\n\t\topt->nhoff = opt->dst1;\n#endif\n\t\treturn 1;\n\t}\n\n\tIP6_INC_STATS_BH(dev_net(dst->dev),\n\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_INHDRERRORS);\n\treturn -1;\n}\n\n/********************************\n  Routing header.\n ********************************/\n\n/* called with rcu_read_lock() */\nstatic int ipv6_rthdr_rcv(struct sk_buff *skb)\n{\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct in6_addr *addr = NULL;\n\tstruct in6_addr daddr;\n\tstruct inet6_dev *idev;\n\tint n, i;\n\tstruct ipv6_rt_hdr *hdr;\n\tstruct rt0_hdr *rthdr;\n\tstruct net *net = dev_net(skb->dev);\n\tint accept_source_route = net->ipv6.devconf_all->accept_source_route;\n\n\tidev = __in6_dev_get(skb->dev);\n\tif (idev && accept_source_route > idev->cnf.accept_source_route)\n\t\taccept_source_route = idev->cnf.accept_source_route;\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + 8) ||\n\t    !pskb_may_pull(skb, (skb_transport_offset(skb) +\n\t\t\t\t ((skb_transport_header(skb)[1] + 1) << 3)))) {\n\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\thdr = (struct ipv6_rt_hdr *)skb_transport_header(skb);\n\n\tif (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr) ||\n\t    skb->pkt_type != PACKET_HOST) {\n\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t IPSTATS_MIB_INADDRERRORS);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\nlooped_back:\n\tif (hdr->segments_left == 0) {\n\t\tswitch (hdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t/* Silently discard type 2 header unless it was\n\t\t\t * processed by own\n\t\t\t */\n\t\t\tif (!addr) {\n\t\t\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t\t IPSTATS_MIB_INADDRERRORS);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\topt->lastopt = opt->srcrt = skb_network_header_len(skb);\n\t\tskb->transport_header += (hdr->hdrlen + 1) << 3;\n\t\topt->dst0 = opt->dst1;\n\t\topt->dst1 = 0;\n\t\topt->nhoff = (&hdr->nexthdr) - skb_network_header(skb);\n\t\treturn 1;\n\t}\n\n\tswitch (hdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\tcase IPV6_SRCRT_TYPE_2:\n\t\tif (accept_source_route < 0)\n\t\t\tgoto unknown_rh;\n\t\t/* Silently discard invalid RTH type 2 */\n\t\tif (hdr->hdrlen != 2 || hdr->segments_left != 1) {\n\t\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n#endif\n\tdefault:\n\t\tgoto unknown_rh;\n\t}\n\n\t/*\n\t *\tThis is the routing header forwarding algorithm from\n\t *\tRFC 2460, page 16.\n\t */\n\n\tn = hdr->hdrlen >> 1;\n\n\tif (hdr->segments_left > n) {\n\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD,\n\t\t\t\t  ((&hdr->segments_left) -\n\t\t\t\t   skb_network_header(skb)));\n\t\treturn -1;\n\t}\n\n\t/* We are about to mangle packet header. Be careful!\n\t   Do not damage packets queued somewhere.\n\t */\n\tif (skb_cloned(skb)) {\n\t\t/* the copy is a forwarded packet */\n\t\tif (pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {\n\t\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t IPSTATS_MIB_OUTDISCARDS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\thdr = (struct ipv6_rt_hdr *)skb_transport_header(skb);\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\ti = n - --hdr->segments_left;\n\n\trthdr = (struct rt0_hdr *) hdr;\n\taddr = rthdr->addr;\n\taddr += i - 1;\n\n\tswitch (hdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\tcase IPV6_SRCRT_TYPE_2:\n\t\tif (xfrm6_input_addr(skb, (xfrm_address_t *)addr,\n\t\t\t\t     (xfrm_address_t *)&ipv6_hdr(skb)->saddr,\n\t\t\t\t     IPPROTO_ROUTING) < 0) {\n\t\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t IPSTATS_MIB_INADDRERRORS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tif (!ipv6_chk_home_addr(dev_net(skb_dst(skb)->dev), addr)) {\n\t\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t IPSTATS_MIB_INADDRERRORS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (ipv6_addr_is_multicast(addr)) {\n\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t IPSTATS_MIB_INADDRERRORS);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\tdaddr = *addr;\n\t*addr = ipv6_hdr(skb)->daddr;\n\tipv6_hdr(skb)->daddr = daddr;\n\n\tskb_dst_drop(skb);\n\tip6_route_input(skb);\n\tif (skb_dst(skb)->error) {\n\t\tskb_push(skb, skb->data - skb_network_header(skb));\n\t\tdst_input(skb);\n\t\treturn -1;\n\t}\n\n\tif (skb_dst(skb)->dev->flags&IFF_LOOPBACK) {\n\t\tif (ipv6_hdr(skb)->hop_limit <= 1) {\n\t\t\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\t\ticmpv6_send(skb, ICMPV6_TIME_EXCEED, ICMPV6_EXC_HOPLIMIT,\n\t\t\t\t    0);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tipv6_hdr(skb)->hop_limit--;\n\t\tgoto looped_back;\n\t}\n\n\tskb_push(skb, skb->data - skb_network_header(skb));\n\tdst_input(skb);\n\treturn -1;\n\nunknown_rh:\n\tIP6_INC_STATS_BH(net, ip6_dst_idev(skb_dst(skb)), IPSTATS_MIB_INHDRERRORS);\n\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD,\n\t\t\t  (&hdr->type) - skb_network_header(skb));\n\treturn -1;\n}\n\nstatic const struct inet6_protocol rthdr_protocol = {\n\t.handler\t=\tipv6_rthdr_rcv,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY,\n};\n\nstatic const struct inet6_protocol destopt_protocol = {\n\t.handler\t=\tipv6_destopt_rcv,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY,\n};\n\nstatic const struct inet6_protocol nodata_protocol = {\n\t.handler\t=\tdst_discard,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY,\n};\n\nint __init ipv6_exthdrs_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&rthdr_protocol, IPPROTO_ROUTING);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_add_protocol(&destopt_protocol, IPPROTO_DSTOPTS);\n\tif (ret)\n\t\tgoto out_rthdr;\n\n\tret = inet6_add_protocol(&nodata_protocol, IPPROTO_NONE);\n\tif (ret)\n\t\tgoto out_destopt;\n\nout:\n\treturn ret;\nout_destopt:\n\tinet6_del_protocol(&destopt_protocol, IPPROTO_DSTOPTS);\nout_rthdr:\n\tinet6_del_protocol(&rthdr_protocol, IPPROTO_ROUTING);\n\tgoto out;\n};\n\nvoid ipv6_exthdrs_exit(void)\n{\n\tinet6_del_protocol(&nodata_protocol, IPPROTO_NONE);\n\tinet6_del_protocol(&destopt_protocol, IPPROTO_DSTOPTS);\n\tinet6_del_protocol(&rthdr_protocol, IPPROTO_ROUTING);\n}\n\n/**********************************\n  Hop-by-hop options.\n **********************************/\n\n/*\n * Note: we cannot rely on skb_dst(skb) before we assign it in ip6_route_input().\n */\nstatic inline struct inet6_dev *ipv6_skb_idev(struct sk_buff *skb)\n{\n\treturn skb_dst(skb) ? ip6_dst_idev(skb_dst(skb)) : __in6_dev_get(skb->dev);\n}\n\nstatic inline struct net *ipv6_skb_net(struct sk_buff *skb)\n{\n\treturn skb_dst(skb) ? dev_net(skb_dst(skb)->dev) : dev_net(skb->dev);\n}\n\n/* Router Alert as of RFC 2711 */\n\nstatic bool ipv6_hop_ra(struct sk_buff *skb, int optoff)\n{\n\tconst unsigned char *nh = skb_network_header(skb);\n\n\tif (nh[optoff + 1] == 2) {\n\t\tIP6CB(skb)->flags |= IP6SKB_ROUTERALERT;\n\t\tmemcpy(&IP6CB(skb)->ra, nh + optoff + 2, sizeof(IP6CB(skb)->ra));\n\t\treturn true;\n\t}\n\tnet_dbg_ratelimited(\"ipv6_hop_ra: wrong RA length %d\\n\",\n\t\t\t    nh[optoff + 1]);\n\tkfree_skb(skb);\n\treturn false;\n}\n\n/* Jumbo payload */\n\nstatic bool ipv6_hop_jumbo(struct sk_buff *skb, int optoff)\n{\n\tconst unsigned char *nh = skb_network_header(skb);\n\tstruct net *net = ipv6_skb_net(skb);\n\tu32 pkt_len;\n\n\tif (nh[optoff + 1] != 4 || (optoff & 3) != 2) {\n\t\tnet_dbg_ratelimited(\"ipv6_hop_jumbo: wrong jumbo opt length/alignment %d\\n\",\n\t\t\t\t    nh[optoff+1]);\n\t\tIP6_INC_STATS_BH(net, ipv6_skb_idev(skb),\n\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\tgoto drop;\n\t}\n\n\tpkt_len = ntohl(*(__be32 *)(nh + optoff + 2));\n\tif (pkt_len <= IPV6_MAXPLEN) {\n\t\tIP6_INC_STATS_BH(net, ipv6_skb_idev(skb),\n\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD, optoff+2);\n\t\treturn false;\n\t}\n\tif (ipv6_hdr(skb)->payload_len) {\n\t\tIP6_INC_STATS_BH(net, ipv6_skb_idev(skb),\n\t\t\t\t IPSTATS_MIB_INHDRERRORS);\n\t\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD, optoff);\n\t\treturn false;\n\t}\n\n\tif (pkt_len > skb->len - sizeof(struct ipv6hdr)) {\n\t\tIP6_INC_STATS_BH(net, ipv6_skb_idev(skb),\n\t\t\t\t IPSTATS_MIB_INTRUNCATEDPKTS);\n\t\tgoto drop;\n\t}\n\n\tif (pskb_trim_rcsum(skb, pkt_len + sizeof(struct ipv6hdr)))\n\t\tgoto drop;\n\n\treturn true;\n\ndrop:\n\tkfree_skb(skb);\n\treturn false;\n}\n\nstatic const struct tlvtype_proc tlvprochopopt_lst[] = {\n\t{\n\t\t.type\t= IPV6_TLV_ROUTERALERT,\n\t\t.func\t= ipv6_hop_ra,\n\t},\n\t{\n\t\t.type\t= IPV6_TLV_JUMBO,\n\t\t.func\t= ipv6_hop_jumbo,\n\t},\n\t{ -1, }\n};\n\nint ipv6_parse_hopopts(struct sk_buff *skb)\n{\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\n\t/*\n\t * skb_network_header(skb) is equal to skb->data, and\n\t * skb_network_header_len(skb) is always equal to\n\t * sizeof(struct ipv6hdr) by definition of\n\t * hop-by-hop options.\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct ipv6hdr) + 8) ||\n\t    !pskb_may_pull(skb, (sizeof(struct ipv6hdr) +\n\t\t\t\t ((skb_transport_header(skb)[1] + 1) << 3)))) {\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\topt->flags |= IP6SKB_HOPBYHOP;\n\tif (ip6_parse_tlv(tlvprochopopt_lst, skb)) {\n\t\tskb->transport_header += (skb_transport_header(skb)[1] + 1) << 3;\n\t\topt = IP6CB(skb);\n\t\topt->nhoff = sizeof(struct ipv6hdr);\n\t\treturn 1;\n\t}\n\treturn -1;\n}\n\n/*\n *\tCreating outbound headers.\n *\n *\t\"build\" functions work when skb is filled from head to tail (datagram)\n *\t\"push\"\tfunctions work when headers are added from tail to head (tcp)\n *\n *\tIn both cases we assume, that caller reserved enough room\n *\tfor headers.\n */\n\nstatic void ipv6_push_rthdr(struct sk_buff *skb, u8 *proto,\n\t\t\t    struct ipv6_rt_hdr *opt,\n\t\t\t    struct in6_addr **addr_p)\n{\n\tstruct rt0_hdr *phdr, *ihdr;\n\tint hops;\n\n\tihdr = (struct rt0_hdr *) opt;\n\n\tphdr = (struct rt0_hdr *) skb_push(skb, (ihdr->rt_hdr.hdrlen + 1) << 3);\n\tmemcpy(phdr, ihdr, sizeof(struct rt0_hdr));\n\n\thops = ihdr->rt_hdr.hdrlen >> 1;\n\n\tif (hops > 1)\n\t\tmemcpy(phdr->addr, ihdr->addr + 1,\n\t\t       (hops - 1) * sizeof(struct in6_addr));\n\n\tphdr->addr[hops - 1] = **addr_p;\n\t*addr_p = ihdr->addr;\n\n\tphdr->rt_hdr.nexthdr = *proto;\n\t*proto = NEXTHDR_ROUTING;\n}\n\nstatic void ipv6_push_exthdr(struct sk_buff *skb, u8 *proto, u8 type, struct ipv6_opt_hdr *opt)\n{\n\tstruct ipv6_opt_hdr *h = (struct ipv6_opt_hdr *)skb_push(skb, ipv6_optlen(opt));\n\n\tmemcpy(h, opt, ipv6_optlen(opt));\n\th->nexthdr = *proto;\n\t*proto = type;\n}\n\nvoid ipv6_push_nfrag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt,\n\t\t\t  u8 *proto,\n\t\t\t  struct in6_addr **daddr)\n{\n\tif (opt->srcrt) {\n\t\tipv6_push_rthdr(skb, proto, opt->srcrt, daddr);\n\t\t/*\n\t\t * IPV6_RTHDRDSTOPTS is ignored\n\t\t * unless IPV6_RTHDR is set (RFC3542).\n\t\t */\n\t\tif (opt->dst0opt)\n\t\t\tipv6_push_exthdr(skb, proto, NEXTHDR_DEST, opt->dst0opt);\n\t}\n\tif (opt->hopopt)\n\t\tipv6_push_exthdr(skb, proto, NEXTHDR_HOP, opt->hopopt);\n}\nEXPORT_SYMBOL(ipv6_push_nfrag_opts);\n\nvoid ipv6_push_frag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt, u8 *proto)\n{\n\tif (opt->dst1opt)\n\t\tipv6_push_exthdr(skb, proto, NEXTHDR_DEST, opt->dst1opt);\n}\n\nstruct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmalloc(sk, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tmemcpy(opt2, opt, opt->tot_len);\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\tatomic_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}\nEXPORT_SYMBOL_GPL(ipv6_dup_options);\n\nstatic int ipv6_renew_option(void *ohdr,\n\t\t\t     struct ipv6_opt_hdr __user *newopt, int newoptlen,\n\t\t\t     int inherit,\n\t\t\t     struct ipv6_opt_hdr **hdr,\n\t\t\t     char **p)\n{\n\tif (inherit) {\n\t\tif (ohdr) {\n\t\t\tmemcpy(*p, ohdr, ipv6_optlen((struct ipv6_opt_hdr *)ohdr));\n\t\t\t*hdr = (struct ipv6_opt_hdr *)*p;\n\t\t\t*p += CMSG_ALIGN(ipv6_optlen(*hdr));\n\t\t}\n\t} else {\n\t\tif (newopt) {\n\t\t\tif (copy_from_user(*p, newopt, newoptlen))\n\t\t\t\treturn -EFAULT;\n\t\t\t*hdr = (struct ipv6_opt_hdr *)*p;\n\t\t\tif (ipv6_optlen(*hdr) > newoptlen)\n\t\t\t\treturn -EINVAL;\n\t\t\t*p += CMSG_ALIGN(newoptlen);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstruct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype,\n\t\t   struct ipv6_opt_hdr __user *newopt, int newoptlen)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\tint err;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt && newoptlen)\n\t\ttot_len += CMSG_ALIGN(newoptlen);\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\tatomic_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\terr = ipv6_renew_option(opt ? opt->hopopt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_HOPOPTS,\n\t\t\t\t&opt2->hopopt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst0opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDRDSTOPTS,\n\t\t\t\t&opt2->dst0opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->srcrt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_RTHDR,\n\t\t\t\t(struct ipv6_opt_hdr **)&opt2->srcrt, &p);\n\tif (err)\n\t\tgoto out;\n\n\terr = ipv6_renew_option(opt ? opt->dst1opt : NULL, newopt, newoptlen,\n\t\t\t\tnewtype != IPV6_DSTOPTS,\n\t\t\t\t&opt2->dst1opt, &p);\n\tif (err)\n\t\tgoto out;\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\nout:\n\tsock_kfree_s(sk, opt2, opt2->tot_len);\n\treturn ERR_PTR(err);\n}\n\nstruct ipv6_txoptions *ipv6_fixup_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t  struct ipv6_txoptions *opt)\n{\n\t/*\n\t * ignore the dest before srcrt unless srcrt is being included.\n\t * --yoshfuji\n\t */\n\tif (opt && opt->dst0opt && !opt->srcrt) {\n\t\tif (opt_space != opt) {\n\t\t\tmemcpy(opt_space, opt, sizeof(*opt_space));\n\t\t\topt = opt_space;\n\t\t}\n\t\topt->opt_nflen -= ipv6_optlen(opt->dst0opt);\n\t\topt->dst0opt = NULL;\n\t}\n\n\treturn opt;\n}\nEXPORT_SYMBOL_GPL(ipv6_fixup_options);\n\n/**\n * fl6_update_dst - update flowi destination address with info given\n *                  by srcrt option, if any.\n *\n * @fl6: flowi6 for which daddr is to be updated\n * @opt: struct ipv6_txoptions in which to look for srcrt opt\n * @orig: copy of original daddr address if modified\n *\n * Returns NULL if no txoptions or no srcrt, otherwise returns orig\n * and initial value of fl6->daddr set in orig\n */\nstruct in6_addr *fl6_update_dst(struct flowi6 *fl6,\n\t\t\t\tconst struct ipv6_txoptions *opt,\n\t\t\t\tstruct in6_addr *orig)\n{\n\tif (!opt || !opt->srcrt)\n\t\treturn NULL;\n\n\t*orig = fl6->daddr;\n\tfl6->daddr = *((struct rt0_hdr *)opt->srcrt)->addr;\n\treturn orig;\n}\nEXPORT_SYMBOL_GPL(fl6_update_dst);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tExtension Header handling for IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\tAndi Kleen\t\t<ak@muc.de>\n *\tAlexey Kuznetsov\t<kuznet@ms2.inr.ac.ru>\n */\n\n/* Changes:\n *\tyoshfuji\t\t: ensure not to overrun while parsing\n *\t\t\t\t  tlv options.\n *\tMitsuru KANDA @USAGI and: Remove ipv6_parse_exthdrs().\n *\tYOSHIFUJI Hideaki @USAGI  Register inbound extension header\n *\t\t\t\t  handlers as inet6_protocol{}.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/netdevice.h>\n#include <linux/in6.h>\n#include <linux/icmpv6.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n\n#include <net/dst.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/rawv6.h>\n#include <net/ndisc.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/calipso.h>\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n#include <net/xfrm.h>\n#endif\n#include <linux/seg6.h>\n#include <net/seg6.h>\n#ifdef CONFIG_IPV6_SEG6_HMAC\n#include <net/seg6_hmac.h>\n#endif\n#include <net/rpl.h>\n#include <linux/ioam6.h>\n#include <linux/ioam6_genl.h>\n#include <net/ioam6.h>\n#include <net/dst_metadata.h>\n\n#include <linux/uaccess.h>\n\n/*********************\n  Generic functions\n *********************/\n\n/* An unknown option is detected, decide what to do */\n\nstatic bool ip6_tlvopt_unknown(struct sk_buff *skb, int optoff,\n\t\t\t       bool disallow_unknowns)\n{\n\tif (disallow_unknowns) {\n\t\t/* If unknown TLVs are disallowed by configuration\n\t\t * then always silently drop packet. Note this also\n\t\t * means no ICMP parameter problem is sent which\n\t\t * could be a good property to mitigate a reflection DOS\n\t\t * attack.\n\t\t */\n\n\t\tgoto drop;\n\t}\n\n\tswitch ((skb_network_header(skb)[optoff] & 0xC0) >> 6) {\n\tcase 0: /* ignore */\n\t\treturn true;\n\n\tcase 1: /* drop packet */\n\t\tbreak;\n\n\tcase 3: /* Send ICMP if not a multicast address and drop packet */\n\t\t/* Actually, it is redundant check. icmp_send\n\t\t   will recheck in any case.\n\t\t */\n\t\tif (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr))\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase 2: /* send ICMP PARM PROB regardless and drop packet */\n\t\ticmpv6_param_prob_reason(skb, ICMPV6_UNK_OPTION, optoff,\n\t\t\t\t\t SKB_DROP_REASON_UNHANDLED_PROTO);\n\t\treturn false;\n\t}\n\ndrop:\n\tkfree_skb_reason(skb, SKB_DROP_REASON_UNHANDLED_PROTO);\n\treturn false;\n}\n\nstatic bool ipv6_hop_ra(struct sk_buff *skb, int optoff);\nstatic bool ipv6_hop_ioam(struct sk_buff *skb, int optoff);\nstatic bool ipv6_hop_jumbo(struct sk_buff *skb, int optoff);\nstatic bool ipv6_hop_calipso(struct sk_buff *skb, int optoff);\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\nstatic bool ipv6_dest_hao(struct sk_buff *skb, int optoff);\n#endif\n\n/* Parse tlv encoded option header (hop-by-hop or destination) */\n\nstatic bool ip6_parse_tlv(bool hopbyhop,\n\t\t\t  struct sk_buff *skb,\n\t\t\t  int max_count)\n{\n\tint len = (skb_transport_header(skb)[1] + 1) << 3;\n\tconst unsigned char *nh = skb_network_header(skb);\n\tint off = skb_network_header_len(skb);\n\tbool disallow_unknowns = false;\n\tint tlv_count = 0;\n\tint padlen = 0;\n\n\tif (unlikely(max_count < 0)) {\n\t\tdisallow_unknowns = true;\n\t\tmax_count = -max_count;\n\t}\n\n\toff += 2;\n\tlen -= 2;\n\n\twhile (len > 0) {\n\t\tint optlen, i;\n\n\t\tif (nh[off] == IPV6_TLV_PAD1) {\n\t\t\tpadlen++;\n\t\t\tif (padlen > 7)\n\t\t\t\tgoto bad;\n\t\t\toff++;\n\t\t\tlen--;\n\t\t\tcontinue;\n\t\t}\n\t\tif (len < 2)\n\t\t\tgoto bad;\n\t\toptlen = nh[off + 1] + 2;\n\t\tif (optlen > len)\n\t\t\tgoto bad;\n\n\t\tif (nh[off] == IPV6_TLV_PADN) {\n\t\t\t/* RFC 2460 states that the purpose of PadN is\n\t\t\t * to align the containing header to multiples\n\t\t\t * of 8. 7 is therefore the highest valid value.\n\t\t\t * See also RFC 4942, Section 2.1.9.5.\n\t\t\t */\n\t\t\tpadlen += optlen;\n\t\t\tif (padlen > 7)\n\t\t\t\tgoto bad;\n\t\t\t/* RFC 4942 recommends receiving hosts to\n\t\t\t * actively check PadN payload to contain\n\t\t\t * only zeroes.\n\t\t\t */\n\t\t\tfor (i = 2; i < optlen; i++) {\n\t\t\t\tif (nh[off + i] != 0)\n\t\t\t\t\tgoto bad;\n\t\t\t}\n\t\t} else {\n\t\t\ttlv_count++;\n\t\t\tif (tlv_count > max_count)\n\t\t\t\tgoto bad;\n\n\t\t\tif (hopbyhop) {\n\t\t\t\tswitch (nh[off]) {\n\t\t\t\tcase IPV6_TLV_ROUTERALERT:\n\t\t\t\t\tif (!ipv6_hop_ra(skb, off))\n\t\t\t\t\t\treturn false;\n\t\t\t\t\tbreak;\n\t\t\t\tcase IPV6_TLV_IOAM:\n\t\t\t\t\tif (!ipv6_hop_ioam(skb, off))\n\t\t\t\t\t\treturn false;\n\n\t\t\t\t\tnh = skb_network_header(skb);\n\t\t\t\t\tbreak;\n\t\t\t\tcase IPV6_TLV_JUMBO:\n\t\t\t\t\tif (!ipv6_hop_jumbo(skb, off))\n\t\t\t\t\t\treturn false;\n\t\t\t\t\tbreak;\n\t\t\t\tcase IPV6_TLV_CALIPSO:\n\t\t\t\t\tif (!ipv6_hop_calipso(skb, off))\n\t\t\t\t\t\treturn false;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tif (!ip6_tlvopt_unknown(skb, off,\n\t\t\t\t\t\t\t\tdisallow_unknowns))\n\t\t\t\t\t\treturn false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tswitch (nh[off]) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\t\tcase IPV6_TLV_HAO:\n\t\t\t\t\tif (!ipv6_dest_hao(skb, off))\n\t\t\t\t\t\treturn false;\n\t\t\t\t\tbreak;\n#endif\n\t\t\t\tdefault:\n\t\t\t\t\tif (!ip6_tlvopt_unknown(skb, off,\n\t\t\t\t\t\t\t\tdisallow_unknowns))\n\t\t\t\t\t\treturn false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tpadlen = 0;\n\t\t}\n\t\toff += optlen;\n\t\tlen -= optlen;\n\t}\n\n\tif (len == 0)\n\t\treturn true;\nbad:\n\tkfree_skb_reason(skb, SKB_DROP_REASON_IP_INHDR);\n\treturn false;\n}\n\n/*****************************\n  Destination options header.\n *****************************/\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\nstatic bool ipv6_dest_hao(struct sk_buff *skb, int optoff)\n{\n\tstruct ipv6_destopt_hao *hao;\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct ipv6hdr *ipv6h = ipv6_hdr(skb);\n\tSKB_DR(reason);\n\tint ret;\n\n\tif (opt->dsthao) {\n\t\tnet_dbg_ratelimited(\"hao duplicated\\n\");\n\t\tgoto discard;\n\t}\n\topt->dsthao = opt->dst1;\n\topt->dst1 = 0;\n\n\thao = (struct ipv6_destopt_hao *)(skb_network_header(skb) + optoff);\n\n\tif (hao->length != 16) {\n\t\tnet_dbg_ratelimited(\"hao invalid option length = %d\\n\",\n\t\t\t\t    hao->length);\n\t\tSKB_DR_SET(reason, IP_INHDR);\n\t\tgoto discard;\n\t}\n\n\tif (!(ipv6_addr_type(&hao->addr) & IPV6_ADDR_UNICAST)) {\n\t\tnet_dbg_ratelimited(\"hao is not an unicast addr: %pI6\\n\",\n\t\t\t\t    &hao->addr);\n\t\tSKB_DR_SET(reason, INVALID_PROTO);\n\t\tgoto discard;\n\t}\n\n\tret = xfrm6_input_addr(skb, (xfrm_address_t *)&ipv6h->daddr,\n\t\t\t       (xfrm_address_t *)&hao->addr, IPPROTO_DSTOPTS);\n\tif (unlikely(ret < 0)) {\n\t\tSKB_DR_SET(reason, XFRM_POLICY);\n\t\tgoto discard;\n\t}\n\n\tif (skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, 0, GFP_ATOMIC))\n\t\t\tgoto discard;\n\n\t\t/* update all variable using below by copied skbuff */\n\t\thao = (struct ipv6_destopt_hao *)(skb_network_header(skb) +\n\t\t\t\t\t\t  optoff);\n\t\tipv6h = ipv6_hdr(skb);\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\tswap(ipv6h->saddr, hao->addr);\n\n\tif (skb->tstamp == 0)\n\t\t__net_timestamp(skb);\n\n\treturn true;\n\n discard:\n\tkfree_skb_reason(skb, reason);\n\treturn false;\n}\n#endif\n\nstatic int ipv6_destopt_rcv(struct sk_buff *skb)\n{\n\tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t__u16 dstbuf;\n#endif\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct net *net = dev_net(skb->dev);\n\tint extlen;\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + 8) ||\n\t    !pskb_may_pull(skb, (skb_transport_offset(skb) +\n\t\t\t\t ((skb_transport_header(skb)[1] + 1) << 3)))) {\n\t\t__IP6_INC_STATS(dev_net(dst_dev(dst)), idev,\n\t\t\t\tIPSTATS_MIB_INHDRERRORS);\nfail_and_free:\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\textlen = (skb_transport_header(skb)[1] + 1) << 3;\n\tif (extlen > net->ipv6.sysctl.max_dst_opts_len)\n\t\tgoto fail_and_free;\n\n\topt->lastopt = opt->dst1 = skb_network_header_len(skb);\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\tdstbuf = opt->dst1;\n#endif\n\n\tif (ip6_parse_tlv(false, skb, net->ipv6.sysctl.max_dst_opts_cnt)) {\n\t\tskb->transport_header += extlen;\n\t\topt = IP6CB(skb);\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\topt->nhoff = dstbuf;\n#else\n\t\topt->nhoff = opt->dst1;\n#endif\n\t\treturn 1;\n\t}\n\n\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\treturn -1;\n}\n\nstatic void seg6_update_csum(struct sk_buff *skb)\n{\n\tstruct ipv6_sr_hdr *hdr;\n\tstruct in6_addr *addr;\n\t__be32 from, to;\n\n\t/* srh is at transport offset and seg_left is already decremented\n\t * but daddr is not yet updated with next segment\n\t */\n\n\thdr = (struct ipv6_sr_hdr *)skb_transport_header(skb);\n\taddr = hdr->segments + hdr->segments_left;\n\n\thdr->segments_left++;\n\tfrom = *(__be32 *)hdr;\n\n\thdr->segments_left--;\n\tto = *(__be32 *)hdr;\n\n\t/* update skb csum with diff resulting from seg_left decrement */\n\n\tupdate_csum_diff4(skb, from, to);\n\n\t/* compute csum diff between current and next segment and update */\n\n\tupdate_csum_diff16(skb, (__be32 *)(&ipv6_hdr(skb)->daddr),\n\t\t\t   (__be32 *)addr);\n}\n\nstatic int ipv6_srh_rcv(struct sk_buff *skb)\n{\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct net *net = dev_net(skb->dev);\n\tstruct ipv6_sr_hdr *hdr;\n\tstruct inet6_dev *idev;\n\tstruct in6_addr *addr;\n\tint accept_seg6;\n\n\thdr = (struct ipv6_sr_hdr *)skb_transport_header(skb);\n\n\tidev = __in6_dev_get(skb->dev);\n\n\taccept_seg6 = min(READ_ONCE(net->ipv6.devconf_all->seg6_enabled),\n\t\t\t  READ_ONCE(idev->cnf.seg6_enabled));\n\n\tif (!accept_seg6) {\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n#ifdef CONFIG_IPV6_SEG6_HMAC\n\tif (!seg6_hmac_validate_skb(skb)) {\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n#endif\n\nlooped_back:\n\tif (hdr->segments_left == 0) {\n\t\tif (hdr->nexthdr == NEXTHDR_IPV6 || hdr->nexthdr == NEXTHDR_IPV4) {\n\t\t\tint offset = (hdr->hdrlen + 1) << 3;\n\n\t\t\tskb_postpull_rcsum(skb, skb_network_header(skb),\n\t\t\t\t\t   skb_network_header_len(skb));\n\t\t\tskb_pull(skb, offset);\n\t\t\tskb_postpull_rcsum(skb, skb_transport_header(skb),\n\t\t\t\t\t   offset);\n\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb_reset_transport_header(skb);\n\t\t\tskb->encapsulation = 0;\n\t\t\tif (hdr->nexthdr == NEXTHDR_IPV4)\n\t\t\t\tskb->protocol = htons(ETH_P_IP);\n\t\t\t__skb_tunnel_rx(skb, skb->dev, net);\n\n\t\t\tnetif_rx(skb);\n\t\t\treturn -1;\n\t\t}\n\n\t\topt->srcrt = skb_network_header_len(skb);\n\t\topt->lastopt = opt->srcrt;\n\t\tskb->transport_header += (hdr->hdrlen + 1) << 3;\n\t\topt->nhoff = (&hdr->nexthdr) - skb_network_header(skb);\n\n\t\treturn 1;\n\t}\n\n\tif (hdr->segments_left >= (hdr->hdrlen >> 1)) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD,\n\t\t\t\t  ((&hdr->segments_left) -\n\t\t\t\t   skb_network_header(skb)));\n\t\treturn -1;\n\t}\n\n\tif (skb_cloned(skb)) {\n\t\tif (pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {\n\t\t\t__IP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\tIPSTATS_MIB_OUTDISCARDS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\n\t\thdr = (struct ipv6_sr_hdr *)skb_transport_header(skb);\n\t}\n\n\thdr->segments_left--;\n\taddr = hdr->segments + hdr->segments_left;\n\n\tskb_push(skb, sizeof(struct ipv6hdr));\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tseg6_update_csum(skb);\n\n\tipv6_hdr(skb)->daddr = *addr;\n\n\tip6_route_input(skb);\n\n\tif (skb_dst(skb)->error) {\n\t\tdst_input(skb);\n\t\treturn -1;\n\t}\n\n\tif (skb_dst_dev(skb)->flags & IFF_LOOPBACK) {\n\t\tif (ipv6_hdr(skb)->hop_limit <= 1) {\n\t\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\t\ticmpv6_send(skb, ICMPV6_TIME_EXCEED,\n\t\t\t\t    ICMPV6_EXC_HOPLIMIT, 0);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tipv6_hdr(skb)->hop_limit--;\n\n\t\tskb_pull(skb, sizeof(struct ipv6hdr));\n\t\tgoto looped_back;\n\t}\n\n\tdst_input(skb);\n\n\treturn -1;\n}\n\nstatic int ipv6_rpl_srh_rcv(struct sk_buff *skb)\n{\n\tstruct ipv6_rpl_sr_hdr *hdr, *ohdr, *chdr;\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct net *net = dev_net(skb->dev);\n\tstruct inet6_dev *idev;\n\tstruct ipv6hdr *oldhdr;\n\tunsigned char *buf;\n\tint accept_rpl_seg;\n\tint i, err;\n\tu64 n = 0;\n\tu32 r;\n\n\tidev = __in6_dev_get(skb->dev);\n\n\taccept_rpl_seg = net->ipv6.devconf_all->rpl_seg_enabled;\n\tif (accept_rpl_seg > idev->cnf.rpl_seg_enabled)\n\t\taccept_rpl_seg = idev->cnf.rpl_seg_enabled;\n\n\tif (!accept_rpl_seg) {\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\nlooped_back:\n\thdr = (struct ipv6_rpl_sr_hdr *)skb_transport_header(skb);\n\n\tif (hdr->segments_left == 0) {\n\t\tif (hdr->nexthdr == NEXTHDR_IPV6) {\n\t\t\tint offset = (hdr->hdrlen + 1) << 3;\n\n\t\t\tskb_postpull_rcsum(skb, skb_network_header(skb),\n\t\t\t\t\t   skb_network_header_len(skb));\n\t\t\tskb_pull(skb, offset);\n\t\t\tskb_postpull_rcsum(skb, skb_transport_header(skb),\n\t\t\t\t\t   offset);\n\n\t\t\tskb_reset_network_header(skb);\n\t\t\tskb_reset_transport_header(skb);\n\t\t\tskb->encapsulation = 0;\n\n\t\t\t__skb_tunnel_rx(skb, skb->dev, net);\n\n\t\t\tnetif_rx(skb);\n\t\t\treturn -1;\n\t\t}\n\n\t\topt->srcrt = skb_network_header_len(skb);\n\t\topt->lastopt = opt->srcrt;\n\t\tskb->transport_header += (hdr->hdrlen + 1) << 3;\n\t\topt->nhoff = (&hdr->nexthdr) - skb_network_header(skb);\n\n\t\treturn 1;\n\t}\n\n\tn = (hdr->hdrlen << 3) - hdr->pad - (16 - hdr->cmpre);\n\tr = do_div(n, (16 - hdr->cmpri));\n\t/* checks if calculation was without remainder and n fits into\n\t * unsigned char which is segments_left field. Should not be\n\t * higher than that.\n\t */\n\tif (r || (n + 1) > 255) {\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\tif (hdr->segments_left > n + 1) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD,\n\t\t\t\t  ((&hdr->segments_left) -\n\t\t\t\t   skb_network_header(skb)));\n\t\treturn -1;\n\t}\n\n\thdr->segments_left--;\n\ti = n - hdr->segments_left;\n\n\tbuf = kcalloc(struct_size(hdr, segments.addr, n + 2), 2, GFP_ATOMIC);\n\tif (unlikely(!buf)) {\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\tohdr = (struct ipv6_rpl_sr_hdr *)buf;\n\tipv6_rpl_srh_decompress(ohdr, hdr, &ipv6_hdr(skb)->daddr, n);\n\tchdr = (struct ipv6_rpl_sr_hdr *)(buf + ((ohdr->hdrlen + 1) << 3));\n\n\tif (ipv6_addr_is_multicast(&ohdr->rpl_segaddr[i])) {\n\t\tkfree_skb(skb);\n\t\tkfree(buf);\n\t\treturn -1;\n\t}\n\n\terr = ipv6_chk_rpl_srh_loop(net, ohdr->rpl_segaddr, n + 1);\n\tif (err) {\n\t\ticmpv6_send(skb, ICMPV6_PARAMPROB, 0, 0);\n\t\tkfree_skb(skb);\n\t\tkfree(buf);\n\t\treturn -1;\n\t}\n\n\tswap(ipv6_hdr(skb)->daddr, ohdr->rpl_segaddr[i]);\n\n\tipv6_rpl_srh_compress(chdr, ohdr, &ipv6_hdr(skb)->daddr, n);\n\n\toldhdr = ipv6_hdr(skb);\n\n\tskb_pull(skb, ((hdr->hdrlen + 1) << 3));\n\tskb_postpull_rcsum(skb, oldhdr,\n\t\t\t   sizeof(struct ipv6hdr) + ((hdr->hdrlen + 1) << 3));\n\tif (unlikely(!hdr->segments_left)) {\n\t\tif (pskb_expand_head(skb, sizeof(struct ipv6hdr) + ((chdr->hdrlen + 1) << 3), 0,\n\t\t\t\t     GFP_ATOMIC)) {\n\t\t\t__IP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)), IPSTATS_MIB_OUTDISCARDS);\n\t\t\tkfree_skb(skb);\n\t\t\tkfree(buf);\n\t\t\treturn -1;\n\t\t}\n\n\t\toldhdr = ipv6_hdr(skb);\n\t}\n\tskb_push(skb, ((chdr->hdrlen + 1) << 3) + sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\tskb_mac_header_rebuild(skb);\n\tskb_set_transport_header(skb, sizeof(struct ipv6hdr));\n\n\tmemmove(ipv6_hdr(skb), oldhdr, sizeof(struct ipv6hdr));\n\tmemcpy(skb_transport_header(skb), chdr, (chdr->hdrlen + 1) << 3);\n\n\tipv6_hdr(skb)->payload_len = htons(skb->len - sizeof(struct ipv6hdr));\n\tskb_postpush_rcsum(skb, ipv6_hdr(skb),\n\t\t\t   sizeof(struct ipv6hdr) + ((chdr->hdrlen + 1) << 3));\n\n\tkfree(buf);\n\n\tip6_route_input(skb);\n\n\tif (skb_dst(skb)->error) {\n\t\tdst_input(skb);\n\t\treturn -1;\n\t}\n\n\tif (skb_dst_dev(skb)->flags & IFF_LOOPBACK) {\n\t\tif (ipv6_hdr(skb)->hop_limit <= 1) {\n\t\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\t\ticmpv6_send(skb, ICMPV6_TIME_EXCEED,\n\t\t\t\t    ICMPV6_EXC_HOPLIMIT, 0);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tipv6_hdr(skb)->hop_limit--;\n\n\t\tskb_pull(skb, sizeof(struct ipv6hdr));\n\t\tgoto looped_back;\n\t}\n\n\tdst_input(skb);\n\n\treturn -1;\n}\n\n/********************************\n  Routing header.\n ********************************/\n\n/* called with rcu_read_lock() */\nstatic int ipv6_rthdr_rcv(struct sk_buff *skb)\n{\n\tstruct inet6_dev *idev = __in6_dev_get(skb->dev);\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct in6_addr *addr = NULL;\n\tint n, i;\n\tstruct ipv6_rt_hdr *hdr;\n\tstruct rt0_hdr *rthdr;\n\tstruct net *net = dev_net(skb->dev);\n\tint accept_source_route;\n\n\taccept_source_route = READ_ONCE(net->ipv6.devconf_all->accept_source_route);\n\n\tif (idev)\n\t\taccept_source_route = min(accept_source_route,\n\t\t\t\t\t  READ_ONCE(idev->cnf.accept_source_route));\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + 8) ||\n\t    !pskb_may_pull(skb, (skb_transport_offset(skb) +\n\t\t\t\t ((skb_transport_header(skb)[1] + 1) << 3)))) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\thdr = (struct ipv6_rt_hdr *)skb_transport_header(skb);\n\n\tif (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr) ||\n\t    skb->pkt_type != PACKET_HOST) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INADDRERRORS);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\tswitch (hdr->type) {\n\tcase IPV6_SRCRT_TYPE_4:\n\t\t/* segment routing */\n\t\treturn ipv6_srh_rcv(skb);\n\tcase IPV6_SRCRT_TYPE_3:\n\t\t/* rpl segment routing */\n\t\treturn ipv6_rpl_srh_rcv(skb);\n\tdefault:\n\t\tbreak;\n\t}\n\nlooped_back:\n\tif (hdr->segments_left == 0) {\n\t\tswitch (hdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t/* Silently discard type 2 header unless it was\n\t\t\t * processed by own\n\t\t\t */\n\t\t\tif (!addr) {\n\t\t\t\t__IP6_INC_STATS(net, idev,\n\t\t\t\t\t\tIPSTATS_MIB_INADDRERRORS);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tbreak;\n#endif\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\topt->lastopt = opt->srcrt = skb_network_header_len(skb);\n\t\tskb->transport_header += (hdr->hdrlen + 1) << 3;\n\t\topt->dst0 = opt->dst1;\n\t\topt->dst1 = 0;\n\t\topt->nhoff = (&hdr->nexthdr) - skb_network_header(skb);\n\t\treturn 1;\n\t}\n\n\tswitch (hdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\tcase IPV6_SRCRT_TYPE_2:\n\t\tif (accept_source_route < 0)\n\t\t\tgoto unknown_rh;\n\t\t/* Silently discard invalid RTH type 2 */\n\t\tif (hdr->hdrlen != 2 || hdr->segments_left != 1) {\n\t\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n#endif\n\tdefault:\n\t\tgoto unknown_rh;\n\t}\n\n\t/*\n\t *\tThis is the routing header forwarding algorithm from\n\t *\tRFC 2460, page 16.\n\t */\n\n\tn = hdr->hdrlen >> 1;\n\n\tif (hdr->segments_left > n) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD,\n\t\t\t\t  ((&hdr->segments_left) -\n\t\t\t\t   skb_network_header(skb)));\n\t\treturn -1;\n\t}\n\n\t/* We are about to mangle packet header. Be careful!\n\t   Do not damage packets queued somewhere.\n\t */\n\tif (skb_cloned(skb)) {\n\t\t/* the copy is a forwarded packet */\n\t\tif (pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {\n\t\t\t__IP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\tIPSTATS_MIB_OUTDISCARDS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\thdr = (struct ipv6_rt_hdr *)skb_transport_header(skb);\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\ti = n - --hdr->segments_left;\n\n\trthdr = (struct rt0_hdr *) hdr;\n\taddr = rthdr->addr;\n\taddr += i - 1;\n\n\tswitch (hdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\tcase IPV6_SRCRT_TYPE_2:\n\t\tif (xfrm6_input_addr(skb, (xfrm_address_t *)addr,\n\t\t\t\t     (xfrm_address_t *)&ipv6_hdr(skb)->saddr,\n\t\t\t\t     IPPROTO_ROUTING) < 0) {\n\t\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INADDRERRORS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tif (!ipv6_chk_home_addr(skb_dst_dev_net(skb), addr)) {\n\t\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INADDRERRORS);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tbreak;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (ipv6_addr_is_multicast(addr)) {\n\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INADDRERRORS);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\tswap(*addr, ipv6_hdr(skb)->daddr);\n\n\tip6_route_input(skb);\n\tif (skb_dst(skb)->error) {\n\t\tskb_push(skb, -skb_network_offset(skb));\n\t\tdst_input(skb);\n\t\treturn -1;\n\t}\n\n\tif (skb_dst_dev(skb)->flags & IFF_LOOPBACK) {\n\t\tif (ipv6_hdr(skb)->hop_limit <= 1) {\n\t\t\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\t\t\ticmpv6_send(skb, ICMPV6_TIME_EXCEED, ICMPV6_EXC_HOPLIMIT,\n\t\t\t\t    0);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -1;\n\t\t}\n\t\tipv6_hdr(skb)->hop_limit--;\n\t\tgoto looped_back;\n\t}\n\n\tskb_push(skb, -skb_network_offset(skb));\n\tdst_input(skb);\n\treturn -1;\n\nunknown_rh:\n\t__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);\n\ticmpv6_param_prob(skb, ICMPV6_HDR_FIELD,\n\t\t\t  (&hdr->type) - skb_network_header(skb));\n\treturn -1;\n}\n\nstatic const struct inet6_protocol rthdr_protocol = {\n\t.handler\t=\tipv6_rthdr_rcv,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY,\n};\n\nstatic const struct inet6_protocol destopt_protocol = {\n\t.handler\t=\tipv6_destopt_rcv,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY,\n};\n\nstatic const struct inet6_protocol nodata_protocol = {\n\t.handler\t=\tdst_discard,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY,\n};\n\nint __init ipv6_exthdrs_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&rthdr_protocol, IPPROTO_ROUTING);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_add_protocol(&destopt_protocol, IPPROTO_DSTOPTS);\n\tif (ret)\n\t\tgoto out_rthdr;\n\n\tret = inet6_add_protocol(&nodata_protocol, IPPROTO_NONE);\n\tif (ret)\n\t\tgoto out_destopt;\n\nout:\n\treturn ret;\nout_destopt:\n\tinet6_del_protocol(&destopt_protocol, IPPROTO_DSTOPTS);\nout_rthdr:\n\tinet6_del_protocol(&rthdr_protocol, IPPROTO_ROUTING);\n\tgoto out;\n};\n\nvoid ipv6_exthdrs_exit(void)\n{\n\tinet6_del_protocol(&nodata_protocol, IPPROTO_NONE);\n\tinet6_del_protocol(&destopt_protocol, IPPROTO_DSTOPTS);\n\tinet6_del_protocol(&rthdr_protocol, IPPROTO_ROUTING);\n}\n\n/**********************************\n  Hop-by-hop options.\n **********************************/\n\n/* Router Alert as of RFC 2711 */\n\nstatic bool ipv6_hop_ra(struct sk_buff *skb, int optoff)\n{\n\tconst unsigned char *nh = skb_network_header(skb);\n\n\tif (nh[optoff + 1] == 2) {\n\t\tIP6CB(skb)->flags |= IP6SKB_ROUTERALERT;\n\t\tmemcpy(&IP6CB(skb)->ra, nh + optoff + 2, sizeof(IP6CB(skb)->ra));\n\t\treturn true;\n\t}\n\tnet_dbg_ratelimited(\"ipv6_hop_ra: wrong RA length %d\\n\",\n\t\t\t    nh[optoff + 1]);\n\tkfree_skb_reason(skb, SKB_DROP_REASON_IP_INHDR);\n\treturn false;\n}\n\n/* IOAM */\n\nstatic bool ipv6_hop_ioam(struct sk_buff *skb, int optoff)\n{\n\tstruct ioam6_trace_hdr *trace;\n\tstruct ioam6_namespace *ns;\n\tstruct ioam6_hdr *hdr;\n\n\t/* Bad alignment (must be 4n-aligned) */\n\tif (optoff & 3)\n\t\tgoto drop;\n\n\t/* Ignore if IOAM is not enabled on ingress */\n\tif (!READ_ONCE(__in6_dev_get(skb->dev)->cnf.ioam6_enabled))\n\t\tgoto ignore;\n\n\t/* Truncated Option header */\n\thdr = (struct ioam6_hdr *)(skb_network_header(skb) + optoff);\n\tif (hdr->opt_len < 2)\n\t\tgoto drop;\n\n\tswitch (hdr->type) {\n\tcase IOAM6_TYPE_PREALLOC:\n\t\t/* Truncated Pre-allocated Trace header */\n\t\tif (hdr->opt_len < 2 + sizeof(*trace))\n\t\t\tgoto drop;\n\n\t\t/* Malformed Pre-allocated Trace header */\n\t\ttrace = (struct ioam6_trace_hdr *)((u8 *)hdr + sizeof(*hdr));\n\t\tif (hdr->opt_len < 2 + sizeof(*trace) + trace->remlen * 4)\n\t\t\tgoto drop;\n\n\t\t/* Ignore if the IOAM namespace is unknown */\n\t\tns = ioam6_namespace(dev_net(skb->dev), trace->namespace_id);\n\t\tif (!ns)\n\t\t\tgoto ignore;\n\n\t\tif (!skb_valid_dst(skb))\n\t\t\tip6_route_input(skb);\n\n\t\t/* About to mangle packet header */\n\t\tif (skb_ensure_writable(skb, optoff + 2 + hdr->opt_len))\n\t\t\tgoto drop;\n\n\t\t/* Trace pointer may have changed */\n\t\ttrace = (struct ioam6_trace_hdr *)(skb_network_header(skb)\n\t\t\t\t\t\t   + optoff + sizeof(*hdr));\n\n\t\tioam6_fill_trace_data(skb, ns, trace, true);\n\n\t\tioam6_event(IOAM6_EVENT_TRACE, dev_net(skb->dev),\n\t\t\t    GFP_ATOMIC, (void *)trace, hdr->opt_len - 2);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\nignore:\n\treturn true;\n\ndrop:\n\tkfree_skb_reason(skb, SKB_DROP_REASON_IP_INHDR);\n\treturn false;\n}\n\n/* Jumbo payload */\n\nstatic bool ipv6_hop_jumbo(struct sk_buff *skb, int optoff)\n{\n\tconst unsigned char *nh = skb_network_header(skb);\n\tSKB_DR(reason);\n\tu32 pkt_len;\n\n\tif (nh[optoff + 1] != 4 || (optoff & 3) != 2) {\n\t\tnet_dbg_ratelimited(\"ipv6_hop_jumbo: wrong jumbo opt length/alignment %d\\n\",\n\t\t\t\t    nh[optoff+1]);\n\t\tSKB_DR_SET(reason, IP_INHDR);\n\t\tgoto drop;\n\t}\n\n\tpkt_len = ntohl(*(__be32 *)(nh + optoff + 2));\n\tif (pkt_len <= IPV6_MAXPLEN) {\n\t\ticmpv6_param_prob_reason(skb, ICMPV6_HDR_FIELD, optoff + 2,\n\t\t\t\t\t SKB_DROP_REASON_IP_INHDR);\n\t\treturn false;\n\t}\n\tif (ipv6_hdr(skb)->payload_len) {\n\t\ticmpv6_param_prob_reason(skb, ICMPV6_HDR_FIELD, optoff,\n\t\t\t\t\t SKB_DROP_REASON_IP_INHDR);\n\t\treturn false;\n\t}\n\n\tif (pkt_len > skb->len - sizeof(struct ipv6hdr)) {\n\t\tSKB_DR_SET(reason, PKT_TOO_SMALL);\n\t\tgoto drop;\n\t}\n\n\tif (pskb_trim_rcsum(skb, pkt_len + sizeof(struct ipv6hdr)))\n\t\tgoto drop;\n\n\tIP6CB(skb)->flags |= IP6SKB_JUMBOGRAM;\n\treturn true;\n\ndrop:\n\tkfree_skb_reason(skb, reason);\n\treturn false;\n}\n\n/* CALIPSO RFC 5570 */\n\nstatic bool ipv6_hop_calipso(struct sk_buff *skb, int optoff)\n{\n\tconst unsigned char *nh = skb_network_header(skb);\n\n\tif (nh[optoff + 1] < 8)\n\t\tgoto drop;\n\n\tif (nh[optoff + 6] * 4 + 8 > nh[optoff + 1])\n\t\tgoto drop;\n\n\tif (!calipso_validate(skb, nh + optoff))\n\t\tgoto drop;\n\n\treturn true;\n\ndrop:\n\tkfree_skb_reason(skb, SKB_DROP_REASON_IP_INHDR);\n\treturn false;\n}\n\nint ipv6_parse_hopopts(struct sk_buff *skb)\n{\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct net *net = dev_net(skb->dev);\n\tint extlen;\n\n\t/*\n\t * skb_network_header(skb) is equal to skb->data, and\n\t * skb_network_header_len(skb) is always equal to\n\t * sizeof(struct ipv6hdr) by definition of\n\t * hop-by-hop options.\n\t */\n\tif (!pskb_may_pull(skb, sizeof(struct ipv6hdr) + 8) ||\n\t    !pskb_may_pull(skb, (sizeof(struct ipv6hdr) +\n\t\t\t\t ((skb_transport_header(skb)[1] + 1) << 3)))) {\nfail_and_free:\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\textlen = (skb_transport_header(skb)[1] + 1) << 3;\n\tif (extlen > net->ipv6.sysctl.max_hbh_opts_len)\n\t\tgoto fail_and_free;\n\n\topt->flags |= IP6SKB_HOPBYHOP;\n\tif (ip6_parse_tlv(true, skb, net->ipv6.sysctl.max_hbh_opts_cnt)) {\n\t\tskb->transport_header += extlen;\n\t\topt = IP6CB(skb);\n\t\topt->nhoff = sizeof(struct ipv6hdr);\n\t\treturn 1;\n\t}\n\treturn -1;\n}\n\n/*\n *\tCreating outbound headers.\n *\n *\t\"build\" functions work when skb is filled from head to tail (datagram)\n *\t\"push\"\tfunctions work when headers are added from tail to head (tcp)\n *\n *\tIn both cases we assume, that caller reserved enough room\n *\tfor headers.\n */\n\nstatic void ipv6_push_rthdr0(struct sk_buff *skb, u8 *proto,\n\t\t\t     struct ipv6_rt_hdr *opt,\n\t\t\t     struct in6_addr **addr_p, struct in6_addr *saddr)\n{\n\tstruct rt0_hdr *phdr, *ihdr;\n\tint hops;\n\n\tihdr = (struct rt0_hdr *) opt;\n\n\tphdr = skb_push(skb, (ihdr->rt_hdr.hdrlen + 1) << 3);\n\tmemcpy(phdr, ihdr, sizeof(struct rt0_hdr));\n\n\thops = ihdr->rt_hdr.hdrlen >> 1;\n\n\tif (hops > 1)\n\t\tmemcpy(phdr->addr, ihdr->addr + 1,\n\t\t       (hops - 1) * sizeof(struct in6_addr));\n\n\tphdr->addr[hops - 1] = **addr_p;\n\t*addr_p = ihdr->addr;\n\n\tphdr->rt_hdr.nexthdr = *proto;\n\t*proto = NEXTHDR_ROUTING;\n}\n\nstatic void ipv6_push_rthdr4(struct sk_buff *skb, u8 *proto,\n\t\t\t     struct ipv6_rt_hdr *opt,\n\t\t\t     struct in6_addr **addr_p, struct in6_addr *saddr)\n{\n\tstruct ipv6_sr_hdr *sr_phdr, *sr_ihdr;\n\tint plen, hops;\n\n\tsr_ihdr = (struct ipv6_sr_hdr *)opt;\n\tplen = (sr_ihdr->hdrlen + 1) << 3;\n\n\tsr_phdr = skb_push(skb, plen);\n\tmemcpy(sr_phdr, sr_ihdr, sizeof(struct ipv6_sr_hdr));\n\n\thops = sr_ihdr->first_segment + 1;\n\tmemcpy(sr_phdr->segments + 1, sr_ihdr->segments + 1,\n\t       (hops - 1) * sizeof(struct in6_addr));\n\n\tsr_phdr->segments[0] = **addr_p;\n\t*addr_p = &sr_ihdr->segments[sr_ihdr->segments_left];\n\n\tif (sr_ihdr->hdrlen > hops * 2) {\n\t\tint tlvs_offset, tlvs_length;\n\n\t\ttlvs_offset = (1 + hops * 2) << 3;\n\t\ttlvs_length = (sr_ihdr->hdrlen - hops * 2) << 3;\n\t\tmemcpy((char *)sr_phdr + tlvs_offset,\n\t\t       (char *)sr_ihdr + tlvs_offset, tlvs_length);\n\t}\n\n#ifdef CONFIG_IPV6_SEG6_HMAC\n\tif (sr_has_hmac(sr_phdr)) {\n\t\tstruct net *net = NULL;\n\n\t\tif (skb->dev)\n\t\t\tnet = dev_net(skb->dev);\n\t\telse if (skb->sk)\n\t\t\tnet = sock_net(skb->sk);\n\n\t\tWARN_ON(!net);\n\n\t\tif (net)\n\t\t\tseg6_push_hmac(net, saddr, sr_phdr);\n\t}\n#endif\n\n\tsr_phdr->nexthdr = *proto;\n\t*proto = NEXTHDR_ROUTING;\n}\n\nstatic void ipv6_push_rthdr(struct sk_buff *skb, u8 *proto,\n\t\t\t    struct ipv6_rt_hdr *opt,\n\t\t\t    struct in6_addr **addr_p, struct in6_addr *saddr)\n{\n\tswitch (opt->type) {\n\tcase IPV6_SRCRT_TYPE_0:\n\tcase IPV6_SRCRT_STRICT:\n\tcase IPV6_SRCRT_TYPE_2:\n\t\tipv6_push_rthdr0(skb, proto, opt, addr_p, saddr);\n\t\tbreak;\n\tcase IPV6_SRCRT_TYPE_4:\n\t\tipv6_push_rthdr4(skb, proto, opt, addr_p, saddr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void ipv6_push_exthdr(struct sk_buff *skb, u8 *proto, u8 type, struct ipv6_opt_hdr *opt)\n{\n\tstruct ipv6_opt_hdr *h = skb_push(skb, ipv6_optlen(opt));\n\n\tmemcpy(h, opt, ipv6_optlen(opt));\n\th->nexthdr = *proto;\n\t*proto = type;\n}\n\nvoid ipv6_push_nfrag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt,\n\t\t\t  u8 *proto,\n\t\t\t  struct in6_addr **daddr, struct in6_addr *saddr)\n{\n\tif (opt->srcrt) {\n\t\tipv6_push_rthdr(skb, proto, opt->srcrt, daddr, saddr);\n\t\t/*\n\t\t * IPV6_RTHDRDSTOPTS is ignored\n\t\t * unless IPV6_RTHDR is set (RFC3542).\n\t\t */\n\t\tif (opt->dst0opt)\n\t\t\tipv6_push_exthdr(skb, proto, NEXTHDR_DEST, opt->dst0opt);\n\t}\n\tif (opt->hopopt)\n\t\tipv6_push_exthdr(skb, proto, NEXTHDR_HOP, opt->hopopt);\n}\n\nvoid ipv6_push_frag_opts(struct sk_buff *skb, struct ipv6_txoptions *opt, u8 *proto)\n{\n\tif (opt->dst1opt)\n\t\tipv6_push_exthdr(skb, proto, NEXTHDR_DEST, opt->dst1opt);\n}\nEXPORT_SYMBOL(ipv6_push_frag_opts);\n\nstruct ipv6_txoptions *\nipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n{\n\tstruct ipv6_txoptions *opt2;\n\n\topt2 = sock_kmemdup(sk, opt, opt->tot_len, GFP_ATOMIC);\n\tif (opt2) {\n\t\tlong dif = (char *)opt2 - (char *)opt;\n\t\tif (opt2->hopopt)\n\t\t\t*((char **)&opt2->hopopt) += dif;\n\t\tif (opt2->dst0opt)\n\t\t\t*((char **)&opt2->dst0opt) += dif;\n\t\tif (opt2->dst1opt)\n\t\t\t*((char **)&opt2->dst1opt) += dif;\n\t\tif (opt2->srcrt)\n\t\t\t*((char **)&opt2->srcrt) += dif;\n\t\trefcount_set(&opt2->refcnt, 1);\n\t}\n\treturn opt2;\n}\nEXPORT_SYMBOL_GPL(ipv6_dup_options);\n\nstatic void ipv6_renew_option(int renewtype,\n\t\t\t      struct ipv6_opt_hdr **dest,\n\t\t\t      struct ipv6_opt_hdr *old,\n\t\t\t      struct ipv6_opt_hdr *new,\n\t\t\t      int newtype, char **p)\n{\n\tstruct ipv6_opt_hdr *src;\n\n\tsrc = (renewtype == newtype ? new : old);\n\tif (!src)\n\t\treturn;\n\n\tmemcpy(*p, src, ipv6_optlen(src));\n\t*dest = (struct ipv6_opt_hdr *)*p;\n\t*p += CMSG_ALIGN(ipv6_optlen(*dest));\n}\n\n/**\n * ipv6_renew_options - replace a specific ext hdr with a new one.\n *\n * @sk: sock from which to allocate memory\n * @opt: original options\n * @newtype: option type to replace in @opt\n * @newopt: new option of type @newtype to replace (user-mem)\n *\n * Returns a new set of options which is a copy of @opt with the\n * option type @newtype replaced with @newopt.\n *\n * @opt may be NULL, in which case a new set of options is returned\n * containing just @newopt.\n *\n * @newopt may be NULL, in which case the specified option type is\n * not copied into the new set of options.\n *\n * The new set of options is allocated from the socket option memory\n * buffer of @sk.\n */\nstruct ipv6_txoptions *\nipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t   int newtype, struct ipv6_opt_hdr *newopt)\n{\n\tint tot_len = 0;\n\tchar *p;\n\tstruct ipv6_txoptions *opt2;\n\n\tif (opt) {\n\t\tif (newtype != IPV6_HOPOPTS && opt->hopopt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->hopopt));\n\t\tif (newtype != IPV6_RTHDRDSTOPTS && opt->dst0opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst0opt));\n\t\tif (newtype != IPV6_RTHDR && opt->srcrt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->srcrt));\n\t\tif (newtype != IPV6_DSTOPTS && opt->dst1opt)\n\t\t\ttot_len += CMSG_ALIGN(ipv6_optlen(opt->dst1opt));\n\t}\n\n\tif (newopt)\n\t\ttot_len += CMSG_ALIGN(ipv6_optlen(newopt));\n\n\tif (!tot_len)\n\t\treturn NULL;\n\n\ttot_len += sizeof(*opt2);\n\topt2 = sock_kmalloc(sk, tot_len, GFP_ATOMIC);\n\tif (!opt2)\n\t\treturn ERR_PTR(-ENOBUFS);\n\n\tmemset(opt2, 0, tot_len);\n\trefcount_set(&opt2->refcnt, 1);\n\topt2->tot_len = tot_len;\n\tp = (char *)(opt2 + 1);\n\n\tipv6_renew_option(IPV6_HOPOPTS, &opt2->hopopt,\n\t\t\t  (opt ? opt->hopopt : NULL),\n\t\t\t  newopt, newtype, &p);\n\tipv6_renew_option(IPV6_RTHDRDSTOPTS, &opt2->dst0opt,\n\t\t\t  (opt ? opt->dst0opt : NULL),\n\t\t\t  newopt, newtype, &p);\n\tipv6_renew_option(IPV6_RTHDR,\n\t\t\t  (struct ipv6_opt_hdr **)&opt2->srcrt,\n\t\t\t  (opt ? (struct ipv6_opt_hdr *)opt->srcrt : NULL),\n\t\t\t  newopt, newtype, &p);\n\tipv6_renew_option(IPV6_DSTOPTS, &opt2->dst1opt,\n\t\t\t  (opt ? opt->dst1opt : NULL),\n\t\t\t  newopt, newtype, &p);\n\n\topt2->opt_nflen = (opt2->hopopt ? ipv6_optlen(opt2->hopopt) : 0) +\n\t\t\t  (opt2->dst0opt ? ipv6_optlen(opt2->dst0opt) : 0) +\n\t\t\t  (opt2->srcrt ? ipv6_optlen(opt2->srcrt) : 0);\n\topt2->opt_flen = (opt2->dst1opt ? ipv6_optlen(opt2->dst1opt) : 0);\n\n\treturn opt2;\n}\n\nstruct ipv6_txoptions *__ipv6_fixup_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t    struct ipv6_txoptions *opt)\n{\n\t/*\n\t * ignore the dest before srcrt unless srcrt is being included.\n\t * --yoshfuji\n\t */\n\tif (opt->dst0opt && !opt->srcrt) {\n\t\tif (opt_space != opt) {\n\t\t\tmemcpy(opt_space, opt, sizeof(*opt_space));\n\t\t\topt = opt_space;\n\t\t}\n\t\topt->opt_nflen -= ipv6_optlen(opt->dst0opt);\n\t\topt->dst0opt = NULL;\n\t}\n\n\treturn opt;\n}\nEXPORT_SYMBOL_GPL(__ipv6_fixup_options);\n\n/**\n * fl6_update_dst - update flowi destination address with info given\n *                  by srcrt option, if any.\n *\n * @fl6: flowi6 for which daddr is to be updated\n * @opt: struct ipv6_txoptions in which to look for srcrt opt\n * @orig: copy of original daddr address if modified\n *\n * Returns NULL if no txoptions or no srcrt, otherwise returns orig\n * and initial value of fl6->daddr set in orig\n */\nstruct in6_addr *fl6_update_dst(struct flowi6 *fl6,\n\t\t\t\tconst struct ipv6_txoptions *opt,\n\t\t\t\tstruct in6_addr *orig)\n{\n\tif (!opt || !opt->srcrt)\n\t\treturn NULL;\n\n\t*orig = fl6->daddr;\n\n\tswitch (opt->srcrt->type) {\n\tcase IPV6_SRCRT_TYPE_0:\n\tcase IPV6_SRCRT_STRICT:\n\tcase IPV6_SRCRT_TYPE_2:\n\t\tfl6->daddr = *((struct rt0_hdr *)opt->srcrt)->addr;\n\t\tbreak;\n\tcase IPV6_SRCRT_TYPE_4:\n\t{\n\t\tstruct ipv6_sr_hdr *srh = (struct ipv6_sr_hdr *)opt->srcrt;\n\n\t\tfl6->daddr = srh->segments[srh->segments_left];\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\treturn orig;\n}\nEXPORT_SYMBOL_GPL(fl6_update_dst);\n", "patch": "@@ -727,6 +727,7 @@ ipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt)\n \t\t\t*((char **)&opt2->dst1opt) += dif;\n \t\tif (opt2->srcrt)\n \t\t\t*((char **)&opt2->srcrt) += dif;\n+\t\tatomic_set(&opt2->refcnt, 1);\n \t}\n \treturn opt2;\n }\n@@ -790,7 +791,7 @@ ipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n \t\treturn ERR_PTR(-ENOBUFS);\n \n \tmemset(opt2, 0, tot_len);\n-\n+\tatomic_set(&opt2->refcnt, 1);\n \topt2->tot_len = tot_len;\n \tp = (char *)(opt2 + 1);\n ", "file_path": "files/2016_8\\87", "file_language": "c", "file_name": "net/ipv6/exthdrs.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/inet6_connection_sock.c", "code": "/*\n * INET        An implementation of the TCP/IP protocol suite for the LINUX\n *             operating system.  INET is implemented using the  BSD Socket\n *             interface as the means of communication with the user level.\n *\n *             Support for INET6 connection oriented protocols.\n *\n * Authors:    See the TCPv6 sources\n *\n *             This program is free software; you can redistribute it and/or\n *             modify it under the terms of the GNU General Public License\n *             as published by the Free Software Foundation; either version\n *             2 of the License, or(at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/in6.h>\n#include <linux/ipv6.h>\n#include <linux/jhash.h>\n#include <linux/slab.h>\n\n#include <net/addrconf.h>\n#include <net/inet_connection_sock.h>\n#include <net/inet_ecn.h>\n#include <net/inet_hashtables.h>\n#include <net/ip6_route.h>\n#include <net/sock.h>\n#include <net/inet6_connection_sock.h>\n\nint inet6_csk_bind_conflict(const struct sock *sk,\n\t\t\t    const struct inet_bind_bucket *tb, bool relax)\n{\n\tconst struct sock *sk2;\n\tint reuse = sk->sk_reuse;\n\tint reuseport = sk->sk_reuseport;\n\tkuid_t uid = sock_i_uid((struct sock *)sk);\n\n\t/* We must walk the whole port owner list in this case. -DaveM */\n\t/*\n\t * See comment in inet_csk_bind_conflict about sock lookup\n\t * vs net namespaces issues.\n\t */\n\tsk_for_each_bound(sk2, &tb->owners) {\n\t\tif (sk != sk2 &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     !sk2->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\n\t\t\tif ((!reuse || !sk2->sk_reuse ||\n\t\t\t     sk2->sk_state == TCP_LISTEN) &&\n\t\t\t    (!reuseport || !sk2->sk_reuseport ||\n\t\t\t     (sk2->sk_state != TCP_TIME_WAIT &&\n\t\t\t      !uid_eq(uid,\n\t\t\t\t      sock_i_uid((struct sock *)sk2))))) {\n\t\t\t\tif (ipv6_rcv_saddr_equal(sk, sk2))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!relax && reuse && sk2->sk_reuse &&\n\t\t\t    sk2->sk_state != TCP_LISTEN &&\n\t\t\t    ipv6_rcv_saddr_equal(sk, sk2))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn sk2 != NULL;\n}\nEXPORT_SYMBOL_GPL(inet6_csk_bind_conflict);\n\nstruct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}\nEXPORT_SYMBOL(inet6_csk_route_req);\n\nvoid inet6_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\n{\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *) uaddr;\n\n\tsin6->sin6_family = AF_INET6;\n\tsin6->sin6_addr = sk->sk_v6_daddr;\n\tsin6->sin6_port\t= inet_sk(sk)->inet_dport;\n\t/* We do not store received flowlabel for TCP */\n\tsin6->sin6_flowinfo = 0;\n\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t  sk->sk_bound_dev_if);\n}\nEXPORT_SYMBOL_GPL(inet6_csk_addr2sockaddr);\n\nstatic inline\nvoid __inet6_csk_dst_store(struct sock *sk, struct dst_entry *dst,\n\t\t\t   const struct in6_addr *daddr,\n\t\t\t   const struct in6_addr *saddr)\n{\n\t__ip6_dst_store(sk, dst, daddr, saddr);\n}\n\nstatic inline\nstruct dst_entry *__inet6_csk_dst_check(struct sock *sk, u32 cookie)\n{\n\treturn __sk_dst_check(sk, cookie);\n}\n\nstatic struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\t__inet6_csk_dst_store(sk, dst, NULL, NULL);\n\t}\n\treturn dst;\n}\n\nint inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tsk->sk_err_soft = -PTR_ERR(dst);\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n\t\t       np->tclass);\n\trcu_read_unlock();\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(inet6_csk_xmit);\n\nstruct dst_entry *inet6_csk_update_pmtu(struct sock *sk, u32 mtu)\n{\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst = inet6_csk_route_socket(sk, &fl6);\n\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\tdst->ops->update_pmtu(dst, sk, NULL, mtu);\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\treturn IS_ERR(dst) ? NULL : dst;\n}\nEXPORT_SYMBOL_GPL(inet6_csk_update_pmtu);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * INET        An implementation of the TCP/IP protocol suite for the LINUX\n *             operating system.  INET is implemented using the  BSD Socket\n *             interface as the means of communication with the user level.\n *\n *             Support for INET6 connection oriented protocols.\n *\n * Authors:    See the TCPv6 sources\n */\n\n#include <linux/module.h>\n#include <linux/in6.h>\n#include <linux/ipv6.h>\n#include <linux/jhash.h>\n#include <linux/slab.h>\n\n#include <net/addrconf.h>\n#include <net/inet_connection_sock.h>\n#include <net/inet_ecn.h>\n#include <net/inet_hashtables.h>\n#include <net/ip6_route.h>\n#include <net/sock.h>\n#include <net/inet6_connection_sock.h>\n#include <net/sock_reuseport.h>\n\nstruct dst_entry *inet6_csk_route_req(const struct sock *sk,\n\t\t\t\t      struct flowi6 *fl6,\n\t\t\t\t      const struct request_sock *req,\n\t\t\t\t      u8 proto)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = proto;\n\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\tfl6->saddr = ireq->ir_v6_loc_addr;\n\tfl6->flowi6_oif = ireq->ir_iif;\n\tfl6->flowi6_mark = ireq->ir_mark;\n\tfl6->fl6_dport = ireq->ir_rmt_port;\n\tfl6->fl6_sport = htons(ireq->ir_num);\n\tfl6->flowi6_uid = sk_uid(sk);\n\tsecurity_req_classify_flow(req, flowi6_to_flowi_common(fl6));\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, fl6, final_p);\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\n\treturn dst;\n}\n\nstatic inline\nstruct dst_entry *__inet6_csk_dst_check(struct sock *sk, u32 cookie)\n{\n\treturn __sk_dst_check(sk, cookie);\n}\n\nstatic struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n\t\t\t\t\t\tstruct flowi6 *fl6)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *final_p, final;\n\tstruct dst_entry *dst;\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->daddr = sk->sk_v6_daddr;\n\tfl6->saddr = np->saddr;\n\tfl6->flowlabel = np->flow_label;\n\tIP6_ECN_flow_xmit(sk, fl6->flowlabel);\n\tfl6->flowi6_oif = sk->sk_bound_dev_if;\n\tfl6->flowi6_mark = sk->sk_mark;\n\tfl6->fl6_sport = inet->inet_sport;\n\tfl6->fl6_dport = inet->inet_dport;\n\tfl6->flowi6_uid = sk_uid(sk);\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(fl6));\n\n\trcu_read_lock();\n\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n\trcu_read_unlock();\n\n\tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n\tif (!dst) {\n\t\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, fl6, final_p);\n\n\t\tif (!IS_ERR(dst))\n\t\t\tip6_dst_store(sk, dst, NULL, false);\n\t}\n\treturn dst;\n}\n\nint inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint res;\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\tif (IS_ERR(dst)) {\n\t\tWRITE_ONCE(sk->sk_err_soft, -PTR_ERR(dst));\n\t\tsk->sk_route_caps = 0;\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(dst);\n\t}\n\n\trcu_read_lock();\n\tskb_dst_set_noref(skb, dst);\n\n\t/* Restore final destination back after routing done */\n\tfl6.daddr = sk->sk_v6_daddr;\n\n\tres = ip6_xmit(sk, skb, &fl6, sk->sk_mark, rcu_dereference(np->opt),\n\t\t       np->tclass, READ_ONCE(sk->sk_priority));\n\trcu_read_unlock();\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(inet6_csk_xmit);\n\nstruct dst_entry *inet6_csk_update_pmtu(struct sock *sk, u32 mtu)\n{\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst = inet6_csk_route_socket(sk, &fl6);\n\n\tif (IS_ERR(dst))\n\t\treturn NULL;\n\tdst->ops->update_pmtu(dst, sk, NULL, mtu, true);\n\n\tdst = inet6_csk_route_socket(sk, &fl6);\n\treturn IS_ERR(dst) ? NULL : dst;\n}\n", "patch": "@@ -78,7 +78,9 @@ struct dst_entry *inet6_csk_route_req(const struct sock *sk,\n \tmemset(fl6, 0, sizeof(*fl6));\n \tfl6->flowi6_proto = proto;\n \tfl6->daddr = ireq->ir_v6_rmt_addr;\n-\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \tfl6->saddr = ireq->ir_v6_loc_addr;\n \tfl6->flowi6_oif = ireq->ir_iif;\n \tfl6->flowi6_mark = ireq->ir_mark;\n@@ -142,7 +144,9 @@ static struct dst_entry *inet6_csk_route_socket(struct sock *sk,\n \tfl6->fl6_dport = inet->inet_dport;\n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\n \n-\tfinal_p = fl6_update_dst(fl6, np->opt, &final);\n+\trcu_read_lock();\n+\tfinal_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);\n+\trcu_read_unlock();\n \n \tdst = __inet6_csk_dst_check(sk, np->dst_cookie);\n \tif (!dst) {\n@@ -175,7 +179,8 @@ int inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused\n \t/* Restore final destination back after routing done */\n \tfl6.daddr = sk->sk_v6_daddr;\n \n-\tres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\n+\tres = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),\n+\t\t       np->tclass);\n \trcu_read_unlock();\n \treturn res;\n }", "file_path": "files/2016_8\\88", "file_language": "c", "file_name": "net/ipv6/inet6_connection_sock.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/ipv6_sockglue.c", "code": "/*\n *\tIPv6 BSD socket options interface\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/net/ipv4/ip_sockglue.c\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n *\n *\tFIXME: Make the setsockopt code POSIX compliant: That is\n *\n *\to\tTruncate getsockopt returns\n *\to\tReturn an optlen of the truncated length if need be\n *\n *\tChanges:\n *\tDavid L Stevens <dlstevens@us.ibm.com>:\n *\t\t- added multicast source filtering API for MLDv2\n */\n\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/mroute6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/init.h>\n#include <linux/sysctl.h>\n#include <linux/netfilter.h>\n#include <linux/slab.h>\n\n#include <net/sock.h>\n#include <net/snmp.h>\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/inet_common.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <net/xfrm.h>\n#include <net/compat.h>\n\n#include <asm/uaccess.h>\n\nstruct ip6_ra_chain *ip6_ra_chain;\nDEFINE_RWLOCK(ip6_ra_lock);\n\nint ip6_ra_control(struct sock *sk, int sel)\n{\n\tstruct ip6_ra_chain *ra, *new_ra, **rap;\n\n\t/* RA packet may be delivered ONLY to IPPROTO_RAW socket */\n\tif (sk->sk_type != SOCK_RAW || inet_sk(sk)->inet_num != IPPROTO_RAW)\n\t\treturn -ENOPROTOOPT;\n\n\tnew_ra = (sel >= 0) ? kmalloc(sizeof(*new_ra), GFP_KERNEL) : NULL;\n\n\twrite_lock_bh(&ip6_ra_lock);\n\tfor (rap = &ip6_ra_chain; (ra = *rap) != NULL; rap = &ra->next) {\n\t\tif (ra->sk == sk) {\n\t\t\tif (sel >= 0) {\n\t\t\t\twrite_unlock_bh(&ip6_ra_lock);\n\t\t\t\tkfree(new_ra);\n\t\t\t\treturn -EADDRINUSE;\n\t\t\t}\n\n\t\t\t*rap = ra->next;\n\t\t\twrite_unlock_bh(&ip6_ra_lock);\n\n\t\t\tsock_put(sk);\n\t\t\tkfree(ra);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (!new_ra) {\n\t\twrite_unlock_bh(&ip6_ra_lock);\n\t\treturn -ENOBUFS;\n\t}\n\tnew_ra->sk = sk;\n\tnew_ra->sel = sel;\n\tnew_ra->next = ra;\n\t*rap = new_ra;\n\tsock_hold(sk);\n\twrite_unlock_bh(&ip6_ra_lock);\n\treturn 0;\n}\n\nstatic\nstruct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_sk(sk)->is_icsk) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,\n\t\t   opt);\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}\n\nstatic bool setsockopt_needs_rtnl(int optname)\n{\n\tswitch (optname) {\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\tcase MCAST_MSFILTER:\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, valbool;\n\tint retv = -ENOPROTOOPT;\n\tbool needs_rtnl = setsockopt_needs_rtnl(optname);\n\n\tif (!optval)\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (get_user(val, (int __user *) optval))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\tif (needs_rtnl)\n\t\trtnl_lock();\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tstruct ipv6_txoptions *opt;\n\t\t\tstruct sk_buff *pktopt;\n\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol != IPPROTO_TCP)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tfl6_free_socklist(sk);\n\t\t\tipv6_sock_mc_close(sk);\n\n\t\t\t/*\n\t\t\t * Sock is moving from IPv6 to IPv4 (sk_prot), so\n\t\t\t * remove it from the refcnt debug socks count in the\n\t\t\t * original family...\n\t\t\t */\n\t\t\tsk_refcnt_debug_dec(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = &tcp_prot;\n\t\t\t\ticsk->icsk_af_ops = &ipv4_specific;\n\t\t\t\tsk->sk_socket->ops = &inet_stream_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\t\t\t\tlocal_bh_disable();\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\t\t\t\tlocal_bh_enable();\n\t\t\t\tsk->sk_prot = prot;\n\t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n\t\t\t\tsk->sk_family = PF_INET;\n\t\t\t}\n\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n\t\t\t\t   NULL);\n\t\t\tif (opt) {\n\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\t\ttxopt_put(opt);\n\t\t\t}\n\t\t\tpktopt = xchg(&np->pktoptions, NULL);\n\t\t\tkfree_skb(pktopt);\n\n\t\t\tsk->sk_destruct = inet_sock_destruct;\n\t\t\t/*\n\t\t\t * ... and add it to the refcnt debug socks count\n\t\t\t * in the new family. -acme\n\t\t\t */\n\t\t\tsk_refcnt_debug_inc(sk);\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tnp->tclass = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !ns_capable(net->user_ns, CAP_NET_ADMIN) &&\n\t\t    !ns_capable(net->user_ns, CAP_NET_RAW)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_sk(sk)->transparent = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\t/* remove any sticky options header with a zero option\n\t\t * length, per RFC3542.\n\t\t */\n\t\tif (optlen == 0)\n\t\t\toptval = NULL;\n\t\telse if (!optval)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t\t optlen & 0x7 || optlen > 8 * 255)\n\t\t\tgoto e_inval;\n\n\t\t/* hop-by-hop / destination options are privileged option */\n\t\tretv = -EPERM;\n\t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n\t\t\tbreak;\n\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\topt = ipv6_renew_options(sk, opt, optname,\n\t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n\t\t\t\t\t optlen);\n\t\tif (IS_ERR(opt)) {\n\t\t\tretv = PTR_ERR(opt);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* routing header option needs extra check */\n\t\tretv = -EINVAL;\n\t\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\t\tif (rthdr->hdrlen != 2 ||\n\t\t\t\t    rthdr->segments_left != 1)\n\t\t\t\t\tgoto sticky_done;\n\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tgoto sticky_done;\n\t\t\t}\n\t\t}\n\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) || !optval)\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_user(&pkt, optval, sizeof(struct in6_pktinfo))) {\n\t\t\t\tretv = -EFAULT;\n\t\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_bound_dev_if && pkt.ipi6_ifindex != sk->sk_bound_dev_if)\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tint junk;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\tatomic_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(opt+1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control = (void *)(opt+1);\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, opt, &junk,\n\t\t\t\t\t     &junk, &junk);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->hop_limit = val;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val > 255 || val < -1)\n\t\t\tgoto e_inval;\n\t\tnp->mcast_hops = (val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val != valbool)\n\t\t\tgoto e_inval;\n\t\tnp->mc_loop = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev = NULL;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (ifindex == 0) {\n\t\t\tnp->ucast_oif = 0;\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tretv = -EADDRNOTAVAIL;\n\t\tif (!dev)\n\t\t\tbreak;\n\t\tdev_put(dev);\n\n\t\tretv = -EINVAL;\n\t\tif (sk->sk_bound_dev_if)\n\t\t\tbreak;\n\n\t\tnp->ucast_oif = ifindex;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\tbreak;\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != val)\n\t\t\t\tgoto e_inval;\n\n\t\t\tdev = dev_get_by_index(net, val);\n\t\t\tif (!dev) {\n\t\t\t\tretv = -ENODEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdev_put(dev);\n\t\t}\n\t\tnp->mcast_oif = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_sk(sk)->is_icsk)\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t{\n\t\tstruct group_req greq;\n\t\tstruct sockaddr_in6 *psin6;\n\n\t\tif (optlen < sizeof(struct group_req))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(&greq, optval, sizeof(struct group_req)))\n\t\t\tbreak;\n\t\tif (greq.gr_group.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\t\tif (optname == MCAST_JOIN_GROUP)\n\t\t\tretv = ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, greq.gr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t{\n\t\tstruct group_source_req greqs;\n\t\tint omode, add;\n\n\t\tif (optlen < sizeof(struct group_source_req))\n\t\t\tgoto e_inval;\n\t\tif (copy_from_user(&greqs, optval, sizeof(greqs))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t\t    greqs.gsr_source.ss_family != AF_INET6) {\n\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\tbreak;\n\t\t}\n\t\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 1;\n\t\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\t\tomode = MCAST_EXCLUDE;\n\t\t\tadd = 0;\n\t\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\t\tstruct sockaddr_in6 *psin6;\n\n\t\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\t\tretv = ipv6_sock_mc_join(sk, greqs.gsr_interface,\n\t\t\t\t\t\t &psin6->sin6_addr);\n\t\t\t/* prior join w/ different source is ok */\n\t\t\tif (retv && retv != -EADDRINUSE)\n\t\t\t\tbreak;\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 1;\n\t\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\t\tomode = MCAST_INCLUDE;\n\t\t\tadd = 0;\n\t\t}\n\t\tretv = ip6_mc_source(add, omode, sk, &greqs);\n\t\tbreak;\n\t}\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter *gsf;\n\n\t\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\t\tgoto e_inval;\n\t\tif (optlen > sysctl_optmem_max) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tgsf = kmalloc(optlen, GFP_KERNEL);\n\t\tif (!gsf) {\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tretv = -EFAULT;\n\t\tif (copy_from_user(gsf, optval, optlen)) {\n\t\t\tkfree(gsf);\n\t\t\tbreak;\n\t\t}\n\t\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\t\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t\t    gsf->gf_numsrc > sysctl_mld_max_msf) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\t\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen) {\n\t\t\tkfree(gsf);\n\t\t\tretv = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tretv = ip6_mc_msfilter(sk, gsf);\n\t\tkfree(gsf);\n\n\t\tbreak;\n\t}\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tbreak;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\tgoto e_inval;\n\t\tnp->pmtudisc = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\tgoto e_inval;\n\t\tnp->frag_size = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->recverr = valbool;\n\t\tif (!val)\n\t\t\tskb_queue_purge(&sk->sk_error_queue);\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->sndflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t    {\n\t\tunsigned int pref = 0;\n\t\tunsigned int prefmask = ~0;\n\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EINVAL;\n\n\t\t/* check PUBLIC/TMP/PUBTMP_DEFAULT conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_PUBLIC|\n\t\t\t       IPV6_PREFER_SRC_TMP|\n\t\t\t       IPV6_PREFER_SRC_PUBTMP_DEFAULT)) {\n\t\tcase IPV6_PREFER_SRC_PUBLIC:\n\t\t\tpref |= IPV6_PREFER_SRC_PUBLIC;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_TMP:\n\t\t\tpref |= IPV6_PREFER_SRC_TMP;\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_PUBTMP_DEFAULT:\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tgoto pref_skip_pubtmp;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~(IPV6_PREFER_SRC_PUBLIC|\n\t\t\t      IPV6_PREFER_SRC_TMP);\npref_skip_pubtmp:\n\n\t\t/* check HOME/COA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_HOME|IPV6_PREFER_SRC_COA)) {\n\t\tcase IPV6_PREFER_SRC_HOME:\n\t\t\tbreak;\n\t\tcase IPV6_PREFER_SRC_COA:\n\t\t\tpref |= IPV6_PREFER_SRC_COA;\n\t\tcase 0:\n\t\t\tgoto pref_skip_coa;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tprefmask &= ~IPV6_PREFER_SRC_COA;\npref_skip_coa:\n\n\t\t/* check CGA/NONCGA conflicts */\n\t\tswitch (val & (IPV6_PREFER_SRC_CGA|IPV6_PREFER_SRC_NONCGA)) {\n\t\tcase IPV6_PREFER_SRC_CGA:\n\t\tcase IPV6_PREFER_SRC_NONCGA:\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto e_inval;\n\t\t}\n\n\t\tnp->srcprefs = (np->srcprefs & prefmask) | pref;\n\t\tretv = 0;\n\n\t\tbreak;\n\t    }\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < 0 || val > 255)\n\t\t\tgoto e_inval;\n\t\tnp->min_hopcount = val;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_DONTFRAG:\n\t\tnp->dontfrag = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tnp->autoflowlabel = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\n\treturn retv;\n\ne_inval:\n\trelease_sock(sk);\n\tif (needs_rtnl)\n\t\trtnl_unlock();\n\treturn -EINVAL;\n}\n\nint ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tint err;\n\n\tif (level == SOL_IP && sk->sk_type != SOCK_RAW)\n\t\treturn udp_prot.setsockopt(sk, level, optname, optval, optlen);\n\n\tif (level != SOL_IPV6)\n\t\treturn -ENOPROTOOPT;\n\n\terr = do_ipv6_setsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IPV6_IPSEC_POLICY &&\n\t\t\toptname != IPV6_XFRM_POLICY) {\n\t\tlock_sock(sk);\n\t\terr = nf_setsockopt(sk, PF_INET6, optname, optval,\n\t\t\t\toptlen);\n\t\trelease_sock(sk);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ipv6_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t   char __user *optval, unsigned int optlen)\n{\n\tint err;\n\n\tif (level == SOL_IP && sk->sk_type != SOCK_RAW) {\n\t\tif (udp_prot.compat_setsockopt != NULL)\n\t\t\treturn udp_prot.compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t\t  optval, optlen);\n\t\treturn udp_prot.setsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\tif (level != SOL_IPV6)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optname >= MCAST_JOIN_GROUP && optname <= MCAST_MSFILTER)\n\t\treturn compat_mc_setsockopt(sk, level, optname, optval, optlen,\n\t\t\tipv6_setsockopt);\n\n\terr = do_ipv6_setsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IPV6_IPSEC_POLICY &&\n\t    optname != IPV6_XFRM_POLICY) {\n\t\tlock_sock(sk);\n\t\terr = compat_nf_setsockopt(sk, PF_INET6, optname,\n\t\t\t\t\t   optval, optlen);\n\t\trelease_sock(sk);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(compat_ipv6_setsockopt);\n#endif\n\nstatic int ipv6_getsockopt_sticky(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t\t\t  int optname, char __user *optval, int len)\n{\n\tstruct ipv6_opt_hdr *hdr;\n\n\tif (!opt)\n\t\treturn 0;\n\n\tswitch (optname) {\n\tcase IPV6_HOPOPTS:\n\t\thdr = opt->hopopt;\n\t\tbreak;\n\tcase IPV6_RTHDRDSTOPTS:\n\t\thdr = opt->dst0opt;\n\t\tbreak;\n\tcase IPV6_RTHDR:\n\t\thdr = (struct ipv6_opt_hdr *)opt->srcrt;\n\t\tbreak;\n\tcase IPV6_DSTOPTS:\n\t\thdr = opt->dst1opt;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\t/* should not happen */\n\t}\n\n\tif (!hdr)\n\t\treturn 0;\n\n\tlen = min_t(unsigned int, len, ipv6_optlen(hdr));\n\tif (copy_to_user(optval, hdr, len))\n\t\treturn -EFAULT;\n\treturn len;\n}\n\nstatic int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen, unsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t{\n\t\tstruct group_filter gsf;\n\t\tint err;\n\n\t\tif (len < GROUP_FILTER_SIZE(0))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&gsf, optval, GROUP_FILTER_SIZE(0)))\n\t\t\treturn -EFAULT;\n\t\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tlock_sock(sk);\n\t\terr = ip6_mc_msfget(sk, &gsf,\n\t\t\t(struct group_filter __user *)optval, optlen);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tmsg.msg_control = optval;\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = flags;\n\n\t\tlock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\trelease_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tstruct in6_pktinfo src_info;\n\t\t\t\tsrc_info.ipi6_ifindex = np->mcast_oif ? np->mcast_oif :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = np->mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t     np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = np->mcast_hops;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn put_user(len, optlen);\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tlock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\trelease_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn put_user(len, optlen);\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_sk(sk)->transparent;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = np->hop_limit;\n\t\telse\n\t\t\tval = np->mcast_hops;\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = sock_net(sk)->ipv6.devconf_all->hop_limit;\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = np->mc_loop;\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = np->mcast_oif;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) np->ucast_oif);\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = np->pmtudisc;\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = np->recverr;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = np->sndflow;\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tval = 0;\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (np->srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (np->srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = np->min_hopcount;\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = np->dontfrag;\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = np->autoflowlabel;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tint err;\n\n\tif (level == SOL_IP && sk->sk_type != SOCK_RAW)\n\t\treturn udp_prot.getsockopt(sk, level, optname, optval, optlen);\n\n\tif (level != SOL_IPV6)\n\t\treturn -ENOPROTOOPT;\n\n\terr = do_ipv6_getsockopt(sk, level, optname, optval, optlen, 0);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IPV6_2292PKTOPTIONS) {\n\t\tint len;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\terr = nf_getsockopt(sk, PF_INET6, optname, optval,\n\t\t\t\t&len);\n\t\trelease_sock(sk);\n\t\tif (err >= 0)\n\t\t\terr = put_user(len, optlen);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ipv6_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tint err;\n\n\tif (level == SOL_IP && sk->sk_type != SOCK_RAW) {\n\t\tif (udp_prot.compat_getsockopt != NULL)\n\t\t\treturn udp_prot.compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t\t  optval, optlen);\n\t\treturn udp_prot.getsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\tif (level != SOL_IPV6)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optname == MCAST_MSFILTER)\n\t\treturn compat_mc_getsockopt(sk, level, optname, optval, optlen,\n\t\t\tipv6_getsockopt);\n\n\terr = do_ipv6_getsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t MSG_CMSG_COMPAT);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IPV6_2292PKTOPTIONS) {\n\t\tint len;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\terr = compat_nf_getsockopt(sk, PF_INET6,\n\t\t\t\t\t   optname, optval, &len);\n\t\trelease_sock(sk);\n\t\tif (err >= 0)\n\t\t\terr = put_user(len, optlen);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(compat_ipv6_getsockopt);\n#endif\n\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tIPv6 BSD socket options interface\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/net/ipv4/ip_sockglue.c\n *\n *\tFIXME: Make the setsockopt code POSIX compliant: That is\n *\n *\to\tTruncate getsockopt returns\n *\to\tReturn an optlen of the truncated length if need be\n *\n *\tChanges:\n *\tDavid L Stevens <dlstevens@us.ibm.com>:\n *\t\t- added multicast source filtering API for MLDv2\n */\n\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/mroute6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/init.h>\n#include <linux/sysctl.h>\n#include <linux/netfilter.h>\n#include <linux/slab.h>\n\n#include <net/sock.h>\n#include <net/snmp.h>\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/inet_common.h>\n#include <net/tcp.h>\n#include <net/udp.h>\n#include <net/udplite.h>\n#include <net/xfrm.h>\n#include <net/compat.h>\n#include <net/seg6.h>\n\n#include <linux/uaccess.h>\n\nstruct ip6_ra_chain *ip6_ra_chain;\nDEFINE_RWLOCK(ip6_ra_lock);\n\nDEFINE_STATIC_KEY_FALSE(ip6_min_hopcount);\n\nint ip6_ra_control(struct sock *sk, int sel)\n{\n\tstruct ip6_ra_chain *ra, *new_ra, **rap;\n\n\t/* RA packet may be delivered ONLY to IPPROTO_RAW socket */\n\tif (sk->sk_type != SOCK_RAW || inet_sk(sk)->inet_num != IPPROTO_RAW)\n\t\treturn -ENOPROTOOPT;\n\n\tnew_ra = (sel >= 0) ? kmalloc(sizeof(*new_ra), GFP_KERNEL) : NULL;\n\tif (sel >= 0 && !new_ra)\n\t\treturn -ENOMEM;\n\n\twrite_lock_bh(&ip6_ra_lock);\n\tfor (rap = &ip6_ra_chain; (ra = *rap) != NULL; rap = &ra->next) {\n\t\tif (ra->sk == sk) {\n\t\t\tif (sel >= 0) {\n\t\t\t\twrite_unlock_bh(&ip6_ra_lock);\n\t\t\t\tkfree(new_ra);\n\t\t\t\treturn -EADDRINUSE;\n\t\t\t}\n\n\t\t\t*rap = ra->next;\n\t\t\twrite_unlock_bh(&ip6_ra_lock);\n\n\t\t\tsock_put(sk);\n\t\t\tkfree(ra);\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (!new_ra) {\n\t\twrite_unlock_bh(&ip6_ra_lock);\n\t\treturn -ENOBUFS;\n\t}\n\tnew_ra->sk = sk;\n\tnew_ra->sel = sel;\n\tnew_ra->next = ra;\n\t*rap = new_ra;\n\tsock_hold(sk);\n\twrite_unlock_bh(&ip6_ra_lock);\n\treturn 0;\n}\n\nstruct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n\t\t\t\t\t   struct ipv6_txoptions *opt)\n{\n\tif (inet_test_bit(IS_ICSK, sk)) {\n\t\tif (opt &&\n\t\t    !((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE)) &&\n\t\t    inet_sk(sk)->inet_daddr != LOOPBACK4_IPV6) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\t\t\ticsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;\n\t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t}\n\t}\n\topt = unrcu_pointer(xchg(&inet6_sk(sk)->opt, RCU_INITIALIZER(opt)));\n\tsk_dst_reset(sk);\n\n\treturn opt;\n}\n\nstatic int copy_group_source_from_sockptr(struct group_source_req *greqs,\n\t\tsockptr_t optval, int optlen)\n{\n\tif (in_compat_syscall()) {\n\t\tstruct compat_group_source_req gr32;\n\n\t\tif (optlen < sizeof(gr32))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(&gr32, optval, sizeof(gr32)))\n\t\t\treturn -EFAULT;\n\t\tgreqs->gsr_interface = gr32.gsr_interface;\n\t\tgreqs->gsr_group = gr32.gsr_group;\n\t\tgreqs->gsr_source = gr32.gsr_source;\n\t} else {\n\t\tif (optlen < sizeof(*greqs))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_sockptr(greqs, optval, sizeof(*greqs)))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic int do_ipv6_mcast_group_source(struct sock *sk, int optname,\n\t\tsockptr_t optval, int optlen)\n{\n\tstruct group_source_req greqs;\n\tint omode, add;\n\tint ret;\n\n\tret = copy_group_source_from_sockptr(&greqs, optval, optlen);\n\tif (ret)\n\t\treturn ret;\n\n\tif (greqs.gsr_group.ss_family != AF_INET6 ||\n\t    greqs.gsr_source.ss_family != AF_INET6)\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (optname == MCAST_BLOCK_SOURCE) {\n\t\tomode = MCAST_EXCLUDE;\n\t\tadd = 1;\n\t} else if (optname == MCAST_UNBLOCK_SOURCE) {\n\t\tomode = MCAST_EXCLUDE;\n\t\tadd = 0;\n\t} else if (optname == MCAST_JOIN_SOURCE_GROUP) {\n\t\tstruct sockaddr_in6 *psin6;\n\t\tint retv;\n\n\t\tpsin6 = (struct sockaddr_in6 *)&greqs.gsr_group;\n\t\tretv = ipv6_sock_mc_join_ssm(sk, greqs.gsr_interface,\n\t\t\t\t\t     &psin6->sin6_addr,\n\t\t\t\t\t     MCAST_INCLUDE);\n\t\t/* prior join w/ different source is ok */\n\t\tif (retv && retv != -EADDRINUSE)\n\t\t\treturn retv;\n\t\tomode = MCAST_INCLUDE;\n\t\tadd = 1;\n\t} else /* MCAST_LEAVE_SOURCE_GROUP */ {\n\t\tomode = MCAST_INCLUDE;\n\t\tadd = 0;\n\t}\n\treturn ip6_mc_source(add, omode, sk, &greqs);\n}\n\nstatic int ipv6_set_mcast_msfilter(struct sock *sk, sockptr_t optval,\n\t\tint optlen)\n{\n\tstruct group_filter *gsf;\n\tint ret;\n\n\tif (optlen < GROUP_FILTER_SIZE(0))\n\t\treturn -EINVAL;\n\tif (optlen > READ_ONCE(sock_net(sk)->core.sysctl_optmem_max))\n\t\treturn -ENOBUFS;\n\n\tgsf = memdup_sockptr(optval, optlen);\n\tif (IS_ERR(gsf))\n\t\treturn PTR_ERR(gsf);\n\n\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\tret = -ENOBUFS;\n\tif (gsf->gf_numsrc >= 0x1ffffffU ||\n\t    gsf->gf_numsrc > sysctl_mld_max_msf)\n\t\tgoto out_free_gsf;\n\n\tret = -EINVAL;\n\tif (GROUP_FILTER_SIZE(gsf->gf_numsrc) > optlen)\n\t\tgoto out_free_gsf;\n\n\tret = ip6_mc_msfilter(sk, gsf, gsf->gf_slist_flex);\nout_free_gsf:\n\tkfree(gsf);\n\treturn ret;\n}\n\nstatic int compat_ipv6_set_mcast_msfilter(struct sock *sk, sockptr_t optval,\n\t\tint optlen)\n{\n\tconst int size0 = offsetof(struct compat_group_filter, gf_slist_flex);\n\tstruct compat_group_filter *gf32;\n\tvoid *p;\n\tint ret;\n\tint n;\n\n\tif (optlen < size0)\n\t\treturn -EINVAL;\n\tif (optlen > READ_ONCE(sock_net(sk)->core.sysctl_optmem_max) - 4)\n\t\treturn -ENOBUFS;\n\n\tp = kmalloc(optlen + 4, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tgf32 = p + 4; /* we want ->gf_group and ->gf_slist_flex aligned */\n\tret = -EFAULT;\n\tif (copy_from_sockptr(gf32, optval, optlen))\n\t\tgoto out_free_p;\n\n\t/* numsrc >= (4G-140)/128 overflow in 32 bits */\n\tret = -ENOBUFS;\n\tn = gf32->gf_numsrc;\n\tif (n >= 0x1ffffffU || n > sysctl_mld_max_msf)\n\t\tgoto out_free_p;\n\n\tret = -EINVAL;\n\tif (offsetof(struct compat_group_filter, gf_slist_flex[n]) > optlen)\n\t\tgoto out_free_p;\n\n\tret = ip6_mc_msfilter(sk, &(struct group_filter){\n\t\t\t.gf_interface = gf32->gf_interface,\n\t\t\t.gf_group = gf32->gf_group,\n\t\t\t.gf_fmode = gf32->gf_fmode,\n\t\t\t.gf_numsrc = gf32->gf_numsrc}, gf32->gf_slist_flex);\n\nout_free_p:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int ipv6_mcast_join_leave(struct sock *sk, int optname,\n\t\tsockptr_t optval, int optlen)\n{\n\tstruct sockaddr_in6 *psin6;\n\tstruct group_req greq;\n\n\tif (optlen < sizeof(greq))\n\t\treturn -EINVAL;\n\tif (copy_from_sockptr(&greq, optval, sizeof(greq)))\n\t\treturn -EFAULT;\n\n\tif (greq.gr_group.ss_family != AF_INET6)\n\t\treturn -EADDRNOTAVAIL;\n\tpsin6 = (struct sockaddr_in6 *)&greq.gr_group;\n\tif (optname == MCAST_JOIN_GROUP)\n\t\treturn ipv6_sock_mc_join(sk, greq.gr_interface,\n\t\t\t\t\t &psin6->sin6_addr);\n\treturn ipv6_sock_mc_drop(sk, greq.gr_interface, &psin6->sin6_addr);\n}\n\nstatic int compat_ipv6_mcast_join_leave(struct sock *sk, int optname,\n\t\tsockptr_t optval, int optlen)\n{\n\tstruct compat_group_req gr32;\n\tstruct sockaddr_in6 *psin6;\n\n\tif (optlen < sizeof(gr32))\n\t\treturn -EINVAL;\n\tif (copy_from_sockptr(&gr32, optval, sizeof(gr32)))\n\t\treturn -EFAULT;\n\n\tif (gr32.gr_group.ss_family != AF_INET6)\n\t\treturn -EADDRNOTAVAIL;\n\tpsin6 = (struct sockaddr_in6 *)&gr32.gr_group;\n\tif (optname == MCAST_JOIN_GROUP)\n\t\treturn ipv6_sock_mc_join(sk, gr32.gr_interface,\n\t\t\t\t\t&psin6->sin6_addr);\n\treturn ipv6_sock_mc_drop(sk, gr32.gr_interface, &psin6->sin6_addr);\n}\n\nstatic int ipv6_set_opt_hdr(struct sock *sk, int optname, sockptr_t optval,\n\t\tint optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_opt_hdr *new = NULL;\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_txoptions *opt;\n\tint err;\n\n\t/* hop-by-hop / destination options are privileged option */\n\tif (optname != IPV6_RTHDR && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW))\n\t\treturn -EPERM;\n\n\t/* remove any sticky options header with a zero option\n\t * length, per RFC3542.\n\t */\n\tif (optlen > 0) {\n\t\tif (sockptr_is_null(optval))\n\t\t\treturn -EINVAL;\n\t\tif (optlen < sizeof(struct ipv6_opt_hdr) ||\n\t\t    optlen & 0x7 ||\n\t\t    optlen > 8 * 255)\n\t\t\treturn -EINVAL;\n\n\t\tnew = memdup_sockptr(optval, optlen);\n\t\tif (IS_ERR(new))\n\t\t\treturn PTR_ERR(new);\n\t\tif (unlikely(ipv6_optlen(new) > optlen)) {\n\t\t\tkfree(new);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\topt = rcu_dereference_protected(np->opt, lockdep_sock_is_held(sk));\n\topt = ipv6_renew_options(sk, opt, optname, new);\n\tkfree(new);\n\tif (IS_ERR(opt))\n\t\treturn PTR_ERR(opt);\n\n\t/* routing header option needs extra check */\n\terr = -EINVAL;\n\tif (optname == IPV6_RTHDR && opt && opt->srcrt) {\n\t\tstruct ipv6_rt_hdr *rthdr = opt->srcrt;\n\t\tswitch (rthdr->type) {\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\tcase IPV6_SRCRT_TYPE_2:\n\t\t\tif (rthdr->hdrlen != 2 || rthdr->segments_left != 1)\n\t\t\t\tgoto sticky_done;\n\t\t\tbreak;\n#endif\n\t\tcase IPV6_SRCRT_TYPE_4:\n\t\t{\n\t\t\tstruct ipv6_sr_hdr *srh =\n\t\t\t\t(struct ipv6_sr_hdr *)opt->srcrt;\n\n\t\t\tif (!seg6_validate_srh(srh, optlen, false))\n\t\t\t\tgoto sticky_done;\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tgoto sticky_done;\n\t\t}\n\t}\n\n\terr = 0;\n\topt = ipv6_update_options(sk, opt);\nsticky_done:\n\tif (opt) {\n\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\ttxopt_put(opt);\n\t}\n\treturn err;\n}\n\nint do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint retv = -ENOPROTOOPT;\n\tint val, valbool;\n\n\tif (sockptr_is_null(optval))\n\t\tval = 0;\n\telse {\n\t\tif (optlen >= sizeof(int)) {\n\t\t\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\t\t\treturn -EFAULT;\n\t\t} else\n\t\t\tval = 0;\n\t}\n\n\tvalbool = (val != 0);\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_setsockopt(sk, optname, optval, optlen);\n\n\t/* Handle options that can be set without locking the socket. */\n\tswitch (optname) {\n\tcase IPV6_UNICAST_HOPS:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val > 255 || val < -1)\n\t\t\treturn -EINVAL;\n\t\tWRITE_ONCE(np->hop_limit, val);\n\t\treturn 0;\n\tcase IPV6_MULTICAST_LOOP:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val != valbool)\n\t\t\treturn -EINVAL;\n\t\tinet6_assign_bit(MC6_LOOP, sk, valbool);\n\t\treturn 0;\n\tcase IPV6_MULTICAST_HOPS:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\treturn retv;\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val > 255 || val < -1)\n\t\t\treturn -EINVAL;\n\t\tWRITE_ONCE(np->mcast_hops,\n\t\t\t   val == -1 ? IPV6_DEFAULT_MCASTHOPS : val);\n\t\treturn 0;\n\tcase IPV6_MTU:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val && val < IPV6_MIN_MTU)\n\t\t\treturn -EINVAL;\n\t\tWRITE_ONCE(np->frag_size, val);\n\t\treturn 0;\n\tcase IPV6_MINHOPCOUNT:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val < 0 || val > 255)\n\t\t\treturn -EINVAL;\n\n\t\tif (val)\n\t\t\tstatic_branch_enable(&ip6_min_hopcount);\n\n\t\t/* tcp_v6_err() and tcp_v6_rcv() might read min_hopcount\n\t\t * while we are changing it.\n\t\t */\n\t\tWRITE_ONCE(np->min_hopcount, val);\n\t\treturn 0;\n\tcase IPV6_RECVERR_RFC4884:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val < 0 || val > 1)\n\t\t\treturn -EINVAL;\n\t\tinet6_assign_bit(RECVERR6_RFC4884, sk, valbool);\n\t\treturn 0;\n\tcase IPV6_MULTICAST_ALL:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tinet6_assign_bit(MC6_ALL, sk, valbool);\n\t\treturn 0;\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tinet6_assign_bit(AUTOFLOWLABEL, sk, valbool);\n\t\tinet6_set_bit(AUTOFLOWLABEL_SET, sk);\n\t\treturn 0;\n\tcase IPV6_DONTFRAG:\n\t\tinet6_assign_bit(DONTFRAG, sk, valbool);\n\t\treturn 0;\n\tcase IPV6_RECVERR:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tinet6_assign_bit(RECVERR6, sk, valbool);\n\t\tif (!val)\n\t\t\tskb_errqueue_purge(&sk->sk_error_queue);\n\t\treturn 0;\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tinet6_assign_bit(RTALERT_ISOLATE, sk, valbool);\n\t\treturn 0;\n\tcase IPV6_MTU_DISCOVER:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val < IPV6_PMTUDISC_DONT || val > IPV6_PMTUDISC_OMIT)\n\t\t\treturn -EINVAL;\n\t\tWRITE_ONCE(np->pmtudisc, val);\n\t\treturn 0;\n\tcase IPV6_FLOWINFO_SEND:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tinet6_assign_bit(SNDFLOW, sk, valbool);\n\t\treturn 0;\n\tcase IPV6_ADDR_PREFERENCES:\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\treturn ip6_sock_set_addr_preferences(sk, val);\n\tcase IPV6_MULTICAST_IF:\n\t\tif (sk->sk_type == SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (optlen < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tif (val) {\n\t\t\tstruct net_device *dev;\n\t\t\tint bound_dev_if, midx;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdev = dev_get_by_index_rcu(net, val);\n\t\t\tif (!dev) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\t\t\tmidx = l3mdev_master_ifindex_rcu(dev);\n\n\t\t\trcu_read_unlock();\n\n\t\t\tbound_dev_if = READ_ONCE(sk->sk_bound_dev_if);\n\t\t\tif (bound_dev_if &&\n\t\t\t    bound_dev_if != val &&\n\t\t\t    (!midx || midx != bound_dev_if))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tWRITE_ONCE(np->mcast_oif, val);\n\t\treturn 0;\n\tcase IPV6_UNICAST_IF:\n\t{\n\t\tstruct net_device *dev;\n\t\tint ifindex;\n\n\t\tif (optlen != sizeof(int))\n\t\t\treturn -EINVAL;\n\n\t\tifindex = (__force int)ntohl((__force __be32)val);\n\t\tif (!ifindex) {\n\t\t\tWRITE_ONCE(np->ucast_oif, 0);\n\t\t\treturn 0;\n\t\t}\n\n\t\tdev = dev_get_by_index(net, ifindex);\n\t\tif (!dev)\n\t\t\treturn -EADDRNOTAVAIL;\n\t\tdev_put(dev);\n\n\t\tif (READ_ONCE(sk->sk_bound_dev_if))\n\t\t\treturn -EINVAL;\n\n\t\tWRITE_ONCE(np->ucast_oif, ifindex);\n\t\treturn 0;\n\t}\n\t}\n\n\tsockopt_lock_sock(sk);\n\n\t/* Another thread has converted the socket into IPv4 with\n\t * IPV6_ADDRFORM concurrently.\n\t */\n\tif (unlikely(sk->sk_family != AF_INET6))\n\t\tgoto unlock;\n\n\tswitch (optname) {\n\n\tcase IPV6_ADDRFORM:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val == PF_INET) {\n\t\t\tif (sk->sk_type == SOCK_RAW)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_protocol == IPPROTO_UDP ||\n\t\t\t    sk->sk_protocol == IPPROTO_UDPLITE) {\n\t\t\t\tstruct udp_sock *up = udp_sk(sk);\n\t\t\t\tif (up->pending == AF_INET6) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tif (sk->sk_prot != &tcpv6_prot) {\n\t\t\t\t\tretv = -EBUSY;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_state != TCP_ESTABLISHED) {\n\t\t\t\tretv = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (ipv6_only_sock(sk) ||\n\t\t\t    !ipv6_addr_v4mapped(&sk->sk_v6_daddr)) {\n\t\t\t\tretv = -EADDRNOTAVAIL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__ipv6_sock_mc_close(sk);\n\t\t\t__ipv6_sock_ac_close(sk);\n\n\t\t\tif (sk->sk_protocol == IPPROTO_TCP) {\n\t\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, &tcp_prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_stream_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, &tcp_prot);\n\t\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv4_specific);\n\t\t\t\tWRITE_ONCE(sk->sk_socket->ops, &inet_stream_ops);\n\t\t\t\tWRITE_ONCE(sk->sk_family, PF_INET);\n\t\t\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\t\t} else {\n\t\t\t\tstruct proto *prot = &udp_prot;\n\n\t\t\t\tif (sk->sk_protocol == IPPROTO_UDPLITE)\n\t\t\t\t\tprot = &udplite_prot;\n\n\t\t\t\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\t\t\t\tsock_prot_inuse_add(net, prot, 1);\n\n\t\t\t\t/* Paired with READ_ONCE(sk->sk_prot) in inet6_dgram_ops */\n\t\t\t\tWRITE_ONCE(sk->sk_prot, prot);\n\t\t\t\tWRITE_ONCE(sk->sk_socket->ops, &inet_dgram_ops);\n\t\t\t\tWRITE_ONCE(sk->sk_family, PF_INET);\n\t\t\t}\n\n\t\t\t/* Disable all options not to allocate memory anymore,\n\t\t\t * but there is still a race.  See the lockless path\n\t\t\t * in udpv6_sendmsg() and ipv6_local_rxpmtu().\n\t\t\t */\n\t\t\tnp->rxopt.all = 0;\n\n\t\t\tinet6_cleanup_sock(sk);\n\n\t\t\tmodule_put(THIS_MODULE);\n\t\t\tretv = 0;\n\t\t\tbreak;\n\t\t}\n\t\tgoto e_inval;\n\n\tcase IPV6_V6ONLY:\n\t\tif (optlen < sizeof(int) ||\n\t\t    inet_sk(sk)->inet_num)\n\t\t\tgoto e_inval;\n\t\tsk->sk_ipv6only = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxoinfo = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxhlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxohlim = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.srcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.osrcrt = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.hopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.ohopopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.dstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.odstopts = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tif (val < -1 || val > 0xff)\n\t\t\tgoto e_inval;\n\t\t/* RFC 3542, 6.5: default traffic class of 0x0 */\n\t\tif (val == -1)\n\t\t\tval = 0;\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tval &= ~INET_ECN_MASK;\n\t\t\tval |= np->tclass & INET_ECN_MASK;\n\t\t}\n\t\tif (np->tclass != val) {\n\t\t\tnp->tclass = val;\n\t\t\tsk_dst_reset(sk);\n\t\t}\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxtclass = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxflow = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxpmtu = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_TRANSPARENT:\n\t\tif (valbool && !sockopt_ns_capable(net->user_ns, CAP_NET_RAW) &&\n\t\t    !sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN)) {\n\t\t\tretv = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we don't have a separate transparent bit for IPV6 we use the one in the IPv4 socket */\n\t\tinet_assign_bit(TRANSPARENT, sk, valbool);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\t/* we also don't have a separate freebind bit for IPV6 */\n\t\tinet_assign_bit(FREEBIND, sk, valbool);\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tnp->rxopt.bits.rxorigdstaddr = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t\tretv = ipv6_set_opt_hdr(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_PKTINFO:\n\t{\n\t\tstruct in6_pktinfo pkt;\n\n\t\tif (optlen == 0)\n\t\t\tgoto e_inval;\n\t\telse if (optlen < sizeof(struct in6_pktinfo) ||\n\t\t\t sockptr_is_null(optval))\n\t\t\tgoto e_inval;\n\n\t\tif (copy_from_sockptr(&pkt, optval, sizeof(pkt))) {\n\t\t\tretv = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!sk_dev_equal_l3scope(sk, pkt.ipi6_ifindex))\n\t\t\tgoto e_inval;\n\n\t\tnp->sticky_pktinfo.ipi6_ifindex = pkt.ipi6_ifindex;\n\t\tnp->sticky_pktinfo.ipi6_addr = pkt.ipi6_addr;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct ipv6_txoptions *opt = NULL;\n\t\tstruct msghdr msg;\n\t\tstruct flowi6 fl6;\n\t\tstruct ipcm6_cookie ipc6;\n\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = sk->sk_mark;\n\n\t\tif (optlen == 0)\n\t\t\tgoto update;\n\n\t\t/* 1K is probably excessive\n\t\t * 1K is surely not enough, 2K per standard header is 16K.\n\t\t */\n\t\tretv = -EINVAL;\n\t\tif (optlen > 64*1024)\n\t\t\tbreak;\n\n\t\topt = sock_kmalloc(sk, sizeof(*opt) + optlen, GFP_KERNEL);\n\t\tretv = -ENOBUFS;\n\t\tif (!opt)\n\t\t\tbreak;\n\n\t\tmemset(opt, 0, sizeof(*opt));\n\t\trefcount_set(&opt->refcnt, 1);\n\t\topt->tot_len = sizeof(*opt) + optlen;\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(opt + 1, optval, optlen))\n\t\t\tgoto done;\n\n\t\tmsg.msg_controllen = optlen;\n\t\tmsg.msg_control_is_user = false;\n\t\tmsg.msg_control = (void *)(opt+1);\n\t\tipc6.opt = opt;\n\n\t\tretv = ip6_datagram_send_ctl(net, sk, &msg, &fl6, &ipc6);\n\t\tif (retv)\n\t\t\tgoto done;\nupdate:\n\t\tretv = 0;\n\t\topt = ipv6_update_options(sk, opt);\ndone:\n\t\tif (opt) {\n\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n\t\t\ttxopt_put(opt);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase IPV6_ADD_MEMBERSHIP:\n\tcase IPV6_DROP_MEMBERSHIP:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EPROTO;\n\t\tif (inet_test_bit(IS_ICSK, sk))\n\t\t\tbreak;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_ADD_MEMBERSHIP)\n\t\t\tretv = ipv6_sock_mc_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_mc_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_multiaddr);\n\t\tbreak;\n\t}\n\tcase IPV6_JOIN_ANYCAST:\n\tcase IPV6_LEAVE_ANYCAST:\n\t{\n\t\tstruct ipv6_mreq mreq;\n\n\t\tif (optlen < sizeof(struct ipv6_mreq))\n\t\t\tgoto e_inval;\n\n\t\tretv = -EFAULT;\n\t\tif (copy_from_sockptr(&mreq, optval, sizeof(struct ipv6_mreq)))\n\t\t\tbreak;\n\n\t\tif (optname == IPV6_JOIN_ANYCAST)\n\t\t\tretv = ipv6_sock_ac_join(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\telse\n\t\t\tretv = ipv6_sock_ac_drop(sk, mreq.ipv6mr_ifindex, &mreq.ipv6mr_acaddr);\n\t\tbreak;\n\t}\n\tcase MCAST_JOIN_GROUP:\n\tcase MCAST_LEAVE_GROUP:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t\t    optlen);\n\t\telse\n\t\t\tretv = ipv6_mcast_join_leave(sk, optname, optval,\n\t\t\t\t\t\t     optlen);\n\t\tbreak;\n\tcase MCAST_JOIN_SOURCE_GROUP:\n\tcase MCAST_LEAVE_SOURCE_GROUP:\n\tcase MCAST_BLOCK_SOURCE:\n\tcase MCAST_UNBLOCK_SOURCE:\n\t\tretv = do_ipv6_mcast_group_source(sk, optname, optval, optlen);\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\tretv = compat_ipv6_set_mcast_msfilter(sk, optval,\n\t\t\t\t\t\t\t      optlen);\n\t\telse\n\t\t\tretv = ipv6_set_mcast_msfilter(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_ROUTER_ALERT:\n\t\tif (optlen < sizeof(int))\n\t\t\tgoto e_inval;\n\t\tretv = ip6_ra_control(sk, val);\n\t\tif (retv == 0)\n\t\t\tinet6_assign_bit(RTALERT, sk, valbool);\n\t\tbreak;\n\tcase IPV6_FLOWLABEL_MGR:\n\t\tretv = ipv6_flowlabel_opt(sk, optval, optlen);\n\t\tbreak;\n\tcase IPV6_IPSEC_POLICY:\n\tcase IPV6_XFRM_POLICY:\n\t\tretv = -EPERM;\n\t\tif (!sockopt_ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\tbreak;\n\t\tretv = xfrm_user_policy(sk, optname, optval, optlen);\n\t\tbreak;\n\n\tcase IPV6_RECVFRAGSIZE:\n\t\tnp->rxopt.bits.recvfragsize = valbool;\n\t\tretv = 0;\n\t\tbreak;\n\t}\n\nunlock:\n\tsockopt_release_sock(sk);\n\n\treturn retv;\n\ne_inval:\n\tretv = -EINVAL;\n\tgoto unlock;\n}\n\nint ipv6_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t    unsigned int optlen)\n{\n\tint err;\n\n\tif (level == SOL_IP && sk->sk_type != SOCK_RAW)\n\t\treturn ip_setsockopt(sk, level, optname, optval, optlen);\n\n\tif (level != SOL_IPV6)\n\t\treturn -ENOPROTOOPT;\n\n\terr = do_ipv6_setsockopt(sk, level, optname, optval, optlen);\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IPV6_IPSEC_POLICY &&\n\t\t\toptname != IPV6_XFRM_POLICY)\n\t\terr = nf_setsockopt(sk, PF_INET6, optname, optval, optlen);\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ipv6_setsockopt);\n\nstatic int ipv6_getsockopt_sticky(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t\t\t  int optname, sockptr_t optval, int len)\n{\n\tstruct ipv6_opt_hdr *hdr;\n\n\tif (!opt)\n\t\treturn 0;\n\n\tswitch (optname) {\n\tcase IPV6_HOPOPTS:\n\t\thdr = opt->hopopt;\n\t\tbreak;\n\tcase IPV6_RTHDRDSTOPTS:\n\t\thdr = opt->dst0opt;\n\t\tbreak;\n\tcase IPV6_RTHDR:\n\t\thdr = (struct ipv6_opt_hdr *)opt->srcrt;\n\t\tbreak;\n\tcase IPV6_DSTOPTS:\n\t\thdr = opt->dst1opt;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\t/* should not happen */\n\t}\n\n\tif (!hdr)\n\t\treturn 0;\n\n\tlen = min_t(unsigned int, len, ipv6_optlen(hdr));\n\tif (copy_to_sockptr(optval, hdr, len))\n\t\treturn -EFAULT;\n\treturn len;\n}\n\nstatic int ipv6_get_msfilter(struct sock *sk, sockptr_t optval,\n\t\t\t     sockptr_t optlen, int len)\n{\n\tconst int size0 = offsetof(struct group_filter, gf_slist_flex);\n\tstruct group_filter gsf;\n\tint num;\n\tint err;\n\n\tif (len < size0)\n\t\treturn -EINVAL;\n\tif (copy_from_sockptr(&gsf, optval, size0))\n\t\treturn -EFAULT;\n\tif (gsf.gf_group.ss_family != AF_INET6)\n\t\treturn -EADDRNOTAVAIL;\n\tnum = gsf.gf_numsrc;\n\tsockopt_lock_sock(sk);\n\terr = ip6_mc_msfget(sk, &gsf, optval, size0);\n\tif (!err) {\n\t\tif (num > gsf.gf_numsrc)\n\t\t\tnum = gsf.gf_numsrc;\n\t\tlen = GROUP_FILTER_SIZE(num);\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)) ||\n\t\t    copy_to_sockptr(optval, &gsf, size0))\n\t\t\terr = -EFAULT;\n\t}\n\tsockopt_release_sock(sk);\n\treturn err;\n}\n\nstatic int compat_ipv6_get_msfilter(struct sock *sk, sockptr_t optval,\n\t\t\t\t    sockptr_t optlen, int len)\n{\n\tconst int size0 = offsetof(struct compat_group_filter, gf_slist_flex);\n\tstruct compat_group_filter gf32;\n\tstruct group_filter gf;\n\tint err;\n\tint num;\n\n\tif (len < size0)\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&gf32, optval, size0))\n\t\treturn -EFAULT;\n\tgf.gf_interface = gf32.gf_interface;\n\tgf.gf_fmode = gf32.gf_fmode;\n\tnum = gf.gf_numsrc = gf32.gf_numsrc;\n\tgf.gf_group = gf32.gf_group;\n\n\tif (gf.gf_group.ss_family != AF_INET6)\n\t\treturn -EADDRNOTAVAIL;\n\n\tsockopt_lock_sock(sk);\n\terr = ip6_mc_msfget(sk, &gf, optval, size0);\n\tsockopt_release_sock(sk);\n\tif (err)\n\t\treturn err;\n\tif (num > gf.gf_numsrc)\n\t\tnum = gf.gf_numsrc;\n\tlen = GROUP_FILTER_SIZE(num) - (sizeof(gf)-sizeof(gf32));\n\tif (copy_to_sockptr(optlen, &len, sizeof(int)) ||\n\t    copy_to_sockptr_offset(optval, offsetof(struct compat_group_filter, gf_fmode),\n\t\t\t\t   &gf.gf_fmode, sizeof(gf32.gf_fmode)) ||\n\t    copy_to_sockptr_offset(optval, offsetof(struct compat_group_filter, gf_numsrc),\n\t\t\t\t   &gf.gf_numsrc, sizeof(gf32.gf_numsrc)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t       sockptr_t optval, sockptr_t optlen)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint len;\n\tint val;\n\n\tif (ip6_mroute_opt(optname))\n\t\treturn ip6_mroute_getsockopt(sk, optname, optval, optlen);\n\n\tif (copy_from_sockptr(&len, optlen, sizeof(int)))\n\t\treturn -EFAULT;\n\tswitch (optname) {\n\tcase IPV6_ADDRFORM:\n\t\tif (sk->sk_protocol != IPPROTO_UDP &&\n\t\t    sk->sk_protocol != IPPROTO_UDPLITE &&\n\t\t    sk->sk_protocol != IPPROTO_TCP)\n\t\t\treturn -ENOPROTOOPT;\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -ENOTCONN;\n\t\tval = sk->sk_family;\n\t\tbreak;\n\tcase MCAST_MSFILTER:\n\t\tif (in_compat_syscall())\n\t\t\treturn compat_ipv6_get_msfilter(sk, optval, optlen, len);\n\t\treturn ipv6_get_msfilter(sk, optval, optlen, len);\n\tcase IPV6_2292PKTOPTIONS:\n\t{\n\t\tstruct msghdr msg;\n\t\tstruct sk_buff *skb;\n\n\t\tif (sk->sk_type != SOCK_STREAM)\n\t\t\treturn -ENOPROTOOPT;\n\n\t\tif (optval.is_kernel) {\n\t\t\tmsg.msg_control_is_user = false;\n\t\t\tmsg.msg_control = optval.kernel;\n\t\t} else {\n\t\t\tmsg.msg_control_is_user = true;\n\t\t\tmsg.msg_control_user = optval.user;\n\t\t}\n\t\tmsg.msg_controllen = len;\n\t\tmsg.msg_flags = 0;\n\n\t\tsockopt_lock_sock(sk);\n\t\tskb = np->pktoptions;\n\t\tif (skb)\n\t\t\tip6_datagram_recv_ctl(sk, &msg, skb);\n\t\tsockopt_release_sock(sk);\n\t\tif (!skb) {\n\t\t\tif (np->rxopt.bits.rxinfo) {\n\t\t\t\tint mcast_oif = READ_ONCE(np->mcast_oif);\n\t\t\t\tstruct in6_pktinfo src_info;\n\n\t\t\t\tsrc_info.ipi6_ifindex = mcast_oif ? :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = mcast_oif ? sk->sk_v6_daddr : np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxhlim) {\n\t\t\t\tint hlim = READ_ONCE(np->mcast_hops);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxtclass) {\n\t\t\t\tint tclass = (int)ip6_tclass(np->rcv_flowinfo);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_TCLASS, sizeof(tclass), &tclass);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxoinfo) {\n\t\t\t\tint mcast_oif = READ_ONCE(np->mcast_oif);\n\t\t\t\tstruct in6_pktinfo src_info;\n\n\t\t\t\tsrc_info.ipi6_ifindex = mcast_oif ? :\n\t\t\t\t\tnp->sticky_pktinfo.ipi6_ifindex;\n\t\t\t\tsrc_info.ipi6_addr = mcast_oif ? sk->sk_v6_daddr :\n\t\t\t\t\t\t\t\t np->sticky_pktinfo.ipi6_addr;\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292PKTINFO, sizeof(src_info), &src_info);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxohlim) {\n\t\t\t\tint hlim = READ_ONCE(np->mcast_hops);\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_2292HOPLIMIT, sizeof(hlim), &hlim);\n\t\t\t}\n\t\t\tif (np->rxopt.bits.rxflow) {\n\t\t\t\t__be32 flowinfo = np->rcv_flowinfo;\n\n\t\t\t\tput_cmsg(&msg, SOL_IPV6, IPV6_FLOWINFO, sizeof(flowinfo), &flowinfo);\n\t\t\t}\n\t\t}\n\t\tlen -= msg.msg_controllen;\n\t\treturn copy_to_sockptr(optlen, &len, sizeof(int));\n\t}\n\tcase IPV6_MTU:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tval = 0;\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tval = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!val)\n\t\t\treturn -ENOTCONN;\n\t\tbreak;\n\t}\n\n\tcase IPV6_V6ONLY:\n\t\tval = sk->sk_ipv6only;\n\t\tbreak;\n\n\tcase IPV6_RECVPKTINFO:\n\t\tval = np->rxopt.bits.rxinfo;\n\t\tbreak;\n\n\tcase IPV6_2292PKTINFO:\n\t\tval = np->rxopt.bits.rxoinfo;\n\t\tbreak;\n\n\tcase IPV6_RECVHOPLIMIT:\n\t\tval = np->rxopt.bits.rxhlim;\n\t\tbreak;\n\n\tcase IPV6_2292HOPLIMIT:\n\t\tval = np->rxopt.bits.rxohlim;\n\t\tbreak;\n\n\tcase IPV6_RECVRTHDR:\n\t\tval = np->rxopt.bits.srcrt;\n\t\tbreak;\n\n\tcase IPV6_2292RTHDR:\n\t\tval = np->rxopt.bits.osrcrt;\n\t\tbreak;\n\n\tcase IPV6_HOPOPTS:\n\tcase IPV6_RTHDRDSTOPTS:\n\tcase IPV6_RTHDR:\n\tcase IPV6_DSTOPTS:\n\t{\n\t\tstruct ipv6_txoptions *opt;\n\n\t\tsockopt_lock_sock(sk);\n\t\topt = rcu_dereference_protected(np->opt,\n\t\t\t\t\t\tlockdep_sock_is_held(sk));\n\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n\t\tsockopt_release_sock(sk);\n\t\t/* check if ipv6_getsockopt_sticky() returns err code */\n\t\tif (len < 0)\n\t\t\treturn len;\n\t\treturn copy_to_sockptr(optlen, &len, sizeof(int));\n\t}\n\n\tcase IPV6_RECVHOPOPTS:\n\t\tval = np->rxopt.bits.hopopts;\n\t\tbreak;\n\n\tcase IPV6_2292HOPOPTS:\n\t\tval = np->rxopt.bits.ohopopts;\n\t\tbreak;\n\n\tcase IPV6_RECVDSTOPTS:\n\t\tval = np->rxopt.bits.dstopts;\n\t\tbreak;\n\n\tcase IPV6_2292DSTOPTS:\n\t\tval = np->rxopt.bits.odstopts;\n\t\tbreak;\n\n\tcase IPV6_TCLASS:\n\t\tval = np->tclass;\n\t\tbreak;\n\n\tcase IPV6_RECVTCLASS:\n\t\tval = np->rxopt.bits.rxtclass;\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO:\n\t\tval = np->rxopt.bits.rxflow;\n\t\tbreak;\n\n\tcase IPV6_RECVPATHMTU:\n\t\tval = np->rxopt.bits.rxpmtu;\n\t\tbreak;\n\n\tcase IPV6_PATHMTU:\n\t{\n\t\tstruct dst_entry *dst;\n\t\tstruct ip6_mtuinfo mtuinfo;\n\n\t\tif (len < sizeof(mtuinfo))\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(mtuinfo);\n\t\tmemset(&mtuinfo, 0, sizeof(mtuinfo));\n\n\t\trcu_read_lock();\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tmtuinfo.ip6m_mtu = dst_mtu(dst);\n\t\trcu_read_unlock();\n\t\tif (!mtuinfo.ip6m_mtu)\n\t\t\treturn -ENOTCONN;\n\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_sockptr(optval, &mtuinfo, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_TRANSPARENT:\n\t\tval = inet_test_bit(TRANSPARENT, sk);\n\t\tbreak;\n\n\tcase IPV6_FREEBIND:\n\t\tval = inet_test_bit(FREEBIND, sk);\n\t\tbreak;\n\n\tcase IPV6_RECVORIGDSTADDR:\n\t\tval = np->rxopt.bits.rxorigdstaddr;\n\t\tbreak;\n\n\tcase IPV6_UNICAST_HOPS:\n\tcase IPV6_MULTICAST_HOPS:\n\t{\n\t\tstruct dst_entry *dst;\n\n\t\tif (optname == IPV6_UNICAST_HOPS)\n\t\t\tval = READ_ONCE(np->hop_limit);\n\t\telse\n\t\t\tval = READ_ONCE(np->mcast_hops);\n\n\t\tif (val < 0) {\n\t\t\trcu_read_lock();\n\t\t\tdst = __sk_dst_get(sk);\n\t\t\tif (dst)\n\t\t\t\tval = ip6_dst_hoplimit(dst);\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (val < 0)\n\t\t\tval = READ_ONCE(sock_net(sk)->ipv6.devconf_all->hop_limit);\n\t\tbreak;\n\t}\n\n\tcase IPV6_MULTICAST_LOOP:\n\t\tval = inet6_test_bit(MC6_LOOP, sk);\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_IF:\n\t\tval = READ_ONCE(np->mcast_oif);\n\t\tbreak;\n\n\tcase IPV6_MULTICAST_ALL:\n\t\tval = inet6_test_bit(MC6_ALL, sk);\n\t\tbreak;\n\n\tcase IPV6_UNICAST_IF:\n\t\tval = (__force int)htonl((__u32) READ_ONCE(np->ucast_oif));\n\t\tbreak;\n\n\tcase IPV6_MTU_DISCOVER:\n\t\tval = READ_ONCE(np->pmtudisc);\n\t\tbreak;\n\n\tcase IPV6_RECVERR:\n\t\tval = inet6_test_bit(RECVERR6, sk);\n\t\tbreak;\n\n\tcase IPV6_FLOWINFO_SEND:\n\t\tval = inet6_test_bit(SNDFLOW, sk);\n\t\tbreak;\n\n\tcase IPV6_FLOWLABEL_MGR:\n\t{\n\t\tstruct in6_flowlabel_req freq;\n\t\tint flags;\n\n\t\tif (len < sizeof(freq))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_sockptr(&freq, optval, sizeof(freq)))\n\t\t\treturn -EFAULT;\n\n\t\tif (freq.flr_action != IPV6_FL_A_GET)\n\t\t\treturn -EINVAL;\n\n\t\tlen = sizeof(freq);\n\t\tflags = freq.flr_flags;\n\n\t\tmemset(&freq, 0, sizeof(freq));\n\n\t\tval = ipv6_flowlabel_opt_get(sk, &freq, flags);\n\t\tif (val < 0)\n\t\t\treturn val;\n\n\t\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_sockptr(optval, &freq, len))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase IPV6_ADDR_PREFERENCES:\n\t\t{\n\t\tu8 srcprefs = READ_ONCE(np->srcprefs);\n\t\tval = 0;\n\n\t\tif (srcprefs & IPV6_PREFER_SRC_TMP)\n\t\t\tval |= IPV6_PREFER_SRC_TMP;\n\t\telse if (srcprefs & IPV6_PREFER_SRC_PUBLIC)\n\t\t\tval |= IPV6_PREFER_SRC_PUBLIC;\n\t\telse {\n\t\t\t/* XXX: should we return system default? */\n\t\t\tval |= IPV6_PREFER_SRC_PUBTMP_DEFAULT;\n\t\t}\n\n\t\tif (srcprefs & IPV6_PREFER_SRC_COA)\n\t\t\tval |= IPV6_PREFER_SRC_COA;\n\t\telse\n\t\t\tval |= IPV6_PREFER_SRC_HOME;\n\t\tbreak;\n\t\t}\n\tcase IPV6_MINHOPCOUNT:\n\t\tval = READ_ONCE(np->min_hopcount);\n\t\tbreak;\n\n\tcase IPV6_DONTFRAG:\n\t\tval = inet6_test_bit(DONTFRAG, sk);\n\t\tbreak;\n\n\tcase IPV6_AUTOFLOWLABEL:\n\t\tval = ip6_autoflowlabel(sock_net(sk), sk);\n\t\tbreak;\n\n\tcase IPV6_RECVFRAGSIZE:\n\t\tval = np->rxopt.bits.recvfragsize;\n\t\tbreak;\n\n\tcase IPV6_ROUTER_ALERT:\n\t\tval = inet6_test_bit(RTALERT, sk);\n\t\tbreak;\n\n\tcase IPV6_ROUTER_ALERT_ISOLATE:\n\t\tval = inet6_test_bit(RTALERT_ISOLATE, sk);\n\t\tbreak;\n\n\tcase IPV6_RECVERR_RFC4884:\n\t\tval = inet6_test_bit(RECVERR6_RFC4884, sk);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\tlen = min_t(unsigned int, sizeof(int), len);\n\tif (copy_to_sockptr(optlen, &len, sizeof(int)))\n\t\treturn -EFAULT;\n\tif (copy_to_sockptr(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint ipv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tint err;\n\n\tif (level == SOL_IP && sk->sk_type != SOCK_RAW)\n\t\treturn ip_getsockopt(sk, level, optname, optval, optlen);\n\n\tif (level != SOL_IPV6)\n\t\treturn -ENOPROTOOPT;\n\n\terr = do_ipv6_getsockopt(sk, level, optname,\n\t\t\t\t USER_SOCKPTR(optval), USER_SOCKPTR(optlen));\n#ifdef CONFIG_NETFILTER\n\t/* we need to exclude all possible ENOPROTOOPTs except default case */\n\tif (err == -ENOPROTOOPT && optname != IPV6_2292PKTOPTIONS) {\n\t\tint len;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\terr = nf_getsockopt(sk, PF_INET6, optname, optval, &len);\n\t\tif (err >= 0)\n\t\t\terr = put_user(len, optlen);\n\t}\n#endif\n\treturn err;\n}\nEXPORT_SYMBOL(ipv6_getsockopt);\n", "patch": "@@ -111,7 +111,8 @@ struct ipv6_txoptions *ipv6_update_options(struct sock *sk,\n \t\t\ticsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);\n \t\t}\n \t}\n-\topt = xchg(&inet6_sk(sk)->opt, opt);\n+\topt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,\n+\t\t   opt);\n \tsk_dst_reset(sk);\n \n \treturn opt;\n@@ -231,9 +232,12 @@ static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n \t\t\t\tsk->sk_socket->ops = &inet_dgram_ops;\n \t\t\t\tsk->sk_family = PF_INET;\n \t\t\t}\n-\t\t\topt = xchg(&np->opt, NULL);\n-\t\t\tif (opt)\n-\t\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\t\topt = xchg((__force struct ipv6_txoptions **)&np->opt,\n+\t\t\t\t   NULL);\n+\t\t\tif (opt) {\n+\t\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\t\ttxopt_put(opt);\n+\t\t\t}\n \t\t\tpktopt = xchg(&np->pktoptions, NULL);\n \t\t\tkfree_skb(pktopt);\n \n@@ -403,7 +407,8 @@ static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n \t\tif (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))\n \t\t\tbreak;\n \n-\t\topt = ipv6_renew_options(sk, np->opt, optname,\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\topt = ipv6_renew_options(sk, opt, optname,\n \t\t\t\t\t (struct ipv6_opt_hdr __user *)optval,\n \t\t\t\t\t optlen);\n \t\tif (IS_ERR(opt)) {\n@@ -432,8 +437,10 @@ static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n sticky_done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \n@@ -486,6 +493,7 @@ static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n \t\t\tbreak;\n \n \t\tmemset(opt, 0, sizeof(*opt));\n+\t\tatomic_set(&opt->refcnt, 1);\n \t\topt->tot_len = sizeof(*opt) + optlen;\n \t\tretv = -EFAULT;\n \t\tif (copy_from_user(opt+1, optval, optlen))\n@@ -502,8 +510,10 @@ static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,\n \t\tretv = 0;\n \t\topt = ipv6_update_options(sk, opt);\n done:\n-\t\tif (opt)\n-\t\t\tsock_kfree_s(sk, opt, opt->tot_len);\n+\t\tif (opt) {\n+\t\t\tatomic_sub(opt->tot_len, &sk->sk_omem_alloc);\n+\t\t\ttxopt_put(opt);\n+\t\t}\n \t\tbreak;\n \t}\n \tcase IPV6_UNICAST_HOPS:\n@@ -1110,10 +1120,11 @@ static int do_ipv6_getsockopt(struct sock *sk, int level, int optname,\n \tcase IPV6_RTHDR:\n \tcase IPV6_DSTOPTS:\n \t{\n+\t\tstruct ipv6_txoptions *opt;\n \n \t\tlock_sock(sk);\n-\t\tlen = ipv6_getsockopt_sticky(sk, np->opt,\n-\t\t\t\t\t     optname, optval, len);\n+\t\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\t\tlen = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);\n \t\trelease_sock(sk);\n \t\t/* check if ipv6_getsockopt_sticky() returns err code */\n \t\tif (len < 0)", "file_path": "files/2016_8\\89", "file_language": "c", "file_name": "net/ipv6/ipv6_sockglue.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/raw.c", "code": "/*\n *\tRAW sockets for IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tAdapted from linux/net/ipv4/raw.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI,H.@USAGI\t:\traw checksum (RFC2292(bis) compliance)\n *\tKazunori MIYAZAWA @USAGI:\tchange process style to use ip6_append_data\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/slab.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/icmpv6.h>\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv6.h>\n#include <linux/skbuff.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/ip6_route.h>\n#include <net/ip6_checksum.h>\n#include <net/addrconf.h>\n#include <net/transp_v6.h>\n#include <net/udp.h>\n#include <net/inet_common.h>\n#include <net/tcp_states.h>\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n#include <net/mip6.h>\n#endif\n#include <linux/mroute6.h>\n\n#include <net/raw.h>\n#include <net/rawv6.h>\n#include <net/xfrm.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/export.h>\n\n#define\tICMPV6_HDRLEN\t4\t/* ICMPv6 header, RFC 4443 Section 2.1 */\n\nstatic struct raw_hashinfo raw_v6_hashinfo = {\n\t.lock = __RW_LOCK_UNLOCKED(raw_v6_hashinfo.lock),\n};\n\nstatic struct sock *__raw_v6_lookup(struct net *net, struct sock *sk,\n\t\tunsigned short num, const struct in6_addr *loc_addr,\n\t\tconst struct in6_addr *rmt_addr, int dif)\n{\n\tbool is_multicast = ipv6_addr_is_multicast(loc_addr);\n\n\tsk_for_each_from(sk)\n\t\tif (inet_sk(sk)->inet_num == num) {\n\n\t\t\tif (!net_eq(sock_net(sk), net))\n\t\t\t\tcontinue;\n\n\t\t\tif (!ipv6_addr_any(&sk->sk_v6_daddr) &&\n\t\t\t    !ipv6_addr_equal(&sk->sk_v6_daddr, rmt_addr))\n\t\t\t\tcontinue;\n\n\t\t\tif (sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif)\n\t\t\t\tcontinue;\n\n\t\t\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\t\tif (ipv6_addr_equal(&sk->sk_v6_rcv_saddr, loc_addr))\n\t\t\t\t\tgoto found;\n\t\t\t\tif (is_multicast &&\n\t\t\t\t    inet6_mc_check(sk, loc_addr, rmt_addr))\n\t\t\t\t\tgoto found;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tgoto found;\n\t\t}\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\n/*\n *\t0 - deliver\n *\t1 - block\n */\nstatic int icmpv6_filter(const struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct icmp6hdr _hdr;\n\tconst struct icmp6hdr *hdr;\n\n\t/* We require only the four bytes of the ICMPv6 header, not any\n\t * additional bytes of message body in \"struct icmp6hdr\".\n\t */\n\thdr = skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t ICMPV6_HDRLEN, &_hdr);\n\tif (hdr) {\n\t\tconst __u32 *data = &raw6_sk(sk)->filter.data[0];\n\t\tunsigned int type = hdr->icmp6_type;\n\n\t\treturn (data[type >> 5] & (1U << (type & 31))) != 0;\n\t}\n\treturn 1;\n}\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\ntypedef int mh_filter_t(struct sock *sock, struct sk_buff *skb);\n\nstatic mh_filter_t __rcu *mh_filter __read_mostly;\n\nint rawv6_mh_filter_register(mh_filter_t filter)\n{\n\trcu_assign_pointer(mh_filter, filter);\n\treturn 0;\n}\nEXPORT_SYMBOL(rawv6_mh_filter_register);\n\nint rawv6_mh_filter_unregister(mh_filter_t filter)\n{\n\tRCU_INIT_POINTER(mh_filter, NULL);\n\tsynchronize_rcu();\n\treturn 0;\n}\nEXPORT_SYMBOL(rawv6_mh_filter_unregister);\n\n#endif\n\n/*\n *\tdemultiplex raw sockets.\n *\t(should consider queueing the skb in the sock receive_queue\n *\twithout calling rawv6.c)\n *\n *\tCaller owns SKB so we must make clones.\n */\nstatic bool ipv6_raw_deliver(struct sk_buff *skb, int nexthdr)\n{\n\tconst struct in6_addr *saddr;\n\tconst struct in6_addr *daddr;\n\tstruct sock *sk;\n\tbool delivered = false;\n\t__u8 hash;\n\tstruct net *net;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = saddr + 1;\n\n\thash = nexthdr & (RAW_HTABLE_SIZE - 1);\n\n\tread_lock(&raw_v6_hashinfo.lock);\n\tsk = sk_head(&raw_v6_hashinfo.ht[hash]);\n\n\tif (!sk)\n\t\tgoto out;\n\n\tnet = dev_net(skb->dev);\n\tsk = __raw_v6_lookup(net, sk, nexthdr, daddr, saddr, inet6_iif(skb));\n\n\twhile (sk) {\n\t\tint filtered;\n\n\t\tdelivered = true;\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_ICMPV6:\n\t\t\tfiltered = icmpv6_filter(sk, skb);\n\t\t\tbreak;\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\tcase IPPROTO_MH:\n\t\t{\n\t\t\t/* XXX: To validate MH only once for each packet,\n\t\t\t * this is placed here. It should be after checking\n\t\t\t * xfrm policy, however it doesn't. The checking xfrm\n\t\t\t * policy is placed in rawv6_rcv() because it is\n\t\t\t * required for each socket.\n\t\t\t */\n\t\t\tmh_filter_t *filter;\n\n\t\t\tfilter = rcu_dereference(mh_filter);\n\t\t\tfiltered = filter ? (*filter)(sk, skb) : 0;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tdefault:\n\t\t\tfiltered = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (filtered < 0)\n\t\t\tbreak;\n\t\tif (filtered == 0) {\n\t\t\tstruct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Not releasing hash table! */\n\t\t\tif (clone) {\n\t\t\t\tnf_reset(clone);\n\t\t\t\trawv6_rcv(sk, clone);\n\t\t\t}\n\t\t}\n\t\tsk = __raw_v6_lookup(net, sk_next(sk), nexthdr, daddr, saddr,\n\t\t\t\t     inet6_iif(skb));\n\t}\nout:\n\tread_unlock(&raw_v6_hashinfo.lock);\n\treturn delivered;\n}\n\nbool raw6_local_deliver(struct sk_buff *skb, int nexthdr)\n{\n\tstruct sock *raw_sk;\n\n\traw_sk = sk_head(&raw_v6_hashinfo.ht[nexthdr & (RAW_HTABLE_SIZE - 1)]);\n\tif (raw_sk && !ipv6_raw_deliver(skb, nexthdr))\n\t\traw_sk = NULL;\n\n\treturn raw_sk != NULL;\n}\n\n/* This cleans up af_inet6 a bit. -DaveM */\nstatic int rawv6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *addr = (struct sockaddr_in6 *) uaddr;\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (addr->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->sin6_addr);\n\n\t/* Raw sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out;\n\n\trcu_read_lock();\n\t/* Check if the address belongs to the host. */\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->sin6_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->sin6_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\tif (!(addr_type & IPV6_ADDR_MULTICAST) &&\n\t\t    !sock_net(sk)->ipv6.sysctl.ip_nonlocal_bind) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->sin6_addr,\n\t\t\t\t\t   dev, 0)) {\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->sin6_addr;\n\tif (!(addr_type & IPV6_ADDR_MULTICAST))\n\t\tnp->saddr = addr->sin6_addr;\n\terr = 0;\nout_unlock:\n\trcu_read_unlock();\nout:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic void rawv6_err(struct sock *sk, struct sk_buff *skb,\n\t       struct inet6_skb_parm *opt,\n\t       u8 type, u8 code, int offset, __be32 info)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint err;\n\tint harderr;\n\n\t/* Report error on raw socket, if:\n\t   1. User requested recverr.\n\t   2. Socket is connected (otherwise the error indication\n\t      is useless without recverr and error is hard.\n\t */\n\tif (!np->recverr && sk->sk_state != TCP_ESTABLISHED)\n\t\treturn;\n\n\tharderr = icmpv6_err_convert(type, code, &err);\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tip6_sk_update_pmtu(skb, sk, info);\n\t\tharderr = (np->pmtudisc == IPV6_PMTUDISC_DO);\n\t}\n\tif (type == NDISC_REDIRECT) {\n\t\tip6_sk_redirect(skb, sk);\n\t\treturn;\n\t}\n\tif (np->recverr) {\n\t\tu8 *payload = skb->data;\n\t\tif (!inet->hdrincl)\n\t\t\tpayload += offset;\n\t\tipv6_icmp_error(sk, skb, err, 0, ntohl(info), payload);\n\t}\n\n\tif (np->recverr || harderr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t}\n}\n\nvoid raw6_icmp_error(struct sk_buff *skb, int nexthdr,\n\t\tu8 type, u8 code, int inner_offset, __be32 info)\n{\n\tstruct sock *sk;\n\tint hash;\n\tconst struct in6_addr *saddr, *daddr;\n\tstruct net *net;\n\n\thash = nexthdr & (RAW_HTABLE_SIZE - 1);\n\n\tread_lock(&raw_v6_hashinfo.lock);\n\tsk = sk_head(&raw_v6_hashinfo.ht[hash]);\n\tif (sk) {\n\t\t/* Note: ipv6_hdr(skb) != skb->data */\n\t\tconst struct ipv6hdr *ip6h = (const struct ipv6hdr *)skb->data;\n\t\tsaddr = &ip6h->saddr;\n\t\tdaddr = &ip6h->daddr;\n\t\tnet = dev_net(skb->dev);\n\n\t\twhile ((sk = __raw_v6_lookup(net, sk, nexthdr, saddr, daddr,\n\t\t\t\t\t\tinet6_iif(skb)))) {\n\t\t\trawv6_err(sk, skb, NULL, type, code,\n\t\t\t\t\tinner_offset, info);\n\t\t\tsk = sk_next(sk);\n\t\t}\n\t}\n\tread_unlock(&raw_v6_hashinfo.lock);\n}\n\nstatic inline int rawv6_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tif ((raw6_sk(sk)->checksum || rcu_access_pointer(sk->sk_filter)) &&\n\t    skb_checksum_complete(skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\t/* Charge it to the socket. */\n\tskb_dst_drop(skb);\n\tif (sock_queue_rcv_skb(sk, skb) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\treturn 0;\n}\n\n/*\n *\tThis is next to useless...\n *\tif we demultiplex in network layer we don't need the extra call\n *\tjust to queue the skb...\n *\tmaybe we could have the network decide upon a hint if it\n *\tshould call raw_rcv for demultiplexing\n */\nint rawv6_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tif (!rp->checksum)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tskb_postpull_rcsum(skb, skb_network_header(skb),\n\t\t\t\t   skb_network_header_len(skb));\n\t\tif (!csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t     &ipv6_hdr(skb)->daddr,\n\t\t\t\t     skb->len, inet->inet_num, skb->csum))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\tif (!skb_csum_unnecessary(skb))\n\t\tskb->csum = ~csum_unfold(csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t\t skb->len,\n\t\t\t\t\t\t\t inet->inet_num, 0));\n\n\tif (inet->hdrincl) {\n\t\tif (skb_checksum_complete(skb)) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tkfree_skb(skb);\n\t\t\treturn NET_RX_DROP;\n\t\t}\n\t}\n\n\trawv6_rcv_skb(sk, skb);\n\treturn 0;\n}\n\n\n/*\n *\tThis should be easy, if there is something there\n *\twe return it, otherwise we block.\n */\n\nstatic int rawv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len, addr_len);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\tif (skb_csum_unnecessary(skb)) {\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\t} else if (msg->msg_flags&MSG_TRUNC) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, 0, msg);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\n\t/* Copy the address. */\n\tif (sin6) {\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = 0;\n\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t\t  inet6_iif(skb));\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = skb->len;\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tskb_kill_datagram(sk, skb, flags);\n\n\t/* Error for blocking case is chosen to masquerade\n\t   as some normal condition.\n\t */\n\terr = (flags&MSG_DONTWAIT) ? -EAGAIN : -EHOSTUNREACH;\n\tgoto out;\n}\n\nstatic int rawv6_push_pending_frames(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t     struct raw6_sock *rp)\n{\n\tstruct sk_buff *skb;\n\tint err = 0;\n\tint offset;\n\tint len;\n\tint total_len;\n\t__wsum tmp_csum;\n\t__sum16 csum;\n\n\tif (!rp->checksum)\n\t\tgoto send;\n\n\tskb = skb_peek(&sk->sk_write_queue);\n\tif (!skb)\n\t\tgoto out;\n\n\toffset = rp->offset;\n\ttotal_len = inet_sk(sk)->cork.base.length;\n\tif (offset >= total_len - 1) {\n\t\terr = -EINVAL;\n\t\tip6_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\t/* should be check HW csum miyazawa */\n\tif (skb_queue_len(&sk->sk_write_queue) == 1) {\n\t\t/*\n\t\t * Only one fragment on the socket.\n\t\t */\n\t\ttmp_csum = skb->csum;\n\t} else {\n\t\tstruct sk_buff *csum_skb = NULL;\n\t\ttmp_csum = 0;\n\n\t\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\t\ttmp_csum = csum_add(tmp_csum, skb->csum);\n\n\t\t\tif (csum_skb)\n\t\t\t\tcontinue;\n\n\t\t\tlen = skb->len - skb_transport_offset(skb);\n\t\t\tif (offset >= len) {\n\t\t\t\toffset -= len;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tcsum_skb = skb;\n\t\t}\n\n\t\tskb = csum_skb;\n\t}\n\n\toffset += skb_transport_offset(skb);\n\tBUG_ON(skb_copy_bits(skb, offset, &csum, 2));\n\n\t/* in case cksum was not initialized */\n\tif (unlikely(csum))\n\t\ttmp_csum = csum_sub(tmp_csum, csum_unfold(csum));\n\n\tcsum = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t       total_len, fl6->flowi6_proto, tmp_csum);\n\n\tif (csum == 0 && fl6->flowi6_proto == IPPROTO_UDP)\n\t\tcsum = CSUM_MANGLED_0;\n\n\tBUG_ON(skb_store_bits(skb, offset, &csum, 2));\n\nsend:\n\terr = ip6_push_pending_frames(sk);\nout:\n\treturn err;\n}\n\nstatic int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,\n\t\t\tstruct flowi6 *fl6, struct dst_entry **dstp,\n\t\t\tunsigned int flags)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6hdr *iph;\n\tstruct sk_buff *skb;\n\tint err;\n\tstruct rt6_info *rt = (struct rt6_info *)*dstp;\n\tint hlen = LL_RESERVED_SPACE(rt->dst.dev);\n\tint tlen = rt->dst.dev->needed_tailroom;\n\n\tif (length > rt->dst.dev->mtu) {\n\t\tipv6_local_error(sk, EMSGSIZE, fl6, rt->dst.dev->mtu);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (flags&MSG_PROBE)\n\t\tgoto out;\n\n\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t  length + hlen + tlen + 15,\n\t\t\t\t  flags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\tgoto error;\n\tskb_reserve(skb, hlen);\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\tskb_dst_set(skb, &rt->dst);\n\t*dstp = NULL;\n\n\tskb_put(skb, length);\n\tskb_reset_network_header(skb);\n\tiph = ipv6_hdr(skb);\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tskb->transport_header = skb->network_header;\n\terr = memcpy_from_msg(iph, msg, length);\n\tif (err)\n\t\tgoto error_fault;\n\n\tIP6_UPD_PO_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUT, skb->len);\n\terr = NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT, net, sk, skb,\n\t\t      NULL, rt->dst.dev, dst_output);\n\tif (err > 0)\n\t\terr = net_xmit_errno(err);\n\tif (err)\n\t\tgoto error;\nout:\n\treturn 0;\n\nerror_fault:\n\terr = -EFAULT;\n\tkfree_skb(skb);\nerror:\n\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\tif (err == -ENOBUFS && !np->recverr)\n\t\terr = 0;\n\treturn err;\n}\n\nstruct raw6_frag_vec {\n\tstruct msghdr *msg;\n\tint hlen;\n\tchar c[4];\n};\n\nstatic int rawv6_probe_proto_opt(struct raw6_frag_vec *rfv, struct flowi6 *fl6)\n{\n\tint err = 0;\n\tswitch (fl6->flowi6_proto) {\n\tcase IPPROTO_ICMPV6:\n\t\trfv->hlen = 2;\n\t\terr = memcpy_from_msg(rfv->c, rfv->msg, rfv->hlen);\n\t\tif (!err) {\n\t\t\tfl6->fl6_icmp_type = rfv->c[0];\n\t\t\tfl6->fl6_icmp_code = rfv->c[1];\n\t\t}\n\t\tbreak;\n\tcase IPPROTO_MH:\n\t\trfv->hlen = 4;\n\t\terr = memcpy_from_msg(rfv->c, rfv->msg, rfv->hlen);\n\t\tif (!err)\n\t\t\tfl6->fl6_mh_type = rfv->c[2];\n\t}\n\treturn err;\n}\n\nstatic int raw6_getfrag(void *from, char *to, int offset, int len, int odd,\n\t\t       struct sk_buff *skb)\n{\n\tstruct raw6_frag_vec *rfv = from;\n\n\tif (offset < rfv->hlen) {\n\t\tint copy = min(rfv->hlen - offset, len);\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\tmemcpy(to, rfv->c + offset, copy);\n\t\telse\n\t\t\tskb->csum = csum_block_add(\n\t\t\t\tskb->csum,\n\t\t\t\tcsum_partial_copy_nocheck(rfv->c + offset,\n\t\t\t\t\t\t\t  to, copy, 0),\n\t\t\t\todd);\n\n\t\todd = 0;\n\t\toffset += copy;\n\t\tto += copy;\n\t\tlen -= copy;\n\n\t\tif (!len)\n\t\t\treturn 0;\n\t}\n\n\toffset -= rfv->hlen;\n\n\treturn ip_generic_getfrag(rfv->msg, to, offset, len, odd, skb);\n}\n\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\trfv.msg = msg;\n\trfv.hlen = 0;\n\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\tif (err)\n\t\tgoto out;\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tif (inet->hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (inet->hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst, msg->msg_flags);\n\telse {\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, hlimit, tclass, opt, &fl6, (struct rt6_info *)dst,\n\t\t\tmsg->msg_flags, dontfrag);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n\nstatic int rawv6_seticmpfilter(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, int optlen)\n{\n\tswitch (optname) {\n\tcase ICMPV6_FILTER:\n\t\tif (optlen > sizeof(struct icmp6_filter))\n\t\t\toptlen = sizeof(struct icmp6_filter);\n\t\tif (copy_from_user(&raw6_sk(sk)->filter, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\treturn 0;\n}\n\nstatic int rawv6_geticmpfilter(struct sock *sk, int level, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tint len;\n\n\tswitch (optname) {\n\tcase ICMPV6_FILTER:\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (len < 0)\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(struct icmp6_filter))\n\t\t\tlen = sizeof(struct icmp6_filter);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &raw6_sk(sk)->filter, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\treturn 0;\n}\n\n\nstatic int do_rawv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tint val;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase IPV6_CHECKSUM:\n\t\tif (inet_sk(sk)->inet_num == IPPROTO_ICMPV6 &&\n\t\t    level == IPPROTO_IPV6) {\n\t\t\t/*\n\t\t\t * RFC3542 tells that IPV6_CHECKSUM socket\n\t\t\t * option in the IPPROTO_IPV6 level is not\n\t\t\t * allowed on ICMPv6 sockets.\n\t\t\t * If you want to set it, use IPPROTO_RAW\n\t\t\t * level IPV6_CHECKSUM socket option\n\t\t\t * (Linux extension).\n\t\t\t */\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* You may get strange result with a positive odd offset;\n\t\t   RFC2292bis agrees with me. */\n\t\tif (val > 0 && (val&1))\n\t\t\treturn -EINVAL;\n\t\tif (val < 0) {\n\t\t\trp->checksum = 0;\n\t\t} else {\n\t\t\trp->checksum = 1;\n\t\t\trp->offset = val;\n\t\t}\n\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int rawv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_seticmpfilter(sk, level, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM)\n\t\t\tbreak;\n\tdefault:\n\t\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\treturn do_rawv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_rawv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t   char __user *optval, unsigned int optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_seticmpfilter(sk, level, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM)\n\t\t\tbreak;\n\tdefault:\n\t\treturn compat_ipv6_setsockopt(sk, level, optname,\n\t\t\t\t\t      optval, optlen);\n\t}\n\treturn do_rawv6_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int do_rawv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase IPV6_CHECKSUM:\n\t\t/*\n\t\t * We allow getsockopt() for IPPROTO_IPV6-level\n\t\t * IPV6_CHECKSUM socket option on ICMPv6 sockets\n\t\t * since RFC3542 is silent about it.\n\t\t */\n\t\tif (rp->checksum == 0)\n\t\t\tval = -1;\n\t\telse\n\t\t\tval = rp->offset;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tlen = min_t(unsigned int, sizeof(int), len);\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int rawv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_geticmpfilter(sk, level, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM)\n\t\t\tbreak;\n\tdefault:\n\t\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\treturn do_rawv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_rawv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t\t   char __user *optval, int __user *optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_geticmpfilter(sk, level, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM)\n\t\t\tbreak;\n\tdefault:\n\t\treturn compat_ipv6_getsockopt(sk, level, optname,\n\t\t\t\t\t      optval, optlen);\n\t}\n\treturn do_rawv6_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int rawv6_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ: {\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCINQ: {\n\t\tstruct sk_buff *skb;\n\t\tint amount = 0;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb_tail_pointer(skb) -\n\t\t\t\tskb_transport_header(skb);\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\n\tdefault:\n#ifdef CONFIG_IPV6_MROUTE\n\t\treturn ip6mr_ioctl(sk, cmd, (void __user *)arg);\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_rawv6_ioctl(struct sock *sk, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\tcase SIOCINQ:\n\t\treturn -ENOIOCTLCMD;\n\tdefault:\n#ifdef CONFIG_IPV6_MROUTE\n\t\treturn ip6mr_compat_ioctl(sk, cmd, compat_ptr(arg));\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n#endif\n\nstatic void rawv6_close(struct sock *sk, long timeout)\n{\n\tif (inet_sk(sk)->inet_num == IPPROTO_RAW)\n\t\tip6_ra_control(sk, -1);\n\tip6mr_sk_done(sk);\n\tsk_common_release(sk);\n}\n\nstatic void raw6_destroy(struct sock *sk)\n{\n\tlock_sock(sk);\n\tip6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tinet6_destroy_sock(sk);\n}\n\nstatic int rawv6_init_sk(struct sock *sk)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\n\tswitch (inet_sk(sk)->inet_num) {\n\tcase IPPROTO_ICMPV6:\n\t\trp->checksum = 1;\n\t\trp->offset   = 2;\n\t\tbreak;\n\tcase IPPROTO_MH:\n\t\trp->checksum = 1;\n\t\trp->offset   = 4;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstruct proto rawv6_prot = {\n\t.name\t\t   = \"RAWv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = rawv6_close,\n\t.destroy\t   = raw6_destroy,\n\t.connect\t   = ip6_datagram_connect_v6_only,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = rawv6_ioctl,\n\t.init\t\t   = rawv6_init_sk,\n\t.setsockopt\t   = rawv6_setsockopt,\n\t.getsockopt\t   = rawv6_getsockopt,\n\t.sendmsg\t   = rawv6_sendmsg,\n\t.recvmsg\t   = rawv6_recvmsg,\n\t.bind\t\t   = rawv6_bind,\n\t.backlog_rcv\t   = rawv6_rcv_skb,\n\t.hash\t\t   = raw_hash_sk,\n\t.unhash\t\t   = raw_unhash_sk,\n\t.obj_size\t   = sizeof(struct raw6_sock),\n\t.h.raw_hash\t   = &raw_v6_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_rawv6_setsockopt,\n\t.compat_getsockopt = compat_rawv6_getsockopt,\n\t.compat_ioctl\t   = compat_rawv6_ioctl,\n#endif\n};\n\n#ifdef CONFIG_PROC_FS\nstatic int raw6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, IPV6_SEQ_DGRAM_HEADER);\n\t} else {\n\t\tstruct sock *sp = v;\n\t\t__u16 srcp  = inet_sk(sp)->inet_num;\n\t\tip6_dgram_sock_seq_show(seq, v, srcp, 0,\n\t\t\t\t\traw_seq_private(seq)->bucket);\n\t}\n\treturn 0;\n}\n\nstatic const struct seq_operations raw6_seq_ops = {\n\t.start =\traw_seq_start,\n\t.next =\t\traw_seq_next,\n\t.stop =\t\traw_seq_stop,\n\t.show =\t\traw6_seq_show,\n};\n\nstatic int raw6_seq_open(struct inode *inode, struct file *file)\n{\n\treturn raw_seq_open(inode, file, &raw_v6_hashinfo, &raw6_seq_ops);\n}\n\nstatic const struct file_operations raw6_seq_fops = {\n\t.owner =\tTHIS_MODULE,\n\t.open =\t\traw6_seq_open,\n\t.read =\t\tseq_read,\n\t.llseek =\tseq_lseek,\n\t.release =\tseq_release_net,\n};\n\nstatic int __net_init raw6_init_net(struct net *net)\n{\n\tif (!proc_create(\"raw6\", S_IRUGO, net->proc_net, &raw6_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __net_exit raw6_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"raw6\", net->proc_net);\n}\n\nstatic struct pernet_operations raw6_net_ops = {\n\t.init = raw6_init_net,\n\t.exit = raw6_exit_net,\n};\n\nint __init raw6_proc_init(void)\n{\n\treturn register_pernet_subsys(&raw6_net_ops);\n}\n\nvoid raw6_proc_exit(void)\n{\n\tunregister_pernet_subsys(&raw6_net_ops);\n}\n#endif\t/* CONFIG_PROC_FS */\n\n/* Same as inet6_dgram_ops, sans udp_poll.  */\nstatic const struct proto_ops inet6_sockraw_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_dgram_connect,\t/* ok\t\t*/\n\t.socketpair\t   = sock_no_socketpair,\t/* a do nothing\t*/\n\t.accept\t\t   = sock_no_accept,\t\t/* a do nothing\t*/\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = datagram_poll,\t\t/* ok\t\t*/\n\t.ioctl\t\t   = inet6_ioctl,\t\t/* must change  */\n\t.listen\t\t   = sock_no_listen,\t\t/* ok\t\t*/\n\t.shutdown\t   = inet_shutdown,\t\t/* ok\t\t*/\n\t.setsockopt\t   = sock_common_setsockopt,\t/* ok\t\t*/\n\t.getsockopt\t   = sock_common_getsockopt,\t/* ok\t\t*/\n\t.sendmsg\t   = inet_sendmsg,\t\t/* ok\t\t*/\n\t.recvmsg\t   = sock_common_recvmsg,\t/* ok\t\t*/\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw rawv6_protosw = {\n\t.type\t\t= SOCK_RAW,\n\t.protocol\t= IPPROTO_IP,\t/* wild card */\n\t.prot\t\t= &rawv6_prot,\n\t.ops\t\t= &inet6_sockraw_ops,\n\t.flags\t\t= INET_PROTOSW_REUSE,\n};\n\nint __init rawv6_init(void)\n{\n\treturn inet6_register_protosw(&rawv6_protosw);\n}\n\nvoid rawv6_exit(void)\n{\n\tinet6_unregister_protosw(&rawv6_protosw);\n}\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tRAW sockets for IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tAdapted from linux/net/ipv4/raw.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI,H.@USAGI\t:\traw checksum (RFC2292(bis) compliance)\n *\tKazunori MIYAZAWA @USAGI:\tchange process style to use ip6_append_data\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/slab.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/icmpv6.h>\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv6.h>\n#include <linux/skbuff.h>\n#include <linux/compat.h>\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/ip6_route.h>\n#include <net/ip6_checksum.h>\n#include <net/addrconf.h>\n#include <net/transp_v6.h>\n#include <net/udp.h>\n#include <net/inet_common.h>\n#include <net/tcp_states.h>\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n#include <net/mip6.h>\n#endif\n#include <linux/mroute6.h>\n\n#include <net/raw.h>\n#include <net/rawv6.h>\n#include <net/xfrm.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/export.h>\n\n#define\tICMPV6_HDRLEN\t4\t/* ICMPv6 header, RFC 4443 Section 2.1 */\n\nstruct raw_hashinfo raw_v6_hashinfo;\nEXPORT_SYMBOL_GPL(raw_v6_hashinfo);\n\nbool raw_v6_match(struct net *net, const struct sock *sk, unsigned short num,\n\t\t  const struct in6_addr *loc_addr,\n\t\t  const struct in6_addr *rmt_addr, int dif, int sdif)\n{\n\tif (inet_sk(sk)->inet_num != num ||\n\t    !net_eq(sock_net(sk), net) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_daddr) &&\n\t     !ipv6_addr_equal(&sk->sk_v6_daddr, rmt_addr)) ||\n\t    !raw_sk_bound_dev_eq(net, sk->sk_bound_dev_if,\n\t\t\t\t dif, sdif))\n\t\treturn false;\n\n\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t    ipv6_addr_equal(&sk->sk_v6_rcv_saddr, loc_addr) ||\n\t    (ipv6_addr_is_multicast(loc_addr) &&\n\t     inet6_mc_check(sk, loc_addr, rmt_addr)))\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(raw_v6_match);\n\n/*\n *\t0 - deliver\n *\t1 - block\n */\nstatic int icmpv6_filter(const struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct icmp6hdr _hdr;\n\tconst struct icmp6hdr *hdr;\n\n\t/* We require only the four bytes of the ICMPv6 header, not any\n\t * additional bytes of message body in \"struct icmp6hdr\".\n\t */\n\thdr = skb_header_pointer(skb, skb_transport_offset(skb),\n\t\t\t\t ICMPV6_HDRLEN, &_hdr);\n\tif (hdr) {\n\t\tconst __u32 *data = &raw6_sk(sk)->filter.data[0];\n\t\tunsigned int type = hdr->icmp6_type;\n\n\t\treturn (data[type >> 5] & (1U << (type & 31))) != 0;\n\t}\n\treturn 1;\n}\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\ntypedef int mh_filter_t(struct sock *sock, struct sk_buff *skb);\n\nstatic mh_filter_t __rcu *mh_filter __read_mostly;\n\nint rawv6_mh_filter_register(mh_filter_t filter)\n{\n\trcu_assign_pointer(mh_filter, filter);\n\treturn 0;\n}\nEXPORT_SYMBOL(rawv6_mh_filter_register);\n\nint rawv6_mh_filter_unregister(mh_filter_t filter)\n{\n\tRCU_INIT_POINTER(mh_filter, NULL);\n\tsynchronize_rcu();\n\treturn 0;\n}\nEXPORT_SYMBOL(rawv6_mh_filter_unregister);\n\n#endif\n\n/*\n *\tdemultiplex raw sockets.\n *\t(should consider queueing the skb in the sock receive_queue\n *\twithout calling rawv6.c)\n *\n *\tCaller owns SKB so we must make clones.\n */\nstatic bool ipv6_raw_deliver(struct sk_buff *skb, int nexthdr)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct in6_addr *saddr;\n\tconst struct in6_addr *daddr;\n\tstruct hlist_head *hlist;\n\tstruct sock *sk;\n\tbool delivered = false;\n\t__u8 hash;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = saddr + 1;\n\n\thash = raw_hashfunc(net, nexthdr);\n\thlist = &raw_v6_hashinfo.ht[hash];\n\trcu_read_lock();\n\tsk_for_each_rcu(sk, hlist) {\n\t\tint filtered;\n\n\t\tif (!raw_v6_match(net, sk, nexthdr, daddr, saddr,\n\t\t\t\t  inet6_iif(skb), inet6_sdif(skb)))\n\t\t\tcontinue;\n\n\t\tif (atomic_read(&sk->sk_rmem_alloc) >=\n\t\t    READ_ONCE(sk->sk_rcvbuf)) {\n\t\t\tsk_drops_inc(sk);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdelivered = true;\n\t\tswitch (nexthdr) {\n\t\tcase IPPROTO_ICMPV6:\n\t\t\tfiltered = icmpv6_filter(sk, skb);\n\t\t\tbreak;\n\n#if IS_ENABLED(CONFIG_IPV6_MIP6)\n\t\tcase IPPROTO_MH:\n\t\t{\n\t\t\t/* XXX: To validate MH only once for each packet,\n\t\t\t * this is placed here. It should be after checking\n\t\t\t * xfrm policy, however it doesn't. The checking xfrm\n\t\t\t * policy is placed in rawv6_rcv() because it is\n\t\t\t * required for each socket.\n\t\t\t */\n\t\t\tmh_filter_t *filter;\n\n\t\t\tfilter = rcu_dereference(mh_filter);\n\t\t\tfiltered = filter ? (*filter)(sk, skb) : 0;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tdefault:\n\t\t\tfiltered = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (filtered < 0)\n\t\t\tbreak;\n\t\tif (filtered == 0) {\n\t\t\tstruct sk_buff *clone = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Not releasing hash table! */\n\t\t\tif (clone)\n\t\t\t\trawv6_rcv(sk, clone);\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn delivered;\n}\n\nbool raw6_local_deliver(struct sk_buff *skb, int nexthdr)\n{\n\treturn ipv6_raw_deliver(skb, nexthdr);\n}\n\n/* This cleans up af_inet6 a bit. -DaveM */\nstatic int rawv6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *addr = (struct sockaddr_in6 *) uaddr;\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (addr->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->sin6_addr);\n\n\t/* Raw sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out;\n\n\trcu_read_lock();\n\t/* Check if the address belongs to the host. */\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->sin6_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->sin6_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\tif (!(addr_type & IPV6_ADDR_MULTICAST) &&\n\t\t    !ipv6_can_nonlocal_bind(sock_net(sk), inet)) {\n\t\t\terr = -EADDRNOTAVAIL;\n\t\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->sin6_addr,\n\t\t\t\t\t   dev, 0)) {\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t}\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->sin6_addr;\n\tif (!(addr_type & IPV6_ADDR_MULTICAST))\n\t\tnp->saddr = addr->sin6_addr;\n\terr = 0;\nout_unlock:\n\trcu_read_unlock();\nout:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic void rawv6_err(struct sock *sk, struct sk_buff *skb,\n\t\t      u8 type, u8 code, int offset, __be32 info)\n{\n\tbool recverr = inet6_test_bit(RECVERR6, sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tint err;\n\tint harderr;\n\n\t/* Report error on raw socket, if:\n\t   1. User requested recverr.\n\t   2. Socket is connected (otherwise the error indication\n\t      is useless without recverr and error is hard.\n\t */\n\tif (!recverr && sk->sk_state != TCP_ESTABLISHED)\n\t\treturn;\n\n\tharderr = icmpv6_err_convert(type, code, &err);\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tip6_sk_update_pmtu(skb, sk, info);\n\t\tharderr = (READ_ONCE(np->pmtudisc) == IPV6_PMTUDISC_DO);\n\t}\n\tif (type == NDISC_REDIRECT) {\n\t\tip6_sk_redirect(skb, sk);\n\t\treturn;\n\t}\n\tif (recverr) {\n\t\tu8 *payload = skb->data;\n\t\tif (!inet_test_bit(HDRINCL, sk))\n\t\t\tpayload += offset;\n\t\tipv6_icmp_error(sk, skb, err, 0, ntohl(info), payload);\n\t}\n\n\tif (recverr || harderr) {\n\t\tsk->sk_err = err;\n\t\tsk_error_report(sk);\n\t}\n}\n\nvoid raw6_icmp_error(struct sk_buff *skb, int nexthdr,\n\t\tu8 type, u8 code, int inner_offset, __be32 info)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct hlist_head *hlist;\n\tstruct sock *sk;\n\tint hash;\n\n\thash = raw_hashfunc(net, nexthdr);\n\thlist = &raw_v6_hashinfo.ht[hash];\n\trcu_read_lock();\n\tsk_for_each_rcu(sk, hlist) {\n\t\t/* Note: ipv6_hdr(skb) != skb->data */\n\t\tconst struct ipv6hdr *ip6h = (const struct ipv6hdr *)skb->data;\n\n\t\tif (!raw_v6_match(net, sk, nexthdr, &ip6h->saddr, &ip6h->daddr,\n\t\t\t\t  inet6_iif(skb), inet6_iif(skb)))\n\t\t\tcontinue;\n\t\trawv6_err(sk, skb, type, code, inner_offset, info);\n\t}\n\trcu_read_unlock();\n}\n\nstatic inline int rawv6_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tenum skb_drop_reason reason;\n\n\tif ((raw6_sk(sk)->checksum || rcu_access_pointer(sk->sk_filter)) &&\n\t    skb_checksum_complete(skb)) {\n\t\tsk_drops_inc(sk);\n\t\tsk_skb_reason_drop(sk, skb, SKB_DROP_REASON_SKB_CSUM);\n\t\treturn NET_RX_DROP;\n\t}\n\n\t/* Charge it to the socket. */\n\tskb_dst_drop(skb);\n\tif (sock_queue_rcv_skb_reason(sk, skb, &reason) < 0) {\n\t\tsk_skb_reason_drop(sk, skb, reason);\n\t\treturn NET_RX_DROP;\n\t}\n\n\treturn 0;\n}\n\n/*\n *\tThis is next to useless...\n *\tif we demultiplex in network layer we don't need the extra call\n *\tjust to queue the skb...\n *\tmaybe we could have the network decide upon a hint if it\n *\tshould call raw_rcv for demultiplexing\n */\nint rawv6_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tsk_drops_inc(sk);\n\t\tsk_skb_reason_drop(sk, skb, SKB_DROP_REASON_XFRM_POLICY);\n\t\treturn NET_RX_DROP;\n\t}\n\tnf_reset_ct(skb);\n\n\tif (!rp->checksum)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tskb_postpull_rcsum(skb, skb_network_header(skb),\n\t\t\t\t   skb_network_header_len(skb));\n\t\tif (!csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t     &ipv6_hdr(skb)->daddr,\n\t\t\t\t     skb->len, inet->inet_num, skb->csum))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\tif (!skb_csum_unnecessary(skb))\n\t\tskb->csum = ~csum_unfold(csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t\t skb->len,\n\t\t\t\t\t\t\t inet->inet_num, 0));\n\n\tif (inet_test_bit(HDRINCL, sk)) {\n\t\tif (skb_checksum_complete(skb)) {\n\t\t\tsk_drops_inc(sk);\n\t\t\tsk_skb_reason_drop(sk, skb, SKB_DROP_REASON_SKB_CSUM);\n\t\t\treturn NET_RX_DROP;\n\t\t}\n\t}\n\n\trawv6_rcv_skb(sk, skb);\n\treturn 0;\n}\n\n\n/*\n *\tThis should be easy, if there is something there\n *\twe return it, otherwise we block.\n */\n\nstatic int rawv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct sk_buff *skb;\n\tsize_t copied;\n\tint err;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len, addr_len);\n\n\tskb = skb_recv_datagram(sk, flags, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\tif (skb_csum_unnecessary(skb)) {\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\t} else if (msg->msg_flags&MSG_TRUNC) {\n\t\tif (__skb_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, 0, msg);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\n\t/* Copy the address. */\n\tif (sin6) {\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = 0;\n\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t\t  inet6_iif(skb));\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\n\tsock_recv_cmsgs(msg, sk, skb);\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = skb->len;\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tskb_kill_datagram(sk, skb, flags);\n\n\t/* Error for blocking case is chosen to masquerade\n\t   as some normal condition.\n\t */\n\terr = (flags&MSG_DONTWAIT) ? -EAGAIN : -EHOSTUNREACH;\n\tgoto out;\n}\n\nstatic int rawv6_push_pending_frames(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t     struct raw6_sock *rp)\n{\n\tstruct ipv6_txoptions *opt;\n\tstruct sk_buff *skb;\n\tint err = 0;\n\tint offset;\n\tint len;\n\tint total_len;\n\t__wsum tmp_csum;\n\t__sum16 csum;\n\n\tif (!rp->checksum)\n\t\tgoto send;\n\n\tskb = skb_peek(&sk->sk_write_queue);\n\tif (!skb)\n\t\tgoto out;\n\n\toffset = rp->offset;\n\ttotal_len = inet_sk(sk)->cork.base.length;\n\topt = inet6_sk(sk)->cork.opt;\n\ttotal_len -= opt ? opt->opt_flen : 0;\n\n\tif (offset >= total_len - 1) {\n\t\terr = -EINVAL;\n\t\tip6_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\t/* should be check HW csum miyazawa */\n\tif (skb_queue_len(&sk->sk_write_queue) == 1) {\n\t\t/*\n\t\t * Only one fragment on the socket.\n\t\t */\n\t\ttmp_csum = skb->csum;\n\t} else {\n\t\tstruct sk_buff *csum_skb = NULL;\n\t\ttmp_csum = 0;\n\n\t\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\t\ttmp_csum = csum_add(tmp_csum, skb->csum);\n\n\t\t\tif (csum_skb)\n\t\t\t\tcontinue;\n\n\t\t\tlen = skb->len - skb_transport_offset(skb);\n\t\t\tif (offset >= len) {\n\t\t\t\toffset -= len;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tcsum_skb = skb;\n\t\t}\n\n\t\tskb = csum_skb;\n\t}\n\n\toffset += skb_transport_offset(skb);\n\terr = skb_copy_bits(skb, offset, &csum, 2);\n\tif (err < 0) {\n\t\tip6_flush_pending_frames(sk);\n\t\tgoto out;\n\t}\n\n\t/* in case cksum was not initialized */\n\tif (unlikely(csum))\n\t\ttmp_csum = csum_sub(tmp_csum, csum_unfold(csum));\n\n\tcsum = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t       total_len, fl6->flowi6_proto, tmp_csum);\n\n\tif (csum == 0 && fl6->flowi6_proto == IPPROTO_UDP)\n\t\tcsum = CSUM_MANGLED_0;\n\n\tBUG_ON(skb_store_bits(skb, offset, &csum, 2));\n\nsend:\n\terr = ip6_push_pending_frames(sk);\nout:\n\treturn err;\n}\n\nstatic int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,\n\t\t\tstruct flowi6 *fl6, struct dst_entry **dstp,\n\t\t\tunsigned int flags, const struct sockcm_cookie *sockc)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6hdr *iph;\n\tstruct sk_buff *skb;\n\tint err;\n\tstruct rt6_info *rt = dst_rt6_info(*dstp);\n\tint hlen = LL_RESERVED_SPACE(rt->dst.dev);\n\tint tlen = rt->dst.dev->needed_tailroom;\n\n\tif (length > rt->dst.dev->mtu) {\n\t\tipv6_local_error(sk, EMSGSIZE, fl6, rt->dst.dev->mtu);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (length < sizeof(struct ipv6hdr))\n\t\treturn -EINVAL;\n\tif (flags&MSG_PROBE)\n\t\tgoto out;\n\n\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t  length + hlen + tlen + 15,\n\t\t\t\t  flags & MSG_DONTWAIT, &err);\n\tif (!skb)\n\t\tgoto error;\n\tskb_reserve(skb, hlen);\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->priority = sockc->priority;\n\tskb->mark = sockc->mark;\n\tskb_set_delivery_type_by_clockid(skb, sockc->transmit_time, sk->sk_clockid);\n\n\tskb_put(skb, length);\n\tskb_reset_network_header(skb);\n\tiph = ipv6_hdr(skb);\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tskb_setup_tx_timestamp(skb, sockc);\n\n\tif (flags & MSG_CONFIRM)\n\t\tskb_set_dst_pending_confirm(skb, 1);\n\n\tskb->transport_header = skb->network_header;\n\terr = memcpy_from_msg(iph, msg, length);\n\tif (err) {\n\t\terr = -EFAULT;\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tskb_dst_set(skb, &rt->dst);\n\t*dstp = NULL;\n\n\t/* if egress device is enslaved to an L3 master device pass the\n\t * skb to its handler for processing\n\t */\n\tskb = l3mdev_ip6_out(sk, skb);\n\tif (unlikely(!skb))\n\t\treturn 0;\n\n\t/* Acquire rcu_read_lock() in case we need to use rt->rt6i_idev\n\t * in the error path. Since skb has been freed, the dst could\n\t * have been queued for deletion.\n\t */\n\trcu_read_lock();\n\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTREQUESTS);\n\terr = NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT, net, sk, skb,\n\t\t      NULL, rt->dst.dev, dst_output);\n\tif (err > 0)\n\t\terr = net_xmit_errno(err);\n\tif (err) {\n\t\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\t\trcu_read_unlock();\n\t\tgoto error_check;\n\t}\n\trcu_read_unlock();\nout:\n\treturn 0;\n\nerror:\n\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\nerror_check:\n\tif (err == -ENOBUFS && !inet6_test_bit(RECVERR6, sk))\n\t\terr = 0;\n\treturn err;\n}\n\nstruct raw6_frag_vec {\n\tstruct msghdr *msg;\n\tint hlen;\n\tchar c[4];\n};\n\nstatic int rawv6_probe_proto_opt(struct raw6_frag_vec *rfv, struct flowi6 *fl6)\n{\n\tint err = 0;\n\tswitch (fl6->flowi6_proto) {\n\tcase IPPROTO_ICMPV6:\n\t\trfv->hlen = 2;\n\t\terr = memcpy_from_msg(rfv->c, rfv->msg, rfv->hlen);\n\t\tif (!err) {\n\t\t\tfl6->fl6_icmp_type = rfv->c[0];\n\t\t\tfl6->fl6_icmp_code = rfv->c[1];\n\t\t}\n\t\tbreak;\n\tcase IPPROTO_MH:\n\t\trfv->hlen = 4;\n\t\terr = memcpy_from_msg(rfv->c, rfv->msg, rfv->hlen);\n\t\tif (!err)\n\t\t\tfl6->fl6_mh_type = rfv->c[2];\n\t}\n\treturn err;\n}\n\nstatic int raw6_getfrag(void *from, char *to, int offset, int len, int odd,\n\t\t       struct sk_buff *skb)\n{\n\tstruct raw6_frag_vec *rfv = from;\n\n\tif (offset < rfv->hlen) {\n\t\tint copy = min(rfv->hlen - offset, len);\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\tmemcpy(to, rfv->c + offset, copy);\n\t\telse\n\t\t\tskb->csum = csum_block_add(\n\t\t\t\tskb->csum,\n\t\t\t\tcsum_partial_copy_nocheck(rfv->c + offset,\n\t\t\t\t\t\t\t  to, copy),\n\t\t\t\todd);\n\n\t\todd = 0;\n\t\toffset += copy;\n\t\tto += copy;\n\t\tlen -= copy;\n\n\t\tif (!len)\n\t\t\treturn 0;\n\t}\n\n\toffset -= rfv->hlen;\n\n\treturn ip_generic_getfrag(rfv->msg, to, offset, len, odd, skb);\n}\n\nstatic int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct raw6_frag_vec rfv;\n\tstruct flowi6 fl6;\n\tstruct ipcm6_cookie ipc6;\n\tint addr_len = msg->msg_namelen;\n\tint hdrincl;\n\tu16 proto;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\thdrincl = inet_test_bit(HDRINCL, sk);\n\n\tipcm6_init_sk(&ipc6, sk);\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = ipc6.sockc.mark;\n\tfl6.flowi6_uid = sk_uid(sk);\n\n\tif (sin6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (sin6->sin6_family && sin6->sin6_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\t/* port is the proto value [0..255] carried in nexthdr */\n\t\tproto = ntohs(sin6->sin6_port);\n\n\t\tif (!proto)\n\t\t\tproto = inet->inet_num;\n\t\telse if (proto != inet->inet_num &&\n\t\t\t inet->inet_num != IPPROTO_RAW)\n\t\t\treturn -EINVAL;\n\n\t\tif (proto > 255)\n\t\t\treturn -EINVAL;\n\n\t\tdaddr = &sin6->sin6_addr;\n\t\tif (inet6_test_bit(SNDFLOW, sk)) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tproto = inet->inet_num;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\t\tipc6.opt = opt;\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, &ipc6);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = proto;\n\tfl6.flowi6_mark = ipc6.sockc.mark;\n\n\tif (!hdrincl) {\n\t\trfv.msg = msg;\n\t\trfv.hlen = 0;\n\t\terr = rawv6_probe_proto_opt(&rfv, &fl6);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = READ_ONCE(np->mcast_oif);\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = READ_ONCE(np->ucast_oif);\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tif (hdrincl)\n\t\tfl6.flowi6_flags |= FLOWI_FLAG_KNOWN_NH;\n\n\tfl6.flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6.flowlabel);\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\tif (ipc6.hlimit < 0)\n\t\tipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tif (hdrincl)\n\t\terr = rawv6_send_hdrinc(sk, msg, len, &fl6, &dst,\n\t\t\t\t\tmsg->msg_flags, &ipc6.sockc);\n\telse {\n\t\tipc6.opt = opt;\n\t\tlock_sock(sk);\n\t\terr = ip6_append_data(sk, raw6_getfrag, &rfv,\n\t\t\tlen, 0, &ipc6, &fl6, dst_rt6_info(dst),\n\t\t\tmsg->msg_flags);\n\n\t\tif (err)\n\t\t\tip6_flush_pending_frames(sk);\n\t\telse if (!(msg->msg_flags & MSG_MORE))\n\t\t\terr = rawv6_push_pending_frames(sk, &fl6, rp);\n\t\trelease_sock(sk);\n\t}\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\treturn err < 0 ? err : len;\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(dst, &fl6.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n\nstatic int rawv6_seticmpfilter(struct sock *sk, int optname,\n\t\t\t       sockptr_t optval, int optlen)\n{\n\tswitch (optname) {\n\tcase ICMPV6_FILTER:\n\t\tif (optlen > sizeof(struct icmp6_filter))\n\t\t\toptlen = sizeof(struct icmp6_filter);\n\t\tif (copy_from_sockptr(&raw6_sk(sk)->filter, optval, optlen))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\treturn 0;\n}\n\nstatic int rawv6_geticmpfilter(struct sock *sk, int optname,\n\t\t\t       char __user *optval, int __user *optlen)\n{\n\tint len;\n\n\tswitch (optname) {\n\tcase ICMPV6_FILTER:\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (len < 0)\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(struct icmp6_filter))\n\t\t\tlen = sizeof(struct icmp6_filter);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &raw6_sk(sk)->filter, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\treturn 0;\n}\n\n\nstatic int do_rawv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tint val;\n\n\tif (optlen < sizeof(val))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&val, optval, sizeof(val)))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase IPV6_HDRINCL:\n\t\tif (sk->sk_type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tinet_assign_bit(HDRINCL, sk, val);\n\t\treturn 0;\n\tcase IPV6_CHECKSUM:\n\t\tif (inet_sk(sk)->inet_num == IPPROTO_ICMPV6 &&\n\t\t    level == IPPROTO_IPV6) {\n\t\t\t/*\n\t\t\t * RFC3542 tells that IPV6_CHECKSUM socket\n\t\t\t * option in the IPPROTO_IPV6 level is not\n\t\t\t * allowed on ICMPv6 sockets.\n\t\t\t * If you want to set it, use IPPROTO_RAW\n\t\t\t * level IPV6_CHECKSUM socket option\n\t\t\t * (Linux extension).\n\t\t\t */\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* You may get strange result with a positive odd offset;\n\t\t   RFC2292bis agrees with me. */\n\t\tif (val > 0 && (val&1))\n\t\t\treturn -EINVAL;\n\t\tif (val < 0) {\n\t\t\trp->checksum = 0;\n\t\t} else {\n\t\t\trp->checksum = 1;\n\t\t\trp->offset = val;\n\t\t}\n\n\t\treturn 0;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int rawv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    sockptr_t optval, unsigned int optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_seticmpfilter(sk, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM ||\n\t\t    optname == IPV6_HDRINCL)\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\treturn do_rawv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\nstatic int do_rawv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase IPV6_HDRINCL:\n\t\tval = inet_test_bit(HDRINCL, sk);\n\t\tbreak;\n\tcase IPV6_CHECKSUM:\n\t\t/*\n\t\t * We allow getsockopt() for IPPROTO_IPV6-level\n\t\t * IPV6_CHECKSUM socket option on ICMPv6 sockets\n\t\t * since RFC3542 is silent about it.\n\t\t */\n\t\tif (rp->checksum == 0)\n\t\t\tval = -1;\n\t\telse\n\t\t\tval = rp->offset;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tlen = min_t(unsigned int, sizeof(int), len);\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int rawv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tswitch (level) {\n\tcase SOL_RAW:\n\t\tbreak;\n\n\tcase SOL_ICMPV6:\n\t\tif (inet_sk(sk)->inet_num != IPPROTO_ICMPV6)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn rawv6_geticmpfilter(sk, optname, optval, optlen);\n\tcase SOL_IPV6:\n\t\tif (optname == IPV6_CHECKSUM ||\n\t\t    optname == IPV6_HDRINCL)\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n\t}\n\n\treturn do_rawv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\nstatic int rawv6_ioctl(struct sock *sk, int cmd, int *karg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ: {\n\t\t*karg = sk_wmem_alloc_get(sk);\n\t\treturn 0;\n\t}\n\tcase SIOCINQ: {\n\t\tstruct sk_buff *skb;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\t*karg = skb->len;\n\t\telse\n\t\t\t*karg = 0;\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn 0;\n\t}\n\n\tdefault:\n#ifdef CONFIG_IPV6_MROUTE\n\t\treturn ip6mr_ioctl(sk, cmd, karg);\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic int compat_rawv6_ioctl(struct sock *sk, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\tcase SIOCINQ:\n\t\treturn -ENOIOCTLCMD;\n\tdefault:\n#ifdef CONFIG_IPV6_MROUTE\n\t\treturn ip6mr_compat_ioctl(sk, cmd, compat_ptr(arg));\n#else\n\t\treturn -ENOIOCTLCMD;\n#endif\n\t}\n}\n#endif\n\nstatic void rawv6_close(struct sock *sk, long timeout)\n{\n\tif (inet_sk(sk)->inet_num == IPPROTO_RAW)\n\t\tip6_ra_control(sk, -1);\n\tip6mr_sk_done(sk);\n\tsk_common_release(sk);\n}\n\nstatic void raw6_destroy(struct sock *sk)\n{\n\tlock_sock(sk);\n\tip6_flush_pending_frames(sk);\n\trelease_sock(sk);\n}\n\nstatic int rawv6_init_sk(struct sock *sk)\n{\n\tstruct raw6_sock *rp = raw6_sk(sk);\n\n\tsk->sk_drop_counters = &rp->drop_counters;\n\tswitch (inet_sk(sk)->inet_num) {\n\tcase IPPROTO_ICMPV6:\n\t\trp->checksum = 1;\n\t\trp->offset   = 2;\n\t\tbreak;\n\tcase IPPROTO_MH:\n\t\trp->checksum = 1;\n\t\trp->offset   = 4;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstruct proto rawv6_prot = {\n\t.name\t\t   = \"RAWv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = rawv6_close,\n\t.destroy\t   = raw6_destroy,\n\t.connect\t   = ip6_datagram_connect_v6_only,\n\t.disconnect\t   = __udp_disconnect,\n\t.ioctl\t\t   = rawv6_ioctl,\n\t.init\t\t   = rawv6_init_sk,\n\t.setsockopt\t   = rawv6_setsockopt,\n\t.getsockopt\t   = rawv6_getsockopt,\n\t.sendmsg\t   = rawv6_sendmsg,\n\t.recvmsg\t   = rawv6_recvmsg,\n\t.bind\t\t   = rawv6_bind,\n\t.backlog_rcv\t   = rawv6_rcv_skb,\n\t.hash\t\t   = raw_hash_sk,\n\t.unhash\t\t   = raw_unhash_sk,\n\t.obj_size\t   = sizeof(struct raw6_sock),\n\t.ipv6_pinfo_offset = offsetof(struct raw6_sock, inet6),\n\t.useroffset\t   = offsetof(struct raw6_sock, filter),\n\t.usersize\t   = sizeof_field(struct raw6_sock, filter),\n\t.h.raw_hash\t   = &raw_v6_hashinfo,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = compat_rawv6_ioctl,\n#endif\n\t.diag_destroy\t   = raw_abort,\n};\n\n#ifdef CONFIG_PROC_FS\nstatic int raw6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, IPV6_SEQ_DGRAM_HEADER);\n\t} else {\n\t\tstruct sock *sp = v;\n\t\t__u16 srcp  = inet_sk(sp)->inet_num;\n\t\tip6_dgram_sock_seq_show(seq, v, srcp, 0,\n\t\t\t\t\traw_seq_private(seq)->bucket);\n\t}\n\treturn 0;\n}\n\nstatic const struct seq_operations raw6_seq_ops = {\n\t.start =\traw_seq_start,\n\t.next =\t\traw_seq_next,\n\t.stop =\t\traw_seq_stop,\n\t.show =\t\traw6_seq_show,\n};\n\nstatic int __net_init raw6_init_net(struct net *net)\n{\n\tif (!proc_create_net_data(\"raw6\", 0444, net->proc_net, &raw6_seq_ops,\n\t\t\tsizeof(struct raw_iter_state), &raw_v6_hashinfo))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __net_exit raw6_exit_net(struct net *net)\n{\n\tremove_proc_entry(\"raw6\", net->proc_net);\n}\n\nstatic struct pernet_operations raw6_net_ops = {\n\t.init = raw6_init_net,\n\t.exit = raw6_exit_net,\n};\n\nint __init raw6_proc_init(void)\n{\n\treturn register_pernet_subsys(&raw6_net_ops);\n}\n\nvoid raw6_proc_exit(void)\n{\n\tunregister_pernet_subsys(&raw6_net_ops);\n}\n#endif\t/* CONFIG_PROC_FS */\n\n/* Same as inet6_dgram_ops, sans udp_poll.  */\nconst struct proto_ops inet6_sockraw_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_dgram_connect,\t/* ok\t\t*/\n\t.socketpair\t   = sock_no_socketpair,\t/* a do nothing\t*/\n\t.accept\t\t   = sock_no_accept,\t\t/* a do nothing\t*/\n\t.getname\t   = inet6_getname,\n\t.poll\t\t   = datagram_poll,\t\t/* ok\t\t*/\n\t.ioctl\t\t   = inet6_ioctl,\t\t/* must change  */\n\t.gettstamp\t   = sock_gettstamp,\n\t.listen\t\t   = sock_no_listen,\t\t/* ok\t\t*/\n\t.shutdown\t   = inet_shutdown,\t\t/* ok\t\t*/\n\t.setsockopt\t   = sock_common_setsockopt,\t/* ok\t\t*/\n\t.getsockopt\t   = sock_common_getsockopt,\t/* ok\t\t*/\n\t.sendmsg\t   = inet_sendmsg,\t\t/* ok\t\t*/\n\t.recvmsg\t   = sock_common_recvmsg,\t/* ok\t\t*/\n\t.mmap\t\t   = sock_no_mmap,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = inet6_compat_ioctl,\n#endif\n};\n\nstatic struct inet_protosw rawv6_protosw = {\n\t.type\t\t= SOCK_RAW,\n\t.protocol\t= IPPROTO_IP,\t/* wild card */\n\t.prot\t\t= &rawv6_prot,\n\t.ops\t\t= &inet6_sockraw_ops,\n\t.flags\t\t= INET_PROTOSW_REUSE,\n};\n\nint __init rawv6_init(void)\n{\n\treturn inet6_register_protosw(&rawv6_protosw);\n}\n\nvoid rawv6_exit(void)\n{\n\tinet6_unregister_protosw(&rawv6_protosw);\n}\n", "patch": "@@ -733,6 +733,7 @@ static int raw6_getfrag(void *from, char *to, int offset, int len, int odd,\n \n static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n {\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ipv6_txoptions opt_space;\n \tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n@@ -839,8 +840,10 @@ static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\tif (!(opt->opt_nflen|opt->opt_flen))\n \t\t\topt = NULL;\n \t}\n-\tif (!opt)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -906,6 +909,7 @@ static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \tdst_release(dst);\n out:\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \treturn err < 0 ? err : len;\n do_confirm:\n \tdst_confirm(dst);", "file_path": "files/2016_8\\90", "file_language": "c", "file_name": "net/ipv6/raw.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/syncookies.c", "code": "/*\n *  IPv6 Syncookies implementation for the Linux kernel\n *\n *  Authors:\n *  Glenn Griffin\t<ggriffin.kernel@gmail.com>\n *\n *  Based on IPv4 implementation by Andi Kleen\n *  linux/net/ipv4/syncookies.c\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n *\n */\n\n#include <linux/tcp.h>\n#include <linux/random.h>\n#include <linux/cryptohash.h>\n#include <linux/kernel.h>\n#include <net/ipv6.h>\n#include <net/tcp.h>\n\n#define COOKIEBITS 24\t/* Upper bits store count */\n#define COOKIEMASK (((__u32)1 << COOKIEBITS) - 1)\n\nstatic u32 syncookie6_secret[2][16-4+SHA_DIGEST_WORDS] __read_mostly;\n\n/* RFC 2460, Section 8.3:\n * [ipv6 tcp] MSS must be computed as the maximum packet size minus 60 [..]\n *\n * Due to IPV6_MIN_MTU=1280 the lowest possible MSS is 1220, which allows\n * using higher values than ipv4 tcp syncookies.\n * The other values are chosen based on ethernet (1500 and 9k MTU), plus\n * one that accounts for common encap (PPPoe) overhead. Table must be sorted.\n */\nstatic __u16 const msstab[] = {\n\t1280 - 60, /* IPV6_MIN_MTU - 60 */\n\t1480 - 60,\n\t1500 - 60,\n\t9000 - 60,\n};\n\nstatic DEFINE_PER_CPU(__u32 [16 + 5 + SHA_WORKSPACE_WORDS],\n\t\t      ipv6_cookie_scratch);\n\nstatic u32 cookie_hash(const struct in6_addr *saddr, const struct in6_addr *daddr,\n\t\t       __be16 sport, __be16 dport, u32 count, int c)\n{\n\t__u32 *tmp;\n\n\tnet_get_random_once(syncookie6_secret, sizeof(syncookie6_secret));\n\n\ttmp  = this_cpu_ptr(ipv6_cookie_scratch);\n\n\t/*\n\t * we have 320 bits of information to hash, copy in the remaining\n\t * 192 bits required for sha_transform, from the syncookie6_secret\n\t * and overwrite the digest with the secret\n\t */\n\tmemcpy(tmp + 10, syncookie6_secret[c], 44);\n\tmemcpy(tmp, saddr, 16);\n\tmemcpy(tmp + 4, daddr, 16);\n\ttmp[8] = ((__force u32)sport << 16) + (__force u32)dport;\n\ttmp[9] = count;\n\tsha_transform(tmp + 16, (__u8 *)tmp, tmp + 16 + 5);\n\n\treturn tmp[17];\n}\n\nstatic __u32 secure_tcp_syn_cookie(const struct in6_addr *saddr,\n\t\t\t\t   const struct in6_addr *daddr,\n\t\t\t\t   __be16 sport, __be16 dport, __u32 sseq,\n\t\t\t\t   __u32 data)\n{\n\tu32 count = tcp_cookie_time();\n\treturn (cookie_hash(saddr, daddr, sport, dport, 0, 0) +\n\t\tsseq + (count << COOKIEBITS) +\n\t\t((cookie_hash(saddr, daddr, sport, dport, count, 1) + data)\n\t\t& COOKIEMASK));\n}\n\nstatic __u32 check_tcp_syn_cookie(__u32 cookie, const struct in6_addr *saddr,\n\t\t\t\t  const struct in6_addr *daddr, __be16 sport,\n\t\t\t\t  __be16 dport, __u32 sseq)\n{\n\t__u32 diff, count = tcp_cookie_time();\n\n\tcookie -= cookie_hash(saddr, daddr, sport, dport, 0, 0) + sseq;\n\n\tdiff = (count - (cookie >> COOKIEBITS)) & ((__u32) -1 >> COOKIEBITS);\n\tif (diff >= MAX_SYNCOOKIE_AGE)\n\t\treturn (__u32)-1;\n\n\treturn (cookie -\n\t\tcookie_hash(saddr, daddr, sport, dport, count - diff, 1))\n\t\t& COOKIEMASK;\n}\n\nu32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,\n\t\t\t      const struct tcphdr *th, __u16 *mssp)\n{\n\tint mssind;\n\tconst __u16 mss = *mssp;\n\n\tfor (mssind = ARRAY_SIZE(msstab) - 1; mssind ; mssind--)\n\t\tif (mss >= msstab[mssind])\n\t\t\tbreak;\n\n\t*mssp = msstab[mssind];\n\n\treturn secure_tcp_syn_cookie(&iph->saddr, &iph->daddr, th->source,\n\t\t\t\t     th->dest, ntohl(th->seq), mssind);\n}\nEXPORT_SYMBOL_GPL(__cookie_v6_init_sequence);\n\n__u32 cookie_v6_init_sequence(const struct sk_buff *skb, __u16 *mssp)\n{\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\n\treturn __cookie_v6_init_sequence(iph, th, mssp);\n}\n\nint __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,\n\t\t      __u32 cookie)\n{\n\t__u32 seq = ntohl(th->seq) - 1;\n\t__u32 mssind = check_tcp_syn_cookie(cookie, &iph->saddr, &iph->daddr,\n\t\t\t\t\t    th->source, th->dest, seq);\n\n\treturn mssind < ARRAY_SIZE(msstab) ? msstab[mssind] : 0;\n}\nEXPORT_SYMBOL_GPL(__cookie_v6_check);\n\nstruct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tstruct inet_request_sock *ireq;\n\tstruct tcp_request_sock *treq;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\tstruct sock *ret = sk;\n\tstruct request_sock *req;\n\tint mss;\n\tstruct dst_entry *dst;\n\t__u8 rcv_wscale;\n\n\tif (!sysctl_tcp_syncookies || !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), th, cookie);\n\tif (mss == 0) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(skb, &tcp_opt, 0, NULL);\n\n\tif (!cookie_timestamp_decode(&tcp_opt))\n\t\tgoto out;\n\n\tret = NULL;\n\treq = inet_reqsk_alloc(&tcp6_request_sock_ops, sk, false);\n\tif (!req)\n\t\tgoto out;\n\n\tireq = inet_rsk(req);\n\ttreq = tcp_rsk(req);\n\ttreq->tfo_listener = false;\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto out_free;\n\n\treq->mss = mss;\n\tireq->ir_rmt_port = th->source;\n\tireq->ir_num = ntohs(th->dest);\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\tireq->ir_iif = sk->sk_bound_dev_if;\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tireq->ir_mark = inet_request_mark(sk, skb);\n\n\treq->num_retrans = 0;\n\tireq->snd_wscale\t= tcp_opt.snd_wscale;\n\tireq->sack_ok\t\t= tcp_opt.sack_ok;\n\tireq->wscale_ok\t\t= tcp_opt.wscale_ok;\n\tireq->tstamp_ok\t\t= tcp_opt.saw_tstamp;\n\treq->ts_recent\t\t= tcp_opt.saw_tstamp ? tcp_opt.rcv_tsval : 0;\n\ttreq->snt_synack.v64\t= 0;\n\ttreq->rcv_isn = ntohl(th->seq) - 1;\n\ttreq->snt_isn = cookie;\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto out_free;\n\t}\n\n\treq->rsk_window_clamp = tp->window_clamp ? :dst_metric(dst, RTAX_WINDOW);\n\ttcp_select_initial_window(tcp_full_space(sk), req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok = cookie_ecn_ok(&tcp_opt, sock_net(sk), dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\n\treturn NULL;\n}\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *  IPv6 Syncookies implementation for the Linux kernel\n *\n *  Authors:\n *  Glenn Griffin\t<ggriffin.kernel@gmail.com>\n *\n *  Based on IPv4 implementation by Andi Kleen\n *  linux/net/ipv4/syncookies.c\n */\n\n#include <linux/tcp.h>\n#include <linux/random.h>\n#include <linux/siphash.h>\n#include <linux/kernel.h>\n#include <net/secure_seq.h>\n#include <net/ipv6.h>\n#include <net/tcp.h>\n\n#define COOKIEBITS 24\t/* Upper bits store count */\n#define COOKIEMASK (((__u32)1 << COOKIEBITS) - 1)\n\nstatic siphash_aligned_key_t syncookie6_secret[2];\n\n/* RFC 2460, Section 8.3:\n * [ipv6 tcp] MSS must be computed as the maximum packet size minus 60 [..]\n *\n * Due to IPV6_MIN_MTU=1280 the lowest possible MSS is 1220, which allows\n * using higher values than ipv4 tcp syncookies.\n * The other values are chosen based on ethernet (1500 and 9k MTU), plus\n * one that accounts for common encap (PPPoe) overhead. Table must be sorted.\n */\nstatic __u16 const msstab[] = {\n\t1280 - 60, /* IPV6_MIN_MTU - 60 */\n\t1480 - 60,\n\t1500 - 60,\n\t9000 - 60,\n};\n\nstatic u32 cookie_hash(const struct in6_addr *saddr,\n\t\t       const struct in6_addr *daddr,\n\t\t       __be16 sport, __be16 dport, u32 count, int c)\n{\n\tconst struct {\n\t\tstruct in6_addr saddr;\n\t\tstruct in6_addr daddr;\n\t\tu32 count;\n\t\t__be16 sport;\n\t\t__be16 dport;\n\t} __aligned(SIPHASH_ALIGNMENT) combined = {\n\t\t.saddr = *saddr,\n\t\t.daddr = *daddr,\n\t\t.count = count,\n\t\t.sport = sport,\n\t\t.dport = dport\n\t};\n\n\tnet_get_random_once(syncookie6_secret, sizeof(syncookie6_secret));\n\treturn siphash(&combined, offsetofend(typeof(combined), dport),\n\t\t       &syncookie6_secret[c]);\n}\n\nstatic __u32 secure_tcp_syn_cookie(const struct in6_addr *saddr,\n\t\t\t\t   const struct in6_addr *daddr,\n\t\t\t\t   __be16 sport, __be16 dport, __u32 sseq,\n\t\t\t\t   __u32 data)\n{\n\tu32 count = tcp_cookie_time();\n\treturn (cookie_hash(saddr, daddr, sport, dport, 0, 0) +\n\t\tsseq + (count << COOKIEBITS) +\n\t\t((cookie_hash(saddr, daddr, sport, dport, count, 1) + data)\n\t\t& COOKIEMASK));\n}\n\nstatic __u32 check_tcp_syn_cookie(__u32 cookie, const struct in6_addr *saddr,\n\t\t\t\t  const struct in6_addr *daddr, __be16 sport,\n\t\t\t\t  __be16 dport, __u32 sseq)\n{\n\t__u32 diff, count = tcp_cookie_time();\n\n\tcookie -= cookie_hash(saddr, daddr, sport, dport, 0, 0) + sseq;\n\n\tdiff = (count - (cookie >> COOKIEBITS)) & ((__u32) -1 >> COOKIEBITS);\n\tif (diff >= MAX_SYNCOOKIE_AGE)\n\t\treturn (__u32)-1;\n\n\treturn (cookie -\n\t\tcookie_hash(saddr, daddr, sport, dport, count - diff, 1))\n\t\t& COOKIEMASK;\n}\n\nu32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,\n\t\t\t      const struct tcphdr *th, __u16 *mssp)\n{\n\tint mssind;\n\tconst __u16 mss = *mssp;\n\n\tfor (mssind = ARRAY_SIZE(msstab) - 1; mssind ; mssind--)\n\t\tif (mss >= msstab[mssind])\n\t\t\tbreak;\n\n\t*mssp = msstab[mssind];\n\n\treturn secure_tcp_syn_cookie(&iph->saddr, &iph->daddr, th->source,\n\t\t\t\t     th->dest, ntohl(th->seq), mssind);\n}\nEXPORT_SYMBOL_GPL(__cookie_v6_init_sequence);\n\n__u32 cookie_v6_init_sequence(const struct sk_buff *skb, __u16 *mssp)\n{\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\n\treturn __cookie_v6_init_sequence(iph, th, mssp);\n}\n\nint __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th)\n{\n\t__u32 cookie = ntohl(th->ack_seq) - 1;\n\t__u32 seq = ntohl(th->seq) - 1;\n\t__u32 mssind;\n\n\tmssind = check_tcp_syn_cookie(cookie, &iph->saddr, &iph->daddr,\n\t\t\t\t      th->source, th->dest, seq);\n\n\treturn mssind < ARRAY_SIZE(msstab) ? msstab[mssind] : 0;\n}\nEXPORT_SYMBOL_GPL(__cookie_v6_check);\n\nstatic struct request_sock *cookie_tcp_check(struct net *net, struct sock *sk,\n\t\t\t\t\t     struct sk_buff *skb)\n{\n\tstruct tcp_options_received tcp_opt;\n\tu32 tsoff = 0;\n\tint mss;\n\n\tif (tcp_synq_no_recent_overflow(sk))\n\t\tgoto out;\n\n\tmss = __cookie_v6_check(ipv6_hdr(skb), tcp_hdr(skb));\n\tif (!mss) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_SYNCOOKIESFAILED);\n\t\tgoto out;\n\t}\n\n\t__NET_INC_STATS(net, LINUX_MIB_SYNCOOKIESRECV);\n\n\t/* check for timestamp cookie support */\n\tmemset(&tcp_opt, 0, sizeof(tcp_opt));\n\ttcp_parse_options(net, skb, &tcp_opt, 0, NULL);\n\n\tif (tcp_opt.saw_tstamp && tcp_opt.rcv_tsecr) {\n\t\ttsoff = secure_tcpv6_ts_off(net,\n\t\t\t\t\t    ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t    ipv6_hdr(skb)->saddr.s6_addr32);\n\t\ttcp_opt.rcv_tsecr -= tsoff;\n\t}\n\n\tif (!cookie_timestamp_decode(net, &tcp_opt))\n\t\tgoto out;\n\n\treturn cookie_tcp_reqsk_alloc(&tcp6_request_sock_ops, sk, skb,\n\t\t\t\t      &tcp_opt, mss, tsoff);\nout:\n\treturn ERR_PTR(-EINVAL);\n}\n\nstruct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_request_sock *ireq;\n\tstruct net *net = sock_net(sk);\n\tstruct request_sock *req;\n\tstruct dst_entry *dst;\n\tstruct sock *ret = sk;\n\t__u8 rcv_wscale;\n\tint full_space;\n\tSKB_DR(reason);\n\n\tif (!READ_ONCE(net->ipv4.sysctl_tcp_syncookies) ||\n\t    !th->ack || th->rst)\n\t\tgoto out;\n\n\tif (cookie_bpf_ok(skb)) {\n\t\treq = cookie_bpf_check(sk, skb);\n\t} else {\n\t\treq = cookie_tcp_check(net, sk, skb);\n\t\tif (IS_ERR(req))\n\t\t\tgoto out;\n\t}\n\tif (!req) {\n\t\tSKB_DR_SET(reason, NO_SOCKET);\n\t\tgoto out_drop;\n\t}\n\n\tireq = inet_rsk(req);\n\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\n\tif (security_inet_conn_request(sk, skb, req)) {\n\t\tSKB_DR_SET(reason, SECURITY_HOOK);\n\t\tgoto out_free;\n\t}\n\n\tif (ipv6_opt_accepted(sk, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t    np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo ||\n\t    np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim) {\n\t\trefcount_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n\n\t/* So that link locals have meaning */\n\tif (!sk->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\ttcp_ao_syncookie(sk, skb, req, AF_INET6);\n\n\t/*\n\t * We need to lookup the dst_entry to get the correct window size.\n\t * This is taken from tcp_v6_syn_recv_sock.  Somebody please enlighten\n\t * me if there is a preferred way.\n\t */\n\t{\n\t\tstruct in6_addr *final_p, final;\n\t\tstruct flowi6 fl6;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n\t\tfl6.saddr = ireq->ir_v6_loc_addr;\n\t\tfl6.flowi6_oif = ireq->ir_iif;\n\t\tfl6.flowi6_mark = ireq->ir_mark;\n\t\tfl6.fl6_dport = ireq->ir_rmt_port;\n\t\tfl6.fl6_sport = inet_sk(sk)->inet_sport;\n\t\tfl6.flowi6_uid = sk_uid(sk);\n\t\tsecurity_req_classify_flow(req, flowi6_to_flowi_common(&fl6));\n\n\t\tdst = ip6_dst_lookup_flow(net, sk, &fl6, final_p);\n\t\tif (IS_ERR(dst)) {\n\t\t\tSKB_DR_SET(reason, IP_OUTNOROUTES);\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\treq->rsk_window_clamp = READ_ONCE(tp->window_clamp) ? :dst_metric(dst, RTAX_WINDOW);\n\t/* limit the window selection if the user enforce a smaller rx buffer */\n\tfull_space = tcp_full_space(sk);\n\tif (sk->sk_userlocks & SOCK_RCVBUF_LOCK &&\n\t    (req->rsk_window_clamp > full_space || req->rsk_window_clamp == 0))\n\t\treq->rsk_window_clamp = full_space;\n\n\ttcp_select_initial_window(sk, full_space, req->mss,\n\t\t\t\t  &req->rsk_rcv_wnd, &req->rsk_window_clamp,\n\t\t\t\t  ireq->wscale_ok, &rcv_wscale,\n\t\t\t\t  dst_metric(dst, RTAX_INITRWND));\n\n\t/* req->syncookie is set true only if ACK is validated\n\t * by BPF kfunc, then, rcv_wscale is already configured.\n\t */\n\tif (!req->syncookie)\n\t\tireq->rcv_wscale = rcv_wscale;\n\tireq->ecn_ok &= cookie_ecn_ok(net, dst);\n\n\tret = tcp_get_cookie_sock(sk, skb, req, dst);\n\tif (!ret) {\n\t\tSKB_DR_SET(reason, NO_SOCKET);\n\t\tgoto out_drop;\n\t}\nout:\n\treturn ret;\nout_free:\n\treqsk_free(req);\nout_drop:\n\tsk_skb_reason_drop(sk, skb, reason);\n\treturn NULL;\n}\n", "patch": "@@ -222,7 +222,7 @@ struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)\n \t\tmemset(&fl6, 0, sizeof(fl6));\n \t\tfl6.flowi6_proto = IPPROTO_TCP;\n \t\tfl6.daddr = ireq->ir_v6_rmt_addr;\n-\t\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\t\tfinal_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);\n \t\tfl6.saddr = ireq->ir_v6_loc_addr;\n \t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n \t\tfl6.flowi6_mark = ireq->ir_mark;", "file_path": "files/2016_8\\91", "file_language": "c", "file_name": "net/ipv6/syncookies.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/tcp_ipv6.c", "code": "/*\n *\tTCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on:\n *\tlinux/net/ipv4/tcp.c\n *\tlinux/net/ipv4/tcp_input.c\n *\tlinux/net/ipv4/tcp_output.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tYOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/tcp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/bottom_half.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/jiffies.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/init.h>\n#include <linux/jhash.h>\n#include <linux/ipsec.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/random.h>\n\n#include <net/tcp.h>\n#include <net/ndisc.h>\n#include <net/inet6_hashtables.h>\n#include <net/inet6_connection_sock.h>\n#include <net/ipv6.h>\n#include <net/transp_v6.h>\n#include <net/addrconf.h>\n#include <net/ip6_route.h>\n#include <net/ip6_checksum.h>\n#include <net/inet_ecn.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n#include <net/snmp.h>\n#include <net/dsfield.h>\n#include <net/timewait_sock.h>\n#include <net/inet_common.h>\n#include <net/secure_seq.h>\n#include <net/tcp_memcontrol.h>\n#include <net/busy_poll.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n\n#include <linux/crypto.h>\n#include <linux/scatterlist.h>\n\nstatic void\ttcp_v6_send_reset(const struct sock *sk, struct sk_buff *skb);\nstatic void\ttcp_v6_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      struct request_sock *req);\n\nstatic int\ttcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);\n\nstatic const struct inet_connection_sock_af_ops ipv6_mapped;\nstatic const struct inet_connection_sock_af_ops ipv6_specific;\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific;\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;\n#else\nstatic struct tcp_md5sig_key *tcp_v6_md5_do_lookup(const struct sock *sk,\n\t\t\t\t\t\t   const struct in6_addr *addr)\n{\n\treturn NULL;\n}\n#endif\n\nstatic void inet6_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\n\tif (dst) {\n\t\tconst struct rt6_info *rt = (const struct rt6_info *)dst;\n\n\t\tdst_hold(dst);\n\t\tsk->sk_rx_dst = dst;\n\t\tinet_sk(sk)->rx_dst_ifindex = skb->skb_iif;\n\t\tinet6_sk(sk)->rx_dst_cookie = rt6_get_cookie(rt);\n\t}\n}\n\nstatic __u32 tcp_v6_init_sequence(const struct sk_buff *skb)\n{\n\treturn secure_tcpv6_sequence_number(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t\t    ipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\t\t    tcp_hdr(skb)->dest,\n\t\t\t\t\t    tcp_hdr(skb)->source);\n}\n\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (np->sndflow) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr))\n\t\tusin->sin6_addr.s6_addr[15] = 0x1;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (sk->sk_bound_dev_if &&\n\t\t\t    sk->sk_bound_dev_if != usin->sin6_scope_id)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\ttp->write_seq = 0;\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type == IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tSOCK_DEBUG(sk, \"connect: ipv4 mapped\\n\");\n\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\ticsk->icsk_af_ops = &ipv6_mapped;\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\ticsk->icsk_af_ops = &ipv6_specific;\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\t\tsk->sk_v6_rcv_saddr = *saddr;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(sk, dst, NULL, NULL);\n\n\tif (tcp_death_row.sysctl_tw_recycle &&\n\t    !tp->rx_opt.ts_recent_stamp &&\n\t    ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr))\n\t\ttcp_fetch_timewait_stamp(sk, dst);\n\n\ticsk->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n\t\t\t\t\t opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(&tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (!tp->write_seq && likely(!tp->repair))\n\t\ttp->write_seq = secure_tcpv6_sequence_number(np->saddr.s6_addr32,\n\t\t\t\t\t\t\t     sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t\t     inet->inet_sport,\n\t\t\t\t\t\t\t     inet->inet_dport);\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\t__sk_dst_reset(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic void tcp_v6_mtu_reduced(struct sock *sk)\n{\n\tstruct dst_entry *dst;\n\n\tif ((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE))\n\t\treturn;\n\n\tdst = inet6_csk_update_pmtu(sk, tcp_sk(sk)->mtu_info);\n\tif (!dst)\n\t\treturn;\n\n\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst)) {\n\t\ttcp_sync_mss(sk, dst_mtu(dst));\n\t\ttcp_simple_retransmit(sk);\n\t}\n}\n\nstatic void tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct tcphdr *th = (struct tcphdr *)(skb->data+offset);\n\tstruct net *net = dev_net(skb->dev);\n\tstruct request_sock *fastopen;\n\tstruct ipv6_pinfo *np;\n\tstruct tcp_sock *tp;\n\t__u32 seq, snd_una;\n\tstruct sock *sk;\n\tint err;\n\n\tsk = __inet6_lookup_established(net, &tcp_hashinfo,\n\t\t\t\t\t&hdr->daddr, th->dest,\n\t\t\t\t\t&hdr->saddr, ntohs(th->source),\n\t\t\t\t\tskb->dev->ifindex);\n\n\tif (!sk) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn;\n\t}\n\tseq = ntohl(th->seq);\n\tif (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\treturn tcp_req_err(sk, seq);\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk) && type != ICMPV6_PKT_TOOBIG)\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (ipv6_hdr(skb)->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto out;\n\t}\n\n\ttp = tcp_sk(sk);\n\t/* XXX (TFO) - tp->snd_una should be ISN (tcp_create_openreq_child() */\n\tfastopen = tp->fastopen_rsk;\n\tsnd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, snd_una, tp->snd_nxt)) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\tif (dst)\n\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\t/* We are not interested in TCP_LISTEN and open_requests\n\t\t * (SYN-ACKs send out by Linux are always <576bytes so\n\t\t * they should go through unfragmented).\n\t\t */\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\tgoto out;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\ttp->mtu_info = ntohl(info);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\ttcp_v6_mtu_reduced(sk);\n\t\telse if (!test_and_set_bit(TCP_MTU_REDUCED_DEFERRED,\n\t\t\t\t\t   &tp->tsq_flags))\n\t\t\tsock_hold(sk);\n\t\tgoto out;\n\t}\n\n\ticmpv6_err_convert(type, code, &err);\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:\n\t\t/* Only in fast or simultaneous open. If a fast open socket is\n\t\t * is already accepted it is treated as a connected one below.\n\t\t */\n\t\tif (fastopen && !fastopen->sk)\n\t\t\tbreak;\n\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tsk->sk_err = err;\n\t\t\tsk->sk_error_report(sk);\t\t/* Wake people up to see the error (see connect in sock.c) */\n\n\t\t\ttcp_done(sk);\n\t\t} else\n\t\t\tsk->sk_err_soft = err;\n\t\tgoto out;\n\t}\n\n\tif (!sock_owned_by_user(sk) && np->recverr) {\n\t\tsk->sk_err = err;\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tsk->sk_err_soft = err;\n\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n}\n\n\nstatic int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct flowi *fl,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      bool attach_req)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct flowi6 *fl6 = &fl->u.ip6;\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet6_csk_route_req(sk, fl6, req,\n\t\t\t\t\t       IPPROTO_TCP)) == NULL)\n\t\tgoto done;\n\n\tskb = tcp_make_synack(sk, dst, req, foc, attach_req);\n\n\tif (skb) {\n\t\t__tcp_v6_send_check(skb, &ireq->ir_v6_loc_addr,\n\t\t\t\t    &ireq->ir_v6_rmt_addr);\n\n\t\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\t\tif (np->repflow && ireq->pktopts)\n\t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n\n\t\terr = ip6_xmit(sk, skb, fl6, rcu_dereference(np->opt),\n\t\t\t       np->tclass);\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\treturn err;\n}\n\n\nstatic void tcp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tkfree_skb(inet_rsk(req)->pktopts);\n}\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic struct tcp_md5sig_key *tcp_v6_md5_do_lookup(const struct sock *sk,\n\t\t\t\t\t\t   const struct in6_addr *addr)\n{\n\treturn tcp_md5_do_lookup(sk, (union tcp_md5_addr *)addr, AF_INET6);\n}\n\nstatic struct tcp_md5sig_key *tcp_v6_md5_lookup(const struct sock *sk,\n\t\t\t\t\t\tconst struct sock *addr_sk)\n{\n\treturn tcp_v6_md5_do_lookup(sk, &addr_sk->sk_v6_daddr);\n}\n\nstatic int tcp_v6_parse_md5_keys(struct sock *sk, char __user *optval,\n\t\t\t\t int optlen)\n{\n\tstruct tcp_md5sig cmd;\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)&cmd.tcpm_addr;\n\n\tif (optlen < sizeof(cmd))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&cmd, optval, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (sin6->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\tif (!cmd.tcpm_keylen) {\n\t\tif (ipv6_addr_v4mapped(&sin6->sin6_addr))\n\t\t\treturn tcp_md5_do_del(sk, (union tcp_md5_addr *)&sin6->sin6_addr.s6_addr32[3],\n\t\t\t\t\t      AF_INET);\n\t\treturn tcp_md5_do_del(sk, (union tcp_md5_addr *)&sin6->sin6_addr,\n\t\t\t\t      AF_INET6);\n\t}\n\n\tif (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)\n\t\treturn -EINVAL;\n\n\tif (ipv6_addr_v4mapped(&sin6->sin6_addr))\n\t\treturn tcp_md5_do_add(sk, (union tcp_md5_addr *)&sin6->sin6_addr.s6_addr32[3],\n\t\t\t\t      AF_INET, cmd.tcpm_key, cmd.tcpm_keylen, GFP_KERNEL);\n\n\treturn tcp_md5_do_add(sk, (union tcp_md5_addr *)&sin6->sin6_addr,\n\t\t\t      AF_INET6, cmd.tcpm_key, cmd.tcpm_keylen, GFP_KERNEL);\n}\n\nstatic int tcp_v6_md5_hash_pseudoheader(struct tcp_md5sig_pool *hp,\n\t\t\t\t\tconst struct in6_addr *daddr,\n\t\t\t\t\tconst struct in6_addr *saddr, int nbytes)\n{\n\tstruct tcp6_pseudohdr *bp;\n\tstruct scatterlist sg;\n\n\tbp = &hp->md5_blk.ip6;\n\t/* 1. TCP pseudo-header (RFC2460) */\n\tbp->saddr = *saddr;\n\tbp->daddr = *daddr;\n\tbp->protocol = cpu_to_be32(IPPROTO_TCP);\n\tbp->len = cpu_to_be32(nbytes);\n\n\tsg_init_one(&sg, bp, sizeof(*bp));\n\treturn crypto_hash_update(&hp->md5_desc, &sg, sizeof(*bp));\n}\n\nstatic int tcp_v6_md5_hash_hdr(char *md5_hash, struct tcp_md5sig_key *key,\n\t\t\t       const struct in6_addr *daddr, struct in6_addr *saddr,\n\t\t\t       const struct tcphdr *th)\n{\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\tif (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nstatic int tcp_v6_md5_hash_skb(char *md5_hash,\n\t\t\t       const struct tcp_md5sig_key *key,\n\t\t\t       const struct sock *sk,\n\t\t\t       const struct sk_buff *skb)\n{\n\tconst struct in6_addr *saddr, *daddr;\n\tstruct tcp_md5sig_pool *hp;\n\tstruct hash_desc *desc;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\n\tif (sk) { /* valid for establish/request sockets */\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else {\n\t\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tsaddr = &ip6h->saddr;\n\t\tdaddr = &ip6h->daddr;\n\t}\n\n\thp = tcp_get_md5sig_pool();\n\tif (!hp)\n\t\tgoto clear_hash_noput;\n\tdesc = &hp->md5_desc;\n\n\tif (crypto_hash_init(desc))\n\t\tgoto clear_hash;\n\n\tif (tcp_v6_md5_hash_pseudoheader(hp, daddr, saddr, skb->len))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_header(hp, th))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_skb_data(hp, skb, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(hp, key))\n\t\tgoto clear_hash;\n\tif (crypto_hash_final(desc, md5_hash))\n\t\tgoto clear_hash;\n\n\ttcp_put_md5sig_pool();\n\treturn 0;\n\nclear_hash:\n\ttcp_put_md5sig_pool();\nclear_hash_noput:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\n#endif\n\nstatic bool tcp_v6_inbound_md5_hash(const struct sock *sk,\n\t\t\t\t    const struct sk_buff *skb)\n{\n#ifdef CONFIG_TCP_MD5SIG\n\tconst __u8 *hash_location = NULL;\n\tstruct tcp_md5sig_key *hash_expected;\n\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tint genhash;\n\tu8 newhash[16];\n\n\thash_expected = tcp_v6_md5_do_lookup(sk, &ip6h->saddr);\n\thash_location = tcp_parse_md5sig_option(th);\n\n\t/* We've parsed the options - do we have a hash? */\n\tif (!hash_expected && !hash_location)\n\t\treturn false;\n\n\tif (hash_expected && !hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5NOTFOUND);\n\t\treturn true;\n\t}\n\n\tif (!hash_expected && hash_location) {\n\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPMD5UNEXPECTED);\n\t\treturn true;\n\t}\n\n\t/* check the signature */\n\tgenhash = tcp_v6_md5_hash_skb(newhash,\n\t\t\t\t      hash_expected,\n\t\t\t\t      NULL, skb);\n\n\tif (genhash || memcmp(hash_location, newhash, 16) != 0) {\n\t\tnet_info_ratelimited(\"MD5 Hash %s for [%pI6c]:%u->[%pI6c]:%u\\n\",\n\t\t\t\t     genhash ? \"failed\" : \"mismatch\",\n\t\t\t\t     &ip6h->saddr, ntohs(th->source),\n\t\t\t\t     &ip6h->daddr, ntohs(th->dest));\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\nstatic void tcp_v6_init_req(struct request_sock *req,\n\t\t\t    const struct sock *sk_listener,\n\t\t\t    struct sk_buff *skb)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = inet6_sk(sk_listener);\n\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\n\t/* So that link locals have meaning */\n\tif (!sk_listener->sk_bound_dev_if &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tif (!TCP_SKB_CB(skb)->tcp_tw_isn &&\n\t    (ipv6_opt_accepted(sk_listener, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t     np->rxopt.bits.rxinfo ||\n\t     np->rxopt.bits.rxoinfo || np->rxopt.bits.rxhlim ||\n\t     np->rxopt.bits.rxohlim || np->repflow)) {\n\t\tatomic_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n}\n\nstatic struct dst_entry *tcp_v6_route_req(const struct sock *sk,\n\t\t\t\t\t  struct flowi *fl,\n\t\t\t\t\t  const struct request_sock *req,\n\t\t\t\t\t  bool *strict)\n{\n\tif (strict)\n\t\t*strict = true;\n\treturn inet6_csk_route_req(sk, &fl->u.ip6, req, IPPROTO_TCP);\n}\n\nstruct request_sock_ops tcp6_request_sock_ops __read_mostly = {\n\t.family\t\t=\tAF_INET6,\n\t.obj_size\t=\tsizeof(struct tcp6_request_sock),\n\t.rtx_syn_ack\t=\ttcp_rtx_synack,\n\t.send_ack\t=\ttcp_v6_reqsk_send_ack,\n\t.destructor\t=\ttcp_v6_reqsk_destructor,\n\t.send_reset\t=\ttcp_v6_send_reset,\n\t.syn_ack_timeout =\ttcp_syn_ack_timeout,\n};\n\nstatic const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {\n\t.mss_clamp\t=\tIPV6_MIN_MTU - sizeof(struct tcphdr) -\n\t\t\t\tsizeof(struct ipv6hdr),\n#ifdef CONFIG_TCP_MD5SIG\n\t.req_md5_lookup\t=\ttcp_v6_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n#endif\n\t.init_req\t=\ttcp_v6_init_req,\n#ifdef CONFIG_SYN_COOKIES\n\t.cookie_init_seq =\tcookie_v6_init_sequence,\n#endif\n\t.route_req\t=\ttcp_v6_route_req,\n\t.init_seq\t=\ttcp_v6_init_sequence,\n\t.send_synack\t=\ttcp_v6_send_synack,\n};\n\nstatic void tcp_v6_send_response(const struct sock *sk, struct sk_buff *skb, u32 seq,\n\t\t\t\t u32 ack, u32 win, u32 tsval, u32 tsecr,\n\t\t\t\t int oif, struct tcp_md5sig_key *key, int rst,\n\t\t\t\t u8 tclass, u32 label)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct tcphdr *t1;\n\tstruct sk_buff *buff;\n\tstruct flowi6 fl6;\n\tstruct net *net = sk ? sock_net(sk) : dev_net(skb_dst(skb)->dev);\n\tstruct sock *ctl_sk = net->ipv6.tcp_sk;\n\tunsigned int tot_len = sizeof(struct tcphdr);\n\tstruct dst_entry *dst;\n\t__be32 *topt;\n\n\tif (tsecr)\n\t\ttot_len += TCPOLEN_TSTAMP_ALIGNED;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key)\n\t\ttot_len += TCPOLEN_MD5SIG_ALIGNED;\n#endif\n\n\tbuff = alloc_skb(MAX_HEADER + sizeof(struct ipv6hdr) + tot_len,\n\t\t\t GFP_ATOMIC);\n\tif (!buff)\n\t\treturn;\n\n\tskb_reserve(buff, MAX_HEADER + sizeof(struct ipv6hdr) + tot_len);\n\n\tt1 = (struct tcphdr *) skb_push(buff, tot_len);\n\tskb_reset_transport_header(buff);\n\n\t/* Swap the send and the receive. */\n\tmemset(t1, 0, sizeof(*t1));\n\tt1->dest = th->source;\n\tt1->source = th->dest;\n\tt1->doff = tot_len / 4;\n\tt1->seq = htonl(seq);\n\tt1->ack_seq = htonl(ack);\n\tt1->ack = !rst || !th->ack;\n\tt1->rst = rst;\n\tt1->window = htons(win);\n\n\ttopt = (__be32 *)(t1 + 1);\n\n\tif (tsecr) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);\n\t\t*topt++ = htonl(tsval);\n\t\t*topt++ = htonl(tsecr);\n\t}\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (key) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\ttcp_v6_md5_hash_hdr((__u8 *)topt, key,\n\t\t\t\t    &ipv6_hdr(skb)->saddr,\n\t\t\t\t    &ipv6_hdr(skb)->daddr, t1);\n\t}\n#endif\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.daddr = ipv6_hdr(skb)->saddr;\n\tfl6.saddr = ipv6_hdr(skb)->daddr;\n\tfl6.flowlabel = label;\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\tbuff->csum = 0;\n\n\t__tcp_v6_send_check(buff, &fl6.saddr, &fl6.daddr);\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tif (rt6_need_strict(&fl6.daddr) && !oif)\n\t\tfl6.flowi6_oif = tcp_v6_iif(skb);\n\telse\n\t\tfl6.flowi6_oif = oif;\n\tfl6.flowi6_mark = IP6_REPLY_MARK(net, skb->mark);\n\tfl6.fl6_dport = t1->dest;\n\tfl6.fl6_sport = t1->source;\n\tsecurity_skb_classify_flow(skb, flowi6_to_flowi(&fl6));\n\n\t/* Pass a socket to ip6_dst_lookup either it is for RST\n\t * Underlying function will use this to retrieve the network\n\t * namespace\n\t */\n\tdst = ip6_dst_lookup_flow(ctl_sk, &fl6, NULL);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(buff, dst);\n\t\tip6_xmit(ctl_sk, buff, &fl6, NULL, tclass);\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);\n\t\tif (rst)\n\t\t\tTCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(buff);\n}\n\nstatic void tcp_v6_send_reset(const struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tu32 seq = 0, ack_seq = 0;\n\tstruct tcp_md5sig_key *key = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tconst __u8 *hash_location = NULL;\n\tstruct ipv6hdr *ipv6h = ipv6_hdr(skb);\n\tunsigned char newhash[16];\n\tint genhash;\n\tstruct sock *sk1 = NULL;\n#endif\n\tint oif;\n\n\tif (th->rst)\n\t\treturn;\n\n\t/* If sk not NULL, it means we did a successful lookup and incoming\n\t * route had to be correct. prequeue might have dropped our dst.\n\t */\n\tif (!sk && !ipv6_unicast_destination(skb))\n\t\treturn;\n\n#ifdef CONFIG_TCP_MD5SIG\n\thash_location = tcp_parse_md5sig_option(th);\n\tif (!sk && hash_location) {\n\t\t/*\n\t\t * active side is lost. Try to find listening socket through\n\t\t * source port, and then find md5 key through listening socket.\n\t\t * we are not loose security here:\n\t\t * Incoming packet is checked with md5 hash with finding key,\n\t\t * no RST generated if md5 hash doesn't match.\n\t\t */\n\t\tsk1 = inet6_lookup_listener(dev_net(skb_dst(skb)->dev),\n\t\t\t\t\t   &tcp_hashinfo, &ipv6h->saddr,\n\t\t\t\t\t   th->source, &ipv6h->daddr,\n\t\t\t\t\t   ntohs(th->source), tcp_v6_iif(skb));\n\t\tif (!sk1)\n\t\t\treturn;\n\n\t\trcu_read_lock();\n\t\tkey = tcp_v6_md5_do_lookup(sk1, &ipv6h->saddr);\n\t\tif (!key)\n\t\t\tgoto release_sk1;\n\n\t\tgenhash = tcp_v6_md5_hash_skb(newhash, key, NULL, skb);\n\t\tif (genhash || memcmp(hash_location, newhash, 16) != 0)\n\t\t\tgoto release_sk1;\n\t} else {\n\t\tkey = sk ? tcp_v6_md5_do_lookup(sk, &ipv6h->saddr) : NULL;\n\t}\n#endif\n\n\tif (th->ack)\n\t\tseq = ntohl(th->ack_seq);\n\telse\n\t\tack_seq = ntohl(th->seq) + th->syn + th->fin + skb->len -\n\t\t\t  (th->doff << 2);\n\n\toif = sk ? sk->sk_bound_dev_if : 0;\n\ttcp_v6_send_response(sk, skb, seq, ack_seq, 0, 0, 0, oif, key, 1, 0, 0);\n\n#ifdef CONFIG_TCP_MD5SIG\nrelease_sk1:\n\tif (sk1) {\n\t\trcu_read_unlock();\n\t\tsock_put(sk1);\n\t}\n#endif\n}\n\nstatic void tcp_v6_send_ack(const struct sock *sk, struct sk_buff *skb, u32 seq,\n\t\t\t    u32 ack, u32 win, u32 tsval, u32 tsecr, int oif,\n\t\t\t    struct tcp_md5sig_key *key, u8 tclass,\n\t\t\t    u32 label)\n{\n\ttcp_v6_send_response(sk, skb, seq, ack, win, tsval, tsecr, oif, key, 0,\n\t\t\t     tclass, label);\n}\n\nstatic void tcp_v6_timewait_ack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\tstruct tcp_timewait_sock *tcptw = tcp_twsk(sk);\n\n\ttcp_v6_send_ack(sk, skb, tcptw->tw_snd_nxt, tcptw->tw_rcv_nxt,\n\t\t\ttcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,\n\t\t\ttcp_time_stamp + tcptw->tw_ts_offset,\n\t\t\ttcptw->tw_ts_recent, tw->tw_bound_dev_if, tcp_twsk_md5_key(tcptw),\n\t\t\ttw->tw_tclass, cpu_to_be32(tw->tw_flowlabel));\n\n\tinet_twsk_put(tw);\n}\n\nstatic void tcp_v6_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req)\n{\n\t/* sk->sk_state == TCP_LISTEN -> for regular TCP_SYN_RECV\n\t * sk->sk_state == TCP_SYN_RECV -> for Fast Open.\n\t */\n\ttcp_v6_send_ack(sk, skb, (sk->sk_state == TCP_LISTEN) ?\n\t\t\ttcp_rsk(req)->snt_isn + 1 : tcp_sk(sk)->snd_nxt,\n\t\t\ttcp_rsk(req)->rcv_nxt, req->rsk_rcv_wnd,\n\t\t\ttcp_time_stamp, req->ts_recent, sk->sk_bound_dev_if,\n\t\t\ttcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->daddr),\n\t\t\t0, 0);\n}\n\n\nstatic struct sock *tcp_v6_cookie_check(struct sock *sk, struct sk_buff *skb)\n{\n#ifdef CONFIG_SYN_COOKIES\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\n\tif (!th->syn)\n\t\tsk = cookie_v6_check(sk, skb);\n#endif\n\treturn sk;\n}\n\nstatic int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\tgoto drop;\n\n\treturn tcp_conn_request(&tcp6_request_sock_ops,\n\t\t\t\t&tcp_request_sock_ipv6_ops, sk, skb);\n\ndrop:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn 0; /* don't send reset */\n}\n\nstatic struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t struct request_sock *req,\n\t\t\t\t\t struct dst_entry *dst,\n\t\t\t\t\t struct request_sock *req_unhash,\n\t\t\t\t\t bool *own_req)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct tcp6_sock *newtcp6sk;\n\tstruct inet_sock *newinet;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n#endif\n\tstruct flowi6 fl6;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t     req_unhash, own_req);\n\n\t\tif (!newsk)\n\t\t\treturn NULL;\n\n\t\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\t\tnewinet = inet_sk(newsk);\n\t\tnewnp = inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#ifdef CONFIG_TCP_MD5SIG\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->ipv6_ac_list = NULL;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = tcp_v6_iif(skb);\n\t\tnewnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;\n\t\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\t\tif (np->repflow)\n\t\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\tireq = inet_rsk(req);\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto out_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_TCP);\n\t\tif (!dst)\n\t\t\tgoto out;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto out_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\t__ip6_dst_store(newsk, dst, NULL, NULL);\n\tinet6_sk_rx_dst_set(newsk, skb);\n\n\tnewtcp6sk = (struct tcp6_sock *)newsk;\n\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tnewsk->sk_v6_daddr = ireq->ir_v6_rmt_addr;\n\tnewnp->saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_v6_rcv_saddr = ireq->ir_v6_loc_addr;\n\tnewsk->sk_bound_dev_if = ireq->ir_iif;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_ac_list = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = tcp_v6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\tif (np->repflow)\n\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\ttcp_ca_openreq_child(newsk, dst);\n\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = dst_metric_advmss(dst);\n\tif (tcp_sk(sk)->rx_opt.user_mss &&\n\t    tcp_sk(sk)->rx_opt.user_mss < newtp->advmss)\n\t\tnewtp->advmss = tcp_sk(sk)->rx_opt.user_mss;\n\n\ttcp_initialize_rcv_mss(newsk);\n\n\tnewinet->inet_daddr = newinet->inet_saddr = LOOPBACK4_IPV6;\n\tnewinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n#ifdef CONFIG_TCP_MD5SIG\n\t/* Copy over the MD5 key from the original socket */\n\tkey = tcp_v6_md5_do_lookup(sk, &newsk->sk_v6_daddr);\n\tif (key) {\n\t\t/* We're using one, so create a matching key\n\t\t * on the newsk structure. If we fail to get\n\t\t * memory, then we end up not copying the key\n\t\t * across. Shucks.\n\t\t */\n\t\ttcp_md5_do_add(newsk, (union tcp_md5_addr *)&newsk->sk_v6_daddr,\n\t\t\t       AF_INET6, key->key, key->keylen,\n\t\t\t       sk_gfp_atomic(sk, GFP_ATOMIC));\n\t}\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0) {\n\t\tinet_csk_prepare_forced_close(newsk);\n\t\ttcp_done(newsk);\n\t\tgoto out;\n\t}\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));\n\tif (*own_req) {\n\t\ttcp_move_syn(newtp, req);\n\n\t\t/* Clone pktoptions received with SYN, if we own the req */\n\t\tif (ireq->pktopts) {\n\t\t\tnewnp->pktoptions = skb_clone(ireq->pktopts,\n\t\t\t\t\t\t      sk_gfp_atomic(sk, GFP_ATOMIC));\n\t\t\tconsume_skb(ireq->pktopts);\n\t\t\tireq->pktopts = NULL;\n\t\t\tif (newnp->pktoptions)\n\t\t\t\tskb_set_owner_r(newnp->pktoptions, newsk);\n\t\t}\n\t}\n\n\treturn newsk;\n\nout_overflow:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nout_nonewsk:\n\tdst_release(dst);\nout:\n\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);\n\treturn NULL;\n}\n\n/* The socket must have it's spinlock held when we get\n * here, unless it is a TCP_LISTEN socket.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nstatic int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct tcp_sock *tp;\n\tstruct sk_buff *opt_skb = NULL;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, tcp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   tcp_v6_hnd_req and tcp_v6_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_do_rcv(sk, skb);\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard;\n\n\t/*\n\t *\tsocket locking is here for SMP purposes as backlog rcv\n\t *\tis currently called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all)\n\t\topt_skb = skb_clone(skb, sk_gfp_atomic(sk, GFP_ATOMIC));\n\n\tif (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */\n\t\tstruct dst_entry *dst = sk->sk_rx_dst;\n\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tif (dst) {\n\t\t\tif (inet_sk(sk)->rx_dst_ifindex != skb->skb_iif ||\n\t\t\t    dst->ops->check(dst, np->rx_dst_cookie) == NULL) {\n\t\t\t\tdst_release(dst);\n\t\t\t\tsk->sk_rx_dst = NULL;\n\t\t\t}\n\t\t}\n\n\t\ttcp_rcv_established(sk, skb, tcp_hdr(skb), skb->len);\n\t\tif (opt_skb)\n\t\t\tgoto ipv6_pktoptions;\n\t\treturn 0;\n\t}\n\n\tif (tcp_checksum_complete(skb))\n\t\tgoto csum_err;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tstruct sock *nsk = tcp_v6_cookie_check(sk, skb);\n\n\t\tif (!nsk)\n\t\t\tgoto discard;\n\n\t\tif (nsk != sk) {\n\t\t\tsock_rps_save_rxhash(nsk, skb);\n\t\t\tsk_mark_napi_id(nsk, skb);\n\t\t\tif (tcp_child_process(sk, nsk, skb))\n\t\t\t\tgoto reset;\n\t\t\tif (opt_skb)\n\t\t\t\t__kfree_skb(opt_skb);\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\tsock_rps_save_rxhash(sk, skb);\n\n\tif (tcp_rcv_state_process(sk, skb))\n\t\tgoto reset;\n\tif (opt_skb)\n\t\tgoto ipv6_pktoptions;\n\treturn 0;\n\nreset:\n\ttcp_v6_send_reset(sk, skb);\ndiscard:\n\tif (opt_skb)\n\t\t__kfree_skb(opt_skb);\n\tkfree_skb(skb);\n\treturn 0;\ncsum_err:\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_CSUMERRORS);\n\tTCP_INC_STATS_BH(sock_net(sk), TCP_MIB_INERRS);\n\tgoto discard;\n\n\nipv6_pktoptions:\n\t/* Do you ask, what is it?\n\n\t   1. skb was enqueued by tcp.\n\t   2. skb is added to tail of read queue, rather than out of order.\n\t   3. socket is not in passive state.\n\t   4. Finally, it really contains options, which user wants to receive.\n\t */\n\ttp = tcp_sk(sk);\n\tif (TCP_SKB_CB(opt_skb)->end_seq == tp->rcv_nxt &&\n\t    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\tif (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo)\n\t\t\tnp->mcast_oif = tcp_v6_iif(opt_skb);\n\t\tif (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim)\n\t\t\tnp->mcast_hops = ipv6_hdr(opt_skb)->hop_limit;\n\t\tif (np->rxopt.bits.rxflow || np->rxopt.bits.rxtclass)\n\t\t\tnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(opt_skb));\n\t\tif (np->repflow)\n\t\t\tnp->flow_label = ip6_flowlabel(ipv6_hdr(opt_skb));\n\t\tif (ipv6_opt_accepted(sk, opt_skb, &TCP_SKB_CB(opt_skb)->header.h6)) {\n\t\t\tskb_set_owner_r(opt_skb, sk);\n\t\t\topt_skb = xchg(&np->pktoptions, opt_skb);\n\t\t} else {\n\t\t\t__kfree_skb(opt_skb);\n\t\t\topt_skb = xchg(&np->pktoptions, NULL);\n\t\t}\n\t}\n\n\tkfree_skb(opt_skb);\n\treturn 0;\n}\n\nstatic void tcp_v6_fill_cb(struct sk_buff *skb, const struct ipv6hdr *hdr,\n\t\t\t   const struct tcphdr *th)\n{\n\t/* This is tricky: we move IP6CB at its correct location into\n\t * TCP_SKB_CB(). It must be done after xfrm6_policy_check(), because\n\t * _decode_session6() uses IP6CB().\n\t * barrier() makes sure compiler won't play aliasing games.\n\t */\n\tmemmove(&TCP_SKB_CB(skb)->header.h6, IP6CB(skb),\n\t\tsizeof(struct inet6_skb_parm));\n\tbarrier();\n\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff*4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->tcp_flags = tcp_flag_byte(th);\n\tTCP_SKB_CB(skb)->tcp_tw_isn = 0;\n\tTCP_SKB_CB(skb)->ip_dsfield = ipv6_get_dsfield(hdr);\n\tTCP_SKB_CB(skb)->sacked = 0;\n}\n\nstatic void tcp_v6_restore_cb(struct sk_buff *skb)\n{\n\t/* We need to move header back to the beginning if xfrm6_policy_check()\n\t * and tcp_v6_fill_cb() are going to be called again.\n\t */\n\tmemmove(IP6CB(skb), &TCP_SKB_CB(skb)->header.h6,\n\t\tsizeof(struct inet6_skb_parm));\n}\n\nstatic int tcp_v6_rcv(struct sk_buff *skb)\n{\n\tconst struct tcphdr *th;\n\tconst struct ipv6hdr *hdr;\n\tstruct sock *sk;\n\tint ret;\n\tstruct net *net = dev_net(skb->dev);\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/*\n\t *\tCount it even if it's bad.\n\t */\n\tTCP_INC_STATS_BH(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr)/4)\n\t\tgoto bad_packet;\n\tif (!pskb_may_pull(skb, th->doff*4))\n\t\tgoto discard_it;\n\n\tif (skb_checksum_init(skb, IPPROTO_TCP, ip6_compute_pseudo))\n\t\tgoto csum_error;\n\n\tth = tcp_hdr(skb);\n\thdr = ipv6_hdr(skb);\n\nlookup:\n\tsk = __inet6_lookup_skb(&tcp_hashinfo, skb, th->source, th->dest,\n\t\t\t\tinet6_iif(skb));\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\nprocess:\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\t\tstruct sock *nsk = NULL;\n\n\t\tsk = req->rsk_listener;\n\t\ttcp_v6_fill_cb(skb, hdr, th);\n\t\tif (tcp_v6_inbound_md5_hash(sk, skb)) {\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_it;\n\t\t}\n\t\tif (likely(sk->sk_state == TCP_LISTEN)) {\n\t\t\tnsk = tcp_check_req(sk, skb, req, false);\n\t\t} else {\n\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\n\t\t\tgoto lookup;\n\t\t}\n\t\tif (!nsk) {\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_it;\n\t\t}\n\t\tif (nsk == sk) {\n\t\t\tsock_hold(sk);\n\t\t\treqsk_put(req);\n\t\t\ttcp_v6_restore_cb(skb);\n\t\t} else if (tcp_child_process(sk, nsk, skb)) {\n\t\t\ttcp_v6_send_reset(nsk, skb);\n\t\t\tgoto discard_it;\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (hdr->hop_limit < inet6_sk(sk)->min_hopcount) {\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPMINTTLDROP);\n\t\tgoto discard_and_relse;\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_and_relse;\n\n\ttcp_v6_fill_cb(skb, hdr, th);\n\n\tif (tcp_v6_inbound_md5_hash(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tret = tcp_v6_do_rcv(sk, skb);\n\t\tgoto put_and_return;\n\t}\n\n\tsk_incoming_cpu_update(sk);\n\n\tbh_lock_sock_nested(sk);\n\ttcp_sk(sk)->segs_in += max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n\t\tif (!tcp_prequeue(sk, skb))\n\t\t\tret = tcp_v6_do_rcv(sk, skb);\n\t} else if (unlikely(sk_add_backlog(sk, skb,\n\t\t\t\t\t   sk->sk_rcvbuf + sk->sk_sndbuf))) {\n\t\tbh_unlock_sock(sk);\n\t\tNET_INC_STATS_BH(net, LINUX_MIB_TCPBACKLOGDROP);\n\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\n\nput_and_return:\n\tsock_put(sk);\n\treturn ret ? -1 : 0;\n\nno_tcp_socket:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\ttcp_v6_fill_cb(skb, hdr, th);\n\n\tif (tcp_checksum_complete(skb)) {\ncsum_error:\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_CSUMERRORS);\nbad_packet:\n\t\tTCP_INC_STATS_BH(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v6_send_reset(NULL, skb);\n\t}\n\ndiscard_it:\n\tkfree_skb(skb);\n\treturn 0;\n\ndiscard_and_relse:\n\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\ttcp_v6_fill_cb(skb, hdr, th);\n\n\tif (tcp_checksum_complete(skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto csum_error;\n\t}\n\n\tswitch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {\n\tcase TCP_TW_SYN:\n\t{\n\t\tstruct sock *sk2;\n\n\t\tsk2 = inet6_lookup_listener(dev_net(skb->dev), &tcp_hashinfo,\n\t\t\t\t\t    &ipv6_hdr(skb)->saddr, th->source,\n\t\t\t\t\t    &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t    ntohs(th->dest), tcp_v6_iif(skb));\n\t\tif (sk2) {\n\t\t\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\t\t\tinet_twsk_deschedule_put(tw);\n\t\t\tsk = sk2;\n\t\t\ttcp_v6_restore_cb(skb);\n\t\t\tgoto process;\n\t\t}\n\t\t/* Fall through to ACK */\n\t}\n\tcase TCP_TW_ACK:\n\t\ttcp_v6_timewait_ack(sk, skb);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\ttcp_v6_restore_cb(skb);\n\t\tgoto no_tcp_socket;\n\tcase TCP_TW_SUCCESS:\n\t\t;\n\t}\n\tgoto discard_it;\n}\n\nstatic void tcp_v6_early_demux(struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *hdr;\n\tconst struct tcphdr *th;\n\tstruct sock *sk;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\treturn;\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + sizeof(struct tcphdr)))\n\t\treturn;\n\n\thdr = ipv6_hdr(skb);\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr) / 4)\n\t\treturn;\n\n\t/* Note : We use inet6_iif() here, not tcp_v6_iif() */\n\tsk = __inet6_lookup_established(dev_net(skb->dev), &tcp_hashinfo,\n\t\t\t\t\t&hdr->saddr, th->source,\n\t\t\t\t\t&hdr->daddr, ntohs(th->dest),\n\t\t\t\t\tinet6_iif(skb));\n\tif (sk) {\n\t\tskb->sk = sk;\n\t\tskb->destructor = sock_edemux;\n\t\tif (sk_fullsock(sk)) {\n\t\t\tstruct dst_entry *dst = READ_ONCE(sk->sk_rx_dst);\n\n\t\t\tif (dst)\n\t\t\t\tdst = dst_check(dst, inet6_sk(sk)->rx_dst_cookie);\n\t\t\tif (dst &&\n\t\t\t    inet_sk(sk)->rx_dst_ifindex == skb->skb_iif)\n\t\t\t\tskb_dst_set_noref(skb, dst);\n\t\t}\n\t}\n}\n\nstatic struct timewait_sock_ops tcp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct tcp6_timewait_sock),\n\t.twsk_unique\t= tcp_twsk_unique,\n\t.twsk_destructor = tcp_twsk_destructor,\n};\n\nstatic const struct inet_connection_sock_af_ops ipv6_specific = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = tcp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.sk_rx_dst_set\t   = inet6_sk_rx_dst_set,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.net_frag_header_len = sizeof(struct frag_hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n\t.mtu_reduced\t   = tcp_v6_mtu_reduced,\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific = {\n\t.md5_lookup\t=\ttcp_v6_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n};\n#endif\n\n/*\n *\tTCP over IPv4 via INET6 API\n */\nstatic const struct inet_connection_sock_af_ops ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = tcp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.sk_rx_dst_set\t   = inet_sk_rx_dst_set,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.addr2sockaddr\t   = inet6_csk_addr2sockaddr,\n\t.sockaddr_len\t   = sizeof(struct sockaddr_in6),\n\t.bind_conflict\t   = inet6_csk_bind_conflict,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n\t.mtu_reduced\t   = tcp_v4_mtu_reduced,\n};\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific = {\n\t.md5_lookup\t=\ttcp_v4_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v4_md5_hash_skb,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n};\n#endif\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int tcp_v6_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_init_sock(sk);\n\n\ticsk->icsk_af_ops = &ipv6_specific;\n\n#ifdef CONFIG_TCP_MD5SIG\n\ttcp_sk(sk)->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\n\treturn 0;\n}\n\nstatic void tcp_v6_destroy_sock(struct sock *sk)\n{\n\ttcp_v4_destroy_sock(sk);\n\tinet6_destroy_sock(sk);\n}\n\n#ifdef CONFIG_PROC_FS\n/* Proc filesystem TCPv6 sock list dumping. */\nstatic void get_openreq6(struct seq_file *seq,\n\t\t\t const struct request_sock *req, int i)\n{\n\tlong ttd = req->rsk_timer.expires - jiffies;\n\tconst struct in6_addr *src = &inet_rsk(req)->ir_v6_loc_addr;\n\tconst struct in6_addr *dest = &inet_rsk(req)->ir_v6_rmt_addr;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5u %8d %d %d %pK\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3],\n\t\t   inet_rsk(req)->ir_num,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3],\n\t\t   ntohs(inet_rsk(req)->ir_rmt_port),\n\t\t   TCP_SYN_RECV,\n\t\t   0, 0, /* could print option size, but that is af dependent. */\n\t\t   1,   /* timers active (only the expire timer) */\n\t\t   jiffies_to_clock_t(ttd),\n\t\t   req->num_timeout,\n\t\t   from_kuid_munged(seq_user_ns(seq),\n\t\t\t\t    sock_i_uid(req->rsk_listener)),\n\t\t   0,  /* non standard timer */\n\t\t   0, /* open_requests have no inode */\n\t\t   0, req);\n}\n\nstatic void get_tcp6_sock(struct seq_file *seq, struct sock *sp, int i)\n{\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\tint timer_active;\n\tunsigned long timer_expires;\n\tconst struct inet_sock *inet = inet_sk(sp);\n\tconst struct tcp_sock *tp = tcp_sk(sp);\n\tconst struct inet_connection_sock *icsk = inet_csk(sp);\n\tconst struct fastopen_queue *fastopenq = &icsk->icsk_accept_queue.fastopenq;\n\tint rx_queue;\n\tint state;\n\n\tdest  = &sp->sk_v6_daddr;\n\tsrc   = &sp->sk_v6_rcv_saddr;\n\tdestp = ntohs(inet->inet_dport);\n\tsrcp  = ntohs(inet->inet_sport);\n\n\tif (icsk->icsk_pending == ICSK_TIME_RETRANS) {\n\t\ttimer_active\t= 1;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (icsk->icsk_pending == ICSK_TIME_PROBE0) {\n\t\ttimer_active\t= 4;\n\t\ttimer_expires\t= icsk->icsk_timeout;\n\t} else if (timer_pending(&sp->sk_timer)) {\n\t\ttimer_active\t= 2;\n\t\ttimer_expires\t= sp->sk_timer.expires;\n\t} else {\n\t\ttimer_active\t= 0;\n\t\ttimer_expires = jiffies;\n\t}\n\n\tstate = sk_state_load(sp);\n\tif (state == TCP_LISTEN)\n\t\trx_queue = sp->sk_ack_backlog;\n\telse\n\t\t/* Because we don't lock the socket,\n\t\t * we might find a transient negative value.\n\t\t */\n\t\trx_queue = max_t(int, tp->rcv_nxt - tp->copied_seq, 0);\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %pK %lu %lu %u %u %d\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   state,\n\t\t   tp->write_seq - tp->snd_una,\n\t\t   rx_queue,\n\t\t   timer_active,\n\t\t   jiffies_delta_to_clock_t(timer_expires - jiffies),\n\t\t   icsk->icsk_retransmits,\n\t\t   from_kuid_munged(seq_user_ns(seq), sock_i_uid(sp)),\n\t\t   icsk->icsk_probes_out,\n\t\t   sock_i_ino(sp),\n\t\t   atomic_read(&sp->sk_refcnt), sp,\n\t\t   jiffies_to_clock_t(icsk->icsk_rto),\n\t\t   jiffies_to_clock_t(icsk->icsk_ack.ato),\n\t\t   (icsk->icsk_ack.quick << 1) | icsk->icsk_ack.pingpong,\n\t\t   tp->snd_cwnd,\n\t\t   state == TCP_LISTEN ?\n\t\t\tfastopenq->max_qlen :\n\t\t\t(tcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh)\n\t\t   );\n}\n\nstatic void get_timewait6_sock(struct seq_file *seq,\n\t\t\t       struct inet_timewait_sock *tw, int i)\n{\n\tlong delta = tw->tw_timer.expires - jiffies;\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\n\tdest = &tw->tw_v6_daddr;\n\tsrc  = &tw->tw_v6_rcv_saddr;\n\tdestp = ntohs(tw->tw_dport);\n\tsrcp  = ntohs(tw->tw_sport);\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %pK\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   tw->tw_substate, 0, 0,\n\t\t   3, jiffies_delta_to_clock_t(delta), 0, 0, 0, 0,\n\t\t   atomic_read(&tw->tw_refcnt), tw);\n}\n\nstatic int tcp6_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st;\n\tstruct sock *sk = v;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq,\n\t\t\t \"  sl  \"\n\t\t\t \"local_address                         \"\n\t\t\t \"remote_address                        \"\n\t\t\t \"st tx_queue rx_queue tr tm->when retrnsmt\"\n\t\t\t \"   uid  timeout inode\\n\");\n\t\tgoto out;\n\t}\n\tst = seq->private;\n\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tget_timewait6_sock(seq, v, st->num);\n\telse if (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\tget_openreq6(seq, v, st->num);\n\telse\n\t\tget_tcp6_sock(seq, v, st->num);\nout:\n\treturn 0;\n}\n\nstatic const struct file_operations tcp6_afinfo_seq_fops = {\n\t.owner   = THIS_MODULE,\n\t.open    = tcp_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release_net\n};\n\nstatic struct tcp_seq_afinfo tcp6_seq_afinfo = {\n\t.name\t\t= \"tcp6\",\n\t.family\t\t= AF_INET6,\n\t.seq_fops\t= &tcp6_afinfo_seq_fops,\n\t.seq_ops\t= {\n\t\t.show\t\t= tcp6_seq_show,\n\t},\n};\n\nint __net_init tcp6_proc_init(struct net *net)\n{\n\treturn tcp_proc_register(net, &tcp6_seq_afinfo);\n}\n\nvoid tcp6_proc_exit(struct net *net)\n{\n\ttcp_proc_unregister(net, &tcp6_seq_afinfo);\n}\n#endif\n\nstatic void tcp_v6_clear_sk(struct sock *sk, int size)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t/* we do not want to clear pinet6 field, because of RCU lookups */\n\tsk_prot_clear_nulls(sk, offsetof(struct inet_sock, pinet6));\n\n\tsize -= offsetof(struct inet_sock, pinet6) + sizeof(inet->pinet6);\n\tmemset(&inet->pinet6 + 1, 0, size);\n}\n\nstruct proto tcpv6_prot = {\n\t.name\t\t\t= \"TCPv6\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= tcp_close,\n\t.connect\t\t= tcp_v6_connect,\n\t.disconnect\t\t= tcp_disconnect,\n\t.accept\t\t\t= inet_csk_accept,\n\t.ioctl\t\t\t= tcp_ioctl,\n\t.init\t\t\t= tcp_v6_init_sock,\n\t.destroy\t\t= tcp_v6_destroy_sock,\n\t.shutdown\t\t= tcp_shutdown,\n\t.setsockopt\t\t= tcp_setsockopt,\n\t.getsockopt\t\t= tcp_getsockopt,\n\t.recvmsg\t\t= tcp_recvmsg,\n\t.sendmsg\t\t= tcp_sendmsg,\n\t.sendpage\t\t= tcp_sendpage,\n\t.backlog_rcv\t\t= tcp_v6_do_rcv,\n\t.release_cb\t\t= tcp_release_cb,\n\t.hash\t\t\t= inet_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.get_port\t\t= inet_csk_get_port,\n\t.enter_memory_pressure\t= tcp_enter_memory_pressure,\n\t.stream_memory_free\t= tcp_stream_memory_free,\n\t.sockets_allocated\t= &tcp_sockets_allocated,\n\t.memory_allocated\t= &tcp_memory_allocated,\n\t.memory_pressure\t= &tcp_memory_pressure,\n\t.orphan_count\t\t= &tcp_orphan_count,\n\t.sysctl_mem\t\t= sysctl_tcp_mem,\n\t.sysctl_wmem\t\t= sysctl_tcp_wmem,\n\t.sysctl_rmem\t\t= sysctl_tcp_rmem,\n\t.max_header\t\t= MAX_TCP_HEADER,\n\t.obj_size\t\t= sizeof(struct tcp6_sock),\n\t.slab_flags\t\t= SLAB_DESTROY_BY_RCU,\n\t.twsk_prot\t\t= &tcp6_timewait_sock_ops,\n\t.rsk_prot\t\t= &tcp6_request_sock_ops,\n\t.h.hashinfo\t\t= &tcp_hashinfo,\n\t.no_autobind\t\t= true,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt\t= compat_tcp_setsockopt,\n\t.compat_getsockopt\t= compat_tcp_getsockopt,\n#endif\n#ifdef CONFIG_MEMCG_KMEM\n\t.proto_cgroup\t\t= tcp_proto_cgroup,\n#endif\n\t.clear_sk\t\t= tcp_v6_clear_sk,\n};\n\nstatic const struct inet6_protocol tcpv6_protocol = {\n\t.early_demux\t=\ttcp_v6_early_demux,\n\t.handler\t=\ttcp_v6_rcv,\n\t.err_handler\t=\ttcp_v6_err,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\nstatic struct inet_protosw tcpv6_protosw = {\n\t.type\t\t=\tSOCK_STREAM,\n\t.protocol\t=\tIPPROTO_TCP,\n\t.prot\t\t=\t&tcpv6_prot,\n\t.ops\t\t=\t&inet6_stream_ops,\n\t.flags\t\t=\tINET_PROTOSW_PERMANENT |\n\t\t\t\tINET_PROTOSW_ICSK,\n};\n\nstatic int __net_init tcpv6_net_init(struct net *net)\n{\n\treturn inet_ctl_sock_create(&net->ipv6.tcp_sk, PF_INET6,\n\t\t\t\t    SOCK_RAW, IPPROTO_TCP, net);\n}\n\nstatic void __net_exit tcpv6_net_exit(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->ipv6.tcp_sk);\n}\n\nstatic void __net_exit tcpv6_net_exit_batch(struct list_head *net_exit_list)\n{\n\tinet_twsk_purge(&tcp_hashinfo, &tcp_death_row, AF_INET6);\n}\n\nstatic struct pernet_operations tcpv6_net_ops = {\n\t.init\t    = tcpv6_net_init,\n\t.exit\t    = tcpv6_net_exit,\n\t.exit_batch = tcpv6_net_exit_batch,\n};\n\nint __init tcpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&tcpv6_protocol, IPPROTO_TCP);\n\tif (ret)\n\t\tgoto out;\n\n\t/* register inet6 protocol */\n\tret = inet6_register_protosw(&tcpv6_protosw);\n\tif (ret)\n\t\tgoto out_tcpv6_protocol;\n\n\tret = register_pernet_subsys(&tcpv6_net_ops);\n\tif (ret)\n\t\tgoto out_tcpv6_protosw;\nout:\n\treturn ret;\n\nout_tcpv6_protosw:\n\tinet6_unregister_protosw(&tcpv6_protosw);\nout_tcpv6_protocol:\n\tinet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);\n\tgoto out;\n}\n\nvoid tcpv6_exit(void)\n{\n\tunregister_pernet_subsys(&tcpv6_net_ops);\n\tinet6_unregister_protosw(&tcpv6_protosw);\n\tinet6_del_protocol(&tcpv6_protocol, IPPROTO_TCP);\n}\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tTCP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on:\n *\tlinux/net/ipv4/tcp.c\n *\tlinux/net/ipv4/tcp_input.c\n *\tlinux/net/ipv4/tcp_output.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *\tYOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/tcp6 to seq_file.\n */\n\n#include <linux/bottom_half.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/jiffies.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/init.h>\n#include <linux/jhash.h>\n#include <linux/ipsec.h>\n#include <linux/times.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/random.h>\n#include <linux/indirect_call_wrapper.h>\n\n#include <net/aligned_data.h>\n#include <net/tcp.h>\n#include <net/ndisc.h>\n#include <net/inet6_hashtables.h>\n#include <net/inet6_connection_sock.h>\n#include <net/ipv6.h>\n#include <net/transp_v6.h>\n#include <net/addrconf.h>\n#include <net/ip6_route.h>\n#include <net/ip6_checksum.h>\n#include <net/inet_ecn.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n#include <net/snmp.h>\n#include <net/dsfield.h>\n#include <net/timewait_sock.h>\n#include <net/inet_common.h>\n#include <net/secure_seq.h>\n#include <net/hotdata.h>\n#include <net/busy_poll.h>\n#include <net/rstreason.h>\n#include <net/psp.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n\n#include <crypto/hash.h>\n#include <linux/scatterlist.h>\n\n#include <trace/events/tcp.h>\n\nstatic void tcp_v6_send_reset(const struct sock *sk, struct sk_buff *skb,\n\t\t\t      enum sk_rst_reason reason);\nstatic void\ttcp_v6_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      struct request_sock *req);\n\nINDIRECT_CALLABLE_SCOPE int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb);\n\nstatic const struct inet_connection_sock_af_ops ipv6_mapped;\nconst struct inet_connection_sock_af_ops ipv6_specific;\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific;\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific;\n#endif\n\n/* Helper returning the inet6 address from a given tcp socket.\n * It can be used in TCP stack instead of inet6_sk(sk).\n * This avoids a dereference and allow compiler optimizations.\n * It is a specialized version of inet6_sk_generic().\n */\n#define tcp_inet6_sk(sk) (&container_of_const(tcp_sk(sk), \\\n\t\t\t\t\t      struct tcp6_sock, tcp)->inet6)\n\nstatic void inet6_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\n\tif (dst && dst_hold_safe(dst)) {\n\t\trcu_assign_pointer(sk->sk_rx_dst, dst);\n\t\tsk->sk_rx_dst_ifindex = skb->skb_iif;\n\t\tsk->sk_rx_dst_cookie = rt6_get_cookie(dst_rt6_info(dst));\n\t}\n}\n\nstatic u32 tcp_v6_init_seq(const struct sk_buff *skb)\n{\n\treturn secure_tcpv6_seq(ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\tipv6_hdr(skb)->saddr.s6_addr32,\n\t\t\t\ttcp_hdr(skb)->dest,\n\t\t\t\ttcp_hdr(skb)->source);\n}\n\nstatic u32 tcp_v6_init_ts_off(const struct net *net, const struct sk_buff *skb)\n{\n\treturn secure_tcpv6_ts_off(net, ipv6_hdr(skb)->daddr.s6_addr32,\n\t\t\t\t   ipv6_hdr(skb)->saddr.s6_addr32);\n}\n\nstatic int tcp_v6_pre_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t      int addr_len)\n{\n\t/* This check is replicated from tcp_v6_connect() and intended to\n\t * prevent BPF program called below from accessing bytes that are out\n\t * of the bound specified by user in addr_len.\n\t */\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tsock_owned_by_me(sk);\n\n\treturn BPF_CGROUP_RUN_PROG_INET6_CONNECT(sk, uaddr, &addr_len);\n}\n\nstatic int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t  int addr_len)\n{\n\tstruct sockaddr_in6 *usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct in6_addr *saddr = NULL, *final_p, final;\n\tstruct inet_timewait_death_row *tcp_death_row;\n\tstruct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct dst_entry *dst;\n\tstruct flowi6 fl6;\n\tint addr_type;\n\tint err;\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (inet6_test_bit(SNDFLOW, sk)) {\n\t\tfl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\tIP6_ECN_flow_init(fl6.flowlabel);\n\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\tstruct ip6_flowlabel *flowlabel;\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t\tfl6_sock_release(flowlabel);\n\t\t}\n\t}\n\n\t/*\n\t *\tconnect() to INADDR_ANY means loopback (BSD'ism).\n\t */\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -ENETUNREACH;\n\n\tif (addr_type&IPV6_ADDR_LINKLOCAL) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\t/* If interface is set while binding, indices\n\t\t\t * must coincide.\n\t\t\t */\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (tp->rx_opt.ts_recent_stamp &&\n\t    !ipv6_addr_equal(&sk->sk_v6_daddr, &usin->sin6_addr)) {\n\t\ttp->rx_opt.ts_recent = 0;\n\t\ttp->rx_opt.ts_recent_stamp = 0;\n\t\tWRITE_ONCE(tp->write_seq, 0);\n\t}\n\n\tsk->sk_v6_daddr = usin->sin6_addr;\n\tnp->flow_label = fl6.flowlabel;\n\n\t/*\n\t *\tTCP over IPv4\n\t */\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tu32 exthdrlen = icsk->icsk_ext_hdr_len;\n\t\tstruct sockaddr_in sin;\n\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -ENETUNREACH;\n\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_port = usin->sin6_port;\n\t\tsin.sin_addr.s_addr = usin->sin6_addr.s6_addr32[3];\n\n\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_mapped);\n\t\tif (sk_is_mptcp(sk))\n\t\t\tmptcpv6_handle_mapped(sk, true);\n\t\tsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\n\t\ttp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\terr = tcp_v4_connect(sk, (struct sockaddr *)&sin, sizeof(sin));\n\n\t\tif (err) {\n\t\t\ticsk->icsk_ext_hdr_len = exthdrlen;\n\t\t\t/* Paired with READ_ONCE() in tcp_(get|set)sockopt() */\n\t\t\tWRITE_ONCE(icsk->icsk_af_ops, &ipv6_specific);\n\t\t\tif (sk_is_mptcp(sk))\n\t\t\t\tmptcpv6_handle_mapped(sk, false);\n\t\t\tsk->sk_backlog_rcv = tcp_v6_do_rcv;\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\n\t\t\ttp->af_specific = &tcp_sock_ipv6_specific;\n#endif\n\t\t\tgoto failure;\n\t\t}\n\t\tnp->saddr = sk->sk_v6_rcv_saddr;\n\n\t\treturn err;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tfl6.daddr = sk->sk_v6_daddr;\n\tfl6.saddr = saddr ? *saddr : np->saddr;\n\tfl6.flowlabel = ip6_make_flowinfo(np->tclass, np->flow_label);\n\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\tfl6.flowi6_mark = sk->sk_mark;\n\tfl6.fl6_dport = usin->sin6_port;\n\tfl6.fl6_sport = inet->inet_sport;\n\tif (IS_ENABLED(CONFIG_IP_ROUTE_MULTIPATH) && !fl6.fl6_sport)\n\t\tfl6.flowi6_flags = FLOWI_FLAG_ANY_SPORT;\n\tfl6.flowi6_uid = sk_uid(sk);\n\n\topt = rcu_dereference_protected(np->opt, lockdep_sock_is_held(sk));\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tdst = ip6_dst_lookup_flow(net, sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto failure;\n\t}\n\n\ttp->tcp_usec_ts = dst_tcp_usec_ts(dst);\n\ttcp_death_row = &sock_net(sk)->ipv4.tcp_death_row;\n\n\tif (!saddr) {\n\t\tsaddr = &fl6.saddr;\n\n\t\terr = inet_bhash2_update_saddr(sk, saddr, AF_INET6);\n\t\tif (err)\n\t\t\tgoto failure;\n\t}\n\n\t/* set the source address */\n\tnp->saddr = *saddr;\n\tinet->inet_rcv_saddr = LOOPBACK4_IPV6;\n\n\tsk->sk_gso_type = SKB_GSO_TCPV6;\n\tip6_dst_store(sk, dst, false, false);\n\n\ticsk->icsk_ext_hdr_len = psp_sk_overhead(sk);\n\tif (opt)\n\t\ticsk->icsk_ext_hdr_len += opt->opt_flen +\n\t\t\t\t\t  opt->opt_nflen;\n\n\ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n\n\tinet->inet_dport = usin->sin6_port;\n\n\ttcp_set_state(sk, TCP_SYN_SENT);\n\terr = inet6_hash_connect(tcp_death_row, sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\tsk_set_txhash(sk);\n\n\tif (likely(!tp->repair)) {\n\t\tif (!tp->write_seq)\n\t\t\tWRITE_ONCE(tp->write_seq,\n\t\t\t\t   secure_tcpv6_seq(np->saddr.s6_addr32,\n\t\t\t\t\t\t    sk->sk_v6_daddr.s6_addr32,\n\t\t\t\t\t\t    inet->inet_sport,\n\t\t\t\t\t\t    inet->inet_dport));\n\t\ttp->tsoffset = secure_tcpv6_ts_off(net, np->saddr.s6_addr32,\n\t\t\t\t\t\t   sk->sk_v6_daddr.s6_addr32);\n\t}\n\n\tif (tcp_fastopen_defer_connect(sk, &err))\n\t\treturn err;\n\tif (err)\n\t\tgoto late_failure;\n\n\terr = tcp_connect(sk);\n\tif (err)\n\t\tgoto late_failure;\n\n\treturn 0;\n\nlate_failure:\n\ttcp_set_state(sk, TCP_CLOSE);\n\tinet_bhash2_reset_saddr(sk);\nfailure:\n\tinet->inet_dport = 0;\n\tsk->sk_route_caps = 0;\n\treturn err;\n}\n\nstatic void tcp_v6_mtu_reduced(struct sock *sk)\n{\n\tstruct dst_entry *dst;\n\tu32 mtu;\n\n\tif ((1 << sk->sk_state) & (TCPF_LISTEN | TCPF_CLOSE))\n\t\treturn;\n\n\tmtu = READ_ONCE(tcp_sk(sk)->mtu_info);\n\n\t/* Drop requests trying to increase our current mss.\n\t * Check done in __ip6_rt_update_pmtu() is too late.\n\t */\n\tif (tcp_mtu_to_mss(sk, mtu) >= tcp_sk(sk)->mss_cache)\n\t\treturn;\n\n\tdst = inet6_csk_update_pmtu(sk, mtu);\n\tif (!dst)\n\t\treturn;\n\n\tif (inet_csk(sk)->icsk_pmtu_cookie > dst_mtu(dst)) {\n\t\ttcp_sync_mss(sk, dst_mtu(dst));\n\t\ttcp_simple_retransmit(sk);\n\t}\n}\n\nstatic int tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\tu8 type, u8 code, int offset, __be32 info)\n{\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct tcphdr *th = (struct tcphdr *)(skb->data+offset);\n\tstruct net *net = dev_net_rcu(skb->dev);\n\tstruct request_sock *fastopen;\n\tstruct ipv6_pinfo *np;\n\tstruct tcp_sock *tp;\n\t__u32 seq, snd_una;\n\tstruct sock *sk;\n\tbool fatal;\n\tint err;\n\n\tsk = __inet6_lookup_established(net, &hdr->daddr, th->dest,\n\t\t\t\t\t&hdr->saddr, ntohs(th->source),\n\t\t\t\t\tskb->dev->ifindex, inet6_sdif(skb));\n\n\tif (!sk) {\n\t\t__ICMP6_INC_STATS(net, __in6_dev_get(skb->dev),\n\t\t\t\t  ICMP6_MIB_INERRORS);\n\t\treturn -ENOENT;\n\t}\n\n\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\t/* To increase the counter of ignored icmps for TCP-AO */\n\t\ttcp_ao_ignore_icmp(sk, AF_INET6, type, code);\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\treturn 0;\n\t}\n\tseq = ntohl(th->seq);\n\tfatal = icmpv6_err_convert(type, code, &err);\n\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\ttcp_req_err(sk, seq, fatal);\n\t\treturn 0;\n\t}\n\n\tif (tcp_ao_ignore_icmp(sk, AF_INET6, type, code)) {\n\t\tsock_put(sk);\n\t\treturn 0;\n\t}\n\n\tbh_lock_sock(sk);\n\tif (sock_owned_by_user(sk) && type != ICMPV6_PKT_TOOBIG)\n\t\t__NET_INC_STATS(net, LINUX_MIB_LOCKDROPPEDICMPS);\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (static_branch_unlikely(&ip6_min_hopcount)) {\n\t\t/* min_hopcount can be changed concurrently from do_ipv6_setsockopt() */\n\t\tif (ipv6_hdr(skb)->hop_limit < READ_ONCE(tcp_inet6_sk(sk)->min_hopcount)) {\n\t\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\ttp = tcp_sk(sk);\n\t/* XXX (TFO) - tp->snd_una should be ISN (tcp_create_openreq_child() */\n\tfastopen = rcu_dereference(tp->fastopen_rsk);\n\tsnd_una = fastopen ? tcp_rsk(fastopen)->snt_isn : tp->snd_una;\n\tif (sk->sk_state != TCP_LISTEN &&\n\t    !between(seq, snd_una, tp->snd_nxt)) {\n\t\t__NET_INC_STATS(net, LINUX_MIB_OUTOFWINDOWICMPS);\n\t\tgoto out;\n\t}\n\n\tnp = tcp_inet6_sk(sk);\n\n\tif (type == NDISC_REDIRECT) {\n\t\tif (!sock_owned_by_user(sk)) {\n\t\t\tstruct dst_entry *dst = __sk_dst_check(sk, np->dst_cookie);\n\n\t\t\tif (dst)\n\t\t\t\tdst->ops->redirect(dst, sk, skb);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tu32 mtu = ntohl(info);\n\n\t\t/* We are not interested in TCP_LISTEN and open_requests\n\t\t * (SYN-ACKs send out by Linux are always <576bytes so\n\t\t * they should go through unfragmented).\n\t\t */\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\tgoto out;\n\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\n\t\tif (mtu < IPV6_MIN_MTU)\n\t\t\tgoto out;\n\n\t\tWRITE_ONCE(tp->mtu_info, mtu);\n\n\t\tif (!sock_owned_by_user(sk))\n\t\t\ttcp_v6_mtu_reduced(sk);\n\t\telse if (!test_and_set_bit(TCP_MTU_REDUCED_DEFERRED,\n\t\t\t\t\t   &sk->sk_tsq_flags))\n\t\t\tsock_hold(sk);\n\t\tgoto out;\n\t}\n\n\n\t/* Might be for an request_sock */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\tcase TCP_SYN_RECV:\n\t\t/* Only in fast or simultaneous open. If a fast open socket is\n\t\t * already accepted it is treated as a connected one below.\n\t\t */\n\t\tif (fastopen && !fastopen->sk)\n\t\t\tbreak;\n\n\t\tipv6_icmp_error(sk, skb, err, th->dest, ntohl(info), (u8 *)th);\n\n\t\tif (!sock_owned_by_user(sk))\n\t\t\ttcp_done_with_error(sk, err);\n\t\telse\n\t\t\tWRITE_ONCE(sk->sk_err_soft, err);\n\t\tgoto out;\n\tcase TCP_LISTEN:\n\t\tbreak;\n\tdefault:\n\t\t/* check if this ICMP message allows revert of backoff.\n\t\t * (see RFC 6069)\n\t\t */\n\t\tif (!fastopen && type == ICMPV6_DEST_UNREACH &&\n\t\t    code == ICMPV6_NOROUTE)\n\t\t\ttcp_ld_RTO_revert(sk, seq);\n\t}\n\n\tif (!sock_owned_by_user(sk) && inet6_test_bit(RECVERR6, sk)) {\n\t\tWRITE_ONCE(sk->sk_err, err);\n\t\tsk_error_report(sk);\n\t} else {\n\t\tWRITE_ONCE(sk->sk_err_soft, err);\n\t}\nout:\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\treturn 0;\n}\n\n\nstatic int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t      struct flowi *fl,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      enum tcp_synack_type synack_type,\n\t\t\t      struct sk_buff *syn_skb)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct flowi6 *fl6 = &fl->u.ip6;\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\tu8 tclass;\n\n\t/* First, grab a route. */\n\tif (!dst && (dst = inet6_csk_route_req(sk, fl6, req,\n\t\t\t\t\t       IPPROTO_TCP)) == NULL)\n\t\tgoto done;\n\n\tskb = tcp_make_synack(sk, dst, req, foc, synack_type, syn_skb);\n\n\tif (skb) {\n\t\ttcp_rsk(req)->syn_ect_snt = np->tclass & INET_ECN_MASK;\n\t\t__tcp_v6_send_check(skb, &ireq->ir_v6_loc_addr,\n\t\t\t\t    &ireq->ir_v6_rmt_addr);\n\n\t\tfl6->daddr = ireq->ir_v6_rmt_addr;\n\t\tif (inet6_test_bit(REPFLOW, sk) && ireq->pktopts)\n\t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n\n\t\ttclass = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reflect_tos) ?\n\t\t\t\t(tcp_rsk(req)->syn_tos & ~INET_ECN_MASK) |\n\t\t\t\t(np->tclass & INET_ECN_MASK) :\n\t\t\t\tnp->tclass;\n\n\t\tif (!INET_ECN_is_capable(tclass) &&\n\t\t    tcp_bpf_ca_needs_ecn((struct sock *)req))\n\t\t\ttclass |= INET_ECN_ECT_0;\n\n\t\trcu_read_lock();\n\t\topt = ireq->ipv6_opt;\n\t\tif (!opt)\n\t\t\topt = rcu_dereference(np->opt);\n\t\terr = ip6_xmit(sk, skb, fl6, skb->mark ? : READ_ONCE(sk->sk_mark),\n\t\t\t       opt, tclass, READ_ONCE(sk->sk_priority));\n\t\trcu_read_unlock();\n\t\terr = net_xmit_eval(err);\n\t}\n\ndone:\n\treturn err;\n}\n\n\nstatic void tcp_v6_reqsk_destructor(struct request_sock *req)\n{\n\tkfree(inet_rsk(req)->ipv6_opt);\n\tconsume_skb(inet_rsk(req)->pktopts);\n}\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic struct tcp_md5sig_key *tcp_v6_md5_do_lookup(const struct sock *sk,\n\t\t\t\t\t\t   const struct in6_addr *addr,\n\t\t\t\t\t\t   int l3index)\n{\n\treturn tcp_md5_do_lookup(sk, l3index,\n\t\t\t\t (union tcp_md5_addr *)addr, AF_INET6);\n}\n\nstatic struct tcp_md5sig_key *tcp_v6_md5_lookup(const struct sock *sk,\n\t\t\t\t\t\tconst struct sock *addr_sk)\n{\n\tint l3index;\n\n\tl3index = l3mdev_master_ifindex_by_index(sock_net(sk),\n\t\t\t\t\t\t addr_sk->sk_bound_dev_if);\n\treturn tcp_v6_md5_do_lookup(sk, &addr_sk->sk_v6_daddr,\n\t\t\t\t    l3index);\n}\n\nstatic int tcp_v6_parse_md5_keys(struct sock *sk, int optname,\n\t\t\t\t sockptr_t optval, int optlen)\n{\n\tstruct tcp_md5sig cmd;\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)&cmd.tcpm_addr;\n\tunion tcp_ao_addr *addr;\n\tint l3index = 0;\n\tu8 prefixlen;\n\tbool l3flag;\n\tu8 flags;\n\n\tif (optlen < sizeof(cmd))\n\t\treturn -EINVAL;\n\n\tif (copy_from_sockptr(&cmd, optval, sizeof(cmd)))\n\t\treturn -EFAULT;\n\n\tif (sin6->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\tflags = cmd.tcpm_flags & TCP_MD5SIG_FLAG_IFINDEX;\n\tl3flag = cmd.tcpm_flags & TCP_MD5SIG_FLAG_IFINDEX;\n\n\tif (optname == TCP_MD5SIG_EXT &&\n\t    cmd.tcpm_flags & TCP_MD5SIG_FLAG_PREFIX) {\n\t\tprefixlen = cmd.tcpm_prefixlen;\n\t\tif (prefixlen > 128 || (ipv6_addr_v4mapped(&sin6->sin6_addr) &&\n\t\t\t\t\tprefixlen > 32))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tprefixlen = ipv6_addr_v4mapped(&sin6->sin6_addr) ? 32 : 128;\n\t}\n\n\tif (optname == TCP_MD5SIG_EXT && cmd.tcpm_ifindex &&\n\t    cmd.tcpm_flags & TCP_MD5SIG_FLAG_IFINDEX) {\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_index_rcu(sock_net(sk), cmd.tcpm_ifindex);\n\t\tif (dev && netif_is_l3_master(dev))\n\t\t\tl3index = dev->ifindex;\n\t\trcu_read_unlock();\n\n\t\t/* ok to reference set/not set outside of rcu;\n\t\t * right now device MUST be an L3 master\n\t\t */\n\t\tif (!dev || !l3index)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!cmd.tcpm_keylen) {\n\t\tif (ipv6_addr_v4mapped(&sin6->sin6_addr))\n\t\t\treturn tcp_md5_do_del(sk, (union tcp_md5_addr *)&sin6->sin6_addr.s6_addr32[3],\n\t\t\t\t\t      AF_INET, prefixlen,\n\t\t\t\t\t      l3index, flags);\n\t\treturn tcp_md5_do_del(sk, (union tcp_md5_addr *)&sin6->sin6_addr,\n\t\t\t\t      AF_INET6, prefixlen, l3index, flags);\n\t}\n\n\tif (cmd.tcpm_keylen > TCP_MD5SIG_MAXKEYLEN)\n\t\treturn -EINVAL;\n\n\tif (ipv6_addr_v4mapped(&sin6->sin6_addr)) {\n\t\taddr = (union tcp_md5_addr *)&sin6->sin6_addr.s6_addr32[3];\n\n\t\t/* Don't allow keys for peers that have a matching TCP-AO key.\n\t\t * See the comment in tcp_ao_add_cmd()\n\t\t */\n\t\tif (tcp_ao_required(sk, addr, AF_INET,\n\t\t\t\t    l3flag ? l3index : -1, false))\n\t\t\treturn -EKEYREJECTED;\n\t\treturn tcp_md5_do_add(sk, addr,\n\t\t\t\t      AF_INET, prefixlen, l3index, flags,\n\t\t\t\t      cmd.tcpm_key, cmd.tcpm_keylen);\n\t}\n\n\taddr = (union tcp_md5_addr *)&sin6->sin6_addr;\n\n\t/* Don't allow keys for peers that have a matching TCP-AO key.\n\t * See the comment in tcp_ao_add_cmd()\n\t */\n\tif (tcp_ao_required(sk, addr, AF_INET6, l3flag ? l3index : -1, false))\n\t\treturn -EKEYREJECTED;\n\n\treturn tcp_md5_do_add(sk, addr, AF_INET6, prefixlen, l3index, flags,\n\t\t\t      cmd.tcpm_key, cmd.tcpm_keylen);\n}\n\nstatic int tcp_v6_md5_hash_headers(struct tcp_sigpool *hp,\n\t\t\t\t   const struct in6_addr *daddr,\n\t\t\t\t   const struct in6_addr *saddr,\n\t\t\t\t   const struct tcphdr *th, int nbytes)\n{\n\tstruct tcp6_pseudohdr *bp;\n\tstruct scatterlist sg;\n\tstruct tcphdr *_th;\n\n\tbp = hp->scratch;\n\t/* 1. TCP pseudo-header (RFC2460) */\n\tbp->saddr = *saddr;\n\tbp->daddr = *daddr;\n\tbp->protocol = cpu_to_be32(IPPROTO_TCP);\n\tbp->len = cpu_to_be32(nbytes);\n\n\t_th = (struct tcphdr *)(bp + 1);\n\tmemcpy(_th, th, sizeof(*th));\n\t_th->check = 0;\n\n\tsg_init_one(&sg, bp, sizeof(*bp) + sizeof(*th));\n\tahash_request_set_crypt(hp->req, &sg, NULL,\n\t\t\t\tsizeof(*bp) + sizeof(*th));\n\treturn crypto_ahash_update(hp->req);\n}\n\nstatic int tcp_v6_md5_hash_hdr(char *md5_hash, const struct tcp_md5sig_key *key,\n\t\t\t       const struct in6_addr *daddr, struct in6_addr *saddr,\n\t\t\t       const struct tcphdr *th)\n{\n\tstruct tcp_sigpool hp;\n\n\tif (tcp_sigpool_start(tcp_md5_sigpool_id, &hp))\n\t\tgoto clear_hash_nostart;\n\n\tif (crypto_ahash_init(hp.req))\n\t\tgoto clear_hash;\n\tif (tcp_v6_md5_hash_headers(&hp, daddr, saddr, th, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(&hp, key))\n\t\tgoto clear_hash;\n\tahash_request_set_crypt(hp.req, NULL, md5_hash, 0);\n\tif (crypto_ahash_final(hp.req))\n\t\tgoto clear_hash;\n\n\ttcp_sigpool_end(&hp);\n\treturn 0;\n\nclear_hash:\n\ttcp_sigpool_end(&hp);\nclear_hash_nostart:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n\nstatic int tcp_v6_md5_hash_skb(char *md5_hash,\n\t\t\t       const struct tcp_md5sig_key *key,\n\t\t\t       const struct sock *sk,\n\t\t\t       const struct sk_buff *skb)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tconst struct in6_addr *saddr, *daddr;\n\tstruct tcp_sigpool hp;\n\n\tif (sk) { /* valid for establish/request sockets */\n\t\tsaddr = &sk->sk_v6_rcv_saddr;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else {\n\t\tconst struct ipv6hdr *ip6h = ipv6_hdr(skb);\n\t\tsaddr = &ip6h->saddr;\n\t\tdaddr = &ip6h->daddr;\n\t}\n\n\tif (tcp_sigpool_start(tcp_md5_sigpool_id, &hp))\n\t\tgoto clear_hash_nostart;\n\n\tif (crypto_ahash_init(hp.req))\n\t\tgoto clear_hash;\n\n\tif (tcp_v6_md5_hash_headers(&hp, daddr, saddr, th, skb->len))\n\t\tgoto clear_hash;\n\tif (tcp_sigpool_hash_skb_data(&hp, skb, th->doff << 2))\n\t\tgoto clear_hash;\n\tif (tcp_md5_hash_key(&hp, key))\n\t\tgoto clear_hash;\n\tahash_request_set_crypt(hp.req, NULL, md5_hash, 0);\n\tif (crypto_ahash_final(hp.req))\n\t\tgoto clear_hash;\n\n\ttcp_sigpool_end(&hp);\n\treturn 0;\n\nclear_hash:\n\ttcp_sigpool_end(&hp);\nclear_hash_nostart:\n\tmemset(md5_hash, 0, 16);\n\treturn 1;\n}\n#endif\n\nstatic void tcp_v6_init_req(struct request_sock *req,\n\t\t\t    const struct sock *sk_listener,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    u32 tw_isn)\n{\n\tbool l3_slave = ipv6_l3mdev_skb(TCP_SKB_CB(skb)->header.h6.flags);\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\tconst struct ipv6_pinfo *np = tcp_inet6_sk(sk_listener);\n\n\tireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\tireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\tireq->ir_rmt_addr = LOOPBACK4_IPV6;\n\tireq->ir_loc_addr = LOOPBACK4_IPV6;\n\n\t/* So that link locals have meaning */\n\tif ((!sk_listener->sk_bound_dev_if || l3_slave) &&\n\t    ipv6_addr_type(&ireq->ir_v6_rmt_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tireq->ir_iif = tcp_v6_iif(skb);\n\n\tif (!tw_isn &&\n\t    (ipv6_opt_accepted(sk_listener, skb, &TCP_SKB_CB(skb)->header.h6) ||\n\t     np->rxopt.bits.rxinfo ||\n\t     np->rxopt.bits.rxoinfo || np->rxopt.bits.rxhlim ||\n\t     np->rxopt.bits.rxohlim || inet6_test_bit(REPFLOW, sk_listener))) {\n\t\trefcount_inc(&skb->users);\n\t\tireq->pktopts = skb;\n\t}\n}\n\nstatic struct dst_entry *tcp_v6_route_req(const struct sock *sk,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  struct flowi *fl,\n\t\t\t\t\t  struct request_sock *req,\n\t\t\t\t\t  u32 tw_isn)\n{\n\ttcp_v6_init_req(req, sk, skb, tw_isn);\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\treturn NULL;\n\n\treturn inet6_csk_route_req(sk, &fl->u.ip6, req, IPPROTO_TCP);\n}\n\nstruct request_sock_ops tcp6_request_sock_ops __read_mostly = {\n\t.family\t\t=\tAF_INET6,\n\t.obj_size\t=\tsizeof(struct tcp6_request_sock),\n\t.send_ack\t=\ttcp_v6_reqsk_send_ack,\n\t.destructor\t=\ttcp_v6_reqsk_destructor,\n\t.send_reset\t=\ttcp_v6_send_reset,\n\t.syn_ack_timeout =\ttcp_syn_ack_timeout,\n};\n\nconst struct tcp_request_sock_ops tcp_request_sock_ipv6_ops = {\n\t.mss_clamp\t=\tIPV6_MIN_MTU - sizeof(struct tcphdr) -\n\t\t\t\tsizeof(struct ipv6hdr),\n#ifdef CONFIG_TCP_MD5SIG\n\t.req_md5_lookup\t=\ttcp_v6_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n#endif\n#ifdef CONFIG_TCP_AO\n\t.ao_lookup\t=\ttcp_v6_ao_lookup_rsk,\n\t.ao_calc_key\t=\ttcp_v6_ao_calc_key_rsk,\n\t.ao_synack_hash =\ttcp_v6_ao_synack_hash,\n#endif\n#ifdef CONFIG_SYN_COOKIES\n\t.cookie_init_seq =\tcookie_v6_init_sequence,\n#endif\n\t.route_req\t=\ttcp_v6_route_req,\n\t.init_seq\t=\ttcp_v6_init_seq,\n\t.init_ts_off\t=\ttcp_v6_init_ts_off,\n\t.send_synack\t=\ttcp_v6_send_synack,\n};\n\nstatic void tcp_v6_send_response(const struct sock *sk, struct sk_buff *skb, u32 seq,\n\t\t\t\t u32 ack, u32 win, u32 tsval, u32 tsecr,\n\t\t\t\t int oif, int rst, u8 tclass, __be32 label,\n\t\t\t\t u32 priority, u32 txhash, struct tcp_key *key)\n{\n\tstruct net *net = sk ? sock_net(sk) : skb_dst_dev_net_rcu(skb);\n\tunsigned int tot_len = sizeof(struct tcphdr);\n\tstruct sock *ctl_sk = net->ipv6.tcp_sk;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\t__be32 mrst = 0, *topt;\n\tstruct dst_entry *dst;\n\tstruct sk_buff *buff;\n\tstruct tcphdr *t1;\n\tstruct flowi6 fl6;\n\tu32 mark = 0;\n\n\tif (tsecr)\n\t\ttot_len += TCPOLEN_TSTAMP_ALIGNED;\n\tif (tcp_key_is_md5(key))\n\t\ttot_len += TCPOLEN_MD5SIG_ALIGNED;\n\tif (tcp_key_is_ao(key))\n\t\ttot_len += tcp_ao_len_aligned(key->ao_key);\n\n#ifdef CONFIG_MPTCP\n\tif (rst && !tcp_key_is_md5(key)) {\n\t\tmrst = mptcp_reset_option(skb);\n\n\t\tif (mrst)\n\t\t\ttot_len += sizeof(__be32);\n\t}\n#endif\n\n\tbuff = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);\n\tif (!buff)\n\t\treturn;\n\n\tskb_reserve(buff, MAX_TCP_HEADER);\n\n\tt1 = skb_push(buff, tot_len);\n\tskb_reset_transport_header(buff);\n\n\t/* Swap the send and the receive. */\n\tmemset(t1, 0, sizeof(*t1));\n\tt1->dest = th->source;\n\tt1->source = th->dest;\n\tt1->doff = tot_len / 4;\n\tt1->seq = htonl(seq);\n\tt1->ack_seq = htonl(ack);\n\tt1->ack = !rst || !th->ack;\n\tt1->rst = rst;\n\tt1->window = htons(win);\n\n\ttopt = (__be32 *)(t1 + 1);\n\n\tif (tsecr) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);\n\t\t*topt++ = htonl(tsval);\n\t\t*topt++ = htonl(tsecr);\n\t}\n\n\tif (mrst)\n\t\t*topt++ = mrst;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tif (tcp_key_is_md5(key)) {\n\t\t*topt++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t\t(TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\ttcp_v6_md5_hash_hdr((__u8 *)topt, key->md5_key,\n\t\t\t\t    &ipv6_hdr(skb)->saddr,\n\t\t\t\t    &ipv6_hdr(skb)->daddr, t1);\n\t}\n#endif\n#ifdef CONFIG_TCP_AO\n\tif (tcp_key_is_ao(key)) {\n\t\t*topt++ = htonl((TCPOPT_AO << 24) |\n\t\t\t\t(tcp_ao_len(key->ao_key) << 16) |\n\t\t\t\t(key->ao_key->sndid << 8) |\n\t\t\t\t(key->rcv_next));\n\n\t\ttcp_ao_hash_hdr(AF_INET6, (char *)topt, key->ao_key,\n\t\t\t\tkey->traffic_key,\n\t\t\t\t(union tcp_ao_addr *)&ipv6_hdr(skb)->saddr,\n\t\t\t\t(union tcp_ao_addr *)&ipv6_hdr(skb)->daddr,\n\t\t\t\tt1, key->sne);\n\t}\n#endif\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\tfl6.daddr = ipv6_hdr(skb)->saddr;\n\tfl6.saddr = ipv6_hdr(skb)->daddr;\n\tfl6.flowlabel = label;\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\n\t__tcp_v6_send_check(buff, &fl6.saddr, &fl6.daddr);\n\n\tfl6.flowi6_proto = IPPROTO_TCP;\n\tif (rt6_need_strict(&fl6.daddr) && !oif)\n\t\tfl6.flowi6_oif = tcp_v6_iif(skb);\n\telse {\n\t\tif (!oif && netif_index_is_l3_master(net, skb->skb_iif))\n\t\t\toif = skb->skb_iif;\n\n\t\tfl6.flowi6_oif = oif;\n\t}\n\n\tif (sk) {\n\t\t/* unconstify the socket only to attach it to buff with care. */\n\t\tskb_set_owner_edemux(buff, (struct sock *)sk);\n\t\tpsp_reply_set_decrypted(buff);\n\n\t\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\t\tmark = inet_twsk(sk)->tw_mark;\n\t\telse\n\t\t\tmark = READ_ONCE(sk->sk_mark);\n\t\tskb_set_delivery_time(buff, tcp_transmit_time(sk), SKB_CLOCK_MONOTONIC);\n\t}\n\tif (txhash) {\n\t\t/* autoflowlabel/skb_get_hash_flowi6 rely on buff->hash */\n\t\tskb_set_hash(buff, txhash, PKT_HASH_TYPE_L4);\n\t}\n\tfl6.flowi6_mark = IP6_REPLY_MARK(net, skb->mark) ?: mark;\n\tfl6.fl6_dport = t1->dest;\n\tfl6.fl6_sport = t1->source;\n\tfl6.flowi6_uid = sock_net_uid(net, sk && sk_fullsock(sk) ? sk : NULL);\n\tsecurity_skb_classify_flow(skb, flowi6_to_flowi_common(&fl6));\n\n\t/* Pass a socket to ip6_dst_lookup either it is for RST\n\t * Underlying function will use this to retrieve the network\n\t * namespace\n\t */\n\tif (sk && sk->sk_state != TCP_TIME_WAIT)\n\t\tdst = ip6_dst_lookup_flow(net, sk, &fl6, NULL); /*sk's xfrm_policy can be referred*/\n\telse\n\t\tdst = ip6_dst_lookup_flow(net, ctl_sk, &fl6, NULL);\n\tif (!IS_ERR(dst)) {\n\t\tskb_dst_set(buff, dst);\n\t\tip6_xmit(ctl_sk, buff, &fl6, fl6.flowi6_mark, NULL,\n\t\t\t tclass, priority);\n\t\tTCP_INC_STATS(net, TCP_MIB_OUTSEGS);\n\t\tif (rst)\n\t\t\tTCP_INC_STATS(net, TCP_MIB_OUTRSTS);\n\t\treturn;\n\t}\n\n\tkfree_skb(buff);\n}\n\nstatic void tcp_v6_send_reset(const struct sock *sk, struct sk_buff *skb,\n\t\t\t      enum sk_rst_reason reason)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct ipv6hdr *ipv6h = ipv6_hdr(skb);\n\tconst __u8 *md5_hash_location = NULL;\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\n\tbool allocated_traffic_key = false;\n#endif\n\tconst struct tcp_ao_hdr *aoh;\n\tstruct tcp_key key = {};\n\tu32 seq = 0, ack_seq = 0;\n\t__be32 label = 0;\n\tu32 priority = 0;\n\tstruct net *net;\n\tu32 txhash = 0;\n\tint oif = 0;\n#ifdef CONFIG_TCP_MD5SIG\n\tunsigned char newhash[16];\n\tint genhash;\n\tstruct sock *sk1 = NULL;\n#endif\n\n\tif (th->rst)\n\t\treturn;\n\n\t/* If sk not NULL, it means we did a successful lookup and incoming\n\t * route had to be correct. prequeue might have dropped our dst.\n\t */\n\tif (!sk && !ipv6_unicast_destination(skb))\n\t\treturn;\n\n\tnet = sk ? sock_net(sk) : skb_dst_dev_net_rcu(skb);\n\t/* Invalid TCP option size or twice included auth */\n\tif (tcp_parse_auth_options(th, &md5_hash_location, &aoh))\n\t\treturn;\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\n\trcu_read_lock();\n#endif\n#ifdef CONFIG_TCP_MD5SIG\n\tif (sk && sk_fullsock(sk)) {\n\t\tint l3index;\n\n\t\t/* sdif set, means packet ingressed via a device\n\t\t * in an L3 domain and inet_iif is set to it.\n\t\t */\n\t\tl3index = tcp_v6_sdif(skb) ? tcp_v6_iif_l3_slave(skb) : 0;\n\t\tkey.md5_key = tcp_v6_md5_do_lookup(sk, &ipv6h->saddr, l3index);\n\t\tif (key.md5_key)\n\t\t\tkey.type = TCP_KEY_MD5;\n\t} else if (md5_hash_location) {\n\t\tint dif = tcp_v6_iif_l3_slave(skb);\n\t\tint sdif = tcp_v6_sdif(skb);\n\t\tint l3index;\n\n\t\t/*\n\t\t * active side is lost. Try to find listening socket through\n\t\t * source port, and then find md5 key through listening socket.\n\t\t * we are not loose security here:\n\t\t * Incoming packet is checked with md5 hash with finding key,\n\t\t * no RST generated if md5 hash doesn't match.\n\t\t */\n\t\tsk1 = inet6_lookup_listener(net, NULL, 0, &ipv6h->saddr, th->source,\n\t\t\t\t\t    &ipv6h->daddr, ntohs(th->source),\n\t\t\t\t\t    dif, sdif);\n\t\tif (!sk1)\n\t\t\tgoto out;\n\n\t\t/* sdif set, means packet ingressed via a device\n\t\t * in an L3 domain and dif is set to it.\n\t\t */\n\t\tl3index = tcp_v6_sdif(skb) ? dif : 0;\n\n\t\tkey.md5_key = tcp_v6_md5_do_lookup(sk1, &ipv6h->saddr, l3index);\n\t\tif (!key.md5_key)\n\t\t\tgoto out;\n\t\tkey.type = TCP_KEY_MD5;\n\n\t\tgenhash = tcp_v6_md5_hash_skb(newhash, key.md5_key, NULL, skb);\n\t\tif (genhash || memcmp(md5_hash_location, newhash, 16) != 0)\n\t\t\tgoto out;\n\t}\n#endif\n\n\tif (th->ack)\n\t\tseq = ntohl(th->ack_seq);\n\telse\n\t\tack_seq = ntohl(th->seq) + th->syn + th->fin + skb->len -\n\t\t\t  (th->doff << 2);\n\n#ifdef CONFIG_TCP_AO\n\tif (aoh) {\n\t\tint l3index;\n\n\t\tl3index = tcp_v6_sdif(skb) ? tcp_v6_iif_l3_slave(skb) : 0;\n\t\tif (tcp_ao_prepare_reset(sk, skb, aoh, l3index, seq,\n\t\t\t\t\t &key.ao_key, &key.traffic_key,\n\t\t\t\t\t &allocated_traffic_key,\n\t\t\t\t\t &key.rcv_next, &key.sne))\n\t\t\tgoto out;\n\t\tkey.type = TCP_KEY_AO;\n\t}\n#endif\n\n\tif (sk) {\n\t\toif = sk->sk_bound_dev_if;\n\t\tif (sk_fullsock(sk)) {\n\t\t\tif (inet6_test_bit(REPFLOW, sk))\n\t\t\t\tlabel = ip6_flowlabel(ipv6h);\n\t\t\tpriority = READ_ONCE(sk->sk_priority);\n\t\t\ttxhash = sk->sk_txhash;\n\t\t}\n\t\tif (sk->sk_state == TCP_TIME_WAIT) {\n\t\t\tlabel = cpu_to_be32(inet_twsk(sk)->tw_flowlabel);\n\t\t\tpriority = inet_twsk(sk)->tw_priority;\n\t\t\ttxhash = inet_twsk(sk)->tw_txhash;\n\t\t}\n\t} else {\n\t\tif (net->ipv6.sysctl.flowlabel_reflect & FLOWLABEL_REFLECT_TCP_RESET)\n\t\t\tlabel = ip6_flowlabel(ipv6h);\n\t}\n\n\ttrace_tcp_send_reset(sk, skb, reason);\n\n\ttcp_v6_send_response(sk, skb, seq, ack_seq, 0, 0, 0, oif, 1,\n\t\t\t     ipv6_get_dsfield(ipv6h) & ~INET_ECN_MASK,\n\t\t\t     label, priority, txhash,\n\t\t\t     &key);\n\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\nout:\n\tif (allocated_traffic_key)\n\t\tkfree(key.traffic_key);\n\trcu_read_unlock();\n#endif\n}\n\nstatic void tcp_v6_send_ack(const struct sock *sk, struct sk_buff *skb, u32 seq,\n\t\t\t    u32 ack, u32 win, u32 tsval, u32 tsecr, int oif,\n\t\t\t    struct tcp_key *key, u8 tclass,\n\t\t\t    __be32 label, u32 priority, u32 txhash)\n{\n\ttcp_v6_send_response(sk, skb, seq, ack, win, tsval, tsecr, oif, 0,\n\t\t\t     tclass, label, priority, txhash, key);\n}\n\nstatic void tcp_v6_timewait_ack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tenum tcp_tw_status tw_status)\n{\n\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\tstruct tcp_timewait_sock *tcptw = tcp_twsk(sk);\n\tu8 tclass = tw->tw_tclass;\n\tstruct tcp_key key = {};\n\n\tif (tw_status == TCP_TW_ACK_OOW)\n\t\ttclass &= ~INET_ECN_MASK;\n#ifdef CONFIG_TCP_AO\n\tstruct tcp_ao_info *ao_info;\n\n\tif (static_branch_unlikely(&tcp_ao_needed.key)) {\n\n\t\t/* FIXME: the segment to-be-acked is not verified yet */\n\t\tao_info = rcu_dereference(tcptw->ao_info);\n\t\tif (ao_info) {\n\t\t\tconst struct tcp_ao_hdr *aoh;\n\n\t\t\t/* Invalid TCP option size or twice included auth */\n\t\t\tif (tcp_parse_auth_options(tcp_hdr(skb), NULL, &aoh))\n\t\t\t\tgoto out;\n\t\t\tif (aoh)\n\t\t\t\tkey.ao_key = tcp_ao_established_key(sk, ao_info,\n\t\t\t\t\t\t\t\t    aoh->rnext_keyid, -1);\n\t\t}\n\t}\n\tif (key.ao_key) {\n\t\tstruct tcp_ao_key *rnext_key;\n\n\t\tkey.traffic_key = snd_other_key(key.ao_key);\n\t\t/* rcv_next switches to our rcv_next */\n\t\trnext_key = READ_ONCE(ao_info->rnext_key);\n\t\tkey.rcv_next = rnext_key->rcvid;\n\t\tkey.sne = READ_ONCE(ao_info->snd_sne);\n\t\tkey.type = TCP_KEY_AO;\n#else\n\tif (0) {\n#endif\n#ifdef CONFIG_TCP_MD5SIG\n\t} else if (static_branch_unlikely(&tcp_md5_needed.key)) {\n\t\tkey.md5_key = tcp_twsk_md5_key(tcptw);\n\t\tif (key.md5_key)\n\t\t\tkey.type = TCP_KEY_MD5;\n#endif\n\t}\n\n\ttcp_v6_send_ack(sk, skb, tcptw->tw_snd_nxt,\n\t\t\tREAD_ONCE(tcptw->tw_rcv_nxt),\n\t\t\ttcptw->tw_rcv_wnd >> tw->tw_rcv_wscale,\n\t\t\ttcp_tw_tsval(tcptw),\n\t\t\tREAD_ONCE(tcptw->tw_ts_recent), tw->tw_bound_dev_if,\n\t\t\t&key, tclass, cpu_to_be32(tw->tw_flowlabel),\n\t\t\ttw->tw_priority, tw->tw_txhash);\n\n#ifdef CONFIG_TCP_AO\nout:\n#endif\n\tinet_twsk_put(tw);\n}\n\nstatic void tcp_v6_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req)\n{\n\tstruct tcp_key key = {};\n\n#ifdef CONFIG_TCP_AO\n\tif (static_branch_unlikely(&tcp_ao_needed.key) &&\n\t    tcp_rsk_used_ao(req)) {\n\t\tconst struct in6_addr *addr = &ipv6_hdr(skb)->saddr;\n\t\tconst struct tcp_ao_hdr *aoh;\n\t\tint l3index;\n\n\t\tl3index = tcp_v6_sdif(skb) ? tcp_v6_iif_l3_slave(skb) : 0;\n\t\t/* Invalid TCP option size or twice included auth */\n\t\tif (tcp_parse_auth_options(tcp_hdr(skb), NULL, &aoh))\n\t\t\treturn;\n\t\tif (!aoh)\n\t\t\treturn;\n\t\tkey.ao_key = tcp_ao_do_lookup(sk, l3index,\n\t\t\t\t\t      (union tcp_ao_addr *)addr,\n\t\t\t\t\t      AF_INET6, aoh->rnext_keyid, -1);\n\t\tif (unlikely(!key.ao_key)) {\n\t\t\t/* Send ACK with any matching MKT for the peer */\n\t\t\tkey.ao_key = tcp_ao_do_lookup(sk, l3index,\n\t\t\t\t\t\t      (union tcp_ao_addr *)addr,\n\t\t\t\t\t\t      AF_INET6, -1, -1);\n\t\t\t/* Matching key disappeared (user removed the key?)\n\t\t\t * let the handshake timeout.\n\t\t\t */\n\t\t\tif (!key.ao_key) {\n\t\t\t\tnet_info_ratelimited(\"TCP-AO key for (%pI6, %d)->(%pI6, %d) suddenly disappeared, won't ACK new connection\\n\",\n\t\t\t\t\t\t     addr,\n\t\t\t\t\t\t     ntohs(tcp_hdr(skb)->source),\n\t\t\t\t\t\t     &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t     ntohs(tcp_hdr(skb)->dest));\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tkey.traffic_key = kmalloc(tcp_ao_digest_size(key.ao_key), GFP_ATOMIC);\n\t\tif (!key.traffic_key)\n\t\t\treturn;\n\n\t\tkey.type = TCP_KEY_AO;\n\t\tkey.rcv_next = aoh->keyid;\n\t\ttcp_v6_ao_calc_key_rsk(key.ao_key, key.traffic_key, req);\n#else\n\tif (0) {\n#endif\n#ifdef CONFIG_TCP_MD5SIG\n\t} else if (static_branch_unlikely(&tcp_md5_needed.key)) {\n\t\tint l3index = tcp_v6_sdif(skb) ? tcp_v6_iif_l3_slave(skb) : 0;\n\n\t\tkey.md5_key = tcp_v6_md5_do_lookup(sk, &ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t   l3index);\n\t\tif (key.md5_key)\n\t\t\tkey.type = TCP_KEY_MD5;\n#endif\n\t}\n\n\t/* sk->sk_state == TCP_LISTEN -> for regular TCP_SYN_RECV\n\t * sk->sk_state == TCP_SYN_RECV -> for Fast Open.\n\t */\n\ttcp_v6_send_ack(sk, skb, (sk->sk_state == TCP_LISTEN) ?\n\t\t\ttcp_rsk(req)->snt_isn + 1 : tcp_sk(sk)->snd_nxt,\n\t\t\ttcp_rsk(req)->rcv_nxt,\n\t\t\ttcp_synack_window(req) >> inet_rsk(req)->rcv_wscale,\n\t\t\ttcp_rsk_tsval(tcp_rsk(req)),\n\t\t\treq->ts_recent, sk->sk_bound_dev_if,\n\t\t\t&key, ipv6_get_dsfield(ipv6_hdr(skb)) & ~INET_ECN_MASK,\n\t\t\t0,\n\t\t\tREAD_ONCE(sk->sk_priority),\n\t\t\tREAD_ONCE(tcp_rsk(req)->txhash));\n\tif (tcp_key_is_ao(&key))\n\t\tkfree(key.traffic_key);\n}\n\n\nstatic struct sock *tcp_v6_cookie_check(struct sock *sk, struct sk_buff *skb)\n{\n#ifdef CONFIG_SYN_COOKIES\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\n\tif (!th->syn)\n\t\tsk = cookie_v6_check(sk, skb);\n#endif\n\treturn sk;\n}\n\nu16 tcp_v6_get_syncookie(struct sock *sk, struct ipv6hdr *iph,\n\t\t\t struct tcphdr *th, u32 *cookie)\n{\n\tu16 mss = 0;\n#ifdef CONFIG_SYN_COOKIES\n\tmss = tcp_get_syncookie_mss(&tcp6_request_sock_ops,\n\t\t\t\t    &tcp_request_sock_ipv6_ops, sk, th);\n\tif (mss) {\n\t\t*cookie = __cookie_v6_init_sequence(iph, th, &mss);\n\t\ttcp_synq_overflow(sk);\n\t}\n#endif\n\treturn mss;\n}\n\nstatic int tcp_v6_conn_request(struct sock *sk, struct sk_buff *skb)\n{\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_conn_request(sk, skb);\n\n\tif (!ipv6_unicast_destination(skb))\n\t\tgoto drop;\n\n\tif (ipv6_addr_v4mapped(&ipv6_hdr(skb)->saddr)) {\n\t\t__IP6_INC_STATS(sock_net(sk), NULL, IPSTATS_MIB_INHDRERRORS);\n\t\treturn 0;\n\t}\n\n\treturn tcp_conn_request(&tcp6_request_sock_ops,\n\t\t\t\t&tcp_request_sock_ipv6_ops, sk, skb);\n\ndrop:\n\ttcp_listendrop(sk);\n\treturn 0; /* don't send reset */\n}\n\nstatic void tcp_v6_restore_cb(struct sk_buff *skb)\n{\n\t/* We need to move header back to the beginning if xfrm6_policy_check()\n\t * and tcp_v6_fill_cb() are going to be called again.\n\t * ip6_datagram_recv_specific_ctl() also expects IP6CB to be there.\n\t */\n\tmemmove(IP6CB(skb), &TCP_SKB_CB(skb)->header.h6,\n\t\tsizeof(struct inet6_skb_parm));\n}\n\nstatic struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t struct request_sock *req,\n\t\t\t\t\t struct dst_entry *dst,\n\t\t\t\t\t struct request_sock *req_unhash,\n\t\t\t\t\t bool *own_req)\n{\n\tstruct inet_request_sock *ireq;\n\tstruct ipv6_pinfo *newnp;\n\tconst struct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct ipv6_txoptions *opt;\n\tstruct inet_sock *newinet;\n\tbool found_dup_sk = false;\n\tstruct tcp_sock *newtp;\n\tstruct sock *newsk;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *key;\n\tint l3index;\n#endif\n\tstruct flowi6 fl6;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t/*\n\t\t *\tv6 mapped\n\t\t */\n\n\t\tnewsk = tcp_v4_syn_recv_sock(sk, skb, req, dst,\n\t\t\t\t\t     req_unhash, own_req);\n\n\t\tif (!newsk)\n\t\t\treturn NULL;\n\n\t\tinet_sk(newsk)->pinet6 = tcp_inet6_sk(newsk);\n\n\t\tnewnp = tcp_inet6_sk(newsk);\n\t\tnewtp = tcp_sk(newsk);\n\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\t\tnewnp->saddr = newsk->sk_v6_rcv_saddr;\n\n\t\tinet_csk(newsk)->icsk_af_ops = &ipv6_mapped;\n\t\tif (sk_is_mptcp(newsk))\n\t\t\tmptcpv6_handle_mapped(newsk, true);\n\t\tnewsk->sk_backlog_rcv = tcp_v4_do_rcv;\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\n\t\tnewtp->af_specific = &tcp_sock_ipv6_mapped_specific;\n#endif\n\n\t\tnewnp->ipv6_mc_list = NULL;\n\t\tnewnp->ipv6_ac_list = NULL;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->pktoptions  = NULL;\n\t\tnewnp->opt\t   = NULL;\n\t\tnewnp->mcast_oif   = inet_iif(skb);\n\t\tnewnp->mcast_hops  = ip_hdr(skb)->ttl;\n\t\tnewnp->rcv_flowinfo = 0;\n\t\tif (inet6_test_bit(REPFLOW, sk))\n\t\t\tnewnp->flow_label = 0;\n\n\t\t/*\n\t\t * No need to charge this sock to the relevant IPv6 refcnt debug socks count\n\t\t * here, tcp_create_openreq_child now does this for us, see the comment in\n\t\t * that function for the gory details. -acme\n\t\t */\n\n\t\t/* It is tricky place. Until this moment IPv4 tcp\n\t\t   worked with IPv6 icsk.icsk_af_ops.\n\t\t   Sync it now.\n\t\t */\n\t\ttcp_sync_mss(newsk, inet_csk(newsk)->icsk_pmtu_cookie);\n\n\t\treturn newsk;\n\t}\n\n\tireq = inet_rsk(req);\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto exit_overflow;\n\n\tif (!dst) {\n\t\tdst = inet6_csk_route_req(sk, &fl6, req, IPPROTO_TCP);\n\t\tif (!dst)\n\t\t\tgoto exit;\n\t}\n\n\tnewsk = tcp_create_openreq_child(sk, req, skb);\n\tif (!newsk)\n\t\tgoto exit_nonewsk;\n\n\t/*\n\t * No need to charge this sock to the relevant IPv6 refcnt debug socks\n\t * count here, tcp_create_openreq_child now does this for us, see the\n\t * comment in that function for the gory details. -acme\n\t */\n\n\tnewsk->sk_gso_type = SKB_GSO_TCPV6;\n\tinet6_sk_rx_dst_set(newsk, skb);\n\n\tinet_sk(newsk)->pinet6 = tcp_inet6_sk(newsk);\n\n\tnewtp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\tnewnp = tcp_inet6_sk(newsk);\n\n\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\n\tip6_dst_store(newsk, dst, false, false);\n\n\tnewnp->saddr = ireq->ir_v6_loc_addr;\n\n\t/* Now IPv6 options...\n\n\t   First: no IPv4 options.\n\t */\n\tnewinet->inet_opt = NULL;\n\tnewnp->ipv6_mc_list = NULL;\n\tnewnp->ipv6_ac_list = NULL;\n\tnewnp->ipv6_fl_list = NULL;\n\n\t/* Clone RX bits */\n\tnewnp->rxopt.all = np->rxopt.all;\n\n\tnewnp->pktoptions = NULL;\n\tnewnp->opt\t  = NULL;\n\tnewnp->mcast_oif  = tcp_v6_iif(skb);\n\tnewnp->mcast_hops = ipv6_hdr(skb)->hop_limit;\n\tnewnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(skb));\n\tif (inet6_test_bit(REPFLOW, sk))\n\t\tnewnp->flow_label = ip6_flowlabel(ipv6_hdr(skb));\n\n\t/* Set ToS of the new socket based upon the value of incoming SYN.\n\t * ECT bits are set later in tcp_init_transfer().\n\t */\n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reflect_tos))\n\t\tnewnp->tclass = tcp_rsk(req)->syn_tos & ~INET_ECN_MASK;\n\n\t/* Clone native IPv6 options from listening socket (if any)\n\n\t   Yes, keeping reference count would be much more clever,\n\t   but we make one more one thing there: reattach optmem\n\t   to newsk.\n\t */\n\topt = ireq->ipv6_opt;\n\tif (!opt)\n\t\topt = rcu_dereference(np->opt);\n\tif (opt) {\n\t\topt = ipv6_dup_options(newsk, opt);\n\t\tRCU_INIT_POINTER(newnp->opt, opt);\n\t}\n\tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n\tif (opt)\n\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n\t\t\t\t\t\t    opt->opt_flen;\n\n\ttcp_ca_openreq_child(newsk, dst);\n\n\ttcp_sync_mss(newsk, dst_mtu(dst));\n\tnewtp->advmss = tcp_mss_clamp(tcp_sk(sk), dst_metric_advmss(dst));\n\n\ttcp_initialize_rcv_mss(newsk);\n\n#ifdef CONFIG_TCP_MD5SIG\n\tl3index = l3mdev_master_ifindex_by_index(sock_net(sk), ireq->ir_iif);\n\n\tif (!tcp_rsk_used_ao(req)) {\n\t\t/* Copy over the MD5 key from the original socket */\n\t\tkey = tcp_v6_md5_do_lookup(sk, &newsk->sk_v6_daddr, l3index);\n\t\tif (key) {\n\t\t\tconst union tcp_md5_addr *addr;\n\n\t\t\taddr = (union tcp_md5_addr *)&newsk->sk_v6_daddr;\n\t\t\tif (tcp_md5_key_copy(newsk, addr, AF_INET6, 128, l3index, key))\n\t\t\t\tgoto put_and_exit;\n\t\t}\n\t}\n#endif\n#ifdef CONFIG_TCP_AO\n\t/* Copy over tcp_ao_info if any */\n\tif (tcp_ao_copy_all_matching(sk, newsk, req, skb, AF_INET6))\n\t\tgoto put_and_exit; /* OOM */\n#endif\n\n\tif (__inet_inherit_port(sk, newsk) < 0)\n\t\tgoto put_and_exit;\n\t*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash),\n\t\t\t\t       &found_dup_sk);\n\tif (*own_req) {\n\t\ttcp_move_syn(newtp, req);\n\n\t\t/* Clone pktoptions received with SYN, if we own the req */\n\t\tif (ireq->pktopts) {\n\t\t\tnewnp->pktoptions = skb_clone_and_charge_r(ireq->pktopts, newsk);\n\t\t\tconsume_skb(ireq->pktopts);\n\t\t\tireq->pktopts = NULL;\n\t\t\tif (newnp->pktoptions)\n\t\t\t\ttcp_v6_restore_cb(newnp->pktoptions);\n\t\t}\n\t} else {\n\t\tif (!req_unhash && found_dup_sk) {\n\t\t\t/* This code path should only be executed in the\n\t\t\t * syncookie case only\n\t\t\t */\n\t\t\tbh_unlock_sock(newsk);\n\t\t\tsock_put(newsk);\n\t\t\tnewsk = NULL;\n\t\t}\n\t}\n\n\treturn newsk;\n\nexit_overflow:\n\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\nexit_nonewsk:\n\tdst_release(dst);\nexit:\n\ttcp_listendrop(sk);\n\treturn NULL;\nput_and_exit:\n\tinet_csk_prepare_forced_close(newsk);\n\ttcp_done(newsk);\n\tgoto exit;\n}\n\nINDIRECT_CALLABLE_DECLARE(struct dst_entry *ipv4_dst_check(struct dst_entry *,\n\t\t\t\t\t\t\t   u32));\n/* The socket must have it's spinlock held when we get\n * here, unless it is a TCP_LISTEN socket.\n *\n * We have a potential double-lock case here, so even when\n * doing backlog processing we use the BH locking scheme.\n * This is because we cannot sleep with the original spinlock\n * held.\n */\nINDIRECT_CALLABLE_SCOPE\nint tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ipv6_pinfo *np = tcp_inet6_sk(sk);\n\tstruct sk_buff *opt_skb = NULL;\n\tenum skb_drop_reason reason;\n\tstruct tcp_sock *tp;\n\n\t/* Imagine: socket is IPv6. IPv4 packet arrives,\n\t   goes to IPv4 receive handler and backlogged.\n\t   From backlog it always goes here. Kerboom...\n\t   Fortunately, tcp_rcv_established and rcv_established\n\t   handle them correctly, but it is not case with\n\t   tcp_v6_hnd_req and tcp_v6_send_reset().   --ANK\n\t */\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\treturn tcp_v4_do_rcv(sk, skb);\n\n\treason = psp_sk_rx_policy_check(sk, skb);\n\tif (reason)\n\t\tgoto err_discard;\n\n\t/*\n\t *\tsocket locking is here for SMP purposes as backlog rcv\n\t *\tis currently called with bh processing disabled.\n\t */\n\n\t/* Do Stevens' IPV6_PKTOPTIONS.\n\n\t   Yes, guys, it is the only place in our code, where we\n\t   may make it not affecting IPv4.\n\t   The rest of code is protocol independent,\n\t   and I do not like idea to uglify IPv4.\n\n\t   Actually, all the idea behind IPV6_PKTOPTIONS\n\t   looks not very well thought. For now we latch\n\t   options, received in the last packet, enqueued\n\t   by tcp. Feel free to propose better solution.\n\t\t\t\t\t       --ANK (980728)\n\t */\n\tif (np->rxopt.all && sk->sk_state != TCP_LISTEN)\n\t\topt_skb = skb_clone_and_charge_r(skb, sk);\n\n\tif (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */\n\t\tstruct dst_entry *dst;\n\n\t\tdst = rcu_dereference_protected(sk->sk_rx_dst,\n\t\t\t\t\t\tlockdep_sock_is_held(sk));\n\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tif (dst) {\n\t\t\tif (sk->sk_rx_dst_ifindex != skb->skb_iif ||\n\t\t\t    INDIRECT_CALL_1(dst->ops->check, ip6_dst_check,\n\t\t\t\t\t    dst, sk->sk_rx_dst_cookie) == NULL) {\n\t\t\t\tRCU_INIT_POINTER(sk->sk_rx_dst, NULL);\n\t\t\t\tdst_release(dst);\n\t\t\t}\n\t\t}\n\n\t\ttcp_rcv_established(sk, skb);\n\t\tif (opt_skb)\n\t\t\tgoto ipv6_pktoptions;\n\t\treturn 0;\n\t}\n\n\tif (tcp_checksum_complete(skb))\n\t\tgoto csum_err;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tstruct sock *nsk = tcp_v6_cookie_check(sk, skb);\n\n\t\tif (nsk != sk) {\n\t\t\tif (nsk) {\n\t\t\t\treason = tcp_child_process(sk, nsk, skb);\n\t\t\t\tif (reason)\n\t\t\t\t\tgoto reset;\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t} else\n\t\tsock_rps_save_rxhash(sk, skb);\n\n\treason = tcp_rcv_state_process(sk, skb);\n\tif (reason)\n\t\tgoto reset;\n\tif (opt_skb)\n\t\tgoto ipv6_pktoptions;\n\treturn 0;\n\nreset:\n\ttcp_v6_send_reset(sk, skb, sk_rst_convert_drop_reason(reason));\ndiscard:\n\tif (opt_skb)\n\t\t__kfree_skb(opt_skb);\n\tsk_skb_reason_drop(sk, skb, reason);\n\treturn 0;\ncsum_err:\n\treason = SKB_DROP_REASON_TCP_CSUM;\n\ttrace_tcp_bad_csum(skb);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\nerr_discard:\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\tgoto discard;\n\n\nipv6_pktoptions:\n\t/* Do you ask, what is it?\n\n\t   1. skb was enqueued by tcp.\n\t   2. skb is added to tail of read queue, rather than out of order.\n\t   3. socket is not in passive state.\n\t   4. Finally, it really contains options, which user wants to receive.\n\t */\n\ttp = tcp_sk(sk);\n\tif (TCP_SKB_CB(opt_skb)->end_seq == tp->rcv_nxt &&\n\t    !((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\tif (np->rxopt.bits.rxinfo || np->rxopt.bits.rxoinfo)\n\t\t\tWRITE_ONCE(np->mcast_oif, tcp_v6_iif(opt_skb));\n\t\tif (np->rxopt.bits.rxhlim || np->rxopt.bits.rxohlim)\n\t\t\tWRITE_ONCE(np->mcast_hops,\n\t\t\t\t   ipv6_hdr(opt_skb)->hop_limit);\n\t\tif (np->rxopt.bits.rxflow || np->rxopt.bits.rxtclass)\n\t\t\tnp->rcv_flowinfo = ip6_flowinfo(ipv6_hdr(opt_skb));\n\t\tif (inet6_test_bit(REPFLOW, sk))\n\t\t\tnp->flow_label = ip6_flowlabel(ipv6_hdr(opt_skb));\n\t\tif (ipv6_opt_accepted(sk, opt_skb, &TCP_SKB_CB(opt_skb)->header.h6)) {\n\t\t\ttcp_v6_restore_cb(opt_skb);\n\t\t\topt_skb = xchg(&np->pktoptions, opt_skb);\n\t\t} else {\n\t\t\t__kfree_skb(opt_skb);\n\t\t\topt_skb = xchg(&np->pktoptions, NULL);\n\t\t}\n\t}\n\n\tconsume_skb(opt_skb);\n\treturn 0;\n}\n\nstatic void tcp_v6_fill_cb(struct sk_buff *skb, const struct ipv6hdr *hdr,\n\t\t\t   const struct tcphdr *th)\n{\n\t/* This is tricky: we move IP6CB at its correct location into\n\t * TCP_SKB_CB(). It must be done after xfrm6_policy_check(), because\n\t * _decode_session6() uses IP6CB().\n\t * barrier() makes sure compiler won't play aliasing games.\n\t */\n\tmemmove(&TCP_SKB_CB(skb)->header.h6, IP6CB(skb),\n\t\tsizeof(struct inet6_skb_parm));\n\tbarrier();\n\n\tTCP_SKB_CB(skb)->seq = ntohl(th->seq);\n\tTCP_SKB_CB(skb)->end_seq = (TCP_SKB_CB(skb)->seq + th->syn + th->fin +\n\t\t\t\t    skb->len - th->doff*4);\n\tTCP_SKB_CB(skb)->ack_seq = ntohl(th->ack_seq);\n\tTCP_SKB_CB(skb)->tcp_flags = tcp_flags_ntohs(th);\n\tTCP_SKB_CB(skb)->ip_dsfield = ipv6_get_dsfield(hdr);\n\tTCP_SKB_CB(skb)->sacked = 0;\n\tTCP_SKB_CB(skb)->has_rxtstamp =\n\t\t\tskb->tstamp || skb_hwtstamps(skb)->hwtstamp;\n}\n\nINDIRECT_CALLABLE_SCOPE int tcp_v6_rcv(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net_rcu(skb->dev);\n\tenum skb_drop_reason drop_reason;\n\tenum tcp_tw_status tw_status;\n\tint sdif = inet6_sdif(skb);\n\tint dif = inet6_iif(skb);\n\tconst struct tcphdr *th;\n\tconst struct ipv6hdr *hdr;\n\tstruct sock *sk = NULL;\n\tbool refcounted;\n\tint ret;\n\tu32 isn;\n\n\tdrop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto discard_it;\n\n\t/*\n\t *\tCount it even if it's bad.\n\t */\n\t__TCP_INC_STATS(net, TCP_MIB_INSEGS);\n\n\tif (!pskb_may_pull(skb, sizeof(struct tcphdr)))\n\t\tgoto discard_it;\n\n\tth = (const struct tcphdr *)skb->data;\n\n\tif (unlikely(th->doff < sizeof(struct tcphdr) / 4)) {\n\t\tdrop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;\n\t\tgoto bad_packet;\n\t}\n\tif (!pskb_may_pull(skb, th->doff*4))\n\t\tgoto discard_it;\n\n\tif (skb_checksum_init(skb, IPPROTO_TCP, ip6_compute_pseudo))\n\t\tgoto csum_error;\n\n\tth = (const struct tcphdr *)skb->data;\n\thdr = ipv6_hdr(skb);\n\nlookup:\n\tsk = __inet6_lookup_skb(skb, __tcp_hdrlen(th),\n\t\t\t\tth->source, th->dest, inet6_iif(skb), sdif,\n\t\t\t\t&refcounted);\n\tif (!sk)\n\t\tgoto no_tcp_socket;\n\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tgoto do_time_wait;\n\n\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\tstruct request_sock *req = inet_reqsk(sk);\n\t\tbool req_stolen = false;\n\t\tstruct sock *nsk;\n\n\t\tsk = req->rsk_listener;\n\t\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\telse\n\t\t\tdrop_reason = tcp_inbound_hash(sk, req, skb,\n\t\t\t\t\t\t       &hdr->saddr, &hdr->daddr,\n\t\t\t\t\t\t       AF_INET6, dif, sdif);\n\t\tif (drop_reason) {\n\t\t\tsk_drops_skbadd(sk, skb);\n\t\t\treqsk_put(req);\n\t\t\tgoto discard_it;\n\t\t}\n\t\tif (tcp_checksum_complete(skb)) {\n\t\t\treqsk_put(req);\n\t\t\tgoto csum_error;\n\t\t}\n\t\tif (unlikely(sk->sk_state != TCP_LISTEN)) {\n\t\t\tnsk = reuseport_migrate_sock(sk, req_to_sk(req), skb);\n\t\t\tif (!nsk) {\n\t\t\t\tinet_csk_reqsk_queue_drop_and_put(sk, req);\n\t\t\t\tgoto lookup;\n\t\t\t}\n\t\t\tsk = nsk;\n\t\t\t/* reuseport_migrate_sock() has already held one sk_refcnt\n\t\t\t * before returning.\n\t\t\t */\n\t\t} else {\n\t\t\tsock_hold(sk);\n\t\t}\n\t\trefcounted = true;\n\t\tnsk = NULL;\n\t\tif (!tcp_filter(sk, skb, &drop_reason)) {\n\t\t\tth = (const struct tcphdr *)skb->data;\n\t\t\thdr = ipv6_hdr(skb);\n\t\t\ttcp_v6_fill_cb(skb, hdr, th);\n\t\t\tnsk = tcp_check_req(sk, skb, req, false, &req_stolen,\n\t\t\t\t\t    &drop_reason);\n\t\t}\n\t\tif (!nsk) {\n\t\t\treqsk_put(req);\n\t\t\tif (req_stolen) {\n\t\t\t\t/* Another cpu got exclusive access to req\n\t\t\t\t * and created a full blown socket.\n\t\t\t\t * Try to feed this packet to this socket\n\t\t\t\t * instead of discarding it.\n\t\t\t\t */\n\t\t\t\ttcp_v6_restore_cb(skb);\n\t\t\t\tsock_put(sk);\n\t\t\t\tgoto lookup;\n\t\t\t}\n\t\t\tgoto discard_and_relse;\n\t\t}\n\t\tnf_reset_ct(skb);\n\t\tif (nsk == sk) {\n\t\t\treqsk_put(req);\n\t\t\ttcp_v6_restore_cb(skb);\n\t\t} else {\n\t\t\tdrop_reason = tcp_child_process(sk, nsk, skb);\n\t\t\tif (drop_reason) {\n\t\t\t\tenum sk_rst_reason rst_reason;\n\n\t\t\t\trst_reason = sk_rst_convert_drop_reason(drop_reason);\n\t\t\t\ttcp_v6_send_reset(nsk, skb, rst_reason);\n\t\t\t\tgoto discard_and_relse;\n\t\t\t}\n\t\t\tsock_put(sk);\n\t\t\treturn 0;\n\t\t}\n\t}\n\nprocess:\n\tif (static_branch_unlikely(&ip6_min_hopcount)) {\n\t\t/* min_hopcount can be changed concurrently from do_ipv6_setsockopt() */\n\t\tif (unlikely(hdr->hop_limit < READ_ONCE(tcp_inet6_sk(sk)->min_hopcount))) {\n\t\t\t__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);\n\t\t\tdrop_reason = SKB_DROP_REASON_TCP_MINTTL;\n\t\t\tgoto discard_and_relse;\n\t\t}\n\t}\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\tgoto discard_and_relse;\n\t}\n\n\tdrop_reason = tcp_inbound_hash(sk, NULL, skb, &hdr->saddr, &hdr->daddr,\n\t\t\t\t       AF_INET6, dif, sdif);\n\tif (drop_reason)\n\t\tgoto discard_and_relse;\n\n\tnf_reset_ct(skb);\n\n\tif (tcp_filter(sk, skb, &drop_reason))\n\t\tgoto discard_and_relse;\n\n\tth = (const struct tcphdr *)skb->data;\n\thdr = ipv6_hdr(skb);\n\ttcp_v6_fill_cb(skb, hdr, th);\n\n\tskb->dev = NULL;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\tret = tcp_v6_do_rcv(sk, skb);\n\t\tgoto put_and_return;\n\t}\n\n\tsk_incoming_cpu_update(sk);\n\n\tbh_lock_sock_nested(sk);\n\ttcp_segs_in(tcp_sk(sk), skb);\n\tret = 0;\n\tif (!sock_owned_by_user(sk)) {\n\t\tret = tcp_v6_do_rcv(sk, skb);\n\t} else {\n\t\tif (tcp_add_backlog(sk, skb, &drop_reason))\n\t\t\tgoto discard_and_relse;\n\t}\n\tbh_unlock_sock(sk);\nput_and_return:\n\tif (refcounted)\n\t\tsock_put(sk);\n\treturn ret ? -1 : 0;\n\nno_tcp_socket:\n\tdrop_reason = SKB_DROP_REASON_NO_SOCKET;\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard_it;\n\n\ttcp_v6_fill_cb(skb, hdr, th);\n\n\tif (tcp_checksum_complete(skb)) {\ncsum_error:\n\t\tdrop_reason = SKB_DROP_REASON_TCP_CSUM;\n\t\ttrace_tcp_bad_csum(skb);\n\t\t__TCP_INC_STATS(net, TCP_MIB_CSUMERRORS);\nbad_packet:\n\t\t__TCP_INC_STATS(net, TCP_MIB_INERRS);\n\t} else {\n\t\ttcp_v6_send_reset(NULL, skb, sk_rst_convert_drop_reason(drop_reason));\n\t}\n\ndiscard_it:\n\tSKB_DR_OR(drop_reason, NOT_SPECIFIED);\n\tsk_skb_reason_drop(sk, skb, drop_reason);\n\treturn 0;\n\ndiscard_and_relse:\n\tsk_drops_skbadd(sk, skb);\n\tif (refcounted)\n\t\tsock_put(sk);\n\tgoto discard_it;\n\ndo_time_wait:\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\t}\n\n\ttcp_v6_fill_cb(skb, hdr, th);\n\n\tif (tcp_checksum_complete(skb)) {\n\t\tinet_twsk_put(inet_twsk(sk));\n\t\tgoto csum_error;\n\t}\n\n\ttw_status = tcp_timewait_state_process(inet_twsk(sk), skb, th, &isn,\n\t\t\t\t\t       &drop_reason);\n\tswitch (tw_status) {\n\tcase TCP_TW_SYN:\n\t{\n\t\tstruct sock *sk2;\n\n\t\tsk2 = inet6_lookup_listener(net, skb, __tcp_hdrlen(th),\n\t\t\t\t\t    &ipv6_hdr(skb)->saddr, th->source,\n\t\t\t\t\t    &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t    ntohs(th->dest),\n\t\t\t\t\t    tcp_v6_iif_l3_slave(skb),\n\t\t\t\t\t    sdif);\n\t\tif (sk2) {\n\t\t\tstruct inet_timewait_sock *tw = inet_twsk(sk);\n\t\t\tinet_twsk_deschedule_put(tw);\n\t\t\tsk = sk2;\n\t\t\ttcp_v6_restore_cb(skb);\n\t\t\trefcounted = false;\n\t\t\t__this_cpu_write(tcp_tw_isn, isn);\n\t\t\tgoto process;\n\t\t}\n\n\t\tdrop_reason = psp_twsk_rx_policy_check(inet_twsk(sk), skb);\n\t\tif (drop_reason)\n\t\t\tbreak;\n\t}\n\t\t/* to ACK */\n\t\tfallthrough;\n\tcase TCP_TW_ACK:\n\tcase TCP_TW_ACK_OOW:\n\t\ttcp_v6_timewait_ack(sk, skb, tw_status);\n\t\tbreak;\n\tcase TCP_TW_RST:\n\t\ttcp_v6_send_reset(sk, skb, SK_RST_REASON_TCP_TIMEWAIT_SOCKET);\n\t\tinet_twsk_deschedule_put(inet_twsk(sk));\n\t\tgoto discard_it;\n\tcase TCP_TW_SUCCESS:\n\t\t;\n\t}\n\tgoto discard_it;\n}\n\nvoid tcp_v6_early_demux(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net_rcu(skb->dev);\n\tconst struct ipv6hdr *hdr;\n\tconst struct tcphdr *th;\n\tstruct sock *sk;\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\treturn;\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) + sizeof(struct tcphdr)))\n\t\treturn;\n\n\thdr = ipv6_hdr(skb);\n\tth = tcp_hdr(skb);\n\n\tif (th->doff < sizeof(struct tcphdr) / 4)\n\t\treturn;\n\n\t/* Note : We use inet6_iif() here, not tcp_v6_iif() */\n\tsk = __inet6_lookup_established(net, &hdr->saddr, th->source,\n\t\t\t\t\t&hdr->daddr, ntohs(th->dest),\n\t\t\t\t\tinet6_iif(skb), inet6_sdif(skb));\n\tif (sk) {\n\t\tskb->sk = sk;\n\t\tskb->destructor = sock_edemux;\n\t\tif (sk_fullsock(sk)) {\n\t\t\tstruct dst_entry *dst = rcu_dereference(sk->sk_rx_dst);\n\n\t\t\tif (dst)\n\t\t\t\tdst = dst_check(dst, sk->sk_rx_dst_cookie);\n\t\t\tif (dst &&\n\t\t\t    sk->sk_rx_dst_ifindex == skb->skb_iif)\n\t\t\t\tskb_dst_set_noref(skb, dst);\n\t\t}\n\t}\n}\n\nstatic struct timewait_sock_ops tcp6_timewait_sock_ops = {\n\t.twsk_obj_size\t= sizeof(struct tcp6_timewait_sock),\n};\n\nINDIRECT_CALLABLE_SCOPE void tcp_v6_send_check(struct sock *sk, struct sk_buff *skb)\n{\n\t__tcp_v6_send_check(skb, &sk->sk_v6_rcv_saddr, &sk->sk_v6_daddr);\n}\n\nconst struct inet_connection_sock_af_ops ipv6_specific = {\n\t.queue_xmit\t   = inet6_csk_xmit,\n\t.send_check\t   = tcp_v6_send_check,\n\t.rebuild_header\t   = inet6_sk_rebuild_header,\n\t.sk_rx_dst_set\t   = inet6_sk_rx_dst_set,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.net_header_len\t   = sizeof(struct ipv6hdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.mtu_reduced\t   = tcp_v6_mtu_reduced,\n};\n\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_specific = {\n#ifdef CONFIG_TCP_MD5SIG\n\t.md5_lookup\t=\ttcp_v6_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v6_md5_hash_skb,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n#endif\n#ifdef CONFIG_TCP_AO\n\t.ao_lookup\t=\ttcp_v6_ao_lookup,\n\t.calc_ao_hash\t=\ttcp_v6_ao_hash_skb,\n\t.ao_parse\t=\ttcp_v6_parse_ao,\n\t.ao_calc_key_sk\t=\ttcp_v6_ao_calc_key_sk,\n#endif\n};\n#endif\n\n/*\n *\tTCP over IPv4 via INET6 API\n */\nstatic const struct inet_connection_sock_af_ops ipv6_mapped = {\n\t.queue_xmit\t   = ip_queue_xmit,\n\t.send_check\t   = tcp_v4_send_check,\n\t.rebuild_header\t   = inet_sk_rebuild_header,\n\t.sk_rx_dst_set\t   = inet_sk_rx_dst_set,\n\t.conn_request\t   = tcp_v6_conn_request,\n\t.syn_recv_sock\t   = tcp_v6_syn_recv_sock,\n\t.net_header_len\t   = sizeof(struct iphdr),\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.mtu_reduced\t   = tcp_v4_mtu_reduced,\n};\n\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\nstatic const struct tcp_sock_af_ops tcp_sock_ipv6_mapped_specific = {\n#ifdef CONFIG_TCP_MD5SIG\n\t.md5_lookup\t=\ttcp_v4_md5_lookup,\n\t.calc_md5_hash\t=\ttcp_v4_md5_hash_skb,\n\t.md5_parse\t=\ttcp_v6_parse_md5_keys,\n#endif\n#ifdef CONFIG_TCP_AO\n\t.ao_lookup\t=\ttcp_v6_ao_lookup,\n\t.calc_ao_hash\t=\ttcp_v4_ao_hash_skb,\n\t.ao_parse\t=\ttcp_v6_parse_ao,\n\t.ao_calc_key_sk\t=\ttcp_v4_ao_calc_key_sk,\n#endif\n};\n\nstatic void tcp6_destruct_sock(struct sock *sk)\n{\n\ttcp_md5_destruct_sock(sk);\n\ttcp_ao_destroy_sock(sk, false);\n\tinet6_sock_destruct(sk);\n}\n#endif\n\n/* NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nstatic int tcp_v6_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_init_sock(sk);\n\n\ticsk->icsk_af_ops = &ipv6_specific;\n\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\n\ttcp_sk(sk)->af_specific = &tcp_sock_ipv6_specific;\n\tsk->sk_destruct = tcp6_destruct_sock;\n#endif\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PROC_FS\n/* Proc filesystem TCPv6 sock list dumping. */\nstatic void get_openreq6(struct seq_file *seq,\n\t\t\t const struct request_sock *req, int i)\n{\n\tlong ttd = req->rsk_timer.expires - jiffies;\n\tconst struct in6_addr *src = &inet_rsk(req)->ir_v6_loc_addr;\n\tconst struct in6_addr *dest = &inet_rsk(req)->ir_v6_rmt_addr;\n\n\tif (ttd < 0)\n\t\tttd = 0;\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5u %8d %d %d %pK\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3],\n\t\t   inet_rsk(req)->ir_num,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3],\n\t\t   ntohs(inet_rsk(req)->ir_rmt_port),\n\t\t   TCP_SYN_RECV,\n\t\t   0, 0, /* could print option size, but that is af dependent. */\n\t\t   1,   /* timers active (only the expire timer) */\n\t\t   jiffies_to_clock_t(ttd),\n\t\t   req->num_timeout,\n\t\t   from_kuid_munged(seq_user_ns(seq),\n\t\t\t\t    sk_uid(req->rsk_listener)),\n\t\t   0,  /* non standard timer */\n\t\t   0, /* open_requests have no inode */\n\t\t   0, req);\n}\n\nstatic void get_tcp6_sock(struct seq_file *seq, struct sock *sp, int i)\n{\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\tint timer_active;\n\tunsigned long timer_expires;\n\tconst struct inet_sock *inet = inet_sk(sp);\n\tconst struct tcp_sock *tp = tcp_sk(sp);\n\tconst struct inet_connection_sock *icsk = inet_csk(sp);\n\tconst struct fastopen_queue *fastopenq = &icsk->icsk_accept_queue.fastopenq;\n\tu8 icsk_pending;\n\tint rx_queue;\n\tint state;\n\n\tdest  = &sp->sk_v6_daddr;\n\tsrc   = &sp->sk_v6_rcv_saddr;\n\tdestp = ntohs(inet->inet_dport);\n\tsrcp  = ntohs(inet->inet_sport);\n\n\ticsk_pending = smp_load_acquire(&icsk->icsk_pending);\n\tif (icsk_pending == ICSK_TIME_RETRANS ||\n\t    icsk_pending == ICSK_TIME_REO_TIMEOUT ||\n\t    icsk_pending == ICSK_TIME_LOSS_PROBE) {\n\t\ttimer_active\t= 1;\n\t\ttimer_expires\t= icsk_timeout(icsk);\n\t} else if (icsk_pending == ICSK_TIME_PROBE0) {\n\t\ttimer_active\t= 4;\n\t\ttimer_expires\t= icsk_timeout(icsk);\n\t} else if (timer_pending(&sp->sk_timer)) {\n\t\ttimer_active\t= 2;\n\t\ttimer_expires\t= sp->sk_timer.expires;\n\t} else {\n\t\ttimer_active\t= 0;\n\t\ttimer_expires = jiffies;\n\t}\n\n\tstate = inet_sk_state_load(sp);\n\tif (state == TCP_LISTEN)\n\t\trx_queue = READ_ONCE(sp->sk_ack_backlog);\n\telse\n\t\t/* Because we don't lock the socket,\n\t\t * we might find a transient negative value.\n\t\t */\n\t\trx_queue = max_t(int, READ_ONCE(tp->rcv_nxt) -\n\t\t\t\t      READ_ONCE(tp->copied_seq), 0);\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5u %8d %lu %d %pK %lu %lu %u %u %d\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   state,\n\t\t   READ_ONCE(tp->write_seq) - tp->snd_una,\n\t\t   rx_queue,\n\t\t   timer_active,\n\t\t   jiffies_delta_to_clock_t(timer_expires - jiffies),\n\t\t   READ_ONCE(icsk->icsk_retransmits),\n\t\t   from_kuid_munged(seq_user_ns(seq), sk_uid(sp)),\n\t\t   READ_ONCE(icsk->icsk_probes_out),\n\t\t   sock_i_ino(sp),\n\t\t   refcount_read(&sp->sk_refcnt), sp,\n\t\t   jiffies_to_clock_t(icsk->icsk_rto),\n\t\t   jiffies_to_clock_t(icsk->icsk_ack.ato),\n\t\t   (icsk->icsk_ack.quick << 1) | inet_csk_in_pingpong_mode(sp),\n\t\t   tcp_snd_cwnd(tp),\n\t\t   state == TCP_LISTEN ?\n\t\t\tfastopenq->max_qlen :\n\t\t\t(tcp_in_initial_slowstart(tp) ? -1 : tp->snd_ssthresh)\n\t\t   );\n}\n\nstatic void get_timewait6_sock(struct seq_file *seq,\n\t\t\t       struct inet_timewait_sock *tw, int i)\n{\n\tlong delta = tw->tw_timer.expires - jiffies;\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\n\tdest = &tw->tw_v6_daddr;\n\tsrc  = &tw->tw_v6_rcv_saddr;\n\tdestp = ntohs(tw->tw_dport);\n\tsrcp  = ntohs(tw->tw_sport);\n\n\tseq_printf(seq,\n\t\t   \"%4d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %d %d %pK\\n\",\n\t\t   i,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   READ_ONCE(tw->tw_substate), 0, 0,\n\t\t   3, jiffies_delta_to_clock_t(delta), 0, 0, 0, 0,\n\t\t   refcount_read(&tw->tw_refcnt), tw);\n}\n\nstatic int tcp6_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct tcp_iter_state *st;\n\tstruct sock *sk = v;\n\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq,\n\t\t\t \"  sl  \"\n\t\t\t \"local_address                         \"\n\t\t\t \"remote_address                        \"\n\t\t\t \"st tx_queue rx_queue tr tm->when retrnsmt\"\n\t\t\t \"   uid  timeout inode\\n\");\n\t\tgoto out;\n\t}\n\tst = seq->private;\n\n\tif (sk->sk_state == TCP_TIME_WAIT)\n\t\tget_timewait6_sock(seq, v, st->num);\n\telse if (sk->sk_state == TCP_NEW_SYN_RECV)\n\t\tget_openreq6(seq, v, st->num);\n\telse\n\t\tget_tcp6_sock(seq, v, st->num);\nout:\n\treturn 0;\n}\n\nstatic const struct seq_operations tcp6_seq_ops = {\n\t.show\t\t= tcp6_seq_show,\n\t.start\t\t= tcp_seq_start,\n\t.next\t\t= tcp_seq_next,\n\t.stop\t\t= tcp_seq_stop,\n};\n\nstatic struct tcp_seq_afinfo tcp6_seq_afinfo = {\n\t.family\t\t= AF_INET6,\n};\n\nint __net_init tcp6_proc_init(struct net *net)\n{\n\tif (!proc_create_net_data(\"tcp6\", 0444, net->proc_net, &tcp6_seq_ops,\n\t\t\tsizeof(struct tcp_iter_state), &tcp6_seq_afinfo))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid tcp6_proc_exit(struct net *net)\n{\n\tremove_proc_entry(\"tcp6\", net->proc_net);\n}\n#endif\n\nstruct proto tcpv6_prot = {\n\t.name\t\t\t= \"TCPv6\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= tcp_close,\n\t.pre_connect\t\t= tcp_v6_pre_connect,\n\t.connect\t\t= tcp_v6_connect,\n\t.disconnect\t\t= tcp_disconnect,\n\t.accept\t\t\t= inet_csk_accept,\n\t.ioctl\t\t\t= tcp_ioctl,\n\t.init\t\t\t= tcp_v6_init_sock,\n\t.destroy\t\t= tcp_v4_destroy_sock,\n\t.shutdown\t\t= tcp_shutdown,\n\t.setsockopt\t\t= tcp_setsockopt,\n\t.getsockopt\t\t= tcp_getsockopt,\n\t.bpf_bypass_getsockopt\t= tcp_bpf_bypass_getsockopt,\n\t.keepalive\t\t= tcp_set_keepalive,\n\t.recvmsg\t\t= tcp_recvmsg,\n\t.sendmsg\t\t= tcp_sendmsg,\n\t.splice_eof\t\t= tcp_splice_eof,\n\t.backlog_rcv\t\t= tcp_v6_do_rcv,\n\t.release_cb\t\t= tcp_release_cb,\n\t.hash\t\t\t= inet_hash,\n\t.unhash\t\t\t= inet_unhash,\n\t.get_port\t\t= inet_csk_get_port,\n\t.put_port\t\t= inet_put_port,\n#ifdef CONFIG_BPF_SYSCALL\n\t.psock_update_sk_prot\t= tcp_bpf_update_proto,\n#endif\n\t.enter_memory_pressure\t= tcp_enter_memory_pressure,\n\t.leave_memory_pressure\t= tcp_leave_memory_pressure,\n\t.stream_memory_free\t= tcp_stream_memory_free,\n\t.sockets_allocated\t= &tcp_sockets_allocated,\n\n\t.memory_allocated\t= &net_aligned_data.tcp_memory_allocated,\n\t.per_cpu_fw_alloc\t= &tcp_memory_per_cpu_fw_alloc,\n\n\t.memory_pressure\t= &tcp_memory_pressure,\n\t.sysctl_mem\t\t= sysctl_tcp_mem,\n\t.sysctl_wmem_offset\t= offsetof(struct net, ipv4.sysctl_tcp_wmem),\n\t.sysctl_rmem_offset\t= offsetof(struct net, ipv4.sysctl_tcp_rmem),\n\t.max_header\t\t= MAX_TCP_HEADER,\n\t.obj_size\t\t= sizeof(struct tcp6_sock),\n\t.ipv6_pinfo_offset = offsetof(struct tcp6_sock, inet6),\n\t.slab_flags\t\t= SLAB_TYPESAFE_BY_RCU,\n\t.twsk_prot\t\t= &tcp6_timewait_sock_ops,\n\t.rsk_prot\t\t= &tcp6_request_sock_ops,\n\t.h.hashinfo\t\t= NULL,\n\t.no_autobind\t\t= true,\n\t.diag_destroy\t\t= tcp_abort,\n};\nEXPORT_SYMBOL_GPL(tcpv6_prot);\n\n\nstatic struct inet_protosw tcpv6_protosw = {\n\t.type\t\t=\tSOCK_STREAM,\n\t.protocol\t=\tIPPROTO_TCP,\n\t.prot\t\t=\t&tcpv6_prot,\n\t.ops\t\t=\t&inet6_stream_ops,\n\t.flags\t\t=\tINET_PROTOSW_PERMANENT |\n\t\t\t\tINET_PROTOSW_ICSK,\n};\n\nstatic int __net_init tcpv6_net_init(struct net *net)\n{\n\tint res;\n\n\tres = inet_ctl_sock_create(&net->ipv6.tcp_sk, PF_INET6,\n\t\t\t\t   SOCK_RAW, IPPROTO_TCP, net);\n\tif (!res)\n\t\tnet->ipv6.tcp_sk->sk_clockid = CLOCK_MONOTONIC;\n\n\treturn res;\n}\n\nstatic void __net_exit tcpv6_net_exit(struct net *net)\n{\n\tinet_ctl_sock_destroy(net->ipv6.tcp_sk);\n}\n\nstatic struct pernet_operations tcpv6_net_ops = {\n\t.init\t    = tcpv6_net_init,\n\t.exit\t    = tcpv6_net_exit,\n};\n\nint __init tcpv6_init(void)\n{\n\tint ret;\n\n\tnet_hotdata.tcpv6_protocol = (struct inet6_protocol) {\n\t\t.handler     = tcp_v6_rcv,\n\t\t.err_handler = tcp_v6_err,\n\t\t.flags\t     = INET6_PROTO_NOPOLICY | INET6_PROTO_FINAL,\n\t};\n\tret = inet6_add_protocol(&net_hotdata.tcpv6_protocol, IPPROTO_TCP);\n\tif (ret)\n\t\tgoto out;\n\n\t/* register inet6 protocol */\n\tret = inet6_register_protosw(&tcpv6_protosw);\n\tif (ret)\n\t\tgoto out_tcpv6_protocol;\n\n\tret = register_pernet_subsys(&tcpv6_net_ops);\n\tif (ret)\n\t\tgoto out_tcpv6_protosw;\n\n\tret = mptcpv6_init();\n\tif (ret)\n\t\tgoto out_tcpv6_pernet_subsys;\n\nout:\n\treturn ret;\n\nout_tcpv6_pernet_subsys:\n\tunregister_pernet_subsys(&tcpv6_net_ops);\nout_tcpv6_protosw:\n\tinet6_unregister_protosw(&tcpv6_protosw);\nout_tcpv6_protocol:\n\tinet6_del_protocol(&net_hotdata.tcpv6_protocol, IPPROTO_TCP);\n\tgoto out;\n}\n\nvoid tcpv6_exit(void)\n{\n\tunregister_pernet_subsys(&tcpv6_net_ops);\n\tinet6_unregister_protosw(&tcpv6_protosw);\n\tinet6_del_protocol(&net_hotdata.tcpv6_protocol, IPPROTO_TCP);\n}\n", "patch": "@@ -120,6 +120,7 @@ static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n \tstruct tcp_sock *tp = tcp_sk(sk);\n \tstruct in6_addr *saddr = NULL, *final_p, final;\n+\tstruct ipv6_txoptions *opt;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n \tint addr_type;\n@@ -235,7 +236,8 @@ static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n \tfl6.fl6_dport = usin->sin6_port;\n \tfl6.fl6_sport = inet->inet_sport;\n \n-\tfinal_p = fl6_update_dst(&fl6, np->opt, &final);\n+\topt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));\n+\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n \n \tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n \n@@ -263,9 +265,9 @@ static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,\n \t\ttcp_fetch_timewait_stamp(sk, dst);\n \n \ticsk->icsk_ext_hdr_len = 0;\n-\tif (np->opt)\n-\t\ticsk->icsk_ext_hdr_len = (np->opt->opt_flen +\n-\t\t\t\t\t  np->opt->opt_nflen);\n+\tif (opt)\n+\t\ticsk->icsk_ext_hdr_len = opt->opt_flen +\n+\t\t\t\t\t opt->opt_nflen;\n \n \ttp->rx_opt.mss_clamp = IPV6_MIN_MTU - sizeof(struct tcphdr) - sizeof(struct ipv6hdr);\n \n@@ -461,7 +463,8 @@ static int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,\n \t\tif (np->repflow && ireq->pktopts)\n \t\t\tfl6->flowlabel = ip6_flowlabel(ipv6_hdr(ireq->pktopts));\n \n-\t\terr = ip6_xmit(sk, skb, fl6, np->opt, np->tclass);\n+\t\terr = ip6_xmit(sk, skb, fl6, rcu_dereference(np->opt),\n+\t\t\t       np->tclass);\n \t\terr = net_xmit_eval(err);\n \t}\n \n@@ -972,6 +975,7 @@ static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *\n \tstruct inet_request_sock *ireq;\n \tstruct ipv6_pinfo *newnp;\n \tconst struct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt;\n \tstruct tcp6_sock *newtcp6sk;\n \tstruct inet_sock *newinet;\n \tstruct tcp_sock *newtp;\n@@ -1098,13 +1102,15 @@ static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *\n \t   but we make one more one thing there: reattach optmem\n \t   to newsk.\n \t */\n-\tif (np->opt)\n-\t\tnewnp->opt = ipv6_dup_options(newsk, np->opt);\n-\n+\topt = rcu_dereference(np->opt);\n+\tif (opt) {\n+\t\topt = ipv6_dup_options(newsk, opt);\n+\t\tRCU_INIT_POINTER(newnp->opt, opt);\n+\t}\n \tinet_csk(newsk)->icsk_ext_hdr_len = 0;\n-\tif (newnp->opt)\n-\t\tinet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +\n-\t\t\t\t\t\t     newnp->opt->opt_flen);\n+\tif (opt)\n+\t\tinet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +\n+\t\t\t\t\t\t    opt->opt_flen;\n \n \ttcp_ca_openreq_child(newsk, dst);\n ", "file_path": "files/2016_8\\92", "file_language": "c", "file_name": "net/ipv6/tcp_ipv6.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/ipv6/udp.c", "code": "/*\n *\tUDP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/ipv4/udp.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *      Kazunori MIYAZAWA @USAGI:       change process style to use ip6_append_data\n *      YOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/udp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/raw.h>\n#include <net/tcp_states.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n#include <net/inet6_hashtables.h>\n#include <net/busy_poll.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <trace/events/skb.h>\n#include \"udp_impl.h\"\n\nstatic u32 udp6_ehashfn(const struct net *net,\n\t\t\tconst struct in6_addr *laddr,\n\t\t\tconst u16 lport,\n\t\t\tconst struct in6_addr *faddr,\n\t\t\tconst __be16 fport)\n{\n\tstatic u32 udp6_ehash_secret __read_mostly;\n\tstatic u32 udp_ipv6_hash_secret __read_mostly;\n\n\tu32 lhash, fhash;\n\n\tnet_get_random_once(&udp6_ehash_secret,\n\t\t\t    sizeof(udp6_ehash_secret));\n\tnet_get_random_once(&udp_ipv6_hash_secret,\n\t\t\t    sizeof(udp_ipv6_hash_secret));\n\n\tlhash = (__force u32)laddr->s6_addr32[3];\n\tfhash = __ipv6_addr_jhash(faddr, udp_ipv6_hash_secret);\n\n\treturn __inet6_ehashfn(lhash, lport, fhash, fport,\n\t\t\t       udp_ipv6_hash_secret + net_hash_mix(net));\n}\n\nint ipv6_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2)\n{\n\tconst struct in6_addr *sk2_rcv_saddr6 = inet6_rcv_saddr(sk2);\n\tint sk2_ipv6only = inet_v6_ipv6only(sk2);\n\tint addr_type = ipv6_addr_type(&sk->sk_v6_rcv_saddr);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t/* if both are mapped, treat as IPv4 */\n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED)\n\t\treturn (!sk2_ipv6only &&\n\t\t\t(!sk->sk_rcv_saddr || !sk2->sk_rcv_saddr ||\n\t\t\t  sk->sk_rcv_saddr == sk2->sk_rcv_saddr));\n\n\tif (addr_type2 == IPV6_ADDR_ANY &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (addr_type == IPV6_ADDR_ANY &&\n\t    !(ipv6_only_sock(sk) && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(&sk->sk_v6_rcv_saddr, sk2_rcv_saddr6))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic u32 udp6_portaddr_hash(const struct net *net,\n\t\t\t      const struct in6_addr *addr6,\n\t\t\t      unsigned int port)\n{\n\tunsigned int hash, mix = net_hash_mix(net);\n\n\tif (ipv6_addr_any(addr6))\n\t\thash = jhash_1word(0, mix);\n\telse if (ipv6_addr_v4mapped(addr6))\n\t\thash = jhash_1word((__force u32)addr6->s6_addr32[3], mix);\n\telse\n\t\thash = jhash2((__force u32 *)addr6->s6_addr32, 4, mix);\n\n\treturn hash ^ port;\n}\n\nint udp_v6_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp6_portaddr_hash(sock_net(sk), &in6addr_any, snum);\n\tunsigned int hash2_partial =\n\t\tudp6_portaddr_hash(sock_net(sk), &sk->sk_v6_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv6_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic void udp_v6_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp6_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net,\n\t\t\t\tunsigned short hnum,\n\t\t\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\t\t\tconst struct in6_addr *daddr, __be16 dport,\n\t\t\t\tint dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6)\n\t\treturn -1;\n\n\tscore = 0;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_rcv_saddr, daddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_daddr, saddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\t const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t const struct in6_addr *daddr,\n\t\t\t\t unsigned short hnum, int dif)\n{\n\tint score;\n\tstruct inet_sock *inet;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6)\n\t\treturn -1;\n\n\tif (!ipv6_addr_equal(&sk->sk_v6_rcv_saddr, daddr))\n\t\treturn -1;\n\n\tscore = 0;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_daddr, saddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_bound_dev_if) {\n\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (sk->sk_incoming_cpu == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp6_lib_lookup2(struct net *net,\n\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\tconst struct in6_addr *daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp6_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t    saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\nstruct sock *__udp6_lib_lookup(struct net *net,\n\t\t\t\t      const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t      const struct in6_addr *daddr, __be16 dport,\n\t\t\t\t      int dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness, matches = 0, reuseport = 0;\n\tu32 hash = 0;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp6_portaddr_hash(net, &in6addr_any, hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  &in6addr_any, hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, hnum, saddr, sport, daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\treuseport = sk->sk_reuseport;\n\t\t\tif (reuseport) {\n\t\t\t\thash = udp6_ehashfn(net, daddr, hnum,\n\t\t\t\t\t\t    saddr, sport);\n\t\t\t\tmatches = 1;\n\t\t\t}\n\t\t} else if (score == badness && reuseport) {\n\t\t\tmatches++;\n\t\t\tif (reciprocal_scale(hash, matches) == 0)\n\t\t\t\tresult = sk;\n\t\t\thash = next_pseudo_random32(hash);\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, hnum, saddr, sport,\n\t\t\t\t\tdaddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(__udp6_lib_lookup);\n\nstatic struct sock *__udp6_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t  __be16 sport, __be16 dport,\n\t\t\t\t\t  struct udp_table *udptable)\n{\n\tstruct sock *sk;\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\n\tsk = skb_steal_sock(skb);\n\tif (unlikely(sk))\n\t\treturn sk;\n\treturn __udp6_lib_lookup(dev_net(skb_dst(skb)->dev), &iph->saddr, sport,\n\t\t\t\t &iph->daddr, dport, inet6_iif(skb),\n\t\t\t\t udptable);\n}\n\nstruct sock *udp6_lib_lookup(struct net *net, const struct in6_addr *saddr, __be16 sport,\n\t\t\t     const struct in6_addr *daddr, __be16 dport, int dif)\n{\n\treturn __udp6_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp6_lib_lookup);\n\n/*\n *\tThis should be easy, if there is something there we\n *\treturn it, otherwise we block.\n */\n\nint udpv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint peeked, off = 0;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len, addr_len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &off, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tcopied = len;\n\tif (copied > ulen)\n\t\tcopied = ulen;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_msg(skb, sizeof(struct udphdr),\n\t\t\t\t\t    msg, copied);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, sizeof(struct udphdr), msg);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\ttrace_kfree_skb(skb, udpv6_recvmsg);\n\t\tif (!peeked) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tif (is_udp4)\n\t\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t   UDP_MIB_INERRORS,\n\t\t\t\t\t\t   is_udplite);\n\t\t\telse\n\t\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t\t    UDP_MIB_INERRORS,\n\t\t\t\t\t\t    is_udplite);\n\t\t}\n\t\tgoto out_free;\n\t}\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    inet6_iif(skb));\n\t\t}\n\t\t*addr_len = sizeof(*sin6);\n\t}\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4) {\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t} else {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_CSUMERRORS, is_udplite);\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\t}\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\t/* starting over for a new packet, but check if we need to yield */\n\tcond_resched();\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nvoid __udp6_lib_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t    u8 type, u8 code, int offset, __be32 info,\n\t\t    struct udp_table *udptable)\n{\n\tstruct ipv6_pinfo *np;\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct in6_addr *saddr = &hdr->saddr;\n\tconst struct in6_addr *daddr = &hdr->daddr;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+offset);\n\tstruct sock *sk;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp6_lib_lookup(net, daddr, uh->dest,\n\t\t\t       saddr, uh->source, inet6_iif(skb), udptable);\n\tif (!sk) {\n\t\tICMP6_INC_STATS_BH(net, __in6_dev_get(skb->dev),\n\t\t\t\t   ICMP6_MIB_INERRORS);\n\t\treturn;\n\t}\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\t\tip6_sk_update_pmtu(skb, sk, info);\n\t}\n\tif (type == NDISC_REDIRECT) {\n\t\tip6_sk_redirect(skb, sk);\n\t\tgoto out;\n\t}\n\n\tnp = inet6_sk(sk);\n\n\tif (!icmpv6_err_convert(type, code, &err) && !np->recverr)\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_ESTABLISHED && !np->recverr)\n\t\tgoto out;\n\n\tif (np->recverr)\n\t\tipv6_icmp_error(sk, skb, err, uh->dest, ntohl(info), (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nstatic int __udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tsk_incoming_cpu_update(sk);\n\t}\n\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tUDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\tkfree_skb(skb);\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic __inline__ void udpv6_err(struct sk_buff *skb,\n\t\t\t\t struct inet6_skb_parm *opt, u8 type,\n\t\t\t\t u8 code, int offset, __be32 info)\n{\n\t__udp6_lib_err(skb, opt, type, code, offset, info, &udp_table);\n}\n\nstatic struct static_key udpv6_encap_needed __read_mostly;\nvoid udpv6_encap_enable(void)\n{\n\tif (!static_key_enabled(&udpv6_encap_needed))\n\t\tstatic_key_slow_inc(&udpv6_encap_needed);\n}\nEXPORT_SYMBOL(udpv6_encap_enable);\n\nint udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = ACCESS_ONCE(up->encap_rcv);\n\t\tif (skb->len > sizeof(struct udphdr) && encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_access_pointer(sk->sk_filter)) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_error;\n\t}\n\n\tif (sk_rcvqueues_full(sk, sk->sk_rcvbuf)) {\n\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t  UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop;\n\t}\n\n\tskb_dst_drop(skb);\n\n\tbh_lock_sock(sk);\n\trc = 0;\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ncsum_error:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic bool __udp_v6_is_mcast_sock(struct net *net, struct sock *sk,\n\t\t\t\t   __be16 loc_port, const struct in6_addr *loc_addr,\n\t\t\t\t   __be16 rmt_port, const struct in6_addr *rmt_addr,\n\t\t\t\t   int dif, unsigned short hnum)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (!net_eq(sock_net(sk), net))\n\t\treturn false;\n\n\tif (udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6 ||\n\t    (inet->inet_dport && inet->inet_dport != rmt_port) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_daddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_daddr, rmt_addr)) ||\n\t    (sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_rcv_saddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_rcv_saddr, loc_addr)))\n\t\treturn false;\n\tif (!inet6_mc_check(sk, loc_addr, rmt_addr))\n\t\treturn false;\n\treturn true;\n}\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tstruct sk_buff *skb1 = NULL;\n\tstruct sock *sk;\n\tunsigned int i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tsk = stack[i];\n\t\tif (likely(!skb1))\n\t\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb1) {\n\t\t\tatomic_inc(&sk->sk_drops);\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t  IS_UDPLITE(sk));\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,\n\t\t\t\t\t  IS_UDPLITE(sk));\n\t\t}\n\n\t\tif (skb1 && udpv6_queue_rcv_skb(sk, skb1) <= 0)\n\t\t\tskb1 = NULL;\n\t\tsock_put(sk);\n\t}\n\tif (unlikely(skb1))\n\t\tkfree_skb(skb1);\n}\n\nstatic void udp6_csum_zero_error(struct sk_buff *skb)\n{\n\t/* RFC 2460 section 8.1 says that we SHOULD log\n\t * this error. Well, it is reasonable.\n\t */\n\tnet_dbg_ratelimited(\"IPv6: udp checksum is 0 for [%pI6c]:%u->[%pI6c]:%u\\n\",\n\t\t\t    &ipv6_hdr(skb)->saddr, ntohs(udp_hdr(skb)->source),\n\t\t\t    &ipv6_hdr(skb)->daddr, ntohs(udp_hdr(skb)->dest));\n}\n\n/*\n * Note: called only from the BH handler context,\n * so we don't need to lock the hashes.\n */\nstatic int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr,\n\t\tstruct udp_table *udptable, int proto)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tconst struct udphdr *uh = udp_hdr(skb);\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(uh->dest);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, hnum);\n\tint dif = inet6_iif(skb);\n\tunsigned int count = 0, offset = offsetof(typeof(*sk), sk_nulls_node);\n\tunsigned int hash2 = 0, hash2_any = 0, use_hash2 = (hslot->count > 10);\n\tbool inner_flushed = false;\n\n\tif (use_hash2) {\n\t\thash2_any = udp6_portaddr_hash(net, &in6addr_any, hnum) &\n\t\t\t    udp_table.mask;\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum) & udp_table.mask;\nstart_lookup:\n\t\thslot = &udp_table.hash2[hash2];\n\t\toffset = offsetof(typeof(*sk), __sk_common.skc_portaddr_node);\n\t}\n\n\tspin_lock(&hslot->lock);\n\tsk_nulls_for_each_entry_offset(sk, node, &hslot->head, offset) {\n\t\tif (__udp_v6_is_mcast_sock(net, sk,\n\t\t\t\t\t   uh->dest, daddr,\n\t\t\t\t\t   uh->source, saddr,\n\t\t\t\t\t   dif, hnum) &&\n\t\t    /* If zero checksum and no_check is not on for\n\t\t     * the socket then skip it.\n\t\t     */\n\t\t    (uh->check || udp_sk(sk)->no_check6_rx)) {\n\t\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\t\tinner_flushed = true;\n\t\t\t\tcount = 0;\n\t\t\t}\n\t\t\tstack[count++] = sk;\n\t\t\tsock_hold(sk);\n\t\t}\n\t}\n\n\tspin_unlock(&hslot->lock);\n\n\t/* Also lookup *:port if we are using hash2 and haven't done so yet. */\n\tif (use_hash2 && hash2 != hash2_any) {\n\t\thash2 = hash2_any;\n\t\tgoto start_lookup;\n\t}\n\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\t} else {\n\t\tif (!inner_flushed)\n\t\t\tUDP_INC_STATS_BH(net, UDP_MIB_IGNOREDMULTI,\n\t\t\t\t\t proto == IPPROTO_UDPLITE);\n\t\tconsume_skb(skb);\n\t}\n\treturn 0;\n}\n\nint __udp6_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tconst struct in6_addr *saddr, *daddr;\n\tu32 ulen = 0;\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto discard;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = &ipv6_hdr(skb)->daddr;\n\tuh = udp_hdr(skb);\n\n\tulen = ntohs(uh->len);\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\n\t\t/* Check for jumbo payload */\n\t\tif (ulen == 0)\n\t\t\tulen = skb->len;\n\n\t\tif (ulen < sizeof(*uh))\n\t\t\tgoto short_packet;\n\n\t\tif (ulen < skb->len) {\n\t\t\tif (pskb_trim_rcsum(skb, ulen))\n\t\t\t\tgoto short_packet;\n\t\t\tsaddr = &ipv6_hdr(skb)->saddr;\n\t\t\tdaddr = &ipv6_hdr(skb)->daddr;\n\t\t\tuh = udp_hdr(skb);\n\t\t}\n\t}\n\n\tif (udp6_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\t/*\n\t *\tMulticast receive code\n\t */\n\tif (ipv6_addr_is_multicast(daddr))\n\t\treturn __udp6_lib_mcast_deliver(net, skb,\n\t\t\t\tsaddr, daddr, udptable, proto);\n\n\t/* Unicast */\n\n\t/*\n\t * check socket cache ... must talk to Alan about his plans\n\t * for sock caches... i'll skip this for now.\n\t */\n\tsk = __udp6_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\tif (sk) {\n\t\tint ret;\n\n\t\tif (!uh->check && !udp_sk(sk)->no_check6_rx) {\n\t\t\tsock_put(sk);\n\t\t\tudp6_csum_zero_error(skb);\n\t\t\tgoto csum_error;\n\t\t}\n\n\t\tif (inet_get_convert_csum(sk) && uh->check && !IS_UDPLITE(sk))\n\t\t\tskb_checksum_try_convert(skb, IPPROTO_UDP, uh->check,\n\t\t\t\t\t\t ip6_compute_pseudo);\n\n\t\tret = udpv6_queue_rcv_skb(sk, skb);\n\t\tsock_put(sk);\n\n\t\t/* a return value > 0 means to resubmit the input, but\n\t\t * it wants the return to be -protocol, or 0\n\t\t */\n\t\tif (ret > 0)\n\t\t\treturn -ret;\n\n\t\treturn 0;\n\t}\n\n\tif (!uh->check) {\n\t\tudp6_csum_zero_error(skb);\n\t\tgoto csum_error;\n\t}\n\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard;\n\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tUDP6_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmpv6_send(skb, ICMPV6_DEST_UNREACH, ICMPV6_PORT_UNREACH, 0);\n\n\tkfree_skb(skb);\n\treturn 0;\n\nshort_packet:\n\tnet_dbg_ratelimited(\"UDP%sv6: short packet: From [%pI6c]:%u %d/%d to [%pI6c]:%u\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t\t    saddr, ntohs(uh->source),\n\t\t\t    ulen, skb->len,\n\t\t\t    daddr, ntohs(uh->dest));\n\tgoto discard;\ncsum_error:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_CSUMERRORS, proto == IPPROTO_UDPLITE);\ndiscard:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic __inline__ int udpv6_rcv(struct sk_buff *skb)\n{\n\treturn __udp6_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nstatic void udp_v6_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending == AF_INET)\n\t\tudp_flush_pending_frames(sk);\n\telse if (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip6_flush_pending_frames(sk);\n\t}\n}\n\n/**\n *\tudp6_hwcsum_outgoing  -  handle outgoing HW checksumming\n *\t@sk:\tsocket we are sending on\n *\t@skb:\tsk_buff containing the filled-in UDP header\n *\t\t(checksum field must be zeroed out)\n */\nstatic void udp6_hwcsum_outgoing(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t const struct in6_addr *saddr,\n\t\t\t\t const struct in6_addr *daddr, int len)\n{\n\tunsigned int offset;\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *frags = skb_shinfo(skb)->frag_list;\n\t__wsum csum = 0;\n\n\tif (!frags) {\n\t\t/* Only one fragment on the socket.  */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\toffset = skb_transport_offset(skb);\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tdo {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t} while ((frags = frags->next));\n\n\t\tuh->check = csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP,\n\t\t\t\t\t    csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\n/*\n *\tSending\n */\n\nstatic int udp_v6_send_skb(struct sk_buff *skb, struct flowi6 *fl6)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\t__wsum csum = 0;\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = fl6->fl6_sport;\n\tuh->dest = fl6->fl6_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (is_udplite)\n\t\tcsum = udplite_csum(skb);\n\telse if (udp_sk(sk)->no_check6_tx) {   /* UDP csum disabled */\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\t\tudp6_hwcsum_outgoing(sk, skb, &fl6->saddr, &fl6->daddr, len);\n\t\tgoto send;\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t\t    len, fl6->flowi6_proto, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip6_send_skb(skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet6_sk(sk)->recverr) {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t    UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t    UDP_MIB_OUTDATAGRAMS, is_udplite);\n\treturn err;\n}\n\nstatic int udp_v6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct flowi6 fl6;\n\tint err = 0;\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_push_pending_frames(sk);\n\n\t/* ip6_finish_skb will release the cork, so make a copy of\n\t * fl6 here.\n\t */\n\tfl6 = inet_sk(sk)->cork.fl.u.ip6;\n\n\tskb = ip6_finish_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_v6_send_skb(skb, &fl6);\n\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\n\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (!flowlabel)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (!flowlabel)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t} else if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), hlimit, tclass, opt,\n\t\t\t\t   &fl6, (struct rt6_info *)dst,\n\t\t\t\t   msg->msg_flags, dontfrag);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, &fl6);\n\t\tgoto release_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info *)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nrelease_dst:\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &sk->sk_v6_daddr) ?\n\t\t\t\t      &sk->sk_v6_daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n\nvoid udpv6_destroy_sock(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tlock_sock(sk);\n\tudp_v6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tif (static_key_false(&udpv6_encap_needed) && up->encap_type) {\n\t\tvoid (*encap_destroy)(struct sock *sk);\n\t\tencap_destroy = ACCESS_ONCE(up->encap_destroy);\n\t\tif (encap_destroy)\n\t\t\tencap_destroy(sk);\n\t}\n\n\tinet6_destroy_sock(sk);\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn compat_ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic const struct inet6_protocol udpv6_protocol = {\n\t.handler\t=\tudpv6_rcv,\n\t.err_handler\t=\tudpv6_err,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\nint udp6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, IPV6_SEQ_DGRAM_HEADER);\n\t} else {\n\t\tint bucket = ((struct udp_iter_state *)seq->private)->bucket;\n\t\tstruct inet_sock *inet = inet_sk(v);\n\t\t__u16 srcp = ntohs(inet->inet_sport);\n\t\t__u16 destp = ntohs(inet->inet_dport);\n\t\tip6_dgram_sock_seq_show(seq, v, srcp, destp, bucket);\n\t}\n\treturn 0;\n}\n\nstatic const struct file_operations udp6_afinfo_seq_fops = {\n\t.owner    = THIS_MODULE,\n\t.open     = udp_seq_open,\n\t.read     = seq_read,\n\t.llseek   = seq_lseek,\n\t.release  = seq_release_net\n};\n\nstatic struct udp_seq_afinfo udp6_seq_afinfo = {\n\t.name\t\t= \"udp6\",\n\t.family\t\t= AF_INET6,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= &udp6_afinfo_seq_fops,\n\t.seq_ops\t= {\n\t\t.show\t\t= udp6_seq_show,\n\t},\n};\n\nint __net_init udp6_proc_init(struct net *net)\n{\n\treturn udp_proc_register(net, &udp6_seq_afinfo);\n}\n\nvoid udp6_proc_exit(struct net *net)\n{\n\tudp_proc_unregister(net, &udp6_seq_afinfo);\n}\n#endif /* CONFIG_PROC_FS */\n\nvoid udp_v6_clear_sk(struct sock *sk, int size)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\t/* we do not want to clear pinet6 field, because of RCU lookups */\n\tsk_prot_clear_portaddr_nulls(sk, offsetof(struct inet_sock, pinet6));\n\n\tsize -= offsetof(struct inet_sock, pinet6) + sizeof(inet->pinet6);\n\tmemset(&inet->pinet6 + 1, 0, size);\n}\n\n/* ------------------------------------------------------------------------ */\n\nstruct proto udpv6_prot = {\n\t.name\t\t   = \"UDPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip6_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udpv6_destroy_sock,\n\t.setsockopt\t   = udpv6_setsockopt,\n\t.getsockopt\t   = udpv6_getsockopt,\n\t.sendmsg\t   = udpv6_sendmsg,\n\t.recvmsg\t   = udpv6_recvmsg,\n\t.backlog_rcv\t   = __udpv6_queue_rcv_skb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v6_rehash,\n\t.get_port\t   = udp_v6_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udpv6_setsockopt,\n\t.compat_getsockopt = compat_udpv6_getsockopt,\n#endif\n\t.clear_sk\t   = udp_v6_clear_sk,\n};\n\nstatic struct inet_protosw udpv6_protosw = {\n\t.type =      SOCK_DGRAM,\n\t.protocol =  IPPROTO_UDP,\n\t.prot =      &udpv6_prot,\n\t.ops =       &inet6_dgram_ops,\n\t.flags =     INET_PROTOSW_PERMANENT,\n};\n\nint __init udpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_register_protosw(&udpv6_protosw);\n\tif (ret)\n\t\tgoto out_udpv6_protocol;\nout:\n\treturn ret;\n\nout_udpv6_protocol:\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tgoto out;\n}\n\nvoid udpv6_exit(void)\n{\n\tinet6_unregister_protosw(&udpv6_protosw);\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n}\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n *\tUDP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/ipv4/udp.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *      Kazunori MIYAZAWA @USAGI:       change process style to use ip6_append_data\n *      YOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/udp6 to seq_file.\n */\n\n#include <linux/bpf-cgroup.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/indirect_call_wrapper.h>\n#include <trace/events/udp.h>\n\n#include <net/addrconf.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/raw.h>\n#include <net/seg6.h>\n#include <net/tcp_states.h>\n#include <net/ip6_checksum.h>\n#include <net/ip6_tunnel.h>\n#include <net/udp_tunnel.h>\n#include <net/xfrm.h>\n#include <net/inet_hashtables.h>\n#include <net/inet6_hashtables.h>\n#include <net/busy_poll.h>\n#include <net/sock_reuseport.h>\n#include <net/gro.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <trace/events/skb.h>\n#include \"udp_impl.h\"\n\nstatic void udpv6_destruct_sock(struct sock *sk)\n{\n\tudp_destruct_common(sk);\n\tinet6_sock_destruct(sk);\n}\n\nint udpv6_init_sock(struct sock *sk)\n{\n\tudp_lib_init_sock(sk);\n\tsk->sk_destruct = udpv6_destruct_sock;\n\tset_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\treturn 0;\n}\n\nINDIRECT_CALLABLE_SCOPE\nu32 udp6_ehashfn(const struct net *net,\n\t\t const struct in6_addr *laddr,\n\t\t const u16 lport,\n\t\t const struct in6_addr *faddr,\n\t\t const __be16 fport)\n{\n\tu32 lhash, fhash;\n\n\tnet_get_random_once(&udp6_ehash_secret,\n\t\t\t    sizeof(udp6_ehash_secret));\n\tnet_get_random_once(&udp_ipv6_hash_secret,\n\t\t\t    sizeof(udp_ipv6_hash_secret));\n\n\tlhash = (__force u32)laddr->s6_addr32[3];\n\tfhash = __ipv6_addr_jhash(faddr, udp_ipv6_hash_secret);\n\n\treturn __inet6_ehashfn(lhash, lport, fhash, fport,\n\t\t\t       udp6_ehash_secret + net_hash_mix(net));\n}\n\nint udp_v6_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tipv6_portaddr_hash(sock_net(sk), &in6addr_any, snum);\n\tunsigned int hash2_partial =\n\t\tipv6_portaddr_hash(sock_net(sk), &sk->sk_v6_rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, hash2_nulladdr);\n}\n\nvoid udp_v6_rehash(struct sock *sk)\n{\n\tu16 new_hash = ipv6_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\tu16 new_hash4;\n\n\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr)) {\n\t\tnew_hash4 = udp_ehashfn(sock_net(sk),\n\t\t\t\t\tsk->sk_rcv_saddr, sk->sk_num,\n\t\t\t\t\tsk->sk_daddr, sk->sk_dport);\n\t} else {\n\t\tnew_hash4 = udp6_ehashfn(sock_net(sk),\n\t\t\t\t\t &sk->sk_v6_rcv_saddr, sk->sk_num,\n\t\t\t\t\t &sk->sk_v6_daddr, sk->sk_dport);\n\t}\n\n\tudp_lib_rehash(sk, new_hash, new_hash4);\n}\n\nstatic int compute_score(struct sock *sk, const struct net *net,\n\t\t\t const struct in6_addr *saddr, __be16 sport,\n\t\t\t const struct in6_addr *daddr, unsigned short hnum,\n\t\t\t int dif, int sdif)\n{\n\tint bound_dev_if, score;\n\tstruct inet_sock *inet;\n\tbool dev_match;\n\n\tif (!net_eq(sock_net(sk), net) ||\n\t    udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6)\n\t\treturn -1;\n\n\tif (!ipv6_addr_equal(&sk->sk_v6_rcv_saddr, daddr))\n\t\treturn -1;\n\n\tscore = 0;\n\tinet = inet_sk(sk);\n\n\tif (inet->inet_dport) {\n\t\tif (inet->inet_dport != sport)\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tif (!ipv6_addr_equal(&sk->sk_v6_daddr, saddr))\n\t\t\treturn -1;\n\t\tscore++;\n\t}\n\n\tbound_dev_if = READ_ONCE(sk->sk_bound_dev_if);\n\tdev_match = udp_sk_bound_dev_eq(net, bound_dev_if, dif, sdif);\n\tif (!dev_match)\n\t\treturn -1;\n\tif (bound_dev_if)\n\t\tscore++;\n\n\tif (READ_ONCE(sk->sk_incoming_cpu) == raw_smp_processor_id())\n\t\tscore++;\n\n\treturn score;\n}\n\n/**\n * udp6_lib_lookup1() - Simplified lookup using primary hash (destination port)\n * @net:\tNetwork namespace\n * @saddr:\tSource address, network order\n * @sport:\tSource port, network order\n * @daddr:\tDestination address, network order\n * @hnum:\tDestination port, host order\n * @dif:\tDestination interface index\n * @sdif:\tDestination bridge port index, if relevant\n * @udptable:\tSet of UDP hash tables\n *\n * Simplified lookup to be used as fallback if no sockets are found due to a\n * potential race between (receive) address change, and lookup happening before\n * the rehash operation. This function ignores SO_REUSEPORT groups while scoring\n * result sockets, because if we have one, we don't need the fallback at all.\n *\n * Called under rcu_read_lock().\n *\n * Return: socket with highest matching score if any, NULL if none\n */\nstatic struct sock *udp6_lib_lookup1(const struct net *net,\n\t\t\t\t     const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t     const struct in6_addr *daddr,\n\t\t\t\t     unsigned int hnum, int dif, int sdif,\n\t\t\t\t     const struct udp_table *udptable)\n{\n\tunsigned int slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot = &udptable->hash[slot];\n\tstruct sock *sk, *result = NULL;\n\tint score, badness = 0;\n\n\tsk_for_each_rcu(sk, &hslot->head) {\n\t\tscore = compute_score(sk, net,\n\t\t\t\t      saddr, sport, daddr, hnum, dif, sdif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t}\n\t}\n\n\treturn result;\n}\n\n/* called with rcu_read_lock() */\nstatic struct sock *udp6_lib_lookup2(const struct net *net,\n\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\tconst struct in6_addr *daddr, unsigned int hnum,\n\t\tint dif, int sdif, struct udp_hslot *hslot2,\n\t\tstruct sk_buff *skb)\n{\n\tstruct sock *sk, *result;\n\tint score, badness;\n\tbool need_rescore;\n\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, &hslot2->head) {\n\t\tneed_rescore = false;\nrescore:\n\t\tscore = compute_score(need_rescore ? result : sk, net, saddr,\n\t\t\t\t      sport, daddr, hnum, dif, sdif);\n\t\tif (score > badness) {\n\t\t\tbadness = score;\n\n\t\t\tif (need_rescore)\n\t\t\t\tcontinue;\n\n\t\t\tif (sk->sk_state == TCP_ESTABLISHED) {\n\t\t\t\tresult = sk;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tresult = inet6_lookup_reuseport(net, sk, skb, sizeof(struct udphdr),\n\t\t\t\t\t\t\tsaddr, sport, daddr, hnum, udp6_ehashfn);\n\t\t\tif (!result) {\n\t\t\t\tresult = sk;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Fall back to scoring if group has connections */\n\t\t\tif (!reuseport_has_conns(sk))\n\t\t\t\treturn result;\n\n\t\t\t/* Reuseport logic returned an error, keep original score. */\n\t\t\tif (IS_ERR(result))\n\t\t\t\tcontinue;\n\n\t\t\t/* compute_score is too long of a function to be\n\t\t\t * inlined, and calling it again here yields\n\t\t\t * measurable overhead for some\n\t\t\t * workloads. Work around it by jumping\n\t\t\t * backwards to rescore 'result'.\n\t\t\t */\n\t\t\tneed_rescore = true;\n\t\t\tgoto rescore;\n\t\t}\n\t}\n\treturn result;\n}\n\n#if IS_ENABLED(CONFIG_BASE_SMALL)\nstatic struct sock *udp6_lib_lookup4(const struct net *net,\n\t\t\t\t     const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t     const struct in6_addr *daddr,\n\t\t\t\t     unsigned int hnum, int dif, int sdif,\n\t\t\t\t     struct udp_table *udptable)\n{\n\treturn NULL;\n}\n\nstatic void udp6_hash4(struct sock *sk)\n{\n}\n#else /* !CONFIG_BASE_SMALL */\nstatic struct sock *udp6_lib_lookup4(const struct net *net,\n\t\t\t\t     const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t     const struct in6_addr *daddr,\n\t\t\t\t     unsigned int hnum, int dif, int sdif,\n\t\t\t\t     struct udp_table *udptable)\n{\n\tconst __portpair ports = INET_COMBINED_PORTS(sport, hnum);\n\tconst struct hlist_nulls_node *node;\n\tstruct udp_hslot *hslot4;\n\tunsigned int hash4, slot;\n\tstruct udp_sock *up;\n\tstruct sock *sk;\n\n\thash4 = udp6_ehashfn(net, daddr, hnum, saddr, sport);\n\tslot = hash4 & udptable->mask;\n\thslot4 = &udptable->hash4[slot];\n\nbegin:\n\tudp_lrpa_for_each_entry_rcu(up, node, &hslot4->nulls_head) {\n\t\tsk = (struct sock *)up;\n\t\tif (inet6_match(net, sk, saddr, daddr, ports, dif, sdif))\n\t\t\treturn sk;\n\t}\n\n\t/* if the nulls value we got at the end of this lookup is not the\n\t * expected one, we must restart lookup. We probably met an item that\n\t * was moved to another chain due to rehash.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\treturn NULL;\n}\n\nstatic void udp6_hash4(struct sock *sk)\n{\n\tstruct net *net = sock_net(sk);\n\tunsigned int hash;\n\n\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr)) {\n\t\tudp4_hash4(sk);\n\t\treturn;\n\t}\n\n\tif (sk_unhashed(sk) || ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\treturn;\n\n\thash = udp6_ehashfn(net, &sk->sk_v6_rcv_saddr, sk->sk_num,\n\t\t\t    &sk->sk_v6_daddr, sk->sk_dport);\n\n\tudp_lib_hash4(sk, hash);\n}\n#endif /* CONFIG_BASE_SMALL */\n\n/* rcu_read_lock() must be held */\nstruct sock *__udp6_lib_lookup(const struct net *net,\n\t\t\t       const struct in6_addr *saddr, __be16 sport,\n\t\t\t       const struct in6_addr *daddr, __be16 dport,\n\t\t\t       int dif, int sdif, struct udp_table *udptable,\n\t\t\t       struct sk_buff *skb)\n{\n\tunsigned short hnum = ntohs(dport);\n\tstruct udp_hslot *hslot2;\n\tstruct sock *result, *sk;\n\tunsigned int hash2;\n\n\thash2 = ipv6_portaddr_hash(net, daddr, hnum);\n\thslot2 = udp_hashslot2(udptable, hash2);\n\n\tif (udp_has_hash4(hslot2)) {\n\t\tresult = udp6_lib_lookup4(net, saddr, sport, daddr, hnum,\n\t\t\t\t\t  dif, sdif, udptable);\n\t\tif (result) /* udp6_lib_lookup4 return sk or NULL */\n\t\t\treturn result;\n\t}\n\n\t/* Lookup connected or non-wildcard sockets */\n\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif, sdif,\n\t\t\t\t  hslot2, skb);\n\tif (!IS_ERR_OR_NULL(result) && result->sk_state == TCP_ESTABLISHED)\n\t\tgoto done;\n\n\t/* Lookup redirect from BPF */\n\tif (static_branch_unlikely(&bpf_sk_lookup_enabled) &&\n\t    udptable == net->ipv4.udp_table) {\n\t\tsk = inet6_lookup_run_sk_lookup(net, IPPROTO_UDP, skb, sizeof(struct udphdr),\n\t\t\t\t\t\tsaddr, sport, daddr, hnum, dif,\n\t\t\t\t\t\tudp6_ehashfn);\n\t\tif (sk) {\n\t\t\tresult = sk;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t/* Got non-wildcard socket or error on first lookup */\n\tif (result)\n\t\tgoto done;\n\n\t/* Lookup wildcard sockets */\n\thash2 = ipv6_portaddr_hash(net, &in6addr_any, hnum);\n\thslot2 = udp_hashslot2(udptable, hash2);\n\n\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t  &in6addr_any, hnum, dif, sdif,\n\t\t\t\t  hslot2, skb);\n\tif (!IS_ERR_OR_NULL(result))\n\t\tgoto done;\n\n\t/* Cover address change/lookup/rehash race: see __udp4_lib_lookup() */\n\tresult = udp6_lib_lookup1(net, saddr, sport, daddr, hnum, dif, sdif,\n\t\t\t\t  udptable);\n\ndone:\n\tif (IS_ERR(result))\n\t\treturn NULL;\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(__udp6_lib_lookup);\n\nstatic struct sock *__udp6_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t  __be16 sport, __be16 dport,\n\t\t\t\t\t  struct udp_table *udptable)\n{\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\n\treturn __udp6_lib_lookup(dev_net(skb->dev), &iph->saddr, sport,\n\t\t\t\t &iph->daddr, dport, inet6_iif(skb),\n\t\t\t\t inet6_sdif(skb), udptable, skb);\n}\n\nstruct sock *udp6_lib_lookup_skb(const struct sk_buff *skb,\n\t\t\t\t __be16 sport, __be16 dport)\n{\n\tconst u16 offset = NAPI_GRO_CB(skb)->network_offsets[skb->encapsulation];\n\tconst struct ipv6hdr *iph = (struct ipv6hdr *)(skb->data + offset);\n\tstruct net *net = dev_net(skb->dev);\n\tint iif, sdif;\n\n\tinet6_get_iif_sdif(skb, &iif, &sdif);\n\n\treturn __udp6_lib_lookup(net, &iph->saddr, sport,\n\t\t\t\t &iph->daddr, dport, iif,\n\t\t\t\t sdif, net->ipv4.udp_table, NULL);\n}\n\n/* Must be called under rcu_read_lock().\n * Does increment socket refcount.\n */\n#if IS_ENABLED(CONFIG_NF_TPROXY_IPV6) || IS_ENABLED(CONFIG_NF_SOCKET_IPV6)\nstruct sock *udp6_lib_lookup(const struct net *net, const struct in6_addr *saddr, __be16 sport,\n\t\t\t     const struct in6_addr *daddr, __be16 dport, int dif)\n{\n\tstruct sock *sk;\n\n\tsk =  __udp6_lib_lookup(net, saddr, sport, daddr, dport,\n\t\t\t\tdif, 0, net->ipv4.udp_table, NULL);\n\tif (sk && !refcount_inc_not_zero(&sk->sk_refcnt))\n\t\tsk = NULL;\n\treturn sk;\n}\nEXPORT_SYMBOL_GPL(udp6_lib_lookup);\n#endif\n\n/* do not use the scratch area len for jumbogram: their length exceeds the\n * scratch area space; note that the IP6CB flags is still in the first\n * cacheline, so checking for jumbograms is cheap\n */\nstatic int udp6_skb_len(struct sk_buff *skb)\n{\n\treturn unlikely(inet6_is_jumbogram(skb)) ? skb->len : udp_skb_len(skb);\n}\n\n/*\n *\tThis should be easy, if there is something there we\n *\treturn it, otherwise we block.\n */\n\nint udpv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t  int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen, copied;\n\tint off, err, peeking = flags & MSG_PEEK;\n\tint is_udplite = IS_UDPLITE(sk);\n\tstruct udp_mib __percpu *mib;\n\tbool checksum_valid = false;\n\tint is_udp4;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tif (np->rxopt.bits.rxpmtu && READ_ONCE(np->rxpmtu))\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len, addr_len);\n\ntry_again:\n\toff = sk_peek_offset(sk, flags);\n\tskb = __skb_recv_udp(sk, flags, &off, &err);\n\tif (!skb)\n\t\treturn err;\n\n\tulen = udp6_skb_len(skb);\n\tcopied = len;\n\tif (copied > ulen - off)\n\t\tcopied = ulen - off;\n\telse if (copied < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\tmib = __UDPX_MIB(sk, is_udp4);\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (copied < ulen || peeking ||\n\t    (is_udplite && UDP_SKB_CB(skb)->partial_cov)) {\n\t\tchecksum_valid = udp_skb_csum_unnecessary(skb) ||\n\t\t\t\t!__udp_lib_checksum_complete(skb);\n\t\tif (!checksum_valid)\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (checksum_valid || udp_skb_csum_unnecessary(skb)) {\n\t\tif (udp_skb_is_linear(skb))\n\t\t\terr = copy_linear_skb(skb, copied, off, &msg->msg_iter);\n\t\telse\n\t\t\terr = skb_copy_datagram_msg(skb, off, msg, copied);\n\t} else {\n\t\terr = skb_copy_and_csum_datagram_msg(skb, off, msg);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (unlikely(err)) {\n\t\tif (!peeking) {\n\t\t\tudp_drops_inc(sk);\n\t\t\tSNMP_INC_STATS(mib, UDP_MIB_INERRORS);\n\t\t}\n\t\tkfree_skb(skb);\n\t\treturn err;\n\t}\n\tif (!peeking)\n\t\tSNMP_INC_STATS(mib, UDP_MIB_INDATAGRAMS);\n\n\tsock_recv_cmsgs(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\n\t\tif (is_udp4) {\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\t\tsin6->sin6_scope_id = 0;\n\t\t} else {\n\t\t\tsin6->sin6_addr = ipv6_hdr(skb)->saddr;\n\t\t\tsin6->sin6_scope_id =\n\t\t\t\tipv6_iface_scope_id(&sin6->sin6_addr,\n\t\t\t\t\t\t    inet6_iif(skb));\n\t\t}\n\t\t*addr_len = sizeof(*sin6);\n\n\t\tBPF_CGROUP_RUN_PROG_UDP6_RECVMSG_LOCK(sk,\n\t\t\t\t\t\t      (struct sockaddr *)sin6,\n\t\t\t\t\t\t      addr_len);\n\t}\n\n\tif (udp_test_bit(GRO_ENABLED, sk))\n\t\tudp_cmsg_recv(msg, sk, skb);\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_common_ctl(sk, msg, skb);\n\n\tif (is_udp4) {\n\t\tif (inet_cmsg_flags(inet))\n\t\t\tip_cmsg_recv_offset(msg, sk, skb,\n\t\t\t\t\t    sizeof(struct udphdr), off);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tip6_datagram_recv_specific_ctl(sk, msg, skb);\n\t}\n\n\terr = copied;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\n\tskb_consume_udp(sk, skb, peeking ? -err : err);\n\treturn err;\n\ncsum_copy_err:\n\tif (!__sk_queue_drop_skb(sk, &udp_sk(sk)->reader_queue, skb, flags,\n\t\t\t\t udp_skb_destructor)) {\n\t\tSNMP_INC_STATS(mib, UDP_MIB_CSUMERRORS);\n\t\tSNMP_INC_STATS(mib, UDP_MIB_INERRORS);\n\t}\n\tkfree_skb_reason(skb, SKB_DROP_REASON_UDP_CSUM);\n\n\t/* starting over for a new packet, but check if we need to yield */\n\tcond_resched();\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nDECLARE_STATIC_KEY_FALSE(udpv6_encap_needed_key);\nvoid udpv6_encap_enable(void)\n{\n\tstatic_branch_inc(&udpv6_encap_needed_key);\n}\nEXPORT_SYMBOL(udpv6_encap_enable);\n\n/* Handler for tunnels with arbitrary destination ports: no socket lookup, go\n * through error handlers in encapsulations looking for a match.\n */\nstatic int __udp6_lib_err_encap_no_sk(struct sk_buff *skb,\n\t\t\t\t      struct inet6_skb_parm *opt,\n\t\t\t\t      u8 type, u8 code, int offset, __be32 info)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_IPTUN_ENCAP_OPS; i++) {\n\t\tint (*handler)(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t\t       u8 type, u8 code, int offset, __be32 info);\n\t\tconst struct ip6_tnl_encap_ops *encap;\n\n\t\tencap = rcu_dereference(ip6tun_encaps[i]);\n\t\tif (!encap)\n\t\t\tcontinue;\n\t\thandler = encap->err_handler;\n\t\tif (handler && !handler(skb, opt, type, code, offset, info))\n\t\t\treturn 0;\n\t}\n\n\treturn -ENOENT;\n}\n\n/* Try to match ICMP errors to UDP tunnels by looking up a socket without\n * reversing source and destination port: this will match tunnels that force the\n * same destination port on both endpoints (e.g. VXLAN, GENEVE). Note that\n * lwtunnels might actually break this assumption by being configured with\n * different destination ports on endpoints, in this case we won't be able to\n * trace ICMP messages back to them.\n *\n * If this doesn't match any socket, probe tunnels with arbitrary destination\n * ports (e.g. FoU, GUE): there, the receiving socket is useless, as the port\n * we've sent packets to won't necessarily match the local destination port.\n *\n * Then ask the tunnel implementation to match the error against a valid\n * association.\n *\n * Return an error if we can't find a match, the socket if we need further\n * processing, zero otherwise.\n */\nstatic struct sock *__udp6_lib_err_encap(struct net *net,\n\t\t\t\t\t const struct ipv6hdr *hdr, int offset,\n\t\t\t\t\t struct udphdr *uh,\n\t\t\t\t\t struct udp_table *udptable,\n\t\t\t\t\t struct sock *sk,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct inet6_skb_parm *opt,\n\t\t\t\t\t u8 type, u8 code, __be32 info)\n{\n\tint (*lookup)(struct sock *sk, struct sk_buff *skb);\n\tint network_offset, transport_offset;\n\tstruct udp_sock *up;\n\n\tnetwork_offset = skb_network_offset(skb);\n\ttransport_offset = skb_transport_offset(skb);\n\n\t/* Network header needs to point to the outer IPv6 header inside ICMP */\n\tskb_reset_network_header(skb);\n\n\t/* Transport header needs to point to the UDP header */\n\tskb_set_transport_header(skb, offset);\n\n\tif (sk) {\n\t\tup = udp_sk(sk);\n\n\t\tlookup = READ_ONCE(up->encap_err_lookup);\n\t\tif (lookup && lookup(sk, skb))\n\t\t\tsk = NULL;\n\n\t\tgoto out;\n\t}\n\n\tsk = __udp6_lib_lookup(net, &hdr->daddr, uh->source,\n\t\t\t       &hdr->saddr, uh->dest,\n\t\t\t       inet6_iif(skb), 0, udptable, skb);\n\tif (sk) {\n\t\tup = udp_sk(sk);\n\n\t\tlookup = READ_ONCE(up->encap_err_lookup);\n\t\tif (!lookup || lookup(sk, skb))\n\t\t\tsk = NULL;\n\t}\n\nout:\n\tif (!sk) {\n\t\tsk = ERR_PTR(__udp6_lib_err_encap_no_sk(skb, opt, type, code,\n\t\t\t\t\t\t\toffset, info));\n\t}\n\n\tskb_set_transport_header(skb, transport_offset);\n\tskb_set_network_header(skb, network_offset);\n\n\treturn sk;\n}\n\nint __udp6_lib_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t   u8 type, u8 code, int offset, __be32 info,\n\t\t   struct udp_table *udptable)\n{\n\tstruct ipv6_pinfo *np;\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct in6_addr *saddr = &hdr->saddr;\n\tconst struct in6_addr *daddr = seg6_get_daddr(skb, opt) ? : &hdr->daddr;\n\tstruct udphdr *uh = (struct udphdr *)(skb->data+offset);\n\tbool tunnel = false;\n\tstruct sock *sk;\n\tint harderr;\n\tint err;\n\tstruct net *net = dev_net(skb->dev);\n\n\tsk = __udp6_lib_lookup(net, daddr, uh->dest, saddr, uh->source,\n\t\t\t       inet6_iif(skb), inet6_sdif(skb), udptable, NULL);\n\n\tif (!sk || READ_ONCE(udp_sk(sk)->encap_type)) {\n\t\t/* No socket for error: try tunnels before discarding */\n\t\tif (static_branch_unlikely(&udpv6_encap_needed_key)) {\n\t\t\tsk = __udp6_lib_err_encap(net, hdr, offset, uh,\n\t\t\t\t\t\t  udptable, sk, skb,\n\t\t\t\t\t\t  opt, type, code, info);\n\t\t\tif (!sk)\n\t\t\t\treturn 0;\n\t\t} else\n\t\t\tsk = ERR_PTR(-ENOENT);\n\n\t\tif (IS_ERR(sk)) {\n\t\t\t__ICMP6_INC_STATS(net, __in6_dev_get(skb->dev),\n\t\t\t\t\t  ICMP6_MIB_INERRORS);\n\t\t\treturn PTR_ERR(sk);\n\t\t}\n\n\t\ttunnel = true;\n\t}\n\n\tharderr = icmpv6_err_convert(type, code, &err);\n\tnp = inet6_sk(sk);\n\n\tif (type == ICMPV6_PKT_TOOBIG) {\n\t\tif (!ip6_sk_accept_pmtu(sk))\n\t\t\tgoto out;\n\t\tip6_sk_update_pmtu(skb, sk, info);\n\t\tif (READ_ONCE(np->pmtudisc) != IPV6_PMTUDISC_DONT)\n\t\t\tharderr = 1;\n\t}\n\tif (type == NDISC_REDIRECT) {\n\t\tif (tunnel) {\n\t\t\tip6_redirect(skb, sock_net(sk), inet6_iif(skb),\n\t\t\t\t     READ_ONCE(sk->sk_mark),\n\t\t\t\t     sk_uid(sk));\n\t\t} else {\n\t\t\tip6_sk_redirect(skb, sk);\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Tunnels don't have an application socket: don't pass errors back */\n\tif (tunnel) {\n\t\tif (udp_sk(sk)->encap_err_rcv)\n\t\t\tudp_sk(sk)->encap_err_rcv(sk, skb, err, uh->dest,\n\t\t\t\t\t\t  ntohl(info), (u8 *)(uh+1));\n\t\tgoto out;\n\t}\n\n\tif (!inet6_test_bit(RECVERR6, sk)) {\n\t\tif (!harderr || sk->sk_state != TCP_ESTABLISHED)\n\t\t\tgoto out;\n\t} else {\n\t\tipv6_icmp_error(sk, skb, err, uh->dest, ntohl(info), (u8 *)(uh+1));\n\t}\n\n\tsk->sk_err = err;\n\tsk_error_report(sk);\nout:\n\treturn 0;\n}\n\nstatic int __udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\tif (!ipv6_addr_any(&sk->sk_v6_daddr)) {\n\t\tsock_rps_save_rxhash(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t\tsk_incoming_cpu_update(sk);\n\t} else {\n\t\tsk_mark_napi_id_once(sk, skb);\n\t}\n\n\trc = __udp_enqueue_schedule_skb(sk, skb);\n\tif (rc < 0) {\n\t\tint is_udplite = IS_UDPLITE(sk);\n\t\tenum skb_drop_reason drop_reason;\n\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM) {\n\t\t\tUDP6_INC_STATS(sock_net(sk),\n\t\t\t\t\t UDP_MIB_RCVBUFERRORS, is_udplite);\n\t\t\tdrop_reason = SKB_DROP_REASON_SOCKET_RCVBUFF;\n\t\t} else {\n\t\t\tUDP6_INC_STATS(sock_net(sk),\n\t\t\t\t       UDP_MIB_MEMERRORS, is_udplite);\n\t\t\tdrop_reason = SKB_DROP_REASON_PROTO_MEM;\n\t\t}\n\t\tUDP6_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\t\ttrace_udp_fail_queue_rcv_skb(rc, sk, skb);\n\t\tsk_skb_reason_drop(sk, skb, drop_reason);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic __inline__ int udpv6_err(struct sk_buff *skb,\n\t\t\t\tstruct inet6_skb_parm *opt, u8 type,\n\t\t\t\tu8 code, int offset, __be32 info)\n{\n\treturn __udp6_lib_err(skb, opt, type, code, offset, info,\n\t\t\t      dev_net(skb->dev)->ipv4.udp_table);\n}\n\nstatic int udpv6_queue_rcv_one_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tenum skb_drop_reason drop_reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tstruct udp_sock *up = udp_sk(sk);\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb)) {\n\t\tdrop_reason = SKB_DROP_REASON_XFRM_POLICY;\n\t\tgoto drop;\n\t}\n\tnf_reset_ct(skb);\n\n\tif (static_branch_unlikely(&udpv6_encap_needed_key) &&\n\t    READ_ONCE(up->encap_type)) {\n\t\tint (*encap_rcv)(struct sock *sk, struct sk_buff *skb);\n\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tencap_rcv = READ_ONCE(up->encap_rcv);\n\t\tif (encap_rcv) {\n\t\t\tint ret;\n\n\t\t\t/* Verify checksum before giving to encap */\n\t\t\tif (udp_lib_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tret = encap_rcv(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\t__UDP6_INC_STATS(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif (udp_test_bit(UDPLITE_RECV_CC, sk) && UDP_SKB_CB(skb)->partial_cov) {\n\t\tu16 pcrlen = READ_ONCE(up->pcrlen);\n\n\t\tif (pcrlen == 0) {          /* full coverage was set  */\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: partial coverage %d while full coverage %d requested\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov < pcrlen) {\n\t\t\tnet_dbg_ratelimited(\"UDPLITE6: coverage %d too small, need min %d\\n\",\n\t\t\t\t\t    UDP_SKB_CB(skb)->cscov, pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tprefetch(&sk->sk_rmem_alloc);\n\tif (rcu_access_pointer(sk->sk_filter) &&\n\t    udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (sk_filter_trim_cap(sk, skb, sizeof(struct udphdr), &drop_reason))\n\t\tgoto drop;\n\n\tudp_csum_pull_header(skb);\n\n\tskb_dst_drop(skb);\n\n\treturn __udpv6_queue_rcv_skb(sk, skb);\n\ncsum_error:\n\tdrop_reason = SKB_DROP_REASON_UDP_CSUM;\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_CSUMERRORS, is_udplite);\ndrop:\n\t__UDP6_INC_STATS(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tudp_drops_inc(sk);\n\tsk_skb_reason_drop(sk, skb, drop_reason);\n\treturn -1;\n}\n\nstatic int udpv6_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct sk_buff *next, *segs;\n\tint ret;\n\n\tif (likely(!udp_unexpected_gso(sk, skb)))\n\t\treturn udpv6_queue_rcv_one_skb(sk, skb);\n\n\t__skb_push(skb, -skb_mac_offset(skb));\n\tsegs = udp_rcv_segment(sk, skb, false);\n\tskb_list_walk_safe(segs, skb, next) {\n\t\t__skb_pull(skb, skb_transport_offset(skb));\n\n\t\tudp_post_segment_fix_csum(skb);\n\t\tret = udpv6_queue_rcv_one_skb(sk, skb);\n\t\tif (ret > 0)\n\t\t\tip6_protocol_deliver_rcu(dev_net(skb->dev), skb, ret,\n\t\t\t\t\t\t true);\n\t}\n\treturn 0;\n}\n\nstatic bool __udp_v6_is_mcast_sock(struct net *net, const struct sock *sk,\n\t\t\t\t   __be16 loc_port, const struct in6_addr *loc_addr,\n\t\t\t\t   __be16 rmt_port, const struct in6_addr *rmt_addr,\n\t\t\t\t   int dif, int sdif, unsigned short hnum)\n{\n\tconst struct inet_sock *inet = inet_sk(sk);\n\n\tif (!net_eq(sock_net(sk), net))\n\t\treturn false;\n\n\tif (udp_sk(sk)->udp_port_hash != hnum ||\n\t    sk->sk_family != PF_INET6 ||\n\t    (inet->inet_dport && inet->inet_dport != rmt_port) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_daddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_daddr, rmt_addr)) ||\n\t    !udp_sk_bound_dev_eq(net, READ_ONCE(sk->sk_bound_dev_if), dif, sdif) ||\n\t    (!ipv6_addr_any(&sk->sk_v6_rcv_saddr) &&\n\t\t    !ipv6_addr_equal(&sk->sk_v6_rcv_saddr, loc_addr)))\n\t\treturn false;\n\tif (!inet6_mc_check(sk, loc_addr, rmt_addr))\n\t\treturn false;\n\treturn true;\n}\n\nstatic void udp6_csum_zero_error(struct sk_buff *skb)\n{\n\t/* RFC 2460 section 8.1 says that we SHOULD log\n\t * this error. Well, it is reasonable.\n\t */\n\tnet_dbg_ratelimited(\"IPv6: udp checksum is 0 for [%pI6c]:%u->[%pI6c]:%u\\n\",\n\t\t\t    &ipv6_hdr(skb)->saddr, ntohs(udp_hdr(skb)->source),\n\t\t\t    &ipv6_hdr(skb)->daddr, ntohs(udp_hdr(skb)->dest));\n}\n\n/*\n * Note: called only from the BH handler context,\n * so we don't need to lock the hashes.\n */\nstatic int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr,\n\t\tstruct udp_table *udptable, int proto)\n{\n\tstruct sock *sk, *first = NULL;\n\tconst struct udphdr *uh = udp_hdr(skb);\n\tunsigned short hnum = ntohs(uh->dest);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, hnum);\n\tunsigned int offset = offsetof(typeof(*sk), sk_node);\n\tunsigned int hash2 = 0, hash2_any = 0, use_hash2 = (hslot->count > 10);\n\tint dif = inet6_iif(skb);\n\tint sdif = inet6_sdif(skb);\n\tstruct hlist_node *node;\n\tstruct sk_buff *nskb;\n\n\tif (use_hash2) {\n\t\thash2_any = ipv6_portaddr_hash(net, &in6addr_any, hnum) &\n\t\t\t    udptable->mask;\n\t\thash2 = ipv6_portaddr_hash(net, daddr, hnum) & udptable->mask;\nstart_lookup:\n\t\thslot = &udptable->hash2[hash2].hslot;\n\t\toffset = offsetof(typeof(*sk), __sk_common.skc_portaddr_node);\n\t}\n\n\tsk_for_each_entry_offset_rcu(sk, node, &hslot->head, offset) {\n\t\tif (!__udp_v6_is_mcast_sock(net, sk, uh->dest, daddr,\n\t\t\t\t\t    uh->source, saddr, dif, sdif,\n\t\t\t\t\t    hnum))\n\t\t\tcontinue;\n\t\t/* If zero checksum and no_check is not on for\n\t\t * the socket then skip it.\n\t\t */\n\t\tif (!uh->check && !udp_get_no_check6_rx(sk))\n\t\t\tcontinue;\n\t\tif (!first) {\n\t\t\tfirst = sk;\n\t\t\tcontinue;\n\t\t}\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (unlikely(!nskb)) {\n\t\t\tudp_drops_inc(sk);\n\t\t\t__UDP6_INC_STATS(net, UDP_MIB_RCVBUFERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t\t__UDP6_INC_STATS(net, UDP_MIB_INERRORS,\n\t\t\t\t\t IS_UDPLITE(sk));\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (udpv6_queue_rcv_skb(sk, nskb) > 0)\n\t\t\tconsume_skb(nskb);\n\t}\n\n\t/* Also lookup *:port if we are using hash2 and haven't done so yet. */\n\tif (use_hash2 && hash2 != hash2_any) {\n\t\thash2 = hash2_any;\n\t\tgoto start_lookup;\n\t}\n\n\tif (first) {\n\t\tif (udpv6_queue_rcv_skb(first, skb) > 0)\n\t\t\tconsume_skb(skb);\n\t} else {\n\t\tkfree_skb(skb);\n\t\t__UDP6_INC_STATS(net, UDP_MIB_IGNOREDMULTI,\n\t\t\t\t proto == IPPROTO_UDPLITE);\n\t}\n\treturn 0;\n}\n\nstatic void udp6_sk_rx_dst_set(struct sock *sk, struct dst_entry *dst)\n{\n\tif (udp_sk_rx_dst_set(sk, dst))\n\t\tsk->sk_rx_dst_cookie = rt6_get_cookie(dst_rt6_info(dst));\n}\n\n/* wrapper for udp_queue_rcv_skb taking care of csum conversion and\n * return code conversion for ip layer consumption\n */\nstatic int udp6_unicast_rcv_skb(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tstruct udphdr *uh)\n{\n\tint ret;\n\n\tif (inet_get_convert_csum(sk) && uh->check && !IS_UDPLITE(sk))\n\t\tskb_checksum_try_convert(skb, IPPROTO_UDP, ip6_compute_pseudo);\n\n\tret = udpv6_queue_rcv_skb(sk, skb);\n\n\t/* a return value > 0 means to resubmit the input */\n\tif (ret > 0)\n\t\treturn ret;\n\treturn 0;\n}\n\nint __udp6_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tenum skb_drop_reason reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tconst struct in6_addr *saddr, *daddr;\n\tstruct net *net = dev_net(skb->dev);\n\tstruct sock *sk = NULL;\n\tstruct udphdr *uh;\n\tbool refcounted;\n\tu32 ulen = 0;\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto discard;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = &ipv6_hdr(skb)->daddr;\n\tuh = udp_hdr(skb);\n\n\tulen = ntohs(uh->len);\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\n\t\t/* Check for jumbo payload */\n\t\tif (ulen == 0)\n\t\t\tulen = skb->len;\n\n\t\tif (ulen < sizeof(*uh))\n\t\t\tgoto short_packet;\n\n\t\tif (ulen < skb->len) {\n\t\t\tif (pskb_trim_rcsum(skb, ulen))\n\t\t\t\tgoto short_packet;\n\t\t\tsaddr = &ipv6_hdr(skb)->saddr;\n\t\t\tdaddr = &ipv6_hdr(skb)->daddr;\n\t\t\tuh = udp_hdr(skb);\n\t\t}\n\t}\n\n\tif (udp6_csum_init(skb, uh, proto))\n\t\tgoto csum_error;\n\n\t/* Check if the socket is already available, e.g. due to early demux */\n\tsk = inet6_steal_sock(net, skb, sizeof(struct udphdr), saddr, uh->source, daddr, uh->dest,\n\t\t\t      &refcounted, udp6_ehashfn);\n\tif (IS_ERR(sk))\n\t\tgoto no_sk;\n\n\tif (sk) {\n\t\tstruct dst_entry *dst = skb_dst(skb);\n\t\tint ret;\n\n\t\tif (unlikely(rcu_dereference(sk->sk_rx_dst) != dst))\n\t\t\tudp6_sk_rx_dst_set(sk, dst);\n\n\t\tif (!uh->check && !udp_get_no_check6_rx(sk)) {\n\t\t\tif (refcounted)\n\t\t\t\tsock_put(sk);\n\t\t\tgoto report_csum_error;\n\t\t}\n\n\t\tret = udp6_unicast_rcv_skb(sk, skb, uh);\n\t\tif (refcounted)\n\t\t\tsock_put(sk);\n\t\treturn ret;\n\t}\n\n\t/*\n\t *\tMulticast receive code\n\t */\n\tif (ipv6_addr_is_multicast(daddr))\n\t\treturn __udp6_lib_mcast_deliver(net, skb,\n\t\t\t\tsaddr, daddr, udptable, proto);\n\n\t/* Unicast */\n\tsk = __udp6_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\tif (sk) {\n\t\tif (!uh->check && !udp_get_no_check6_rx(sk))\n\t\t\tgoto report_csum_error;\n\t\treturn udp6_unicast_rcv_skb(sk, skb, uh);\n\t}\nno_sk:\n\treason = SKB_DROP_REASON_NO_SOCKET;\n\n\tif (!uh->check)\n\t\tgoto report_csum_error;\n\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\tgoto discard;\n\tnf_reset_ct(skb);\n\n\tif (udp_lib_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\t__UDP6_INC_STATS(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);\n\ticmpv6_send(skb, ICMPV6_DEST_UNREACH, ICMPV6_PORT_UNREACH, 0);\n\n\tsk_skb_reason_drop(sk, skb, reason);\n\treturn 0;\n\nshort_packet:\n\tif (reason == SKB_DROP_REASON_NOT_SPECIFIED)\n\t\treason = SKB_DROP_REASON_PKT_TOO_SMALL;\n\tnet_dbg_ratelimited(\"UDP%sv6: short packet: From [%pI6c]:%u %d/%d to [%pI6c]:%u\\n\",\n\t\t\t    proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t\t    saddr, ntohs(uh->source),\n\t\t\t    ulen, skb->len,\n\t\t\t    daddr, ntohs(uh->dest));\n\tgoto discard;\n\nreport_csum_error:\n\tudp6_csum_zero_error(skb);\ncsum_error:\n\tif (reason == SKB_DROP_REASON_NOT_SPECIFIED)\n\t\treason = SKB_DROP_REASON_UDP_CSUM;\n\t__UDP6_INC_STATS(net, UDP_MIB_CSUMERRORS, proto == IPPROTO_UDPLITE);\ndiscard:\n\t__UDP6_INC_STATS(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tsk_skb_reason_drop(sk, skb, reason);\n\treturn 0;\n}\n\n\nstatic struct sock *__udp6_lib_demux_lookup(struct net *net,\n\t\t\t__be16 loc_port, const struct in6_addr *loc_addr,\n\t\t\t__be16 rmt_port, const struct in6_addr *rmt_addr,\n\t\t\tint dif, int sdif)\n{\n\tstruct udp_table *udptable = net->ipv4.udp_table;\n\tunsigned short hnum = ntohs(loc_port);\n\tstruct udp_hslot *hslot2;\n\tunsigned int hash2;\n\t__portpair ports;\n\tstruct sock *sk;\n\n\thash2 = ipv6_portaddr_hash(net, loc_addr, hnum);\n\thslot2 = udp_hashslot2(udptable, hash2);\n\tports = INET_COMBINED_PORTS(rmt_port, hnum);\n\n\tudp_portaddr_for_each_entry_rcu(sk, &hslot2->head) {\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    inet6_match(net, sk, rmt_addr, loc_addr, ports, dif, sdif))\n\t\t\treturn sk;\n\t\t/* Only check first socket in chain */\n\t\tbreak;\n\t}\n\treturn NULL;\n}\n\nvoid udp_v6_early_demux(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tconst struct udphdr *uh;\n\tstruct sock *sk;\n\tstruct dst_entry *dst;\n\tint dif = skb->dev->ifindex;\n\tint sdif = inet6_sdif(skb);\n\n\tif (!pskb_may_pull(skb, skb_transport_offset(skb) +\n\t    sizeof(struct udphdr)))\n\t\treturn;\n\n\tuh = udp_hdr(skb);\n\n\tif (skb->pkt_type == PACKET_HOST)\n\t\tsk = __udp6_lib_demux_lookup(net, uh->dest,\n\t\t\t\t\t     &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t     uh->source, &ipv6_hdr(skb)->saddr,\n\t\t\t\t\t     dif, sdif);\n\telse\n\t\treturn;\n\n\tif (!sk)\n\t\treturn;\n\n\tskb->sk = sk;\n\tDEBUG_NET_WARN_ON_ONCE(sk_is_refcounted(sk));\n\tskb->destructor = sock_pfree;\n\tdst = rcu_dereference(sk->sk_rx_dst);\n\n\tif (dst)\n\t\tdst = dst_check(dst, sk->sk_rx_dst_cookie);\n\tif (dst) {\n\t\t/* set noref for now.\n\t\t * any place which wants to hold dst has to call\n\t\t * dst_hold_safe()\n\t\t */\n\t\tskb_dst_set_noref(skb, dst);\n\t}\n}\n\nINDIRECT_CALLABLE_SCOPE int udpv6_rcv(struct sk_buff *skb)\n{\n\treturn __udp6_lib_rcv(skb, dev_net(skb->dev)->ipv4.udp_table, IPPROTO_UDP);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nstatic void udp_v6_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending == AF_INET)\n\t\tudp_flush_pending_frames(sk);\n\telse if (up->pending) {\n\t\tup->len = 0;\n\t\tWRITE_ONCE(up->pending, 0);\n\t\tip6_flush_pending_frames(sk);\n\t}\n}\n\nstatic int udpv6_pre_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t     int addr_len)\n{\n\tif (addr_len < offsetofend(struct sockaddr, sa_family))\n\t\treturn -EINVAL;\n\t/* The following checks are replicated from __ip6_datagram_connect()\n\t * and intended to prevent BPF program called below from accessing\n\t * bytes that are out of the bound specified by user in addr_len.\n\t */\n\tif (uaddr->sa_family == AF_INET) {\n\t\tif (ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\treturn udp_pre_connect(sk, uaddr, addr_len);\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\treturn BPF_CGROUP_RUN_PROG_INET6_CONNECT_LOCK(sk, uaddr, &addr_len);\n}\n\nstatic int udpv6_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tint res;\n\n\tlock_sock(sk);\n\tres = __ip6_datagram_connect(sk, uaddr, addr_len);\n\tif (!res)\n\t\tudp6_hash4(sk);\n\trelease_sock(sk);\n\treturn res;\n}\n\n/**\n *\tudp6_hwcsum_outgoing  -  handle outgoing HW checksumming\n *\t@sk:\tsocket we are sending on\n *\t@skb:\tsk_buff containing the filled-in UDP header\n *\t\t(checksum field must be zeroed out)\n *\t@saddr: source address\n *\t@daddr: destination address\n *\t@len:\tlength of packet\n */\nstatic void udp6_hwcsum_outgoing(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t const struct in6_addr *saddr,\n\t\t\t\t const struct in6_addr *daddr, int len)\n{\n\tunsigned int offset;\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *frags = skb_shinfo(skb)->frag_list;\n\t__wsum csum = 0;\n\n\tif (!frags) {\n\t\t/* Only one fragment on the socket.  */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\toffset = skb_transport_offset(skb);\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\tcsum = skb->csum;\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tdo {\n\t\t\tcsum = csum_add(csum, frags->csum);\n\t\t} while ((frags = frags->next));\n\n\t\tuh->check = csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP,\n\t\t\t\t\t    csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\n/*\n *\tSending\n */\n\nstatic int udp_v6_send_skb(struct sk_buff *skb, struct flowi6 *fl6,\n\t\t\t   struct inet_cork *cork)\n{\n\tstruct sock *sk = skb->sk;\n\tstruct udphdr *uh;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\t__wsum csum = 0;\n\tint offset = skb_transport_offset(skb);\n\tint len = skb->len - offset;\n\tint datalen = len - sizeof(*uh);\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = fl6->fl6_sport;\n\tuh->dest = fl6->fl6_dport;\n\tuh->len = htons(len);\n\tuh->check = 0;\n\n\tif (cork->gso_size) {\n\t\tconst int hlen = skb_network_header_len(skb) +\n\t\t\t\t sizeof(struct udphdr);\n\n\t\tif (hlen + min(datalen, cork->gso_size) > cork->fragsize) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EMSGSIZE;\n\t\t}\n\t\tif (datalen > cork->gso_size * UDP_MAX_SEGMENTS) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (udp_get_no_check6_tx(sk)) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (is_udplite || dst_xfrm(skb_dst(skb))) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (datalen > cork->gso_size) {\n\t\t\tskb_shinfo(skb)->gso_size = cork->gso_size;\n\t\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP_L4;\n\t\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(datalen,\n\t\t\t\t\t\t\t\t cork->gso_size);\n\n\t\t\t/* Don't checksum the payload, skb will get segmented */\n\t\t\tgoto csum_partial;\n\t\t}\n\t}\n\n\tif (is_udplite)\n\t\tcsum = udplite_csum(skb);\n\telse if (udp_get_no_check6_tx(sk)) {   /* UDP csum disabled */\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tgoto send;\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\ncsum_partial:\n\t\tudp6_hwcsum_outgoing(sk, skb, &fl6->saddr, &fl6->daddr, len);\n\t\tgoto send;\n\t} else\n\t\tcsum = udp_csum(skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t\t    len, fl6->flowi6_proto, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip6_send_skb(skb);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet6_test_bit(RECVERR6, sk)) {\n\t\t\tUDP6_INC_STATS(sock_net(sk),\n\t\t\t\t       UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else {\n\t\tUDP6_INC_STATS(sock_net(sk),\n\t\t\t       UDP_MIB_OUTDATAGRAMS, is_udplite);\n\t}\n\treturn err;\n}\n\nstatic int udp_v6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct udp_sock  *up = udp_sk(sk);\n\tint err = 0;\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_push_pending_frames(sk);\n\n\tskb = ip6_finish_skb(sk);\n\tif (!skb)\n\t\tgoto out;\n\n\terr = udp_v6_send_skb(skb, &inet_sk(sk)->cork.fl.u.ip6,\n\t\t\t      &inet_sk(sk)->cork.base);\nout:\n\tup->len = 0;\n\tWRITE_ONCE(up->pending, 0);\n\treturn err;\n}\n\nint udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct inet_cork_full cork;\n\tstruct flowi6 *fl6 = &cork.fl.u.ip6;\n\tstruct dst_entry *dst;\n\tstruct ipcm6_cookie ipc6;\n\tint addr_len = msg->msg_namelen;\n\tbool connected = false;\n\tint ulen = len;\n\tint corkreq = udp_test_bit(CORK, sk) || msg->msg_flags & MSG_MORE;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\tipcm6_init_sk(&ipc6, sk);\n\tipc6.gso_size = READ_ONCE(up->gso_size);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tif (ipv6_addr_any(daddr) &&\n\t\t\t    ipv6_addr_v4mapped(&np->saddr))\n\t\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t\t       daddr);\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!READ_ONCE(up->pending)) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\terr = ipv6_only_sock(sk) ?\n\t\t\t\t-ENETUNREACH : udp_sendmsg(sk, msg, len);\n\t\t\tmsg->msg_name = sin6;\n\t\t\tmsg->msg_namelen = addr_len;\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\tif (READ_ONCE(up->pending)) {\n\t\tif (READ_ONCE(up->pending) == AF_INET)\n\t\t\treturn udp_sendmsg(sk, msg, len);\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(fl6, 0, sizeof(*fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6->fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (inet6_test_bit(SNDFLOW, sk)) {\n\t\t\tfl6->flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6->flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6->flowlabel);\n\t\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    __ipv6_addr_needs_scope_id(__ipv6_addr_type(daddr)))\n\t\t\tfl6->flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6->fl6_dport = inet->inet_dport;\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6->flowlabel = np->flow_label;\n\t\tconnected = true;\n\t}\n\n\tif (!fl6->flowi6_oif)\n\t\tfl6->flowi6_oif = READ_ONCE(sk->sk_bound_dev_if);\n\n\tif (!fl6->flowi6_oif)\n\t\tfl6->flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6->flowi6_uid = sk_uid(sk);\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\t\tipc6.opt = opt;\n\n\t\terr = udp_cmsg_send(sk, msg, &ipc6.gso_size);\n\t\tif (err > 0) {\n\t\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, fl6,\n\t\t\t\t\t\t    &ipc6);\n\t\t\tconnected = false;\n\t\t}\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6->flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6->flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\tipc6.opt = opt;\n\n\tfl6->flowi6_proto = sk->sk_protocol;\n\tfl6->flowi6_mark = ipc6.sockc.mark;\n\tfl6->daddr = *daddr;\n\tif (ipv6_addr_any(&fl6->saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6->saddr = np->saddr;\n\tfl6->fl6_sport = inet->inet_sport;\n\n\tif (cgroup_bpf_enabled(CGROUP_UDP6_SENDMSG) && !connected) {\n\t\terr = BPF_CGROUP_RUN_PROG_UDP6_SENDMSG_LOCK(sk,\n\t\t\t\t\t   (struct sockaddr *)sin6,\n\t\t\t\t\t   &addr_len,\n\t\t\t\t\t   &fl6->saddr);\n\t\tif (err)\n\t\t\tgoto out_no_dst;\n\t\tif (sin6) {\n\t\t\tif (ipv6_addr_v4mapped(&sin6->sin6_addr)) {\n\t\t\t\t/* BPF program rewrote IPv6-only by IPv4-mapped\n\t\t\t\t * IPv6. It's currently unsupported.\n\t\t\t\t */\n\t\t\t\terr = -ENOTSUPP;\n\t\t\t\tgoto out_no_dst;\n\t\t\t}\n\t\t\tif (sin6->sin6_port == 0) {\n\t\t\t\t/* BPF program set invalid port. Reject it. */\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_no_dst;\n\t\t\t}\n\t\t\tfl6->fl6_dport = sin6->sin6_port;\n\t\t\tfl6->daddr = sin6->sin6_addr;\n\t\t}\n\t}\n\n\tif (ipv6_addr_any(&fl6->daddr))\n\t\tfl6->daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\n\tfinal_p = fl6_update_dst(fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = false;\n\n\tif (!fl6->flowi6_oif && ipv6_addr_is_multicast(&fl6->daddr)) {\n\t\tfl6->flowi6_oif = READ_ONCE(np->mcast_oif);\n\t\tconnected = false;\n\t} else if (!fl6->flowi6_oif)\n\t\tfl6->flowi6_oif = READ_ONCE(np->ucast_oif);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(fl6));\n\n\tfl6->flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6->flowlabel);\n\n\tdst = ip6_sk_dst_lookup_flow(sk, fl6, final_p, connected);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (ipc6.hlimit < 0)\n\t\tipc6.hlimit = ip6_sk_dst_hoplimit(np, fl6, dst);\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\t/* Lockless fast path for the non-corking case */\n\tif (!corkreq) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = ip6_make_skb(sk, getfrag, msg, ulen,\n\t\t\t\t   sizeof(struct udphdr), &ipc6,\n\t\t\t\t   dst_rt6_info(dst),\n\t\t\t\t   msg->msg_flags, &cork);\n\t\terr = PTR_ERR(skb);\n\t\tif (!IS_ERR_OR_NULL(skb))\n\t\t\terr = udp_v6_send_skb(skb, fl6, &cork.base);\n\t\t/* ip6_make_skb steals dst reference */\n\t\tgoto out_no_dst;\n\t}\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tnet_dbg_ratelimited(\"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tWRITE_ONCE(up->pending, AF_INET6);\n\ndo_append_data:\n\tup->len += ulen;\n\terr = ip6_append_data(sk, getfrag, msg, ulen, sizeof(struct udphdr),\n\t\t\t      &ipc6, fl6, dst_rt6_info(dst),\n\t\t\t      corkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tWRITE_ONCE(up->pending, 0);\n\n\tif (err > 0)\n\t\terr = inet6_test_bit(RECVERR6, sk) ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\n\nout:\n\tdst_release(dst);\nout_no_dst:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS(sock_net(sk),\n\t\t\t       UDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(dst, &fl6->daddr);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\nEXPORT_SYMBOL(udpv6_sendmsg);\n\nstatic void udpv6_splice_eof(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (!READ_ONCE(up->pending) || udp_test_bit(CORK, sk))\n\t\treturn;\n\n\tlock_sock(sk);\n\tif (up->pending && !udp_test_bit(CORK, sk))\n\t\tudp_v6_push_pending_frames(sk);\n\trelease_sock(sk);\n}\n\nvoid udpv6_destroy_sock(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tlock_sock(sk);\n\n\t/* protects from races with udp_abort() */\n\tsock_set_flag(sk, SOCK_DEAD);\n\tudp_v6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tif (static_branch_unlikely(&udpv6_encap_needed_key)) {\n\t\tif (up->encap_type) {\n\t\t\tvoid (*encap_destroy)(struct sock *sk);\n\t\t\tencap_destroy = READ_ONCE(up->encap_destroy);\n\t\t\tif (encap_destroy)\n\t\t\t\tencap_destroy(sk);\n\t\t}\n\t\tif (udp_test_bit(ENCAP_ENABLED, sk)) {\n\t\t\tstatic_branch_dec(&udpv6_encap_needed_key);\n\t\t\tudp_encap_disable();\n\t\t\tudp_tunnel_cleanup_gro(sk);\n\t\t}\n\t}\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udpv6_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t     unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE || level == SOL_SOCKET)\n\t\treturn udp_lib_setsockopt(sk, level, optname,\n\t\t\t\t\t  optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\nint udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\nint udp6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq, IPV6_SEQ_DGRAM_HEADER);\n\t} else {\n\t\tint bucket = ((struct udp_iter_state *)seq->private)->bucket;\n\t\tconst struct inet_sock *inet = inet_sk((const struct sock *)v);\n\t\t__u16 srcp = ntohs(inet->inet_sport);\n\t\t__u16 destp = ntohs(inet->inet_dport);\n\t\t__ip6_dgram_sock_seq_show(seq, v, srcp, destp,\n\t\t\t\t\t  udp_rqueue_get(v), bucket);\n\t}\n\treturn 0;\n}\n\nconst struct seq_operations udp6_seq_ops = {\n\t.start\t\t= udp_seq_start,\n\t.next\t\t= udp_seq_next,\n\t.stop\t\t= udp_seq_stop,\n\t.show\t\t= udp6_seq_show,\n};\nEXPORT_SYMBOL(udp6_seq_ops);\n\nstatic struct udp_seq_afinfo udp6_seq_afinfo = {\n\t.family\t\t= AF_INET6,\n\t.udp_table\t= NULL,\n};\n\nint __net_init udp6_proc_init(struct net *net)\n{\n\tif (!proc_create_net_data(\"udp6\", 0444, net->proc_net, &udp6_seq_ops,\n\t\t\tsizeof(struct udp_iter_state), &udp6_seq_afinfo))\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid udp6_proc_exit(struct net *net)\n{\n\tremove_proc_entry(\"udp6\", net->proc_net);\n}\n#endif /* CONFIG_PROC_FS */\n\n/* ------------------------------------------------------------------------ */\n\nstruct proto udpv6_prot = {\n\t.name\t\t\t= \"UDPv6\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.close\t\t\t= udp_lib_close,\n\t.pre_connect\t\t= udpv6_pre_connect,\n\t.connect\t\t= udpv6_connect,\n\t.disconnect\t\t= udp_disconnect,\n\t.ioctl\t\t\t= udp_ioctl,\n\t.init\t\t\t= udpv6_init_sock,\n\t.destroy\t\t= udpv6_destroy_sock,\n\t.setsockopt\t\t= udpv6_setsockopt,\n\t.getsockopt\t\t= udpv6_getsockopt,\n\t.sendmsg\t\t= udpv6_sendmsg,\n\t.recvmsg\t\t= udpv6_recvmsg,\n\t.splice_eof\t\t= udpv6_splice_eof,\n\t.release_cb\t\t= ip6_datagram_release_cb,\n\t.hash\t\t\t= udp_lib_hash,\n\t.unhash\t\t\t= udp_lib_unhash,\n\t.rehash\t\t\t= udp_v6_rehash,\n\t.get_port\t\t= udp_v6_get_port,\n\t.put_port\t\t= udp_lib_unhash,\n#ifdef CONFIG_BPF_SYSCALL\n\t.psock_update_sk_prot\t= udp_bpf_update_proto,\n#endif\n\n\t.memory_allocated\t= &net_aligned_data.udp_memory_allocated,\n\t.per_cpu_fw_alloc\t= &udp_memory_per_cpu_fw_alloc,\n\n\t.sysctl_mem\t\t= sysctl_udp_mem,\n\t.sysctl_wmem_offset     = offsetof(struct net, ipv4.sysctl_udp_wmem_min),\n\t.sysctl_rmem_offset     = offsetof(struct net, ipv4.sysctl_udp_rmem_min),\n\t.obj_size\t\t= sizeof(struct udp6_sock),\n\t.ipv6_pinfo_offset = offsetof(struct udp6_sock, inet6),\n\t.h.udp_table\t\t= NULL,\n\t.diag_destroy\t\t= udp_abort,\n};\n\nstatic struct inet_protosw udpv6_protosw = {\n\t.type =      SOCK_DGRAM,\n\t.protocol =  IPPROTO_UDP,\n\t.prot =      &udpv6_prot,\n\t.ops =       &inet6_dgram_ops,\n\t.flags =     INET_PROTOSW_PERMANENT,\n};\n\nint __init udpv6_init(void)\n{\n\tint ret;\n\n\tnet_hotdata.udpv6_protocol = (struct inet6_protocol) {\n\t\t.handler     = udpv6_rcv,\n\t\t.err_handler = udpv6_err,\n\t\t.flags\t     = INET6_PROTO_NOPOLICY | INET6_PROTO_FINAL,\n\t};\n\tret = inet6_add_protocol(&net_hotdata.udpv6_protocol, IPPROTO_UDP);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_register_protosw(&udpv6_protosw);\n\tif (ret)\n\t\tgoto out_udpv6_protocol;\nout:\n\treturn ret;\n\nout_udpv6_protocol:\n\tinet6_del_protocol(&net_hotdata.udpv6_protocol, IPPROTO_UDP);\n\tgoto out;\n}\n\nvoid udpv6_exit(void)\n{\n\tinet6_unregister_protosw(&udpv6_protosw);\n\tinet6_del_protocol(&net_hotdata.udpv6_protocol, IPPROTO_UDP);\n}\n", "patch": "@@ -1110,6 +1110,7 @@ int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \tDECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n \tstruct ipv6_txoptions *opt = NULL;\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ip6_flowlabel *flowlabel = NULL;\n \tstruct flowi6 fl6;\n \tstruct dst_entry *dst;\n@@ -1263,8 +1264,10 @@ int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\t\topt = NULL;\n \t\tconnected = 0;\n \t}\n-\tif (!opt)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -1373,6 +1376,7 @@ int udpv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n out:\n \tdst_release(dst);\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \tif (!err)\n \t\treturn len;\n \t/*", "file_path": "files/2016_8\\93", "file_language": "c", "file_name": "net/ipv6/udp.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/45f6fad84cc305103b28d73482b344d7f5b76f39/net/l2tp/l2tp_ip6.c", "code": "/*\n * L2TPv3 IP encapsulation support for IPv6\n *\n * Copyright (c) 2012 Katalix Systems Ltd\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/icmp.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/random.h>\n#include <linux/socket.h>\n#include <linux/l2tp.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/udp.h>\n#include <net/inet_common.h>\n#include <net/inet_hashtables.h>\n#include <net/tcp_states.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n\n#include <net/transp_v6.h>\n#include <net/addrconf.h>\n#include <net/ip6_route.h>\n\n#include \"l2tp_core.h\"\n\nstruct l2tp_ip6_sock {\n\t/* inet_sock has to be the first member of l2tp_ip6_sock */\n\tstruct inet_sock\tinet;\n\n\tu32\t\t\tconn_id;\n\tu32\t\t\tpeer_conn_id;\n\n\t/* ipv6_pinfo has to be the last member of l2tp_ip6_sock, see\n\t   inet6_sk_generic */\n\tstruct ipv6_pinfo\tinet6;\n};\n\nstatic DEFINE_RWLOCK(l2tp_ip6_lock);\nstatic struct hlist_head l2tp_ip6_table;\nstatic struct hlist_head l2tp_ip6_bind_table;\n\nstatic inline struct l2tp_ip6_sock *l2tp_ip6_sk(const struct sock *sk)\n{\n\treturn (struct l2tp_ip6_sock *)sk;\n}\n\nstatic struct sock *__l2tp_ip6_bind_lookup(struct net *net,\n\t\t\t\t\t   struct in6_addr *laddr,\n\t\t\t\t\t   int dif, u32 tunnel_id)\n{\n\tstruct sock *sk;\n\n\tsk_for_each_bound(sk, &l2tp_ip6_bind_table) {\n\t\tconst struct in6_addr *addr = inet6_rcv_saddr(sk);\n\t\tstruct l2tp_ip6_sock *l2tp = l2tp_ip6_sk(sk);\n\n\t\tif (l2tp == NULL)\n\t\t\tcontinue;\n\n\t\tif ((l2tp->conn_id == tunnel_id) &&\n\t\t    net_eq(sock_net(sk), net) &&\n\t\t    !(addr && ipv6_addr_equal(addr, laddr)) &&\n\t\t    !(sk->sk_bound_dev_if && sk->sk_bound_dev_if != dif))\n\t\t\tgoto found;\n\t}\n\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\nstatic inline struct sock *l2tp_ip6_bind_lookup(struct net *net,\n\t\t\t\t\t\tstruct in6_addr *laddr,\n\t\t\t\t\t\tint dif, u32 tunnel_id)\n{\n\tstruct sock *sk = __l2tp_ip6_bind_lookup(net, laddr, dif, tunnel_id);\n\tif (sk)\n\t\tsock_hold(sk);\n\n\treturn sk;\n}\n\n/* When processing receive frames, there are two cases to\n * consider. Data frames consist of a non-zero session-id and an\n * optional cookie. Control frames consist of a regular L2TP header\n * preceded by 32-bits of zeros.\n *\n * L2TPv3 Session Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                           Session ID                          |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Cookie (optional, maximum 64 bits)...\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *                                                                 |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * L2TPv3 Control Message Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                      (32 bits of zeros)                       |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |T|L|x|x|S|x|x|x|x|x|x|x|  Ver  |             Length            |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                     Control Connection ID                     |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Ns              |               Nr              |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * All control frames are passed to userspace.\n */\nstatic int l2tp_ip6_recv(struct sk_buff *skb)\n{\n\tstruct sock *sk;\n\tu32 session_id;\n\tu32 tunnel_id;\n\tunsigned char *ptr, *optr;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel = NULL;\n\tint length;\n\n\t/* Point to L2TP header */\n\toptr = ptr = skb->data;\n\n\tif (!pskb_may_pull(skb, 4))\n\t\tgoto discard;\n\n\tsession_id = ntohl(*((__be32 *) ptr));\n\tptr += 4;\n\n\t/* RFC3931: L2TP/IP packets have the first 4 bytes containing\n\t * the session_id. If it is 0, the packet is a L2TP control\n\t * frame and the session_id value can be discarded.\n\t */\n\tif (session_id == 0) {\n\t\t__skb_pull(skb, 4);\n\t\tgoto pass_up;\n\t}\n\n\t/* Ok, this is a data packet. Lookup the session. */\n\tsession = l2tp_session_find(&init_net, NULL, session_id);\n\tif (session == NULL)\n\t\tgoto discard;\n\n\ttunnel = session->tunnel;\n\tif (tunnel == NULL)\n\t\tgoto discard;\n\n\t/* Trace packet contents, if enabled */\n\tif (tunnel->debug & L2TP_MSG_DATA) {\n\t\tlength = min(32u, skb->len);\n\t\tif (!pskb_may_pull(skb, length))\n\t\t\tgoto discard;\n\n\t\tpr_debug(\"%s: ip recv\\n\", tunnel->name);\n\t\tprint_hex_dump_bytes(\"\", DUMP_PREFIX_OFFSET, ptr, length);\n\t}\n\n\tl2tp_recv_common(session, skb, ptr, optr, 0, skb->len,\n\t\t\t tunnel->recv_payload_hook);\n\treturn 0;\n\npass_up:\n\t/* Get the tunnel_id from the L2TP header */\n\tif (!pskb_may_pull(skb, 12))\n\t\tgoto discard;\n\n\tif ((skb->data[0] & 0xc0) != 0xc0)\n\t\tgoto discard;\n\n\ttunnel_id = ntohl(*(__be32 *) &skb->data[4]);\n\ttunnel = l2tp_tunnel_find(&init_net, tunnel_id);\n\tif (tunnel != NULL)\n\t\tsk = tunnel->sock;\n\telse {\n\t\tstruct ipv6hdr *iph = ipv6_hdr(skb);\n\n\t\tread_lock_bh(&l2tp_ip6_lock);\n\t\tsk = __l2tp_ip6_bind_lookup(&init_net, &iph->daddr,\n\t\t\t\t\t    0, tunnel_id);\n\t\tread_unlock_bh(&l2tp_ip6_lock);\n\t}\n\n\tif (sk == NULL)\n\t\tgoto discard;\n\n\tsock_hold(sk);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_put;\n\n\tnf_reset(skb);\n\n\treturn sk_receive_skb(sk, skb, 1);\n\ndiscard_put:\n\tsock_put(sk);\n\ndiscard:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int l2tp_ip6_open(struct sock *sk)\n{\n\t/* Prevent autobind. We don't have ports. */\n\tinet_sk(sk)->inet_num = IPPROTO_L2TP;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_node(sk, &l2tp_ip6_table);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\treturn 0;\n}\n\nstatic void l2tp_ip6_close(struct sock *sk, long timeout)\n{\n\twrite_lock_bh(&l2tp_ip6_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsk_common_release(sk);\n}\n\nstatic void l2tp_ip6_destroy_sock(struct sock *sk)\n{\n\tstruct l2tp_tunnel *tunnel = l2tp_sock_to_tunnel(sk);\n\n\tlock_sock(sk);\n\tip6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tif (tunnel) {\n\t\tl2tp_tunnel_closeall(tunnel);\n\t\tsock_put(sk);\n\t}\n\n\tinet6_destroy_sock(sk);\n}\n\nstatic int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *) uaddr;\n\t__be32 v4addr = 0;\n\tint addr_type;\n\tint err;\n\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\treturn -EINVAL;\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = -EADDRINUSE;\n\tread_lock_bh(&l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(&init_net, &addr->l2tp_addr,\n\t\t\t\t   sk->sk_bound_dev_if, addr->l2tp_conn_id))\n\t\tgoto out_in_use;\n\tread_unlock_bh(&l2tp_ip6_lock);\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t\t    addr->l2tp_scope_id) {\n\t\t\t\t/* Override any existing binding, if another\n\t\t\t\t * one is supplied by user.\n\t\t\t\t */\n\t\t\t\tsk->sk_bound_dev_if = addr->l2tp_scope_id;\n\t\t\t}\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t   interface */\n\t\t\tif (!sk->sk_bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk),\n\t\t\t\t\t\t   sk->sk_bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\tinet->inet_rcv_saddr = inet->inet_saddr = v4addr;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\treturn err;\n\nout_in_use:\n\tread_unlock_bh(&l2tp_ip6_lock);\n\treturn err;\n}\n\nstatic int l2tp_ip6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t    int addr_len)\n{\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *) uaddr;\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct in6_addr\t*daddr;\n\tint\taddr_type;\n\tint rc;\n\n\tif (sock_flag(sk, SOCK_ZAPPED)) /* Must bind first - autobinding does not work */\n\t\treturn -EINVAL;\n\n\tif (addr_len < sizeof(*lsa))\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EINVAL;\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tdaddr = &usin->sin6_addr;\n\t\tif (ipv4_is_multicast(daddr->s6_addr32[3]))\n\t\t\treturn -EINVAL;\n\t}\n\n\trc = ip6_datagram_connect(sk, uaddr, addr_len);\n\n\tlock_sock(sk);\n\n\tl2tp_ip6_sk(sk)->peer_conn_id = lsa->l2tp_conn_id;\n\n\twrite_lock_bh(&l2tp_ip6_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\tsk_add_bind_node(sk, &l2tp_ip6_bind_table);\n\twrite_unlock_bh(&l2tp_ip6_lock);\n\n\trelease_sock(sk);\n\n\treturn rc;\n}\n\nstatic int l2tp_ip6_disconnect(struct sock *sk, int flags)\n{\n\tif (sock_flag(sk, SOCK_ZAPPED))\n\t\treturn 0;\n\n\treturn udp_disconnect(sk, flags);\n}\n\nstatic int l2tp_ip6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int *uaddr_len, int peer)\n{\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct l2tp_ip6_sock *lsk = l2tp_ip6_sk(sk);\n\n\tlsa->l2tp_family = AF_INET6;\n\tlsa->l2tp_flowinfo = 0;\n\tlsa->l2tp_scope_id = 0;\n\tlsa->l2tp_unused = 0;\n\tif (peer) {\n\t\tif (!lsk->peer_conn_id)\n\t\t\treturn -ENOTCONN;\n\t\tlsa->l2tp_conn_id = lsk->peer_conn_id;\n\t\tlsa->l2tp_addr = sk->sk_v6_daddr;\n\t\tif (np->sndflow)\n\t\t\tlsa->l2tp_flowinfo = np->flow_label;\n\t} else {\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\t\tlsa->l2tp_addr = np->saddr;\n\t\telse\n\t\t\tlsa->l2tp_addr = sk->sk_v6_rcv_saddr;\n\n\t\tlsa->l2tp_conn_id = lsk->conn_id;\n\t}\n\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tlsa->l2tp_scope_id = sk->sk_bound_dev_if;\n\t*uaddr_len = sizeof(*lsa);\n\treturn 0;\n}\n\nstatic int l2tp_ip6_backlog_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\t/* Charge it to the socket, dropping if the queue is full. */\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0)\n\t\tgoto drop;\n\n\treturn 0;\n\ndrop:\n\tIP_INC_STATS(&init_net, IPSTATS_MIB_INDISCARDS);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic int l2tp_ip6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\t__be32 *transhdr = NULL;\n\tint err = 0;\n\n\tskb = skb_peek(&sk->sk_write_queue);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\ttranshdr = (__be32 *)skb_transport_header(skb);\n\t*transhdr = 0;\n\n\terr = ip6_push_pending_frames(sk);\n\nout:\n\treturn err;\n}\n\n/* Userspace will call sendmsg() on the tunnel socket to send L2TP\n * control frames.\n */\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tint addr_len = msg->msg_namelen;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen = len + transhdrlen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, opt,\n\t\t\t\t\t    &hlimit, &tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->ucast_oif;\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_dst_lookup_flow(sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0)\n\t\thlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, hlimit, tclass, opt,\n\t\t\t      &fl6, (struct rt6_info *)dst,\n\t\t\t      msg->msg_flags, dontfrag);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n\nstatic int l2tp_ip6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t    int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (addr_len)\n\t\t*addr_len = sizeof(*lsa);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (lsa) {\n\t\tlsa->l2tp_family = AF_INET6;\n\t\tlsa->l2tp_unused = 0;\n\t\tlsa->l2tp_addr = ipv6_hdr(skb)->saddr;\n\t\tlsa->l2tp_flowinfo = 0;\n\t\tlsa->l2tp_scope_id = 0;\n\t\tlsa->l2tp_conn_id = 0;\n\t\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tlsa->l2tp_scope_id = inet6_iif(skb);\n\t}\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}\n\nstatic struct proto l2tp_ip6_prot = {\n\t.name\t\t   = \"L2TP/IPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.init\t\t   = l2tp_ip6_open,\n\t.close\t\t   = l2tp_ip6_close,\n\t.bind\t\t   = l2tp_ip6_bind,\n\t.connect\t   = l2tp_ip6_connect,\n\t.disconnect\t   = l2tp_ip6_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = l2tp_ip6_destroy_sock,\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.sendmsg\t   = l2tp_ip6_sendmsg,\n\t.recvmsg\t   = l2tp_ip6_recvmsg,\n\t.backlog_rcv\t   = l2tp_ip6_backlog_recv,\n\t.hash\t\t   = inet_hash,\n\t.unhash\t\t   = inet_unhash,\n\t.obj_size\t   = sizeof(struct l2tp_ip6_sock),\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_ipv6_setsockopt,\n\t.compat_getsockopt = compat_ipv6_getsockopt,\n#endif\n};\n\nstatic const struct proto_ops l2tp_ip6_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = l2tp_ip6_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet6_ioctl,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n\t.sendpage\t   = sock_no_sendpage,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_sock_common_setsockopt,\n\t.compat_getsockopt = compat_sock_common_getsockopt,\n#endif\n};\n\nstatic struct inet_protosw l2tp_ip6_protosw = {\n\t.type\t\t= SOCK_DGRAM,\n\t.protocol\t= IPPROTO_L2TP,\n\t.prot\t\t= &l2tp_ip6_prot,\n\t.ops\t\t= &l2tp_ip6_ops,\n};\n\nstatic struct inet6_protocol l2tp_ip6_protocol __read_mostly = {\n\t.handler\t= l2tp_ip6_recv,\n};\n\nstatic int __init l2tp_ip6_init(void)\n{\n\tint err;\n\n\tpr_info(\"L2TP IP encapsulation support for IPv6 (L2TPv3)\\n\");\n\n\terr = proto_register(&l2tp_ip6_prot, 1);\n\tif (err != 0)\n\t\tgoto out;\n\n\terr = inet6_add_protocol(&l2tp_ip6_protocol, IPPROTO_L2TP);\n\tif (err)\n\t\tgoto out1;\n\n\tinet6_register_protosw(&l2tp_ip6_protosw);\n\treturn 0;\n\nout1:\n\tproto_unregister(&l2tp_ip6_prot);\nout:\n\treturn err;\n}\n\nstatic void __exit l2tp_ip6_exit(void)\n{\n\tinet6_unregister_protosw(&l2tp_ip6_protosw);\n\tinet6_del_protocol(&l2tp_ip6_protocol, IPPROTO_L2TP);\n\tproto_unregister(&l2tp_ip6_prot);\n}\n\nmodule_init(l2tp_ip6_init);\nmodule_exit(l2tp_ip6_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Chris Elston <celston@katalix.com>\");\nMODULE_DESCRIPTION(\"L2TP IP encapsulation for IPv6\");\nMODULE_VERSION(\"1.0\");\n\n/* Use the value of SOCK_DGRAM (2) directory, because __stringify doesn't like\n * enums\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 2, IPPROTO_L2TP);\nMODULE_ALIAS_NET_PF_PROTO(PF_INET6, IPPROTO_L2TP);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* L2TPv3 IP encapsulation support for IPv6\n *\n * Copyright (c) 2012 Katalix Systems Ltd\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/icmp.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/random.h>\n#include <linux/socket.h>\n#include <linux/l2tp.h>\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <net/sock.h>\n#include <net/ip.h>\n#include <net/icmp.h>\n#include <net/udp.h>\n#include <net/inet_common.h>\n#include <net/tcp_states.h>\n#include <net/protocol.h>\n#include <net/xfrm.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n#include <net/transp_v6.h>\n#include <net/addrconf.h>\n#include <net/ip6_route.h>\n\n#include \"l2tp_core.h\"\n\n/* per-net private data for this module */\nstatic unsigned int l2tp_ip6_net_id;\nstruct l2tp_ip6_net {\n\trwlock_t l2tp_ip6_lock;\n\tstruct hlist_head l2tp_ip6_table;\n\tstruct hlist_head l2tp_ip6_bind_table;\n};\n\nstruct l2tp_ip6_sock {\n\t/* inet_sock has to be the first member of l2tp_ip6_sock */\n\tstruct inet_sock\tinet;\n\n\tu32\t\t\tconn_id;\n\tu32\t\t\tpeer_conn_id;\n\n\tstruct ipv6_pinfo\tinet6;\n};\n\nstatic struct l2tp_ip6_sock *l2tp_ip6_sk(const struct sock *sk)\n{\n\treturn (struct l2tp_ip6_sock *)sk;\n}\n\nstatic struct l2tp_ip6_net *l2tp_ip6_pernet(const struct net *net)\n{\n\treturn net_generic(net, l2tp_ip6_net_id);\n}\n\nstatic struct sock *__l2tp_ip6_bind_lookup(const struct net *net,\n\t\t\t\t\t   const struct in6_addr *laddr,\n\t\t\t\t\t   const struct in6_addr *raddr,\n\t\t\t\t\t   int dif, u32 tunnel_id)\n{\n\tstruct l2tp_ip6_net *pn = l2tp_ip6_pernet(net);\n\tstruct sock *sk;\n\n\tsk_for_each_bound(sk, &pn->l2tp_ip6_bind_table) {\n\t\tconst struct in6_addr *sk_laddr = inet6_rcv_saddr(sk);\n\t\tconst struct in6_addr *sk_raddr = &sk->sk_v6_daddr;\n\t\tconst struct l2tp_ip6_sock *l2tp = l2tp_ip6_sk(sk);\n\t\tint bound_dev_if;\n\n\t\tif (!net_eq(sock_net(sk), net))\n\t\t\tcontinue;\n\n\t\tbound_dev_if = READ_ONCE(sk->sk_bound_dev_if);\n\t\tif (bound_dev_if && dif && bound_dev_if != dif)\n\t\t\tcontinue;\n\n\t\tif (sk_laddr && !ipv6_addr_any(sk_laddr) &&\n\t\t    !ipv6_addr_any(laddr) && !ipv6_addr_equal(sk_laddr, laddr))\n\t\t\tcontinue;\n\n\t\tif (!ipv6_addr_any(sk_raddr) && raddr &&\n\t\t    !ipv6_addr_any(raddr) && !ipv6_addr_equal(sk_raddr, raddr))\n\t\t\tcontinue;\n\n\t\tif (l2tp->conn_id != tunnel_id)\n\t\t\tcontinue;\n\n\t\tgoto found;\n\t}\n\n\tsk = NULL;\nfound:\n\treturn sk;\n}\n\n/* When processing receive frames, there are two cases to\n * consider. Data frames consist of a non-zero session-id and an\n * optional cookie. Control frames consist of a regular L2TP header\n * preceded by 32-bits of zeros.\n *\n * L2TPv3 Session Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                           Session ID                          |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Cookie (optional, maximum 64 bits)...\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *                                                                 |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * L2TPv3 Control Message Header Over IP\n *\n *  0                   1                   2                   3\n *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                      (32 bits of zeros)                       |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |T|L|x|x|S|x|x|x|x|x|x|x|  Ver  |             Length            |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |                     Control Connection ID                     |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n * |               Ns              |               Nr              |\n * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n *\n * All control frames are passed to userspace.\n */\nstatic int l2tp_ip6_recv(struct sk_buff *skb)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct l2tp_ip6_net *pn;\n\tstruct sock *sk;\n\tu32 session_id;\n\tu32 tunnel_id;\n\tunsigned char *ptr, *optr;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel = NULL;\n\tstruct ipv6hdr *iph;\n\n\tpn = l2tp_ip6_pernet(net);\n\n\tif (!pskb_may_pull(skb, 4))\n\t\tgoto discard;\n\n\t/* Point to L2TP header */\n\toptr = skb->data;\n\tptr = skb->data;\n\tsession_id = ntohl(*((__be32 *)ptr));\n\tptr += 4;\n\n\t/* RFC3931: L2TP/IP packets have the first 4 bytes containing\n\t * the session_id. If it is 0, the packet is a L2TP control\n\t * frame and the session_id value can be discarded.\n\t */\n\tif (session_id == 0) {\n\t\t__skb_pull(skb, 4);\n\t\tgoto pass_up;\n\t}\n\n\t/* Ok, this is a data packet. Lookup the session. */\n\tsession = l2tp_v3_session_get(net, NULL, session_id);\n\tif (!session)\n\t\tgoto discard;\n\n\ttunnel = session->tunnel;\n\tif (!tunnel)\n\t\tgoto discard_sess;\n\n\tif (l2tp_v3_ensure_opt_in_linear(session, skb, &ptr, &optr))\n\t\tgoto discard_sess;\n\n\tl2tp_recv_common(session, skb, ptr, optr, 0, skb->len);\n\tl2tp_session_put(session);\n\n\treturn 0;\n\npass_up:\n\t/* Get the tunnel_id from the L2TP header */\n\tif (!pskb_may_pull(skb, 12))\n\t\tgoto discard;\n\n\tif ((skb->data[0] & 0xc0) != 0xc0)\n\t\tgoto discard;\n\n\ttunnel_id = ntohl(*(__be32 *)&skb->data[4]);\n\tiph = ipv6_hdr(skb);\n\n\tread_lock_bh(&pn->l2tp_ip6_lock);\n\tsk = __l2tp_ip6_bind_lookup(net, &iph->daddr, &iph->saddr,\n\t\t\t\t    inet6_iif(skb), tunnel_id);\n\tif (!sk) {\n\t\tread_unlock_bh(&pn->l2tp_ip6_lock);\n\t\tgoto discard;\n\t}\n\tsock_hold(sk);\n\tread_unlock_bh(&pn->l2tp_ip6_lock);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto discard_put;\n\n\tnf_reset_ct(skb);\n\n\treturn sk_receive_skb(sk, skb, 1);\n\ndiscard_sess:\n\tl2tp_session_put(session);\n\tgoto discard;\n\ndiscard_put:\n\tsock_put(sk);\n\ndiscard:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic int l2tp_ip6_hash(struct sock *sk)\n{\n\tstruct l2tp_ip6_net *pn = l2tp_ip6_pernet(sock_net(sk));\n\n\tif (sk_unhashed(sk)) {\n\t\twrite_lock_bh(&pn->l2tp_ip6_lock);\n\t\tsk_add_node(sk, &pn->l2tp_ip6_table);\n\t\twrite_unlock_bh(&pn->l2tp_ip6_lock);\n\t}\n\treturn 0;\n}\n\nstatic void l2tp_ip6_unhash(struct sock *sk)\n{\n\tstruct l2tp_ip6_net *pn = l2tp_ip6_pernet(sock_net(sk));\n\n\tif (sk_unhashed(sk))\n\t\treturn;\n\twrite_lock_bh(&pn->l2tp_ip6_lock);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&pn->l2tp_ip6_lock);\n}\n\nstatic int l2tp_ip6_open(struct sock *sk)\n{\n\t/* Prevent autobind. We don't have ports. */\n\tinet_sk(sk)->inet_num = IPPROTO_L2TP;\n\n\tl2tp_ip6_hash(sk);\n\treturn 0;\n}\n\nstatic void l2tp_ip6_close(struct sock *sk, long timeout)\n{\n\tstruct l2tp_ip6_net *pn = l2tp_ip6_pernet(sock_net(sk));\n\n\twrite_lock_bh(&pn->l2tp_ip6_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&pn->l2tp_ip6_lock);\n\n\tsk_common_release(sk);\n}\n\nstatic void l2tp_ip6_destroy_sock(struct sock *sk)\n{\n\tstruct l2tp_tunnel *tunnel;\n\n\tlock_sock(sk);\n\tip6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\ttunnel = l2tp_sk_to_tunnel(sk);\n\tif (tunnel) {\n\t\tl2tp_tunnel_delete(tunnel);\n\t\tl2tp_tunnel_put(tunnel);\n\t}\n}\n\nstatic int l2tp_ip6_bind(struct sock *sk, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_l2tpip6 *addr = (struct sockaddr_l2tpip6 *)uaddr;\n\tstruct net *net = sock_net(sk);\n\tstruct l2tp_ip6_net *pn;\n\t__be32 v4addr = 0;\n\tint bound_dev_if;\n\tint addr_type;\n\tint err;\n\n\tpn = l2tp_ip6_pernet(net);\n\n\tif (addr->l2tp_family != AF_INET6)\n\t\treturn -EINVAL;\n\tif (addr_len < sizeof(*addr))\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&addr->l2tp_addr);\n\n\t/* l2tp_ip6 sockets are IPv6 only */\n\tif (addr_type == IPV6_ADDR_MAPPED)\n\t\treturn -EADDRNOTAVAIL;\n\n\t/* L2TP is point-point, not multicast */\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EADDRNOTAVAIL;\n\n\tlock_sock(sk);\n\n\terr = -EINVAL;\n\tif (!sock_flag(sk, SOCK_ZAPPED))\n\t\tgoto out_unlock;\n\n\tif (sk->sk_state != TCP_CLOSE)\n\t\tgoto out_unlock;\n\n\tbound_dev_if = sk->sk_bound_dev_if;\n\n\t/* Check if the address belongs to the host. */\n\trcu_read_lock();\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tstruct net_device *dev = NULL;\n\n\t\tif (addr_type & IPV6_ADDR_LINKLOCAL) {\n\t\t\tif (addr->l2tp_scope_id)\n\t\t\t\tbound_dev_if = addr->l2tp_scope_id;\n\n\t\t\t/* Binding to link-local address requires an\n\t\t\t * interface.\n\t\t\t */\n\t\t\tif (!bound_dev_if)\n\t\t\t\tgoto out_unlock_rcu;\n\n\t\t\terr = -ENODEV;\n\t\t\tdev = dev_get_by_index_rcu(sock_net(sk), bound_dev_if);\n\t\t\tif (!dev)\n\t\t\t\tgoto out_unlock_rcu;\n\t\t}\n\n\t\t/* ipv4 addr of the socket is invalid.  Only the\n\t\t * unspecified and mapped address have a v4 equivalent.\n\t\t */\n\t\tv4addr = LOOPBACK4_IPV6;\n\t\terr = -EADDRNOTAVAIL;\n\t\tif (!ipv6_chk_addr(sock_net(sk), &addr->l2tp_addr, dev, 0))\n\t\t\tgoto out_unlock_rcu;\n\t}\n\trcu_read_unlock();\n\n\twrite_lock_bh(&pn->l2tp_ip6_lock);\n\tif (__l2tp_ip6_bind_lookup(net, &addr->l2tp_addr, NULL, bound_dev_if,\n\t\t\t\t   addr->l2tp_conn_id)) {\n\t\twrite_unlock_bh(&pn->l2tp_ip6_lock);\n\t\terr = -EADDRINUSE;\n\t\tgoto out_unlock;\n\t}\n\n\tinet->inet_saddr = v4addr;\n\tinet->inet_rcv_saddr = v4addr;\n\tsk->sk_bound_dev_if = bound_dev_if;\n\tsk->sk_v6_rcv_saddr = addr->l2tp_addr;\n\tnp->saddr = addr->l2tp_addr;\n\n\tl2tp_ip6_sk(sk)->conn_id = addr->l2tp_conn_id;\n\n\tsk_add_bind_node(sk, &pn->l2tp_ip6_bind_table);\n\tsk_del_node_init(sk);\n\twrite_unlock_bh(&pn->l2tp_ip6_lock);\n\n\tsock_reset_flag(sk, SOCK_ZAPPED);\n\trelease_sock(sk);\n\treturn 0;\n\nout_unlock_rcu:\n\trcu_read_unlock();\nout_unlock:\n\trelease_sock(sk);\n\n\treturn err;\n}\n\nstatic int l2tp_ip6_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t    int addr_len)\n{\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *)uaddr;\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *)uaddr;\n\tstruct in6_addr\t*daddr;\n\tint\taddr_type;\n\tint rc;\n\tstruct l2tp_ip6_net *pn;\n\n\tif (addr_len < sizeof(*lsa))\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EINVAL;\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\tif (addr_type & IPV6_ADDR_MULTICAST)\n\t\treturn -EINVAL;\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tdaddr = &usin->sin6_addr;\n\t\tif (ipv4_is_multicast(daddr->s6_addr32[3]))\n\t\t\treturn -EINVAL;\n\t}\n\n\tlock_sock(sk);\n\n\t /* Must bind first - autobinding does not work */\n\tif (sock_flag(sk, SOCK_ZAPPED)) {\n\t\trc = -EINVAL;\n\t\tgoto out_sk;\n\t}\n\n\trc = __ip6_datagram_connect(sk, uaddr, addr_len);\n\tif (rc < 0)\n\t\tgoto out_sk;\n\n\tl2tp_ip6_sk(sk)->peer_conn_id = lsa->l2tp_conn_id;\n\n\tpn = l2tp_ip6_pernet(sock_net(sk));\n\twrite_lock_bh(&pn->l2tp_ip6_lock);\n\thlist_del_init(&sk->sk_bind_node);\n\tsk_add_bind_node(sk, &pn->l2tp_ip6_bind_table);\n\twrite_unlock_bh(&pn->l2tp_ip6_lock);\n\nout_sk:\n\trelease_sock(sk);\n\n\treturn rc;\n}\n\nstatic int l2tp_ip6_disconnect(struct sock *sk, int flags)\n{\n\tif (sock_flag(sk, SOCK_ZAPPED))\n\t\treturn 0;\n\n\treturn __udp_disconnect(sk, flags);\n}\n\nstatic int l2tp_ip6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int peer)\n{\n\tstruct sockaddr_l2tpip6 *lsa = (struct sockaddr_l2tpip6 *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct l2tp_ip6_sock *lsk = l2tp_ip6_sk(sk);\n\n\tlsa->l2tp_family = AF_INET6;\n\tlsa->l2tp_flowinfo = 0;\n\tlsa->l2tp_scope_id = 0;\n\tlsa->l2tp_unused = 0;\n\tif (peer) {\n\t\tif (!lsk->peer_conn_id)\n\t\t\treturn -ENOTCONN;\n\t\tlsa->l2tp_conn_id = lsk->peer_conn_id;\n\t\tlsa->l2tp_addr = sk->sk_v6_daddr;\n\t\tif (inet6_test_bit(SNDFLOW, sk))\n\t\t\tlsa->l2tp_flowinfo = np->flow_label;\n\t} else {\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr))\n\t\t\tlsa->l2tp_addr = np->saddr;\n\t\telse\n\t\t\tlsa->l2tp_addr = sk->sk_v6_rcv_saddr;\n\n\t\tlsa->l2tp_conn_id = lsk->conn_id;\n\t}\n\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\tlsa->l2tp_scope_id = READ_ONCE(sk->sk_bound_dev_if);\n\treturn sizeof(*lsa);\n}\n\nstatic int l2tp_ip6_backlog_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tint rc;\n\n\t/* Charge it to the socket, dropping if the queue is full. */\n\trc = sock_queue_rcv_skb(sk, skb);\n\tif (rc < 0)\n\t\tgoto drop;\n\n\treturn 0;\n\ndrop:\n\tIP_INC_STATS(sock_net(sk), IPSTATS_MIB_INDISCARDS);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic int l2tp_ip6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\t__be32 *transhdr = NULL;\n\tint err = 0;\n\n\tskb = skb_peek(&sk->sk_write_queue);\n\tif (!skb)\n\t\tgoto out;\n\n\ttranshdr = (__be32 *)skb_transport_header(skb);\n\t*transhdr = 0;\n\n\terr = ip6_push_pending_frames(sk);\n\nout:\n\treturn err;\n}\n\n/* Userspace will call sendmsg() on the tunnel socket to send L2TP\n * control frames.\n */\nstatic int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6_txoptions *opt_to_free = NULL;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct flowi6 fl6;\n\tstruct ipcm6_cookie ipc6;\n\tint addr_len = msg->msg_namelen;\n\tint transhdrlen = 4; /* zero session-id */\n\tint ulen;\n\tint err;\n\n\t/* Rough check on arithmetic overflow,\n\t * better check is made in ip6_append_data().\n\t */\n\tif (len > INT_MAX - transhdrlen)\n\t\treturn -EMSGSIZE;\n\n\t/* Mirror BSD error message compatibility */\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Get and verify the address */\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tfl6.flowi6_mark = READ_ONCE(sk->sk_mark);\n\tfl6.flowi6_uid = sk->sk_uid;\n\n\tipcm6_init_sk(&ipc6, sk);\n\n\tif (lsa) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn -EINVAL;\n\n\t\tif (lsa->l2tp_family && lsa->l2tp_family != AF_INET6)\n\t\t\treturn -EAFNOSUPPORT;\n\n\t\tdaddr = &lsa->l2tp_addr;\n\t\tif (inet6_test_bit(SNDFLOW, sk)) {\n\t\t\tfl6.flowlabel = lsa->l2tp_flowinfo & IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel & IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &sk->sk_v6_daddr))\n\t\t\tdaddr = &sk->sk_v6_daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    lsa->l2tp_scope_id &&\n\t\t    ipv6_addr_type(daddr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = lsa->l2tp_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tdaddr = &sk->sk_v6_daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t}\n\n\tif (fl6.flowi6_oif == 0)\n\t\tfl6.flowi6_oif = READ_ONCE(sk->sk_bound_dev_if);\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(struct ipv6_txoptions);\n\t\tipc6.opt = opt;\n\n\t\terr = ip6_datagram_send_ctl(sock_net(sk), sk, msg, &fl6, &ipc6);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel & IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (IS_ERR(flowlabel))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen | opt->opt_flen))\n\t\t\topt = NULL;\n\t}\n\n\tif (!opt) {\n\t\topt = txopt_get(np);\n\t\topt_to_free = opt;\n\t}\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\tipc6.opt = opt;\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tfl6.daddr = *daddr;\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tfl6.saddr = np->saddr;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr))\n\t\tfl6.flowi6_oif = READ_ONCE(np->mcast_oif);\n\telse if (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = READ_ONCE(np->ucast_oif);\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi_common(&fl6));\n\n\tfl6.flowlabel = ip6_make_flowinfo(ipc6.tclass, fl6.flowlabel);\n\n\tdst = ip6_dst_lookup_flow(sock_net(sk), sk, &fl6, final_p);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tgoto out;\n\t}\n\n\tif (ipc6.hlimit < 0)\n\t\tipc6.hlimit = ip6_sk_dst_hoplimit(np, &fl6, dst);\n\n\tif (msg->msg_flags & MSG_CONFIRM)\n\t\tgoto do_confirm;\n\nback_from_confirm:\n\tlock_sock(sk);\n\tulen = len + (skb_queue_empty(&sk->sk_write_queue) ? transhdrlen : 0);\n\terr = ip6_append_data(sk, ip_generic_getfrag, msg,\n\t\t\t      ulen, transhdrlen, &ipc6,\n\t\t\t      &fl6, dst_rt6_info(dst),\n\t\t\t      msg->msg_flags);\n\tif (err)\n\t\tip6_flush_pending_frames(sk);\n\telse if (!(msg->msg_flags & MSG_MORE))\n\t\terr = l2tp_ip6_push_pending_frames(sk);\n\trelease_sock(sk);\ndone:\n\tdst_release(dst);\nout:\n\tfl6_sock_release(flowlabel);\n\ttxopt_put(opt_to_free);\n\n\treturn err < 0 ? err : len;\n\ndo_confirm:\n\tif (msg->msg_flags & MSG_PROBE)\n\t\tdst_confirm_neigh(dst, &fl6.daddr);\n\tif (!(msg->msg_flags & MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto done;\n}\n\nstatic int l2tp_ip6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t    int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n\tsize_t copied = 0;\n\tint err = -EOPNOTSUPP;\n\tstruct sk_buff *skb;\n\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len, addr_len);\n\n\tskb = skb_recv_datagram(sk, flags, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\terr = skb_copy_datagram_msg(skb, 0, msg, copied);\n\tif (err)\n\t\tgoto done;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (lsa) {\n\t\tlsa->l2tp_family = AF_INET6;\n\t\tlsa->l2tp_unused = 0;\n\t\tlsa->l2tp_addr = ipv6_hdr(skb)->saddr;\n\t\tlsa->l2tp_flowinfo = 0;\n\t\tlsa->l2tp_scope_id = 0;\n\t\tlsa->l2tp_conn_id = 0;\n\t\tif (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tlsa->l2tp_scope_id = inet6_iif(skb);\n\t\t*addr_len = sizeof(*lsa);\n\t}\n\n\tif (np->rxopt.all)\n\t\tip6_datagram_recv_ctl(sk, msg, skb);\n\n\tif (flags & MSG_TRUNC)\n\t\tcopied = skb->len;\ndone:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err ? err : copied;\n}\n\nstatic struct proto l2tp_ip6_prot = {\n\t.name\t\t   = \"L2TP/IPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.init\t\t   = l2tp_ip6_open,\n\t.close\t\t   = l2tp_ip6_close,\n\t.bind\t\t   = l2tp_ip6_bind,\n\t.connect\t   = l2tp_ip6_connect,\n\t.disconnect\t   = l2tp_ip6_disconnect,\n\t.ioctl\t\t   = l2tp_ioctl,\n\t.destroy\t   = l2tp_ip6_destroy_sock,\n\t.setsockopt\t   = ipv6_setsockopt,\n\t.getsockopt\t   = ipv6_getsockopt,\n\t.sendmsg\t   = l2tp_ip6_sendmsg,\n\t.recvmsg\t   = l2tp_ip6_recvmsg,\n\t.backlog_rcv\t   = l2tp_ip6_backlog_recv,\n\t.hash\t\t   = l2tp_ip6_hash,\n\t.unhash\t\t   = l2tp_ip6_unhash,\n\t.obj_size\t   = sizeof(struct l2tp_ip6_sock),\n\t.ipv6_pinfo_offset = offsetof(struct l2tp_ip6_sock, inet6),\n};\n\nstatic const struct proto_ops l2tp_ip6_ops = {\n\t.family\t\t   = PF_INET6,\n\t.owner\t\t   = THIS_MODULE,\n\t.release\t   = inet6_release,\n\t.bind\t\t   = inet6_bind,\n\t.connect\t   = inet_dgram_connect,\n\t.socketpair\t   = sock_no_socketpair,\n\t.accept\t\t   = sock_no_accept,\n\t.getname\t   = l2tp_ip6_getname,\n\t.poll\t\t   = datagram_poll,\n\t.ioctl\t\t   = inet6_ioctl,\n\t.gettstamp\t   = sock_gettstamp,\n\t.listen\t\t   = sock_no_listen,\n\t.shutdown\t   = inet_shutdown,\n\t.setsockopt\t   = sock_common_setsockopt,\n\t.getsockopt\t   = sock_common_getsockopt,\n\t.sendmsg\t   = inet_sendmsg,\n\t.recvmsg\t   = sock_common_recvmsg,\n\t.mmap\t\t   = sock_no_mmap,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t   = inet6_compat_ioctl,\n#endif\n};\n\nstatic struct inet_protosw l2tp_ip6_protosw = {\n\t.type\t\t= SOCK_DGRAM,\n\t.protocol\t= IPPROTO_L2TP,\n\t.prot\t\t= &l2tp_ip6_prot,\n\t.ops\t\t= &l2tp_ip6_ops,\n};\n\nstatic struct inet6_protocol l2tp_ip6_protocol __read_mostly = {\n\t.handler\t= l2tp_ip6_recv,\n};\n\nstatic __net_init int l2tp_ip6_init_net(struct net *net)\n{\n\tstruct l2tp_ip6_net *pn = net_generic(net, l2tp_ip6_net_id);\n\n\trwlock_init(&pn->l2tp_ip6_lock);\n\tINIT_HLIST_HEAD(&pn->l2tp_ip6_table);\n\tINIT_HLIST_HEAD(&pn->l2tp_ip6_bind_table);\n\treturn 0;\n}\n\nstatic __net_exit void l2tp_ip6_exit_net(struct net *net)\n{\n\tstruct l2tp_ip6_net *pn = l2tp_ip6_pernet(net);\n\n\twrite_lock_bh(&pn->l2tp_ip6_lock);\n\tWARN_ON_ONCE(hlist_count_nodes(&pn->l2tp_ip6_table) != 0);\n\tWARN_ON_ONCE(hlist_count_nodes(&pn->l2tp_ip6_bind_table) != 0);\n\twrite_unlock_bh(&pn->l2tp_ip6_lock);\n}\n\nstatic struct pernet_operations l2tp_ip6_net_ops = {\n\t.init = l2tp_ip6_init_net,\n\t.exit = l2tp_ip6_exit_net,\n\t.id   = &l2tp_ip6_net_id,\n\t.size = sizeof(struct l2tp_ip6_net),\n};\n\nstatic int __init l2tp_ip6_init(void)\n{\n\tint err;\n\n\tpr_info(\"L2TP IP encapsulation support for IPv6 (L2TPv3)\\n\");\n\n\terr = register_pernet_device(&l2tp_ip6_net_ops);\n\tif (err)\n\t\tgoto out;\n\n\terr = proto_register(&l2tp_ip6_prot, 1);\n\tif (err != 0)\n\t\tgoto out1;\n\n\terr = inet6_add_protocol(&l2tp_ip6_protocol, IPPROTO_L2TP);\n\tif (err)\n\t\tgoto out2;\n\n\tinet6_register_protosw(&l2tp_ip6_protosw);\n\treturn 0;\n\nout2:\n\tproto_unregister(&l2tp_ip6_prot);\nout1:\n\tunregister_pernet_device(&l2tp_ip6_net_ops);\nout:\n\treturn err;\n}\n\nstatic void __exit l2tp_ip6_exit(void)\n{\n\tinet6_unregister_protosw(&l2tp_ip6_protosw);\n\tinet6_del_protocol(&l2tp_ip6_protocol, IPPROTO_L2TP);\n\tproto_unregister(&l2tp_ip6_prot);\n\tunregister_pernet_device(&l2tp_ip6_net_ops);\n}\n\nmodule_init(l2tp_ip6_init);\nmodule_exit(l2tp_ip6_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Chris Elston <celston@katalix.com>\");\nMODULE_DESCRIPTION(\"L2TP IP encapsulation for IPv6\");\nMODULE_VERSION(\"1.0\");\n\n/* Use the values of SOCK_DGRAM (2) as type and IPPROTO_L2TP (115) as protocol,\n * because __stringify doesn't like enums\n */\nMODULE_ALIAS_NET_PF_PROTO_TYPE(PF_INET6, 115, 2);\nMODULE_ALIAS_NET_PF_PROTO(PF_INET6, 115);\n", "patch": "@@ -486,6 +486,7 @@ static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \tDECLARE_SOCKADDR(struct sockaddr_l2tpip6 *, lsa, msg->msg_name);\n \tstruct in6_addr *daddr, *final_p, final;\n \tstruct ipv6_pinfo *np = inet6_sk(sk);\n+\tstruct ipv6_txoptions *opt_to_free = NULL;\n \tstruct ipv6_txoptions *opt = NULL;\n \tstruct ip6_flowlabel *flowlabel = NULL;\n \tstruct dst_entry *dst = NULL;\n@@ -575,8 +576,10 @@ static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \t\t\topt = NULL;\n \t}\n \n-\tif (opt == NULL)\n-\t\topt = np->opt;\n+\tif (!opt) {\n+\t\topt = txopt_get(np);\n+\t\topt_to_free = opt;\n+\t}\n \tif (flowlabel)\n \t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n \topt = ipv6_fixup_options(&opt_space, opt);\n@@ -631,6 +634,7 @@ static int l2tp_ip6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)\n \tdst_release(dst);\n out:\n \tfl6_sock_release(flowlabel);\n+\ttxopt_put(opt_to_free);\n \n \treturn err < 0 ? err : len;\n ", "file_path": "files/2016_8\\94", "file_language": "c", "file_name": "net/l2tp/l2tp_ip6.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 34, "cve_id": "CVE-2016-6156", "cwe_id": ["CWE-362"], "cve_language": "C", "cve_description": "Race condition in the ec_device_ioctl_xcmd function in drivers/platform/chrome/cros_ec_dev.c in the Linux kernel before 4.7 allows local users to cause a denial of service (out-of-bounds array access) by changing a certain size value, aka a \"double fetch\" vulnerability.", "cvss": "5.1", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "HIGH", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "NONE", "A": "HIGH", "commit_id": "096cdc6f52225835ff503f987a0d68ef770bb78e", "commit_message": "platform/chrome: cros_ec_dev - double fetch bug in ioctl\n\nWe verify \"u_cmd.outsize\" and \"u_cmd.insize\" but we need to make sure\nthat those values have not changed between the two copy_from_user()\ncalls.  Otherwise it could lead to a buffer overflow.\n\nAdditionally, cros_ec_cmd_xfer() can set s_cmd->insize to a lower value.\nWe should use the new smaller value so we don't copy too much data to\nthe user.\n\nReported-by: Pengfei Wang <wpengfeinudt@gmail.com>\nFixes: a841178445bb ('mfd: cros_ec: Use a zero-length array for command data')\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nReviewed-by: Kees Cook <keescook@chromium.org>\nTested-by: Gwendal Grignou <gwendal@chromium.org>\nCc: <stable@vger.kernel.org> # v4.2+\nSigned-off-by: Olof Johansson <olof@lixom.net>", "commit_date": "2016-07-05T21:01:52Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/096cdc6f52225835ff503f987a0d68ef770bb78e", "html_url": "https://github.com/torvalds/linux/commit/096cdc6f52225835ff503f987a0d68ef770bb78e", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "33688abb2802ff3a230bd2441f765477b94cc89e", "url_before": "https://api.github.com/repos/torvalds/linux/commits/33688abb2802ff3a230bd2441f765477b94cc89e", "html_url_before": "https://github.com/torvalds/linux/commit/33688abb2802ff3a230bd2441f765477b94cc89e"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/096cdc6f52225835ff503f987a0d68ef770bb78e/drivers/platform/chrome/cros_ec_dev.c", "code": "/*\n * cros_ec_dev - expose the Chrome OS Embedded Controller to user-space\n *\n * Copyright (C) 2014 Google, Inc.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program. If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/fs.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n\n#include \"cros_ec_dev.h\"\n\n/* Device variables */\n#define CROS_MAX_DEV 128\nstatic int ec_major;\n\nstatic const struct attribute_group *cros_ec_groups[] = {\n\t&cros_ec_attr_group,\n\t&cros_ec_lightbar_attr_group,\n\t&cros_ec_vbc_attr_group,\n\tNULL,\n};\n\nstatic struct class cros_class = {\n\t.owner          = THIS_MODULE,\n\t.name           = \"chromeos\",\n\t.dev_groups     = cros_ec_groups,\n};\n\n/* Basic communication */\nstatic int ec_get_version(struct cros_ec_dev *ec, char *str, int maxlen)\n{\n\tstruct ec_response_get_version *resp;\n\tstatic const char * const current_image_name[] = {\n\t\t\"unknown\", \"read-only\", \"read-write\", \"invalid\",\n\t};\n\tstruct cros_ec_command *msg;\n\tint ret;\n\n\tmsg = kmalloc(sizeof(*msg) + sizeof(*resp), GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\tmsg->version = 0;\n\tmsg->command = EC_CMD_GET_VERSION + ec->cmd_offset;\n\tmsg->insize = sizeof(*resp);\n\tmsg->outsize = 0;\n\n\tret = cros_ec_cmd_xfer(ec->ec_dev, msg);\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (msg->result != EC_RES_SUCCESS) {\n\t\tsnprintf(str, maxlen,\n\t\t\t \"%s\\nUnknown EC version: EC returned %d\\n\",\n\t\t\t CROS_EC_DEV_VERSION, msg->result);\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tresp = (struct ec_response_get_version *)msg->data;\n\tif (resp->current_image >= ARRAY_SIZE(current_image_name))\n\t\tresp->current_image = 3; /* invalid */\n\n\tsnprintf(str, maxlen, \"%s\\n%s\\n%s\\n%s\\n\", CROS_EC_DEV_VERSION,\n\t\t resp->version_string_ro, resp->version_string_rw,\n\t\t current_image_name[resp->current_image]);\n\n\tret = 0;\nexit:\n\tkfree(msg);\n\treturn ret;\n}\n\n/* Device file ops */\nstatic int ec_device_open(struct inode *inode, struct file *filp)\n{\n\tstruct cros_ec_dev *ec = container_of(inode->i_cdev,\n\t\t\t\t\t      struct cros_ec_dev, cdev);\n\tfilp->private_data = ec;\n\tnonseekable_open(inode, filp);\n\treturn 0;\n}\n\nstatic int ec_device_release(struct inode *inode, struct file *filp)\n{\n\treturn 0;\n}\n\nstatic ssize_t ec_device_read(struct file *filp, char __user *buffer,\n\t\t\t      size_t length, loff_t *offset)\n{\n\tstruct cros_ec_dev *ec = filp->private_data;\n\tchar msg[sizeof(struct ec_response_get_version) +\n\t\t sizeof(CROS_EC_DEV_VERSION)];\n\tsize_t count;\n\tint ret;\n\n\tif (*offset != 0)\n\t\treturn 0;\n\n\tret = ec_get_version(ec, msg, sizeof(msg));\n\tif (ret)\n\t\treturn ret;\n\n\tcount = min(length, strlen(msg));\n\n\tif (copy_to_user(buffer, msg, count))\n\t\treturn -EFAULT;\n\n\t*offset = count;\n\treturn count;\n}\n\n/* Ioctls */\nstatic long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n{\n\tlong ret;\n\tstruct cros_ec_command u_cmd;\n\tstruct cros_ec_command *s_cmd;\n\n\tif (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))\n\t\treturn -EFAULT;\n\n\tif ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||\n\t    (u_cmd.insize > EC_MAX_MSG_BYTES))\n\t\treturn -EINVAL;\n\n\ts_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),\n\t\t\tGFP_KERNEL);\n\tif (!s_cmd)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tif (u_cmd.outsize != s_cmd->outsize ||\n\t    u_cmd.insize != s_cmd->insize) {\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\ts_cmd->command += ec->cmd_offset;\n\tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n\t/* Only copy data to userland if data was received. */\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n\t\tret = -EFAULT;\nexit:\n\tkfree(s_cmd);\n\treturn ret;\n}\n\nstatic long ec_device_ioctl_readmem(struct cros_ec_dev *ec, void __user *arg)\n{\n\tstruct cros_ec_device *ec_dev = ec->ec_dev;\n\tstruct cros_ec_readmem s_mem = { };\n\tlong num;\n\n\t/* Not every platform supports direct reads */\n\tif (!ec_dev->cmd_readmem)\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&s_mem, arg, sizeof(s_mem)))\n\t\treturn -EFAULT;\n\n\tnum = ec_dev->cmd_readmem(ec_dev, s_mem.offset, s_mem.bytes,\n\t\t\t\t  s_mem.buffer);\n\tif (num <= 0)\n\t\treturn num;\n\n\tif (copy_to_user((void __user *)arg, &s_mem, sizeof(s_mem)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long ec_device_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t    unsigned long arg)\n{\n\tstruct cros_ec_dev *ec = filp->private_data;\n\n\tif (_IOC_TYPE(cmd) != CROS_EC_DEV_IOC)\n\t\treturn -ENOTTY;\n\n\tswitch (cmd) {\n\tcase CROS_EC_DEV_IOCXCMD:\n\t\treturn ec_device_ioctl_xcmd(ec, (void __user *)arg);\n\tcase CROS_EC_DEV_IOCRDMEM:\n\t\treturn ec_device_ioctl_readmem(ec, (void __user *)arg);\n\t}\n\n\treturn -ENOTTY;\n}\n\n/* Module initialization */\nstatic const struct file_operations fops = {\n\t.open = ec_device_open,\n\t.release = ec_device_release,\n\t.read = ec_device_read,\n\t.unlocked_ioctl = ec_device_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = ec_device_ioctl,\n#endif\n};\n\nstatic void __remove(struct device *dev)\n{\n\tstruct cros_ec_dev *ec = container_of(dev, struct cros_ec_dev,\n\t\t\t\t\t      class_dev);\n\tkfree(ec);\n}\n\nstatic int ec_device_probe(struct platform_device *pdev)\n{\n\tint retval = -ENOMEM;\n\tstruct device *dev = &pdev->dev;\n\tstruct cros_ec_platform *ec_platform = dev_get_platdata(dev);\n\tdev_t devno = MKDEV(ec_major, pdev->id);\n\tstruct cros_ec_dev *ec = kzalloc(sizeof(*ec), GFP_KERNEL);\n\n\tif (!ec)\n\t\treturn retval;\n\n\tdev_set_drvdata(dev, ec);\n\tec->ec_dev = dev_get_drvdata(dev->parent);\n\tec->dev = dev;\n\tec->cmd_offset = ec_platform->cmd_offset;\n\tdevice_initialize(&ec->class_dev);\n\tcdev_init(&ec->cdev, &fops);\n\n\t/*\n\t * Add the character device\n\t * Link cdev to the class device to be sure device is not used\n\t * before unbinding it.\n\t */\n\tec->cdev.kobj.parent = &ec->class_dev.kobj;\n\tretval = cdev_add(&ec->cdev, devno, 1);\n\tif (retval) {\n\t\tdev_err(dev, \": failed to add character device\\n\");\n\t\tgoto cdev_add_failed;\n\t}\n\n\t/*\n\t * Add the class device\n\t * Link to the character device for creating the /dev entry\n\t * in devtmpfs.\n\t */\n\tec->class_dev.devt = ec->cdev.dev;\n\tec->class_dev.class = &cros_class;\n\tec->class_dev.parent = dev;\n\tec->class_dev.release = __remove;\n\n\tretval = dev_set_name(&ec->class_dev, \"%s\", ec_platform->ec_name);\n\tif (retval) {\n\t\tdev_err(dev, \"dev_set_name failed => %d\\n\", retval);\n\t\tgoto set_named_failed;\n\t}\n\n\tretval = device_add(&ec->class_dev);\n\tif (retval) {\n\t\tdev_err(dev, \"device_register failed => %d\\n\", retval);\n\t\tgoto dev_reg_failed;\n\t}\n\n\treturn 0;\n\ndev_reg_failed:\nset_named_failed:\n\tdev_set_drvdata(dev, NULL);\n\tcdev_del(&ec->cdev);\ncdev_add_failed:\n\tkfree(ec);\n\treturn retval;\n}\n\nstatic int ec_device_remove(struct platform_device *pdev)\n{\n\tstruct cros_ec_dev *ec = dev_get_drvdata(&pdev->dev);\n\tcdev_del(&ec->cdev);\n\tdevice_unregister(&ec->class_dev);\n\treturn 0;\n}\n\nstatic const struct platform_device_id cros_ec_id[] = {\n\t{ \"cros-ec-ctl\", 0 },\n\t{ /* sentinel */ },\n};\nMODULE_DEVICE_TABLE(platform, cros_ec_id);\n\nstatic struct platform_driver cros_ec_dev_driver = {\n\t.driver = {\n\t\t.name = \"cros-ec-ctl\",\n\t},\n\t.probe = ec_device_probe,\n\t.remove = ec_device_remove,\n};\n\nstatic int __init cros_ec_dev_init(void)\n{\n\tint ret;\n\tdev_t dev = 0;\n\n\tret  = class_register(&cros_class);\n\tif (ret) {\n\t\tpr_err(CROS_EC_DEV_NAME \": failed to register device class\\n\");\n\t\treturn ret;\n\t}\n\n\t/* Get a range of minor numbers (starting with 0) to work with */\n\tret = alloc_chrdev_region(&dev, 0, CROS_MAX_DEV, CROS_EC_DEV_NAME);\n\tif (ret < 0) {\n\t\tpr_err(CROS_EC_DEV_NAME \": alloc_chrdev_region() failed\\n\");\n\t\tgoto failed_chrdevreg;\n\t}\n\tec_major = MAJOR(dev);\n\n\t/* Register the driver */\n\tret = platform_driver_register(&cros_ec_dev_driver);\n\tif (ret < 0) {\n\t\tpr_warn(CROS_EC_DEV_NAME \": can't register driver: %d\\n\", ret);\n\t\tgoto failed_devreg;\n\t}\n\treturn 0;\n\nfailed_devreg:\n\tunregister_chrdev_region(MKDEV(ec_major, 0), CROS_MAX_DEV);\nfailed_chrdevreg:\n\tclass_unregister(&cros_class);\n\treturn ret;\n}\n\nstatic void __exit cros_ec_dev_exit(void)\n{\n\tplatform_driver_unregister(&cros_ec_dev_driver);\n\tunregister_chrdev(ec_major, CROS_EC_DEV_NAME);\n\tclass_unregister(&cros_class);\n}\n\nmodule_init(cros_ec_dev_init);\nmodule_exit(cros_ec_dev_exit);\n\nMODULE_AUTHOR(\"Bill Richardson <wfrichar@chromium.org>\");\nMODULE_DESCRIPTION(\"Userspace interface to the Chrome OS Embedded Controller\");\nMODULE_VERSION(\"1.0\");\nMODULE_LICENSE(\"GPL\");\n", "code_before": "/*\n * cros_ec_dev - expose the Chrome OS Embedded Controller to user-space\n *\n * Copyright (C) 2014 Google, Inc.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program. If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/fs.h>\n#include <linux/mfd/core.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/pm.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n\n#include \"cros_ec_debugfs.h\"\n#include \"cros_ec_dev.h\"\n\n#define DRV_NAME \"cros-ec-dev\"\n\n/* Device variables */\n#define CROS_MAX_DEV 128\nstatic int ec_major;\n\nstatic const struct attribute_group *cros_ec_groups[] = {\n\t&cros_ec_attr_group,\n\t&cros_ec_lightbar_attr_group,\n\t&cros_ec_vbc_attr_group,\n\tNULL,\n};\n\nstatic struct class cros_class = {\n\t.owner          = THIS_MODULE,\n\t.name           = \"chromeos\",\n\t.dev_groups     = cros_ec_groups,\n};\n\n/* Basic communication */\nstatic int ec_get_version(struct cros_ec_dev *ec, char *str, int maxlen)\n{\n\tstruct ec_response_get_version *resp;\n\tstatic const char * const current_image_name[] = {\n\t\t\"unknown\", \"read-only\", \"read-write\", \"invalid\",\n\t};\n\tstruct cros_ec_command *msg;\n\tint ret;\n\n\tmsg = kmalloc(sizeof(*msg) + sizeof(*resp), GFP_KERNEL);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\n\tmsg->version = 0;\n\tmsg->command = EC_CMD_GET_VERSION + ec->cmd_offset;\n\tmsg->insize = sizeof(*resp);\n\tmsg->outsize = 0;\n\n\tret = cros_ec_cmd_xfer(ec->ec_dev, msg);\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (msg->result != EC_RES_SUCCESS) {\n\t\tsnprintf(str, maxlen,\n\t\t\t \"%s\\nUnknown EC version: EC returned %d\\n\",\n\t\t\t CROS_EC_DEV_VERSION, msg->result);\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tresp = (struct ec_response_get_version *)msg->data;\n\tif (resp->current_image >= ARRAY_SIZE(current_image_name))\n\t\tresp->current_image = 3; /* invalid */\n\n\tsnprintf(str, maxlen, \"%s\\n%s\\n%s\\n%s\\n\", CROS_EC_DEV_VERSION,\n\t\t resp->version_string_ro, resp->version_string_rw,\n\t\t current_image_name[resp->current_image]);\n\n\tret = 0;\nexit:\n\tkfree(msg);\n\treturn ret;\n}\n\nstatic int cros_ec_check_features(struct cros_ec_dev *ec, int feature)\n{\n\tstruct cros_ec_command *msg;\n\tint ret;\n\n\tif (ec->features[0] == -1U && ec->features[1] == -1U) {\n\t\t/* features bitmap not read yet */\n\n\t\tmsg = kmalloc(sizeof(*msg) + sizeof(ec->features), GFP_KERNEL);\n\t\tif (!msg)\n\t\t\treturn -ENOMEM;\n\n\t\tmsg->version = 0;\n\t\tmsg->command = EC_CMD_GET_FEATURES + ec->cmd_offset;\n\t\tmsg->insize = sizeof(ec->features);\n\t\tmsg->outsize = 0;\n\n\t\tret = cros_ec_cmd_xfer(ec->ec_dev, msg);\n\t\tif (ret < 0 || msg->result != EC_RES_SUCCESS) {\n\t\t\tdev_warn(ec->dev, \"cannot get EC features: %d/%d\\n\",\n\t\t\t\t ret, msg->result);\n\t\t\tmemset(ec->features, 0, sizeof(ec->features));\n\t\t}\n\n\t\tmemcpy(ec->features, msg->data, sizeof(ec->features));\n\n\t\tdev_dbg(ec->dev, \"EC features %08x %08x\\n\",\n\t\t\tec->features[0], ec->features[1]);\n\n\t\tkfree(msg);\n\t}\n\n\treturn ec->features[feature / 32] & EC_FEATURE_MASK_0(feature);\n}\n\n/* Device file ops */\nstatic int ec_device_open(struct inode *inode, struct file *filp)\n{\n\tstruct cros_ec_dev *ec = container_of(inode->i_cdev,\n\t\t\t\t\t      struct cros_ec_dev, cdev);\n\tfilp->private_data = ec;\n\tnonseekable_open(inode, filp);\n\treturn 0;\n}\n\nstatic int ec_device_release(struct inode *inode, struct file *filp)\n{\n\treturn 0;\n}\n\nstatic ssize_t ec_device_read(struct file *filp, char __user *buffer,\n\t\t\t      size_t length, loff_t *offset)\n{\n\tstruct cros_ec_dev *ec = filp->private_data;\n\tchar msg[sizeof(struct ec_response_get_version) +\n\t\t sizeof(CROS_EC_DEV_VERSION)];\n\tsize_t count;\n\tint ret;\n\n\tif (*offset != 0)\n\t\treturn 0;\n\n\tret = ec_get_version(ec, msg, sizeof(msg));\n\tif (ret)\n\t\treturn ret;\n\n\tcount = min(length, strlen(msg));\n\n\tif (copy_to_user(buffer, msg, count))\n\t\treturn -EFAULT;\n\n\t*offset = count;\n\treturn count;\n}\n\n/* Ioctls */\nstatic long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n{\n\tlong ret;\n\tstruct cros_ec_command u_cmd;\n\tstruct cros_ec_command *s_cmd;\n\n\tif (copy_from_user(&u_cmd, arg, sizeof(u_cmd)))\n\t\treturn -EFAULT;\n\n\tif ((u_cmd.outsize > EC_MAX_MSG_BYTES) ||\n\t    (u_cmd.insize > EC_MAX_MSG_BYTES))\n\t\treturn -EINVAL;\n\n\ts_cmd = kmalloc(sizeof(*s_cmd) + max(u_cmd.outsize, u_cmd.insize),\n\t\t\tGFP_KERNEL);\n\tif (!s_cmd)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(s_cmd, arg, sizeof(*s_cmd) + u_cmd.outsize)) {\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tif (u_cmd.outsize != s_cmd->outsize ||\n\t    u_cmd.insize != s_cmd->insize) {\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\ts_cmd->command += ec->cmd_offset;\n\tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n\t/* Only copy data to userland if data was received. */\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n\t\tret = -EFAULT;\nexit:\n\tkfree(s_cmd);\n\treturn ret;\n}\n\nstatic long ec_device_ioctl_readmem(struct cros_ec_dev *ec, void __user *arg)\n{\n\tstruct cros_ec_device *ec_dev = ec->ec_dev;\n\tstruct cros_ec_readmem s_mem = { };\n\tlong num;\n\n\t/* Not every platform supports direct reads */\n\tif (!ec_dev->cmd_readmem)\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&s_mem, arg, sizeof(s_mem)))\n\t\treturn -EFAULT;\n\n\tnum = ec_dev->cmd_readmem(ec_dev, s_mem.offset, s_mem.bytes,\n\t\t\t\t  s_mem.buffer);\n\tif (num <= 0)\n\t\treturn num;\n\n\tif (copy_to_user((void __user *)arg, &s_mem, sizeof(s_mem)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long ec_device_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t    unsigned long arg)\n{\n\tstruct cros_ec_dev *ec = filp->private_data;\n\n\tif (_IOC_TYPE(cmd) != CROS_EC_DEV_IOC)\n\t\treturn -ENOTTY;\n\n\tswitch (cmd) {\n\tcase CROS_EC_DEV_IOCXCMD:\n\t\treturn ec_device_ioctl_xcmd(ec, (void __user *)arg);\n\tcase CROS_EC_DEV_IOCRDMEM:\n\t\treturn ec_device_ioctl_readmem(ec, (void __user *)arg);\n\t}\n\n\treturn -ENOTTY;\n}\n\n/* Module initialization */\nstatic const struct file_operations fops = {\n\t.open = ec_device_open,\n\t.release = ec_device_release,\n\t.read = ec_device_read,\n\t.unlocked_ioctl = ec_device_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = ec_device_ioctl,\n#endif\n};\n\nstatic void __remove(struct device *dev)\n{\n\tstruct cros_ec_dev *ec = container_of(dev, struct cros_ec_dev,\n\t\t\t\t\t      class_dev);\n\tkfree(ec);\n}\n\nstatic void cros_ec_sensors_register(struct cros_ec_dev *ec)\n{\n\t/*\n\t * Issue a command to get the number of sensor reported.\n\t * Build an array of sensors driver and register them all.\n\t */\n\tint ret, i, id, sensor_num;\n\tstruct mfd_cell *sensor_cells;\n\tstruct cros_ec_sensor_platform *sensor_platforms;\n\tint sensor_type[MOTIONSENSE_TYPE_MAX];\n\tstruct ec_params_motion_sense *params;\n\tstruct ec_response_motion_sense *resp;\n\tstruct cros_ec_command *msg;\n\n\tmsg = kzalloc(sizeof(struct cros_ec_command) +\n\t\t      max(sizeof(*params), sizeof(*resp)), GFP_KERNEL);\n\tif (msg == NULL)\n\t\treturn;\n\n\tmsg->version = 2;\n\tmsg->command = EC_CMD_MOTION_SENSE_CMD + ec->cmd_offset;\n\tmsg->outsize = sizeof(*params);\n\tmsg->insize = sizeof(*resp);\n\n\tparams = (struct ec_params_motion_sense *)msg->data;\n\tparams->cmd = MOTIONSENSE_CMD_DUMP;\n\n\tret = cros_ec_cmd_xfer(ec->ec_dev, msg);\n\tif (ret < 0 || msg->result != EC_RES_SUCCESS) {\n\t\tdev_warn(ec->dev, \"cannot get EC sensor information: %d/%d\\n\",\n\t\t\t ret, msg->result);\n\t\tgoto error;\n\t}\n\n\tresp = (struct ec_response_motion_sense *)msg->data;\n\tsensor_num = resp->dump.sensor_count;\n\t/* Allocate 2 extra sensors in case lid angle or FIFO are needed */\n\tsensor_cells = kzalloc(sizeof(struct mfd_cell) * (sensor_num + 2),\n\t\t\t       GFP_KERNEL);\n\tif (sensor_cells == NULL)\n\t\tgoto error;\n\n\tsensor_platforms = kzalloc(sizeof(struct cros_ec_sensor_platform) *\n\t\t  (sensor_num + 1), GFP_KERNEL);\n\tif (sensor_platforms == NULL)\n\t\tgoto error_platforms;\n\n\tmemset(sensor_type, 0, sizeof(sensor_type));\n\tid = 0;\n\tfor (i = 0; i < sensor_num; i++) {\n\t\tparams->cmd = MOTIONSENSE_CMD_INFO;\n\t\tparams->info.sensor_num = i;\n\t\tret = cros_ec_cmd_xfer(ec->ec_dev, msg);\n\t\tif (ret < 0 || msg->result != EC_RES_SUCCESS) {\n\t\t\tdev_warn(ec->dev, \"no info for EC sensor %d : %d/%d\\n\",\n\t\t\t\t i, ret, msg->result);\n\t\t\tcontinue;\n\t\t}\n\t\tswitch (resp->info.type) {\n\t\tcase MOTIONSENSE_TYPE_ACCEL:\n\t\t\tsensor_cells[id].name = \"cros-ec-accel\";\n\t\t\tbreak;\n\t\tcase MOTIONSENSE_TYPE_BARO:\n\t\t\tsensor_cells[id].name = \"cros-ec-baro\";\n\t\t\tbreak;\n\t\tcase MOTIONSENSE_TYPE_GYRO:\n\t\t\tsensor_cells[id].name = \"cros-ec-gyro\";\n\t\t\tbreak;\n\t\tcase MOTIONSENSE_TYPE_MAG:\n\t\t\tsensor_cells[id].name = \"cros-ec-mag\";\n\t\t\tbreak;\n\t\tcase MOTIONSENSE_TYPE_PROX:\n\t\t\tsensor_cells[id].name = \"cros-ec-prox\";\n\t\t\tbreak;\n\t\tcase MOTIONSENSE_TYPE_LIGHT:\n\t\t\tsensor_cells[id].name = \"cros-ec-light\";\n\t\t\tbreak;\n\t\tcase MOTIONSENSE_TYPE_ACTIVITY:\n\t\t\tsensor_cells[id].name = \"cros-ec-activity\";\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(ec->dev, \"unknown type %d\\n\", resp->info.type);\n\t\t\tcontinue;\n\t\t}\n\t\tsensor_platforms[id].sensor_num = i;\n\t\tsensor_cells[id].id = sensor_type[resp->info.type];\n\t\tsensor_cells[id].platform_data = &sensor_platforms[id];\n\t\tsensor_cells[id].pdata_size =\n\t\t\tsizeof(struct cros_ec_sensor_platform);\n\n\t\tsensor_type[resp->info.type]++;\n\t\tid++;\n\t}\n\tif (sensor_type[MOTIONSENSE_TYPE_ACCEL] >= 2) {\n\t\tsensor_platforms[id].sensor_num = sensor_num;\n\n\t\tsensor_cells[id].name = \"cros-ec-angle\";\n\t\tsensor_cells[id].id = 0;\n\t\tsensor_cells[id].platform_data = &sensor_platforms[id];\n\t\tsensor_cells[id].pdata_size =\n\t\t\tsizeof(struct cros_ec_sensor_platform);\n\t\tid++;\n\t}\n\tif (cros_ec_check_features(ec, EC_FEATURE_MOTION_SENSE_FIFO)) {\n\t\tsensor_cells[id].name = \"cros-ec-ring\";\n\t\tid++;\n\t}\n\n\tret = mfd_add_devices(ec->dev, 0, sensor_cells, id,\n\t\t\t      NULL, 0, NULL);\n\tif (ret)\n\t\tdev_err(ec->dev, \"failed to add EC sensors\\n\");\n\n\tkfree(sensor_platforms);\nerror_platforms:\n\tkfree(sensor_cells);\nerror:\n\tkfree(msg);\n}\n\nstatic int ec_device_probe(struct platform_device *pdev)\n{\n\tint retval = -ENOMEM;\n\tstruct device *dev = &pdev->dev;\n\tstruct cros_ec_platform *ec_platform = dev_get_platdata(dev);\n\tstruct cros_ec_dev *ec = kzalloc(sizeof(*ec), GFP_KERNEL);\n\n\tif (!ec)\n\t\treturn retval;\n\n\tdev_set_drvdata(dev, ec);\n\tec->ec_dev = dev_get_drvdata(dev->parent);\n\tec->dev = dev;\n\tec->cmd_offset = ec_platform->cmd_offset;\n\tec->features[0] = -1U; /* Not cached yet */\n\tec->features[1] = -1U; /* Not cached yet */\n\tdevice_initialize(&ec->class_dev);\n\tcdev_init(&ec->cdev, &fops);\n\n\t/*\n\t * Add the class device\n\t * Link to the character device for creating the /dev entry\n\t * in devtmpfs.\n\t */\n\tec->class_dev.devt = MKDEV(ec_major, pdev->id);\n\tec->class_dev.class = &cros_class;\n\tec->class_dev.parent = dev;\n\tec->class_dev.release = __remove;\n\n\tretval = dev_set_name(&ec->class_dev, \"%s\", ec_platform->ec_name);\n\tif (retval) {\n\t\tdev_err(dev, \"dev_set_name failed => %d\\n\", retval);\n\t\tgoto failed;\n\t}\n\n\tretval = cdev_device_add(&ec->cdev, &ec->class_dev);\n\tif (retval) {\n\t\tdev_err(dev, \"cdev_device_add failed => %d\\n\", retval);\n\t\tgoto failed;\n\t}\n\n\tif (cros_ec_debugfs_init(ec))\n\t\tdev_warn(dev, \"failed to create debugfs directory\\n\");\n\n\t/* check whether this EC is a sensor hub. */\n\tif (cros_ec_check_features(ec, EC_FEATURE_MOTION_SENSE))\n\t\tcros_ec_sensors_register(ec);\n\n\t/* Take control of the lightbar from the EC. */\n\tlb_manual_suspend_ctrl(ec, 1);\n\n\treturn 0;\n\nfailed:\n\tput_device(&ec->class_dev);\n\treturn retval;\n}\n\nstatic int ec_device_remove(struct platform_device *pdev)\n{\n\tstruct cros_ec_dev *ec = dev_get_drvdata(&pdev->dev);\n\n\t/* Let the EC take over the lightbar again. */\n\tlb_manual_suspend_ctrl(ec, 0);\n\n\tcros_ec_debugfs_remove(ec);\n\n\tcdev_del(&ec->cdev);\n\tdevice_unregister(&ec->class_dev);\n\treturn 0;\n}\n\nstatic const struct platform_device_id cros_ec_id[] = {\n\t{ DRV_NAME, 0 },\n\t{ /* sentinel */ },\n};\nMODULE_DEVICE_TABLE(platform, cros_ec_id);\n\nstatic __maybe_unused int ec_device_suspend(struct device *dev)\n{\n\tstruct cros_ec_dev *ec = dev_get_drvdata(dev);\n\n\tlb_suspend(ec);\n\n\treturn 0;\n}\n\nstatic __maybe_unused int ec_device_resume(struct device *dev)\n{\n\tstruct cros_ec_dev *ec = dev_get_drvdata(dev);\n\n\tlb_resume(ec);\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops cros_ec_dev_pm_ops = {\n#ifdef CONFIG_PM_SLEEP\n\t.suspend = ec_device_suspend,\n\t.resume = ec_device_resume,\n#endif\n};\n\nstatic struct platform_driver cros_ec_dev_driver = {\n\t.driver = {\n\t\t.name = DRV_NAME,\n\t\t.pm = &cros_ec_dev_pm_ops,\n\t},\n\t.probe = ec_device_probe,\n\t.remove = ec_device_remove,\n};\n\nstatic int __init cros_ec_dev_init(void)\n{\n\tint ret;\n\tdev_t dev = 0;\n\n\tret  = class_register(&cros_class);\n\tif (ret) {\n\t\tpr_err(CROS_EC_DEV_NAME \": failed to register device class\\n\");\n\t\treturn ret;\n\t}\n\n\t/* Get a range of minor numbers (starting with 0) to work with */\n\tret = alloc_chrdev_region(&dev, 0, CROS_MAX_DEV, CROS_EC_DEV_NAME);\n\tif (ret < 0) {\n\t\tpr_err(CROS_EC_DEV_NAME \": alloc_chrdev_region() failed\\n\");\n\t\tgoto failed_chrdevreg;\n\t}\n\tec_major = MAJOR(dev);\n\n\t/* Register the driver */\n\tret = platform_driver_register(&cros_ec_dev_driver);\n\tif (ret < 0) {\n\t\tpr_warn(CROS_EC_DEV_NAME \": can't register driver: %d\\n\", ret);\n\t\tgoto failed_devreg;\n\t}\n\treturn 0;\n\nfailed_devreg:\n\tunregister_chrdev_region(MKDEV(ec_major, 0), CROS_MAX_DEV);\nfailed_chrdevreg:\n\tclass_unregister(&cros_class);\n\treturn ret;\n}\n\nstatic void __exit cros_ec_dev_exit(void)\n{\n\tplatform_driver_unregister(&cros_ec_dev_driver);\n\tunregister_chrdev(ec_major, CROS_EC_DEV_NAME);\n\tclass_unregister(&cros_class);\n}\n\nmodule_init(cros_ec_dev_init);\nmodule_exit(cros_ec_dev_exit);\n\nMODULE_ALIAS(\"platform:\" DRV_NAME);\nMODULE_AUTHOR(\"Bill Richardson <wfrichar@chromium.org>\");\nMODULE_DESCRIPTION(\"Userspace interface to the Chrome OS Embedded Controller\");\nMODULE_VERSION(\"1.0\");\nMODULE_LICENSE(\"GPL\");\n", "patch": "@@ -151,13 +151,19 @@ static long ec_device_ioctl_xcmd(struct cros_ec_dev *ec, void __user *arg)\n \t\tgoto exit;\n \t}\n \n+\tif (u_cmd.outsize != s_cmd->outsize ||\n+\t    u_cmd.insize != s_cmd->insize) {\n+\t\tret = -EINVAL;\n+\t\tgoto exit;\n+\t}\n+\n \ts_cmd->command += ec->cmd_offset;\n \tret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);\n \t/* Only copy data to userland if data was received. */\n \tif (ret < 0)\n \t\tgoto exit;\n \n-\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))\n+\tif (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))\n \t\tret = -EFAULT;\n exit:\n \tkfree(s_cmd);", "file_path": "files/2016_8\\95", "file_language": "c", "file_name": "drivers/platform/chrome/cros_ec_dev.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 35, "cve_id": "CVE-2016-6187", "cwe_id": ["CWE-119", "CWE-264"], "cve_language": "C", "cve_description": "The apparmor_setprocattr function in security/apparmor/lsm.c in the Linux kernel before 4.6.5 does not validate the buffer size, which allows local users to gain privileges by triggering an AppArmor setprocattr hook.", "cvss": "7.8", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "30a46a4647fd1df9cf52e43bf467f0d9265096ca", "commit_message": "apparmor: fix oops, validate buffer size in apparmor_setprocattr()\n\nWhen proc_pid_attr_write() was changed to use memdup_user apparmor's\n(interface violating) assumption that the setprocattr buffer was always\na single page was violated.\n\nThe size test is not strictly speaking needed as proc_pid_attr_write()\nwill reject anything larger, but for the sake of robustness we can keep\nit in.\n\nSMACK and SELinux look safe to me, but somebody else should probably\nhave a look just in case.\n\nBased on original patch from Vegard Nossum <vegard.nossum@oracle.com>\nmodified for the case that apparmor provides null termination.\n\nFixes: bb646cdb12e75d82258c2f2e7746d5952d3e321a\nReported-by: Vegard Nossum <vegard.nossum@oracle.com>\nCc: Al Viro <viro@zeniv.linux.org.uk>\nCc: John Johansen <john.johansen@canonical.com>\nCc: Paul Moore <paul@paul-moore.com>\nCc: Stephen Smalley <sds@tycho.nsa.gov>\nCc: Eric Paris <eparis@parisplace.org>\nCc: Casey Schaufler <casey@schaufler-ca.com>\nCc: stable@kernel.org\nSigned-off-by: John Johansen <john.johansen@canonical.com>\nReviewed-by: Tyler Hicks <tyhicks@canonical.com>\nSigned-off-by: James Morris <james.l.morris@oracle.com>", "commit_date": "2016-07-08T00:26:25Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/30a46a4647fd1df9cf52e43bf467f0d9265096ca", "html_url": "https://github.com/torvalds/linux/commit/30a46a4647fd1df9cf52e43bf467f0d9265096ca", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "ac904ae6e6f0a56be7b9a1cf66fbd50dd025fb06", "url_before": "https://api.github.com/repos/torvalds/linux/commits/ac904ae6e6f0a56be7b9a1cf66fbd50dd025fb06", "html_url_before": "https://github.com/torvalds/linux/commit/ac904ae6e6f0a56be7b9a1cf66fbd50dd025fb06"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/30a46a4647fd1df9cf52e43bf467f0d9265096ca/security/apparmor/lsm.c", "code": "/*\n * AppArmor security module\n *\n * This file contains AppArmor LSM hooks.\n *\n * Copyright (C) 1998-2008 Novell/SUSE\n * Copyright 2009-2010 Canonical Ltd.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License as\n * published by the Free Software Foundation, version 2 of the\n * License.\n */\n\n#include <linux/lsm_hooks.h>\n#include <linux/moduleparam.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/ptrace.h>\n#include <linux/ctype.h>\n#include <linux/sysctl.h>\n#include <linux/audit.h>\n#include <linux/user_namespace.h>\n#include <net/sock.h>\n\n#include \"include/apparmor.h\"\n#include \"include/apparmorfs.h\"\n#include \"include/audit.h\"\n#include \"include/capability.h\"\n#include \"include/context.h\"\n#include \"include/file.h\"\n#include \"include/ipc.h\"\n#include \"include/path.h\"\n#include \"include/policy.h\"\n#include \"include/procattr.h\"\n\n/* Flag indicating whether initialization completed */\nint apparmor_initialized __initdata;\n\n/*\n * LSM hook functions\n */\n\n/*\n * free the associated aa_task_cxt and put its profiles\n */\nstatic void apparmor_cred_free(struct cred *cred)\n{\n\taa_free_task_context(cred_cxt(cred));\n\tcred_cxt(cred) = NULL;\n}\n\n/*\n * allocate the apparmor part of blank credentials\n */\nstatic int apparmor_cred_alloc_blank(struct cred *cred, gfp_t gfp)\n{\n\t/* freed by apparmor_cred_free */\n\tstruct aa_task_cxt *cxt = aa_alloc_task_context(gfp);\n\tif (!cxt)\n\t\treturn -ENOMEM;\n\n\tcred_cxt(cred) = cxt;\n\treturn 0;\n}\n\n/*\n * prepare new aa_task_cxt for modification by prepare_cred block\n */\nstatic int apparmor_cred_prepare(struct cred *new, const struct cred *old,\n\t\t\t\t gfp_t gfp)\n{\n\t/* freed by apparmor_cred_free */\n\tstruct aa_task_cxt *cxt = aa_alloc_task_context(gfp);\n\tif (!cxt)\n\t\treturn -ENOMEM;\n\n\taa_dup_task_context(cxt, cred_cxt(old));\n\tcred_cxt(new) = cxt;\n\treturn 0;\n}\n\n/*\n * transfer the apparmor data to a blank set of creds\n */\nstatic void apparmor_cred_transfer(struct cred *new, const struct cred *old)\n{\n\tconst struct aa_task_cxt *old_cxt = cred_cxt(old);\n\tstruct aa_task_cxt *new_cxt = cred_cxt(new);\n\n\taa_dup_task_context(new_cxt, old_cxt);\n}\n\nstatic int apparmor_ptrace_access_check(struct task_struct *child,\n\t\t\t\t\tunsigned int mode)\n{\n\treturn aa_ptrace(current, child, mode);\n}\n\nstatic int apparmor_ptrace_traceme(struct task_struct *parent)\n{\n\treturn aa_ptrace(parent, current, PTRACE_MODE_ATTACH);\n}\n\n/* Derived from security/commoncap.c:cap_capget */\nstatic int apparmor_capget(struct task_struct *target, kernel_cap_t *effective,\n\t\t\t   kernel_cap_t *inheritable, kernel_cap_t *permitted)\n{\n\tstruct aa_profile *profile;\n\tconst struct cred *cred;\n\n\trcu_read_lock();\n\tcred = __task_cred(target);\n\tprofile = aa_cred_profile(cred);\n\n\t/*\n\t * cap_capget is stacked ahead of this and will\n\t * initialize effective and permitted.\n\t */\n\tif (!unconfined(profile) && !COMPLAIN_MODE(profile)) {\n\t\t*effective = cap_intersect(*effective, profile->caps.allow);\n\t\t*permitted = cap_intersect(*permitted, profile->caps.allow);\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n}\n\nstatic int apparmor_capable(const struct cred *cred, struct user_namespace *ns,\n\t\t\t    int cap, int audit)\n{\n\tstruct aa_profile *profile;\n\tint error = 0;\n\n\tprofile = aa_cred_profile(cred);\n\tif (!unconfined(profile))\n\t\terror = aa_capable(profile, cap, audit);\n\treturn error;\n}\n\n/**\n * common_perm - basic common permission check wrapper fn for paths\n * @op: operation being checked\n * @path: path to check permission of  (NOT NULL)\n * @mask: requested permissions mask\n * @cond: conditional info for the permission request  (NOT NULL)\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm(int op, const struct path *path, u32 mask,\n\t\t       struct path_cond *cond)\n{\n\tstruct aa_profile *profile;\n\tint error = 0;\n\n\tprofile = __aa_current_profile();\n\tif (!unconfined(profile))\n\t\terror = aa_path_perm(op, profile, path, 0, mask, cond);\n\n\treturn error;\n}\n\n/**\n * common_perm_dir_dentry - common permission wrapper when path is dir, dentry\n * @op: operation being checked\n * @dir: directory of the dentry  (NOT NULL)\n * @dentry: dentry to check  (NOT NULL)\n * @mask: requested permissions mask\n * @cond: conditional info for the permission request  (NOT NULL)\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm_dir_dentry(int op, const struct path *dir,\n\t\t\t\t  struct dentry *dentry, u32 mask,\n\t\t\t\t  struct path_cond *cond)\n{\n\tstruct path path = { dir->mnt, dentry };\n\n\treturn common_perm(op, &path, mask, cond);\n}\n\n/**\n * common_perm_path - common permission wrapper when mnt, dentry\n * @op: operation being checked\n * @path: location to check (NOT NULL)\n * @mask: requested permissions mask\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic inline int common_perm_path(int op, const struct path *path, u32 mask)\n{\n\tstruct path_cond cond = { d_backing_inode(path->dentry)->i_uid,\n\t\t\t\t  d_backing_inode(path->dentry)->i_mode\n\t};\n\tif (!mediated_filesystem(path->dentry))\n\t\treturn 0;\n\n\treturn common_perm(op, path, mask, &cond);\n}\n\n/**\n * common_perm_rm - common permission wrapper for operations doing rm\n * @op: operation being checked\n * @dir: directory that the dentry is in  (NOT NULL)\n * @dentry: dentry being rm'd  (NOT NULL)\n * @mask: requested permission mask\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm_rm(int op, const struct path *dir,\n\t\t\t  struct dentry *dentry, u32 mask)\n{\n\tstruct inode *inode = d_backing_inode(dentry);\n\tstruct path_cond cond = { };\n\n\tif (!inode || !mediated_filesystem(dentry))\n\t\treturn 0;\n\n\tcond.uid = inode->i_uid;\n\tcond.mode = inode->i_mode;\n\n\treturn common_perm_dir_dentry(op, dir, dentry, mask, &cond);\n}\n\n/**\n * common_perm_create - common permission wrapper for operations doing create\n * @op: operation being checked\n * @dir: directory that dentry will be created in  (NOT NULL)\n * @dentry: dentry to create   (NOT NULL)\n * @mask: request permission mask\n * @mode: created file mode\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm_create(int op, const struct path *dir,\n\t\t\t      struct dentry *dentry, u32 mask, umode_t mode)\n{\n\tstruct path_cond cond = { current_fsuid(), mode };\n\n\tif (!mediated_filesystem(dir->dentry))\n\t\treturn 0;\n\n\treturn common_perm_dir_dentry(op, dir, dentry, mask, &cond);\n}\n\nstatic int apparmor_path_unlink(const struct path *dir, struct dentry *dentry)\n{\n\treturn common_perm_rm(OP_UNLINK, dir, dentry, AA_MAY_DELETE);\n}\n\nstatic int apparmor_path_mkdir(const struct path *dir, struct dentry *dentry,\n\t\t\t       umode_t mode)\n{\n\treturn common_perm_create(OP_MKDIR, dir, dentry, AA_MAY_CREATE,\n\t\t\t\t  S_IFDIR);\n}\n\nstatic int apparmor_path_rmdir(const struct path *dir, struct dentry *dentry)\n{\n\treturn common_perm_rm(OP_RMDIR, dir, dentry, AA_MAY_DELETE);\n}\n\nstatic int apparmor_path_mknod(const struct path *dir, struct dentry *dentry,\n\t\t\t       umode_t mode, unsigned int dev)\n{\n\treturn common_perm_create(OP_MKNOD, dir, dentry, AA_MAY_CREATE, mode);\n}\n\nstatic int apparmor_path_truncate(const struct path *path)\n{\n\treturn common_perm_path(OP_TRUNC, path, MAY_WRITE | AA_MAY_META_WRITE);\n}\n\nstatic int apparmor_path_symlink(const struct path *dir, struct dentry *dentry,\n\t\t\t\t const char *old_name)\n{\n\treturn common_perm_create(OP_SYMLINK, dir, dentry, AA_MAY_CREATE,\n\t\t\t\t  S_IFLNK);\n}\n\nstatic int apparmor_path_link(struct dentry *old_dentry, const struct path *new_dir,\n\t\t\t      struct dentry *new_dentry)\n{\n\tstruct aa_profile *profile;\n\tint error = 0;\n\n\tif (!mediated_filesystem(old_dentry))\n\t\treturn 0;\n\n\tprofile = aa_current_profile();\n\tif (!unconfined(profile))\n\t\terror = aa_path_link(profile, old_dentry, new_dir, new_dentry);\n\treturn error;\n}\n\nstatic int apparmor_path_rename(const struct path *old_dir, struct dentry *old_dentry,\n\t\t\t\tconst struct path *new_dir, struct dentry *new_dentry)\n{\n\tstruct aa_profile *profile;\n\tint error = 0;\n\n\tif (!mediated_filesystem(old_dentry))\n\t\treturn 0;\n\n\tprofile = aa_current_profile();\n\tif (!unconfined(profile)) {\n\t\tstruct path old_path = { old_dir->mnt, old_dentry };\n\t\tstruct path new_path = { new_dir->mnt, new_dentry };\n\t\tstruct path_cond cond = { d_backing_inode(old_dentry)->i_uid,\n\t\t\t\t\t  d_backing_inode(old_dentry)->i_mode\n\t\t};\n\n\t\terror = aa_path_perm(OP_RENAME_SRC, profile, &old_path, 0,\n\t\t\t\t     MAY_READ | AA_MAY_META_READ | MAY_WRITE |\n\t\t\t\t     AA_MAY_META_WRITE | AA_MAY_DELETE,\n\t\t\t\t     &cond);\n\t\tif (!error)\n\t\t\terror = aa_path_perm(OP_RENAME_DEST, profile, &new_path,\n\t\t\t\t\t     0, MAY_WRITE | AA_MAY_META_WRITE |\n\t\t\t\t\t     AA_MAY_CREATE, &cond);\n\n\t}\n\treturn error;\n}\n\nstatic int apparmor_path_chmod(const struct path *path, umode_t mode)\n{\n\treturn common_perm_path(OP_CHMOD, path, AA_MAY_CHMOD);\n}\n\nstatic int apparmor_path_chown(const struct path *path, kuid_t uid, kgid_t gid)\n{\n\treturn common_perm_path(OP_CHOWN, path, AA_MAY_CHOWN);\n}\n\nstatic int apparmor_inode_getattr(const struct path *path)\n{\n\treturn common_perm_path(OP_GETATTR, path, AA_MAY_META_READ);\n}\n\nstatic int apparmor_file_open(struct file *file, const struct cred *cred)\n{\n\tstruct aa_file_cxt *fcxt = file->f_security;\n\tstruct aa_profile *profile;\n\tint error = 0;\n\n\tif (!mediated_filesystem(file->f_path.dentry))\n\t\treturn 0;\n\n\t/* If in exec, permission is handled by bprm hooks.\n\t * Cache permissions granted by the previous exec check, with\n\t * implicit read and executable mmap which are required to\n\t * actually execute the image.\n\t */\n\tif (current->in_execve) {\n\t\tfcxt->allow = MAY_EXEC | MAY_READ | AA_EXEC_MMAP;\n\t\treturn 0;\n\t}\n\n\tprofile = aa_cred_profile(cred);\n\tif (!unconfined(profile)) {\n\t\tstruct inode *inode = file_inode(file);\n\t\tstruct path_cond cond = { inode->i_uid, inode->i_mode };\n\n\t\terror = aa_path_perm(OP_OPEN, profile, &file->f_path, 0,\n\t\t\t\t     aa_map_file_to_perms(file), &cond);\n\t\t/* todo cache full allowed permissions set and state */\n\t\tfcxt->allow = aa_map_file_to_perms(file);\n\t}\n\n\treturn error;\n}\n\nstatic int apparmor_file_alloc_security(struct file *file)\n{\n\t/* freed by apparmor_file_free_security */\n\tfile->f_security = aa_alloc_file_context(GFP_KERNEL);\n\tif (!file->f_security)\n\t\treturn -ENOMEM;\n\treturn 0;\n\n}\n\nstatic void apparmor_file_free_security(struct file *file)\n{\n\tstruct aa_file_cxt *cxt = file->f_security;\n\n\taa_free_file_context(cxt);\n}\n\nstatic int common_file_perm(int op, struct file *file, u32 mask)\n{\n\tstruct aa_file_cxt *fcxt = file->f_security;\n\tstruct aa_profile *profile, *fprofile = aa_cred_profile(file->f_cred);\n\tint error = 0;\n\n\tBUG_ON(!fprofile);\n\n\tif (!file->f_path.mnt ||\n\t    !mediated_filesystem(file->f_path.dentry))\n\t\treturn 0;\n\n\tprofile = __aa_current_profile();\n\n\t/* revalidate access, if task is unconfined, or the cached cred\n\t * doesn't match or if the request is for more permissions than\n\t * was granted.\n\t *\n\t * Note: the test for !unconfined(fprofile) is to handle file\n\t *       delegation from unconfined tasks\n\t */\n\tif (!unconfined(profile) && !unconfined(fprofile) &&\n\t    ((fprofile != profile) || (mask & ~fcxt->allow)))\n\t\terror = aa_file_perm(op, profile, file, mask);\n\n\treturn error;\n}\n\nstatic int apparmor_file_permission(struct file *file, int mask)\n{\n\treturn common_file_perm(OP_FPERM, file, mask);\n}\n\nstatic int apparmor_file_lock(struct file *file, unsigned int cmd)\n{\n\tu32 mask = AA_MAY_LOCK;\n\n\tif (cmd == F_WRLCK)\n\t\tmask |= MAY_WRITE;\n\n\treturn common_file_perm(OP_FLOCK, file, mask);\n}\n\nstatic int common_mmap(int op, struct file *file, unsigned long prot,\n\t\t       unsigned long flags)\n{\n\tint mask = 0;\n\n\tif (!file || !file->f_security)\n\t\treturn 0;\n\n\tif (prot & PROT_READ)\n\t\tmask |= MAY_READ;\n\t/*\n\t * Private mappings don't require write perms since they don't\n\t * write back to the files\n\t */\n\tif ((prot & PROT_WRITE) && !(flags & MAP_PRIVATE))\n\t\tmask |= MAY_WRITE;\n\tif (prot & PROT_EXEC)\n\t\tmask |= AA_EXEC_MMAP;\n\n\treturn common_file_perm(op, file, mask);\n}\n\nstatic int apparmor_mmap_file(struct file *file, unsigned long reqprot,\n\t\t\t      unsigned long prot, unsigned long flags)\n{\n\treturn common_mmap(OP_FMMAP, file, prot, flags);\n}\n\nstatic int apparmor_file_mprotect(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long reqprot, unsigned long prot)\n{\n\treturn common_mmap(OP_FMPROT, vma->vm_file, prot,\n\t\t\t   !(vma->vm_flags & VM_SHARED) ? MAP_PRIVATE : 0);\n}\n\nstatic int apparmor_getprocattr(struct task_struct *task, char *name,\n\t\t\t\tchar **value)\n{\n\tint error = -ENOENT;\n\t/* released below */\n\tconst struct cred *cred = get_task_cred(task);\n\tstruct aa_task_cxt *cxt = cred_cxt(cred);\n\tstruct aa_profile *profile = NULL;\n\n\tif (strcmp(name, \"current\") == 0)\n\t\tprofile = aa_get_newest_profile(cxt->profile);\n\telse if (strcmp(name, \"prev\") == 0  && cxt->previous)\n\t\tprofile = aa_get_newest_profile(cxt->previous);\n\telse if (strcmp(name, \"exec\") == 0 && cxt->onexec)\n\t\tprofile = aa_get_newest_profile(cxt->onexec);\n\telse\n\t\terror = -EINVAL;\n\n\tif (profile)\n\t\terror = aa_getprocattr(profile, value);\n\n\taa_put_profile(profile);\n\tput_cred(cred);\n\n\treturn error;\n}\n\nstatic int apparmor_setprocattr(struct task_struct *task, char *name,\n\t\t\t\tvoid *value, size_t size)\n{\n\tstruct common_audit_data sa;\n\tstruct apparmor_audit_data aad = {0,};\n\tchar *command, *largs = NULL, *args = value;\n\tsize_t arg_size;\n\tint error;\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\t/* task can only write its own attributes */\n\tif (current != task)\n\t\treturn -EACCES;\n\n\t/* AppArmor requires that the buffer must be null terminated atm */\n\tif (args[size - 1] != '\\0') {\n\t\t/* null terminate */\n\t\tlargs = args = kmalloc(size + 1, GFP_KERNEL);\n\t\tif (!args)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(args, value, size);\n\t\targs[size] = '\\0';\n\t}\n\n\terror = -EINVAL;\n\targs = strim(args);\n\tcommand = strsep(&args, \" \");\n\tif (!args)\n\t\tgoto out;\n\targs = skip_spaces(args);\n\tif (!*args)\n\t\tgoto out;\n\n\targ_size = size - (args - (char *) value);\n\tif (strcmp(name, \"current\") == 0) {\n\t\tif (strcmp(command, \"changehat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t !AA_DO_TEST);\n\t\t} else if (strcmp(command, \"permhat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t AA_DO_TEST);\n\t\t} else if (strcmp(command, \"changeprofile\") == 0) {\n\t\t\terror = aa_setprocattr_changeprofile(args, !AA_ONEXEC,\n\t\t\t\t\t\t\t     !AA_DO_TEST);\n\t\t} else if (strcmp(command, \"permprofile\") == 0) {\n\t\t\terror = aa_setprocattr_changeprofile(args, !AA_ONEXEC,\n\t\t\t\t\t\t\t     AA_DO_TEST);\n\t\t} else\n\t\t\tgoto fail;\n\t} else if (strcmp(name, \"exec\") == 0) {\n\t\tif (strcmp(command, \"exec\") == 0)\n\t\t\terror = aa_setprocattr_changeprofile(args, AA_ONEXEC,\n\t\t\t\t\t\t\t     !AA_DO_TEST);\n\t\telse\n\t\t\tgoto fail;\n\t} else\n\t\t/* only support the \"current\" and \"exec\" process attributes */\n\t\tgoto fail;\n\n\tif (!error)\n\t\terror = size;\nout:\n\tkfree(largs);\n\treturn error;\n\nfail:\n\tsa.type = LSM_AUDIT_DATA_NONE;\n\tsa.aad = &aad;\n\taad.profile = aa_current_profile();\n\taad.op = OP_SETPROCATTR;\n\taad.info = name;\n\taad.error = error = -EINVAL;\n\taa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);\n\tgoto out;\n}\n\nstatic int apparmor_task_setrlimit(struct task_struct *task,\n\t\tunsigned int resource, struct rlimit *new_rlim)\n{\n\tstruct aa_profile *profile = __aa_current_profile();\n\tint error = 0;\n\n\tif (!unconfined(profile))\n\t\terror = aa_task_setrlimit(profile, task, resource, new_rlim);\n\n\treturn error;\n}\n\nstatic struct security_hook_list apparmor_hooks[] = {\n\tLSM_HOOK_INIT(ptrace_access_check, apparmor_ptrace_access_check),\n\tLSM_HOOK_INIT(ptrace_traceme, apparmor_ptrace_traceme),\n\tLSM_HOOK_INIT(capget, apparmor_capget),\n\tLSM_HOOK_INIT(capable, apparmor_capable),\n\n\tLSM_HOOK_INIT(path_link, apparmor_path_link),\n\tLSM_HOOK_INIT(path_unlink, apparmor_path_unlink),\n\tLSM_HOOK_INIT(path_symlink, apparmor_path_symlink),\n\tLSM_HOOK_INIT(path_mkdir, apparmor_path_mkdir),\n\tLSM_HOOK_INIT(path_rmdir, apparmor_path_rmdir),\n\tLSM_HOOK_INIT(path_mknod, apparmor_path_mknod),\n\tLSM_HOOK_INIT(path_rename, apparmor_path_rename),\n\tLSM_HOOK_INIT(path_chmod, apparmor_path_chmod),\n\tLSM_HOOK_INIT(path_chown, apparmor_path_chown),\n\tLSM_HOOK_INIT(path_truncate, apparmor_path_truncate),\n\tLSM_HOOK_INIT(inode_getattr, apparmor_inode_getattr),\n\n\tLSM_HOOK_INIT(file_open, apparmor_file_open),\n\tLSM_HOOK_INIT(file_permission, apparmor_file_permission),\n\tLSM_HOOK_INIT(file_alloc_security, apparmor_file_alloc_security),\n\tLSM_HOOK_INIT(file_free_security, apparmor_file_free_security),\n\tLSM_HOOK_INIT(mmap_file, apparmor_mmap_file),\n\tLSM_HOOK_INIT(file_mprotect, apparmor_file_mprotect),\n\tLSM_HOOK_INIT(file_lock, apparmor_file_lock),\n\n\tLSM_HOOK_INIT(getprocattr, apparmor_getprocattr),\n\tLSM_HOOK_INIT(setprocattr, apparmor_setprocattr),\n\n\tLSM_HOOK_INIT(cred_alloc_blank, apparmor_cred_alloc_blank),\n\tLSM_HOOK_INIT(cred_free, apparmor_cred_free),\n\tLSM_HOOK_INIT(cred_prepare, apparmor_cred_prepare),\n\tLSM_HOOK_INIT(cred_transfer, apparmor_cred_transfer),\n\n\tLSM_HOOK_INIT(bprm_set_creds, apparmor_bprm_set_creds),\n\tLSM_HOOK_INIT(bprm_committing_creds, apparmor_bprm_committing_creds),\n\tLSM_HOOK_INIT(bprm_committed_creds, apparmor_bprm_committed_creds),\n\tLSM_HOOK_INIT(bprm_secureexec, apparmor_bprm_secureexec),\n\n\tLSM_HOOK_INIT(task_setrlimit, apparmor_task_setrlimit),\n};\n\n/*\n * AppArmor sysfs module parameters\n */\n\nstatic int param_set_aabool(const char *val, const struct kernel_param *kp);\nstatic int param_get_aabool(char *buffer, const struct kernel_param *kp);\n#define param_check_aabool param_check_bool\nstatic const struct kernel_param_ops param_ops_aabool = {\n\t.flags = KERNEL_PARAM_OPS_FL_NOARG,\n\t.set = param_set_aabool,\n\t.get = param_get_aabool\n};\n\nstatic int param_set_aauint(const char *val, const struct kernel_param *kp);\nstatic int param_get_aauint(char *buffer, const struct kernel_param *kp);\n#define param_check_aauint param_check_uint\nstatic const struct kernel_param_ops param_ops_aauint = {\n\t.set = param_set_aauint,\n\t.get = param_get_aauint\n};\n\nstatic int param_set_aalockpolicy(const char *val, const struct kernel_param *kp);\nstatic int param_get_aalockpolicy(char *buffer, const struct kernel_param *kp);\n#define param_check_aalockpolicy param_check_bool\nstatic const struct kernel_param_ops param_ops_aalockpolicy = {\n\t.flags = KERNEL_PARAM_OPS_FL_NOARG,\n\t.set = param_set_aalockpolicy,\n\t.get = param_get_aalockpolicy\n};\n\nstatic int param_set_audit(const char *val, struct kernel_param *kp);\nstatic int param_get_audit(char *buffer, struct kernel_param *kp);\n\nstatic int param_set_mode(const char *val, struct kernel_param *kp);\nstatic int param_get_mode(char *buffer, struct kernel_param *kp);\n\n/* Flag values, also controllable via /sys/module/apparmor/parameters\n * We define special types as we want to do additional mediation.\n */\n\n/* AppArmor global enforcement switch - complain, enforce, kill */\nenum profile_mode aa_g_profile_mode = APPARMOR_ENFORCE;\nmodule_param_call(mode, param_set_mode, param_get_mode,\n\t\t  &aa_g_profile_mode, S_IRUSR | S_IWUSR);\n\n/* Debug mode */\nbool aa_g_debug;\nmodule_param_named(debug, aa_g_debug, aabool, S_IRUSR | S_IWUSR);\n\n/* Audit mode */\nenum audit_mode aa_g_audit;\nmodule_param_call(audit, param_set_audit, param_get_audit,\n\t\t  &aa_g_audit, S_IRUSR | S_IWUSR);\n\n/* Determines if audit header is included in audited messages.  This\n * provides more context if the audit daemon is not running\n */\nbool aa_g_audit_header = 1;\nmodule_param_named(audit_header, aa_g_audit_header, aabool,\n\t\t   S_IRUSR | S_IWUSR);\n\n/* lock out loading/removal of policy\n * TODO: add in at boot loading of policy, which is the only way to\n *       load policy, if lock_policy is set\n */\nbool aa_g_lock_policy;\nmodule_param_named(lock_policy, aa_g_lock_policy, aalockpolicy,\n\t\t   S_IRUSR | S_IWUSR);\n\n/* Syscall logging mode */\nbool aa_g_logsyscall;\nmodule_param_named(logsyscall, aa_g_logsyscall, aabool, S_IRUSR | S_IWUSR);\n\n/* Maximum pathname length before accesses will start getting rejected */\nunsigned int aa_g_path_max = 2 * PATH_MAX;\nmodule_param_named(path_max, aa_g_path_max, aauint, S_IRUSR | S_IWUSR);\n\n/* Determines how paranoid loading of policy is and how much verification\n * on the loaded policy is done.\n */\nbool aa_g_paranoid_load = 1;\nmodule_param_named(paranoid_load, aa_g_paranoid_load, aabool,\n\t\t   S_IRUSR | S_IWUSR);\n\n/* Boot time disable flag */\nstatic bool apparmor_enabled = CONFIG_SECURITY_APPARMOR_BOOTPARAM_VALUE;\nmodule_param_named(enabled, apparmor_enabled, bool, S_IRUGO);\n\nstatic int __init apparmor_enabled_setup(char *str)\n{\n\tunsigned long enabled;\n\tint error = kstrtoul(str, 0, &enabled);\n\tif (!error)\n\t\tapparmor_enabled = enabled ? 1 : 0;\n\treturn 1;\n}\n\n__setup(\"apparmor=\", apparmor_enabled_setup);\n\n/* set global flag turning off the ability to load policy */\nstatic int param_set_aalockpolicy(const char *val, const struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\tif (aa_g_lock_policy)\n\t\treturn -EACCES;\n\treturn param_set_bool(val, kp);\n}\n\nstatic int param_get_aalockpolicy(char *buffer, const struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\treturn param_get_bool(buffer, kp);\n}\n\nstatic int param_set_aabool(const char *val, const struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\treturn param_set_bool(val, kp);\n}\n\nstatic int param_get_aabool(char *buffer, const struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\treturn param_get_bool(buffer, kp);\n}\n\nstatic int param_set_aauint(const char *val, const struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\treturn param_set_uint(val, kp);\n}\n\nstatic int param_get_aauint(char *buffer, const struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\treturn param_get_uint(buffer, kp);\n}\n\nstatic int param_get_audit(char *buffer, struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\n\treturn sprintf(buffer, \"%s\", audit_mode_names[aa_g_audit]);\n}\n\nstatic int param_set_audit(const char *val, struct kernel_param *kp)\n{\n\tint i;\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\n\tif (!val)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < AUDIT_MAX_INDEX; i++) {\n\t\tif (strcmp(val, audit_mode_names[i]) == 0) {\n\t\t\taa_g_audit = i;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int param_get_mode(char *buffer, struct kernel_param *kp)\n{\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\n\treturn sprintf(buffer, \"%s\", aa_profile_mode_names[aa_g_profile_mode]);\n}\n\nstatic int param_set_mode(const char *val, struct kernel_param *kp)\n{\n\tint i;\n\tif (!capable(CAP_MAC_ADMIN))\n\t\treturn -EPERM;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\n\tif (!val)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < APPARMOR_MODE_NAMES_MAX_INDEX; i++) {\n\t\tif (strcmp(val, aa_profile_mode_names[i]) == 0) {\n\t\t\taa_g_profile_mode = i;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\n\n/*\n * AppArmor init functions\n */\n\n/**\n * set_init_cxt - set a task context and profile on the first task.\n *\n * TODO: allow setting an alternate profile than unconfined\n */\nstatic int __init set_init_cxt(void)\n{\n\tstruct cred *cred = (struct cred *)current->real_cred;\n\tstruct aa_task_cxt *cxt;\n\n\tcxt = aa_alloc_task_context(GFP_KERNEL);\n\tif (!cxt)\n\t\treturn -ENOMEM;\n\n\tcxt->profile = aa_get_profile(root_ns->unconfined);\n\tcred_cxt(cred) = cxt;\n\n\treturn 0;\n}\n\nstatic int __init apparmor_init(void)\n{\n\tint error;\n\n\tif (!apparmor_enabled || !security_module_enable(\"apparmor\")) {\n\t\taa_info_message(\"AppArmor disabled by boot time parameter\");\n\t\tapparmor_enabled = 0;\n\t\treturn 0;\n\t}\n\n\terror = aa_alloc_root_ns();\n\tif (error) {\n\t\tAA_ERROR(\"Unable to allocate default profile namespace\\n\");\n\t\tgoto alloc_out;\n\t}\n\n\terror = set_init_cxt();\n\tif (error) {\n\t\tAA_ERROR(\"Failed to set context on init task\\n\");\n\t\taa_free_root_ns();\n\t\tgoto alloc_out;\n\t}\n\tsecurity_add_hooks(apparmor_hooks, ARRAY_SIZE(apparmor_hooks));\n\n\t/* Report that AppArmor successfully initialized */\n\tapparmor_initialized = 1;\n\tif (aa_g_profile_mode == APPARMOR_COMPLAIN)\n\t\taa_info_message(\"AppArmor initialized: complain mode enabled\");\n\telse if (aa_g_profile_mode == APPARMOR_KILL)\n\t\taa_info_message(\"AppArmor initialized: kill mode enabled\");\n\telse\n\t\taa_info_message(\"AppArmor initialized\");\n\n\treturn error;\n\nalloc_out:\n\taa_destroy_aafs();\n\n\tapparmor_enabled = 0;\n\treturn error;\n}\n\nsecurity_initcall(apparmor_init);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n * AppArmor security module\n *\n * This file contains AppArmor LSM hooks.\n *\n * Copyright (C) 1998-2008 Novell/SUSE\n * Copyright 2009-2010 Canonical Ltd.\n */\n\n#include <linux/lsm_hooks.h>\n#include <linux/moduleparam.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/mount.h>\n#include <linux/namei.h>\n#include <linux/ptrace.h>\n#include <linux/ctype.h>\n#include <linux/sysctl.h>\n#include <linux/audit.h>\n#include <linux/user_namespace.h>\n#include <linux/netfilter_ipv4.h>\n#include <linux/netfilter_ipv6.h>\n#include <linux/zstd.h>\n#include <net/sock.h>\n#include <uapi/linux/mount.h>\n#include <uapi/linux/lsm.h>\n\n#include \"include/af_unix.h\"\n#include \"include/apparmor.h\"\n#include \"include/apparmorfs.h\"\n#include \"include/audit.h\"\n#include \"include/capability.h\"\n#include \"include/cred.h\"\n#include \"include/file.h\"\n#include \"include/ipc.h\"\n#include \"include/net.h\"\n#include \"include/path.h\"\n#include \"include/label.h\"\n#include \"include/policy.h\"\n#include \"include/policy_ns.h\"\n#include \"include/procattr.h\"\n#include \"include/mount.h\"\n#include \"include/secid.h\"\n\n/* Flag indicating whether initialization completed */\nint apparmor_initialized;\n\nunion aa_buffer {\n\tstruct list_head list;\n\tDECLARE_FLEX_ARRAY(char, buffer);\n};\n\nstruct aa_local_cache {\n\tunsigned int hold;\n\tunsigned int count;\n\tstruct list_head head;\n};\n\n#define RESERVE_COUNT 2\nstatic int reserve_count = RESERVE_COUNT;\nstatic int buffer_count;\n\nstatic LIST_HEAD(aa_global_buffers);\nstatic DEFINE_SPINLOCK(aa_buffers_lock);\nstatic DEFINE_PER_CPU(struct aa_local_cache, aa_local_buffers);\n\n/*\n * LSM hook functions\n */\n\n/*\n * put the associated labels\n */\nstatic void apparmor_cred_free(struct cred *cred)\n{\n\taa_put_label(cred_label(cred));\n\tset_cred_label(cred, NULL);\n}\n\n/*\n * allocate the apparmor part of blank credentials\n */\nstatic int apparmor_cred_alloc_blank(struct cred *cred, gfp_t gfp)\n{\n\tset_cred_label(cred, NULL);\n\treturn 0;\n}\n\n/*\n * prepare new cred label for modification by prepare_cred block\n */\nstatic int apparmor_cred_prepare(struct cred *new, const struct cred *old,\n\t\t\t\t gfp_t gfp)\n{\n\tset_cred_label(new, aa_get_newest_label(cred_label(old)));\n\treturn 0;\n}\n\n/*\n * transfer the apparmor data to a blank set of creds\n */\nstatic void apparmor_cred_transfer(struct cred *new, const struct cred *old)\n{\n\tset_cred_label(new, aa_get_newest_label(cred_label(old)));\n}\n\nstatic void apparmor_task_free(struct task_struct *task)\n{\n\n\taa_free_task_ctx(task_ctx(task));\n}\n\nstatic int apparmor_task_alloc(struct task_struct *task,\n\t\t\t       u64 clone_flags)\n{\n\tstruct aa_task_ctx *new = task_ctx(task);\n\n\taa_dup_task_ctx(new, task_ctx(current));\n\n\treturn 0;\n}\n\nstatic int apparmor_ptrace_access_check(struct task_struct *child,\n\t\t\t\t\tunsigned int mode)\n{\n\tstruct aa_label *tracer, *tracee;\n\tconst struct cred *cred;\n\tint error;\n\tbool needput;\n\n\tcred = get_task_cred(child);\n\ttracee = cred_label(cred);\t/* ref count on cred */\n\ttracer = __begin_current_label_crit_section(&needput);\n\terror = aa_may_ptrace(current_cred(), tracer, cred, tracee,\n\t\t\t(mode & PTRACE_MODE_READ) ? AA_PTRACE_READ\n\t\t\t\t\t\t  : AA_PTRACE_TRACE);\n\t__end_current_label_crit_section(tracer, needput);\n\tput_cred(cred);\n\n\treturn error;\n}\n\nstatic int apparmor_ptrace_traceme(struct task_struct *parent)\n{\n\tstruct aa_label *tracer, *tracee;\n\tconst struct cred *cred;\n\tint error;\n\tbool needput;\n\n\ttracee = __begin_current_label_crit_section(&needput);\n\tcred = get_task_cred(parent);\n\ttracer = cred_label(cred);\t/* ref count on cred */\n\terror = aa_may_ptrace(cred, tracer, current_cred(), tracee,\n\t\t\t      AA_PTRACE_TRACE);\n\tput_cred(cred);\n\t__end_current_label_crit_section(tracee, needput);\n\n\treturn error;\n}\n\n/* Derived from security/commoncap.c:cap_capget */\nstatic int apparmor_capget(const struct task_struct *target, kernel_cap_t *effective,\n\t\t\t   kernel_cap_t *inheritable, kernel_cap_t *permitted)\n{\n\tstruct aa_label *label;\n\tconst struct cred *cred;\n\n\trcu_read_lock();\n\tcred = __task_cred(target);\n\tlabel = aa_get_newest_cred_label(cred);\n\n\t/*\n\t * cap_capget is stacked ahead of this and will\n\t * initialize effective and permitted.\n\t */\n\tif (!unconfined(label)) {\n\t\tstruct aa_profile *profile;\n\t\tstruct label_it i;\n\n\t\tlabel_for_each_confined(i, label, profile) {\n\t\t\tkernel_cap_t allowed;\n\n\t\t\tallowed = aa_profile_capget(profile);\n\t\t\t*effective = cap_intersect(*effective, allowed);\n\t\t\t*permitted = cap_intersect(*permitted, allowed);\n\t\t}\n\t}\n\trcu_read_unlock();\n\taa_put_label(label);\n\n\treturn 0;\n}\n\nstatic int apparmor_capable(const struct cred *cred, struct user_namespace *ns,\n\t\t\t    int cap, unsigned int opts)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\n\tlabel = aa_get_newest_cred_label(cred);\n\tif (!unconfined(label))\n\t\terror = aa_capable(cred, label, cap, opts);\n\taa_put_label(label);\n\n\treturn error;\n}\n\n/**\n * common_perm - basic common permission check wrapper fn for paths\n * @op: operation being checked\n * @path: path to check permission of  (NOT NULL)\n * @mask: requested permissions mask\n * @cond: conditional info for the permission request  (NOT NULL)\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm(const char *op, const struct path *path, u32 mask,\n\t\t       struct path_cond *cond)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\tif (!unconfined(label))\n\t\terror = aa_path_perm(op, current_cred(), label, path, 0, mask,\n\t\t\t\t     cond);\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\n/**\n * common_perm_cond - common permission wrapper around inode cond\n * @op: operation being checked\n * @path: location to check (NOT NULL)\n * @mask: requested permissions mask\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm_cond(const char *op, const struct path *path, u32 mask)\n{\n\tvfsuid_t vfsuid = i_uid_into_vfsuid(mnt_idmap(path->mnt),\n\t\t\t\t\t    d_backing_inode(path->dentry));\n\tstruct path_cond cond = {\n\t\tvfsuid_into_kuid(vfsuid),\n\t\td_backing_inode(path->dentry)->i_mode\n\t};\n\n\tif (!path_mediated_fs(path->dentry))\n\t\treturn 0;\n\n\treturn common_perm(op, path, mask, &cond);\n}\n\n/**\n * common_perm_dir_dentry - common permission wrapper when path is dir, dentry\n * @op: operation being checked\n * @dir: directory of the dentry  (NOT NULL)\n * @dentry: dentry to check  (NOT NULL)\n * @mask: requested permissions mask\n * @cond: conditional info for the permission request  (NOT NULL)\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm_dir_dentry(const char *op, const struct path *dir,\n\t\t\t\t  struct dentry *dentry, u32 mask,\n\t\t\t\t  struct path_cond *cond)\n{\n\tstruct path path = { .mnt = dir->mnt, .dentry = dentry };\n\n\treturn common_perm(op, &path, mask, cond);\n}\n\n/**\n * common_perm_rm - common permission wrapper for operations doing rm\n * @op: operation being checked\n * @dir: directory that the dentry is in  (NOT NULL)\n * @dentry: dentry being rm'd  (NOT NULL)\n * @mask: requested permission mask\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm_rm(const char *op, const struct path *dir,\n\t\t\t  struct dentry *dentry, u32 mask)\n{\n\tstruct inode *inode = d_backing_inode(dentry);\n\tstruct path_cond cond = { };\n\tvfsuid_t vfsuid;\n\n\tif (!inode || !path_mediated_fs(dentry))\n\t\treturn 0;\n\n\tvfsuid = i_uid_into_vfsuid(mnt_idmap(dir->mnt), inode);\n\tcond.uid = vfsuid_into_kuid(vfsuid);\n\tcond.mode = inode->i_mode;\n\n\treturn common_perm_dir_dentry(op, dir, dentry, mask, &cond);\n}\n\n/**\n * common_perm_create - common permission wrapper for operations doing create\n * @op: operation being checked\n * @dir: directory that dentry will be created in  (NOT NULL)\n * @dentry: dentry to create   (NOT NULL)\n * @mask: request permission mask\n * @mode: created file mode\n *\n * Returns: %0 else error code if error or permission denied\n */\nstatic int common_perm_create(const char *op, const struct path *dir,\n\t\t\t      struct dentry *dentry, u32 mask, umode_t mode)\n{\n\tstruct path_cond cond = { current_fsuid(), mode };\n\n\tif (!path_mediated_fs(dir->dentry))\n\t\treturn 0;\n\n\treturn common_perm_dir_dentry(op, dir, dentry, mask, &cond);\n}\n\nstatic int apparmor_path_unlink(const struct path *dir, struct dentry *dentry)\n{\n\treturn common_perm_rm(OP_UNLINK, dir, dentry, AA_MAY_DELETE);\n}\n\nstatic int apparmor_path_mkdir(const struct path *dir, struct dentry *dentry,\n\t\t\t       umode_t mode)\n{\n\treturn common_perm_create(OP_MKDIR, dir, dentry, AA_MAY_CREATE,\n\t\t\t\t  S_IFDIR);\n}\n\nstatic int apparmor_path_rmdir(const struct path *dir, struct dentry *dentry)\n{\n\treturn common_perm_rm(OP_RMDIR, dir, dentry, AA_MAY_DELETE);\n}\n\nstatic int apparmor_path_mknod(const struct path *dir, struct dentry *dentry,\n\t\t\t       umode_t mode, unsigned int dev)\n{\n\treturn common_perm_create(OP_MKNOD, dir, dentry, AA_MAY_CREATE, mode);\n}\n\nstatic int apparmor_path_truncate(const struct path *path)\n{\n\treturn common_perm_cond(OP_TRUNC, path, MAY_WRITE | AA_MAY_SETATTR);\n}\n\nstatic int apparmor_file_truncate(struct file *file)\n{\n\treturn apparmor_path_truncate(&file->f_path);\n}\n\nstatic int apparmor_path_symlink(const struct path *dir, struct dentry *dentry,\n\t\t\t\t const char *old_name)\n{\n\treturn common_perm_create(OP_SYMLINK, dir, dentry, AA_MAY_CREATE,\n\t\t\t\t  S_IFLNK);\n}\n\nstatic int apparmor_path_link(struct dentry *old_dentry, const struct path *new_dir,\n\t\t\t      struct dentry *new_dentry)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\n\tif (!path_mediated_fs(old_dentry))\n\t\treturn 0;\n\n\tlabel = begin_current_label_crit_section();\n\tif (!unconfined(label))\n\t\terror = aa_path_link(current_cred(), label, old_dentry, new_dir,\n\t\t\t\t     new_dentry);\n\tend_current_label_crit_section(label);\n\n\treturn error;\n}\n\nstatic int apparmor_path_rename(const struct path *old_dir, struct dentry *old_dentry,\n\t\t\t\tconst struct path *new_dir, struct dentry *new_dentry,\n\t\t\t\tconst unsigned int flags)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\n\tif (!path_mediated_fs(old_dentry))\n\t\treturn 0;\n\tif ((flags & RENAME_EXCHANGE) && !path_mediated_fs(new_dentry))\n\t\treturn 0;\n\n\tlabel = begin_current_label_crit_section();\n\tif (!unconfined(label)) {\n\t\tstruct mnt_idmap *idmap = mnt_idmap(old_dir->mnt);\n\t\tvfsuid_t vfsuid;\n\t\tstruct path old_path = { .mnt = old_dir->mnt,\n\t\t\t\t\t .dentry = old_dentry };\n\t\tstruct path new_path = { .mnt = new_dir->mnt,\n\t\t\t\t\t .dentry = new_dentry };\n\t\tstruct path_cond cond = {\n\t\t\t.mode = d_backing_inode(old_dentry)->i_mode\n\t\t};\n\t\tvfsuid = i_uid_into_vfsuid(idmap, d_backing_inode(old_dentry));\n\t\tcond.uid = vfsuid_into_kuid(vfsuid);\n\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tstruct path_cond cond_exchange = {\n\t\t\t\t.mode = d_backing_inode(new_dentry)->i_mode,\n\t\t\t};\n\t\t\tvfsuid = i_uid_into_vfsuid(idmap, d_backing_inode(old_dentry));\n\t\t\tcond_exchange.uid = vfsuid_into_kuid(vfsuid);\n\n\t\t\terror = aa_path_perm(OP_RENAME_SRC, current_cred(),\n\t\t\t\t\t     label, &new_path, 0,\n\t\t\t\t\t     MAY_READ | AA_MAY_GETATTR | MAY_WRITE |\n\t\t\t\t\t     AA_MAY_SETATTR | AA_MAY_DELETE,\n\t\t\t\t\t     &cond_exchange);\n\t\t\tif (!error)\n\t\t\t\terror = aa_path_perm(OP_RENAME_DEST, current_cred(),\n\t\t\t\t\t\t     label, &old_path,\n\t\t\t\t\t\t     0, MAY_WRITE | AA_MAY_SETATTR |\n\t\t\t\t\t\t     AA_MAY_CREATE, &cond_exchange);\n\t\t}\n\n\t\tif (!error)\n\t\t\terror = aa_path_perm(OP_RENAME_SRC, current_cred(),\n\t\t\t\t\t     label, &old_path, 0,\n\t\t\t\t\t     MAY_READ | AA_MAY_GETATTR | MAY_WRITE |\n\t\t\t\t\t     AA_MAY_SETATTR | AA_MAY_DELETE,\n\t\t\t\t\t     &cond);\n\t\tif (!error)\n\t\t\terror = aa_path_perm(OP_RENAME_DEST, current_cred(),\n\t\t\t\t\t     label, &new_path,\n\t\t\t\t\t     0, MAY_WRITE | AA_MAY_SETATTR |\n\t\t\t\t\t     AA_MAY_CREATE, &cond);\n\n\t}\n\tend_current_label_crit_section(label);\n\n\treturn error;\n}\n\nstatic int apparmor_path_chmod(const struct path *path, umode_t mode)\n{\n\treturn common_perm_cond(OP_CHMOD, path, AA_MAY_CHMOD);\n}\n\nstatic int apparmor_path_chown(const struct path *path, kuid_t uid, kgid_t gid)\n{\n\treturn common_perm_cond(OP_CHOWN, path, AA_MAY_CHOWN);\n}\n\nstatic int apparmor_inode_getattr(const struct path *path)\n{\n\treturn common_perm_cond(OP_GETATTR, path, AA_MAY_GETATTR);\n}\n\nstatic int apparmor_file_open(struct file *file)\n{\n\tstruct aa_file_ctx *fctx = file_ctx(file);\n\tstruct aa_label *label;\n\tint error = 0;\n\tbool needput;\n\n\tif (!path_mediated_fs(file->f_path.dentry))\n\t\treturn 0;\n\n\t/* If in exec, permission is handled by bprm hooks.\n\t * Cache permissions granted by the previous exec check, with\n\t * implicit read and executable mmap which are required to\n\t * actually execute the image.\n\t *\n\t * Illogically, FMODE_EXEC is in f_flags, not f_mode.\n\t */\n\tif (file->f_flags & __FMODE_EXEC) {\n\t\tfctx->allow = MAY_EXEC | MAY_READ | AA_EXEC_MMAP;\n\t\treturn 0;\n\t}\n\n\tlabel = aa_get_newest_cred_label_condref(file->f_cred, &needput);\n\tif (!unconfined(label)) {\n\t\tstruct mnt_idmap *idmap = file_mnt_idmap(file);\n\t\tstruct inode *inode = file_inode(file);\n\t\tvfsuid_t vfsuid;\n\t\tstruct path_cond cond = {\n\t\t\t.mode = inode->i_mode,\n\t\t};\n\t\tvfsuid = i_uid_into_vfsuid(idmap, inode);\n\t\tcond.uid = vfsuid_into_kuid(vfsuid);\n\n\t\terror = aa_path_perm(OP_OPEN, file->f_cred,\n\t\t\t\t     label, &file->f_path, 0,\n\t\t\t\t     aa_map_file_to_perms(file), &cond);\n\t\t/* todo cache full allowed permissions set and state */\n\t\tfctx->allow = aa_map_file_to_perms(file);\n\t}\n\taa_put_label_condref(label, needput);\n\n\treturn error;\n}\n\nstatic int apparmor_file_alloc_security(struct file *file)\n{\n\tstruct aa_file_ctx *ctx = file_ctx(file);\n\tstruct aa_label *label = begin_current_label_crit_section();\n\n\tspin_lock_init(&ctx->lock);\n\trcu_assign_pointer(ctx->label, aa_get_label(label));\n\tend_current_label_crit_section(label);\n\treturn 0;\n}\n\nstatic void apparmor_file_free_security(struct file *file)\n{\n\tstruct aa_file_ctx *ctx = file_ctx(file);\n\n\tif (ctx)\n\t\taa_put_label(rcu_access_pointer(ctx->label));\n}\n\nstatic int common_file_perm(const char *op, struct file *file, u32 mask,\n\t\t\t    bool in_atomic)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\tbool needput;\n\n\t/* don't reaudit files closed during inheritance */\n\tif (unlikely(file->f_path.dentry == aa_null.dentry))\n\t\treturn -EACCES;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\terror = aa_file_perm(op, current_cred(), label, file, mask, in_atomic);\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\nstatic int apparmor_file_receive(struct file *file)\n{\n\treturn common_file_perm(OP_FRECEIVE, file, aa_map_file_to_perms(file),\n\t\t\t\tfalse);\n}\n\nstatic int apparmor_file_permission(struct file *file, int mask)\n{\n\treturn common_file_perm(OP_FPERM, file, mask, false);\n}\n\nstatic int apparmor_file_lock(struct file *file, unsigned int cmd)\n{\n\tu32 mask = AA_MAY_LOCK;\n\n\tif (cmd == F_WRLCK)\n\t\tmask |= MAY_WRITE;\n\n\treturn common_file_perm(OP_FLOCK, file, mask, false);\n}\n\nstatic int common_mmap(const char *op, struct file *file, unsigned long prot,\n\t\t       unsigned long flags, bool in_atomic)\n{\n\tint mask = 0;\n\n\tif (!file || !file_ctx(file))\n\t\treturn 0;\n\n\tif (prot & PROT_READ)\n\t\tmask |= MAY_READ;\n\t/*\n\t * Private mappings don't require write perms since they don't\n\t * write back to the files\n\t */\n\tif ((prot & PROT_WRITE) && !(flags & MAP_PRIVATE))\n\t\tmask |= MAY_WRITE;\n\tif (prot & PROT_EXEC)\n\t\tmask |= AA_EXEC_MMAP;\n\n\treturn common_file_perm(op, file, mask, in_atomic);\n}\n\nstatic int apparmor_mmap_file(struct file *file, unsigned long reqprot,\n\t\t\t      unsigned long prot, unsigned long flags)\n{\n\treturn common_mmap(OP_FMMAP, file, prot, flags, GFP_ATOMIC);\n}\n\nstatic int apparmor_file_mprotect(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long reqprot, unsigned long prot)\n{\n\treturn common_mmap(OP_FMPROT, vma->vm_file, prot,\n\t\t\t   !(vma->vm_flags & VM_SHARED) ? MAP_PRIVATE : 0,\n\t\t\t   false);\n}\n\n#ifdef CONFIG_IO_URING\nstatic const char *audit_uring_mask(u32 mask)\n{\n\tif (mask & AA_MAY_CREATE_SQPOLL)\n\t\treturn \"sqpoll\";\n\tif (mask & AA_MAY_OVERRIDE_CRED)\n\t\treturn \"override_creds\";\n\treturn \"\";\n}\n\nstatic void audit_uring_cb(struct audit_buffer *ab, void *va)\n{\n\tstruct apparmor_audit_data *ad = aad_of_va(va);\n\n\tif (ad->request & AA_URING_PERM_MASK) {\n\t\taudit_log_format(ab, \" requested=\\\"%s\\\"\",\n\t\t\t\t audit_uring_mask(ad->request));\n\t\tif (ad->denied & AA_URING_PERM_MASK) {\n\t\t\taudit_log_format(ab, \" denied=\\\"%s\\\"\",\n\t\t\t\t\t audit_uring_mask(ad->denied));\n\t\t}\n\t}\n\tif (ad->uring.target) {\n\t\taudit_log_format(ab, \" tcontext=\");\n\t\taa_label_xaudit(ab, labels_ns(ad->subj_label),\n\t\t\t\tad->uring.target,\n\t\t\t\tFLAGS_NONE, GFP_ATOMIC);\n\t}\n}\n\nstatic int profile_uring(struct aa_profile *profile, u32 request,\n\t\t\t struct aa_label *new, int cap,\n\t\t\t struct apparmor_audit_data *ad)\n{\n\tunsigned int state;\n\tstruct aa_ruleset *rules;\n\tint error = 0;\n\n\tAA_BUG(!profile);\n\n\trules = profile->label.rules[0];\n\tstate = RULE_MEDIATES(rules, AA_CLASS_IO_URING);\n\tif (state) {\n\t\tstruct aa_perms perms = { };\n\n\t\tif (new) {\n\t\t\taa_label_match(profile, rules, new, state,\n\t\t\t\t       false, request, &perms);\n\t\t} else {\n\t\t\tperms = *aa_lookup_perms(rules->policy, state);\n\t\t}\n\t\taa_apply_modes_to_perms(profile, &perms);\n\t\terror = aa_check_perms(profile, &perms, request, ad,\n\t\t\t\t       audit_uring_cb);\n\t}\n\n\treturn error;\n}\n\n/**\n * apparmor_uring_override_creds - check the requested cred override\n * @new: the target creds\n *\n * Check to see if the current task is allowed to override it's credentials\n * to service an io_uring operation.\n */\nstatic int apparmor_uring_override_creds(const struct cred *new)\n{\n\tstruct aa_profile *profile;\n\tstruct aa_label *label;\n\tint error;\n\tbool needput;\n\tDEFINE_AUDIT_DATA(ad, LSM_AUDIT_DATA_NONE, AA_CLASS_IO_URING,\n\t\t\t  OP_URING_OVERRIDE);\n\n\tad.uring.target = cred_label(new);\n\tlabel = __begin_current_label_crit_section(&needput);\n\terror = fn_for_each(label, profile,\n\t\t\tprofile_uring(profile, AA_MAY_OVERRIDE_CRED,\n\t\t\t\t      cred_label(new), CAP_SYS_ADMIN, &ad));\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\n/**\n * apparmor_uring_sqpoll - check if a io_uring polling thread can be created\n *\n * Check to see if the current task is allowed to create a new io_uring\n * kernel polling thread.\n */\nstatic int apparmor_uring_sqpoll(void)\n{\n\tstruct aa_profile *profile;\n\tstruct aa_label *label;\n\tint error;\n\tbool needput;\n\tDEFINE_AUDIT_DATA(ad, LSM_AUDIT_DATA_NONE, AA_CLASS_IO_URING,\n\t\t\t  OP_URING_SQPOLL);\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\terror = fn_for_each(label, profile,\n\t\t\tprofile_uring(profile, AA_MAY_CREATE_SQPOLL,\n\t\t\t\t      NULL, CAP_SYS_ADMIN, &ad));\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n#endif /* CONFIG_IO_URING */\n\nstatic int apparmor_sb_mount(const char *dev_name, const struct path *path,\n\t\t\t     const char *type, unsigned long flags, void *data)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\tbool needput;\n\n\t/* Discard magic */\n\tif ((flags & MS_MGC_MSK) == MS_MGC_VAL)\n\t\tflags &= ~MS_MGC_MSK;\n\n\tflags &= ~AA_MS_IGNORE_MASK;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\tif (!unconfined(label)) {\n\t\tif (flags & MS_REMOUNT)\n\t\t\terror = aa_remount(current_cred(), label, path, flags,\n\t\t\t\t\t   data);\n\t\telse if (flags & MS_BIND)\n\t\t\terror = aa_bind_mount(current_cred(), label, path,\n\t\t\t\t\t      dev_name, flags);\n\t\telse if (flags & (MS_SHARED | MS_PRIVATE | MS_SLAVE |\n\t\t\t\t  MS_UNBINDABLE))\n\t\t\terror = aa_mount_change_type(current_cred(), label,\n\t\t\t\t\t\t     path, flags);\n\t\telse if (flags & MS_MOVE)\n\t\t\terror = aa_move_mount_old(current_cred(), label, path,\n\t\t\t\t\t\t  dev_name);\n\t\telse\n\t\t\terror = aa_new_mount(current_cred(), label, dev_name,\n\t\t\t\t\t     path, type, flags, data);\n\t}\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\nstatic int apparmor_move_mount(const struct path *from_path,\n\t\t\t       const struct path *to_path)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\tif (!unconfined(label))\n\t\terror = aa_move_mount(current_cred(), label, from_path,\n\t\t\t\t      to_path);\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\nstatic int apparmor_sb_umount(struct vfsmount *mnt, int flags)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\tif (!unconfined(label))\n\t\terror = aa_umount(current_cred(), label, mnt, flags);\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\nstatic int apparmor_sb_pivotroot(const struct path *old_path,\n\t\t\t\t const struct path *new_path)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\n\tlabel = aa_get_current_label();\n\tif (!unconfined(label))\n\t\terror = aa_pivotroot(current_cred(), label, old_path, new_path);\n\taa_put_label(label);\n\n\treturn error;\n}\n\nstatic int apparmor_getselfattr(unsigned int attr, struct lsm_ctx __user *lx,\n\t\t\t\tu32 *size, u32 flags)\n{\n\tint error = -ENOENT;\n\tstruct aa_task_ctx *ctx = task_ctx(current);\n\tstruct aa_label *label = NULL;\n\tchar *value = NULL;\n\n\tswitch (attr) {\n\tcase LSM_ATTR_CURRENT:\n\t\tlabel = aa_get_newest_label(cred_label(current_cred()));\n\t\tbreak;\n\tcase LSM_ATTR_PREV:\n\t\tif (ctx->previous)\n\t\t\tlabel = aa_get_newest_label(ctx->previous);\n\t\tbreak;\n\tcase LSM_ATTR_EXEC:\n\t\tif (ctx->onexec)\n\t\t\tlabel = aa_get_newest_label(ctx->onexec);\n\t\tbreak;\n\tdefault:\n\t\terror = -EOPNOTSUPP;\n\t\tbreak;\n\t}\n\n\tif (label) {\n\t\terror = aa_getprocattr(label, &value, false);\n\t\tif (error > 0)\n\t\t\terror = lsm_fill_user_ctx(lx, size, value, error,\n\t\t\t\t\t\t  LSM_ID_APPARMOR, 0);\n\t\tkfree(value);\n\t}\n\n\taa_put_label(label);\n\n\tif (error < 0)\n\t\treturn error;\n\treturn 1;\n}\n\nstatic int apparmor_getprocattr(struct task_struct *task, const char *name,\n\t\t\t\tchar **value)\n{\n\tint error = -ENOENT;\n\t/* released below */\n\tconst struct cred *cred = get_task_cred(task);\n\tstruct aa_task_ctx *ctx = task_ctx(current);\n\tstruct aa_label *label = NULL;\n\n\tif (strcmp(name, \"current\") == 0)\n\t\tlabel = aa_get_newest_label(cred_label(cred));\n\telse if (strcmp(name, \"prev\") == 0  && ctx->previous)\n\t\tlabel = aa_get_newest_label(ctx->previous);\n\telse if (strcmp(name, \"exec\") == 0 && ctx->onexec)\n\t\tlabel = aa_get_newest_label(ctx->onexec);\n\telse\n\t\terror = -EINVAL;\n\n\tif (label)\n\t\terror = aa_getprocattr(label, value, true);\n\n\taa_put_label(label);\n\tput_cred(cred);\n\n\treturn error;\n}\n\nstatic int do_setattr(u64 attr, void *value, size_t size)\n{\n\tchar *command, *largs = NULL, *args = value;\n\tsize_t arg_size;\n\tint error;\n\tDEFINE_AUDIT_DATA(ad, LSM_AUDIT_DATA_NONE, AA_CLASS_NONE,\n\t\t\t  OP_SETPROCATTR);\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\t/* AppArmor requires that the buffer must be null terminated atm */\n\tif (args[size - 1] != '\\0') {\n\t\t/* null terminate */\n\t\tlargs = args = kmalloc(size + 1, GFP_KERNEL);\n\t\tif (!args)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(args, value, size);\n\t\targs[size] = '\\0';\n\t}\n\n\terror = -EINVAL;\n\targs = strim(args);\n\tcommand = strsep(&args, \" \");\n\tif (!args)\n\t\tgoto out;\n\targs = skip_spaces(args);\n\tif (!*args)\n\t\tgoto out;\n\n\targ_size = size - (args - (largs ? largs : (char *) value));\n\tif (attr == LSM_ATTR_CURRENT) {\n\t\tif (strcmp(command, \"changehat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t AA_CHANGE_NOFLAGS);\n\t\t} else if (strcmp(command, \"permhat\") == 0) {\n\t\t\terror = aa_setprocattr_changehat(args, arg_size,\n\t\t\t\t\t\t\t AA_CHANGE_TEST);\n\t\t} else if (strcmp(command, \"changeprofile\") == 0) {\n\t\t\terror = aa_change_profile(args, AA_CHANGE_NOFLAGS);\n\t\t} else if (strcmp(command, \"permprofile\") == 0) {\n\t\t\terror = aa_change_profile(args, AA_CHANGE_TEST);\n\t\t} else if (strcmp(command, \"stack\") == 0) {\n\t\t\terror = aa_change_profile(args, AA_CHANGE_STACK);\n\t\t} else\n\t\t\tgoto fail;\n\t} else if (attr == LSM_ATTR_EXEC) {\n\t\tif (strcmp(command, \"exec\") == 0)\n\t\t\terror = aa_change_profile(args, AA_CHANGE_ONEXEC);\n\t\telse if (strcmp(command, \"stack\") == 0)\n\t\t\terror = aa_change_profile(args, (AA_CHANGE_ONEXEC |\n\t\t\t\t\t\t\t AA_CHANGE_STACK));\n\t\telse\n\t\t\tgoto fail;\n\t} else\n\t\t/* only support the \"current\" and \"exec\" process attributes */\n\t\tgoto fail;\n\n\tif (!error)\n\t\terror = size;\nout:\n\tkfree(largs);\n\treturn error;\n\nfail:\n\tad.subj_label = begin_current_label_crit_section();\n\tif (attr == LSM_ATTR_CURRENT)\n\t\tad.info = \"current\";\n\telse if (attr == LSM_ATTR_EXEC)\n\t\tad.info = \"exec\";\n\telse\n\t\tad.info = \"invalid\";\n\tad.error = error = -EINVAL;\n\taa_audit_msg(AUDIT_APPARMOR_DENIED, &ad, NULL);\n\tend_current_label_crit_section(ad.subj_label);\n\tgoto out;\n}\n\nstatic int apparmor_setselfattr(unsigned int attr, struct lsm_ctx *ctx,\n\t\t\t\tu32 size, u32 flags)\n{\n\tint rc;\n\n\tif (attr != LSM_ATTR_CURRENT && attr != LSM_ATTR_EXEC)\n\t\treturn -EOPNOTSUPP;\n\n\trc = do_setattr(attr, ctx->ctx, ctx->ctx_len);\n\tif (rc > 0)\n\t\treturn 0;\n\treturn rc;\n}\n\nstatic int apparmor_setprocattr(const char *name, void *value,\n\t\t\t\tsize_t size)\n{\n\tint attr = lsm_name_to_attr(name);\n\n\tif (attr)\n\t\treturn do_setattr(attr, value, size);\n\treturn -EINVAL;\n}\n\n/**\n * apparmor_bprm_committing_creds - do task cleanup on committing new creds\n * @bprm: binprm for the exec  (NOT NULL)\n */\nstatic void apparmor_bprm_committing_creds(const struct linux_binprm *bprm)\n{\n\tstruct aa_label *label = aa_current_raw_label();\n\tstruct aa_label *new_label = cred_label(bprm->cred);\n\n\t/* bail out if unconfined or not changing profile */\n\tif ((new_label->proxy == label->proxy) ||\n\t    (unconfined(new_label)))\n\t\treturn;\n\n\taa_inherit_files(bprm->cred, current->files);\n\n\tcurrent->pdeath_signal = 0;\n\n\t/* reset soft limits and set hard limits for the new label */\n\t__aa_transition_rlimits(label, new_label);\n}\n\n/**\n * apparmor_bprm_committed_creds() - do cleanup after new creds committed\n * @bprm: binprm for the exec  (NOT NULL)\n */\nstatic void apparmor_bprm_committed_creds(const struct linux_binprm *bprm)\n{\n\t/* clear out temporary/transitional state from the context */\n\taa_clear_task_ctx_trans(task_ctx(current));\n\n\treturn;\n}\n\nstatic void apparmor_current_getlsmprop_subj(struct lsm_prop *prop)\n{\n\tstruct aa_label *label;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\tprop->apparmor.label = label;\n\t__end_current_label_crit_section(label, needput);\n}\n\nstatic void apparmor_task_getlsmprop_obj(struct task_struct *p,\n\t\t\t\t\t  struct lsm_prop *prop)\n{\n\tstruct aa_label *label = aa_get_task_label(p);\n\n\tprop->apparmor.label = label;\n\taa_put_label(label);\n}\n\nstatic int apparmor_task_setrlimit(struct task_struct *task,\n\t\tunsigned int resource, struct rlimit *new_rlim)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\n\tif (!unconfined(label))\n\t\terror = aa_task_setrlimit(current_cred(), label, task,\n\t\t\t\t\t  resource, new_rlim);\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\nstatic int apparmor_task_kill(struct task_struct *target, struct kernel_siginfo *info,\n\t\t\t      int sig, const struct cred *cred)\n{\n\tconst struct cred *tc;\n\tstruct aa_label *cl, *tl;\n\tint error;\n\tbool needput;\n\n\ttc = get_task_cred(target);\n\ttl = aa_get_newest_cred_label(tc);\n\tif (cred) {\n\t\t/*\n\t\t * Dealing with USB IO specific behavior\n\t\t */\n\t\tcl = aa_get_newest_cred_label(cred);\n\t\terror = aa_may_signal(cred, cl, tc, tl, sig);\n\t\taa_put_label(cl);\n\t} else {\n\t\tcl = __begin_current_label_crit_section(&needput);\n\t\terror = aa_may_signal(current_cred(), cl, tc, tl, sig);\n\t\t__end_current_label_crit_section(cl, needput);\n\t}\n\taa_put_label(tl);\n\tput_cred(tc);\n\n\treturn error;\n}\n\nstatic int apparmor_userns_create(const struct cred *cred)\n{\n\tstruct aa_label *label;\n\tstruct aa_profile *profile;\n\tint error = 0;\n\tDEFINE_AUDIT_DATA(ad, LSM_AUDIT_DATA_TASK, AA_CLASS_NS,\n\t\t\t  OP_USERNS_CREATE);\n\n\tad.subj_cred = current_cred();\n\n\tlabel = begin_current_label_crit_section();\n\tif (!unconfined(label)) {\n\t\terror = fn_for_each(label, profile,\n\t\t\t\t    aa_profile_ns_perm(profile, &ad,\n\t\t\t\t\t\t       AA_USERNS_CREATE));\n\t}\n\tend_current_label_crit_section(label);\n\n\treturn error;\n}\n\nstatic int apparmor_sk_alloc_security(struct sock *sk, int family, gfp_t gfp)\n{\n\tstruct aa_sk_ctx *ctx = aa_sock(sk);\n\tstruct aa_label *label;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\t//spin_lock_init(&ctx->lock);\n\trcu_assign_pointer(ctx->label, aa_get_label(label));\n\trcu_assign_pointer(ctx->peer, NULL);\n\trcu_assign_pointer(ctx->peer_lastupdate, NULL);\n\t__end_current_label_crit_section(label, needput);\n\treturn 0;\n}\n\nstatic void apparmor_sk_free_security(struct sock *sk)\n{\n\tstruct aa_sk_ctx *ctx = aa_sock(sk);\n\n\t/* dead these won't be updated any more */\n\taa_put_label(rcu_dereference_protected(ctx->label, true));\n\taa_put_label(rcu_dereference_protected(ctx->peer, true));\n\taa_put_label(rcu_dereference_protected(ctx->peer_lastupdate, true));\n}\n\n/**\n * apparmor_sk_clone_security - clone the sk_security field\n * @sk: sock to have security cloned\n * @newsk: sock getting clone\n */\nstatic void apparmor_sk_clone_security(const struct sock *sk,\n\t\t\t\t       struct sock *newsk)\n{\n\tstruct aa_sk_ctx *ctx = aa_sock(sk);\n\tstruct aa_sk_ctx *new = aa_sock(newsk);\n\n\t/* not actually in use yet */\n\tif (rcu_access_pointer(ctx->label) != rcu_access_pointer(new->label)) {\n\t\taa_put_label(rcu_dereference_protected(new->label, true));\n\t\trcu_assign_pointer(new->label, aa_get_label_rcu(&ctx->label));\n\t}\n\n\tif (rcu_access_pointer(ctx->peer) != rcu_access_pointer(new->peer)) {\n\t\taa_put_label(rcu_dereference_protected(new->peer, true));\n\t\trcu_assign_pointer(new->peer, aa_get_label_rcu(&ctx->peer));\n\t}\n\n\tif (rcu_access_pointer(ctx->peer_lastupdate) != rcu_access_pointer(new->peer_lastupdate)) {\n\t\taa_put_label(rcu_dereference_protected(new->peer_lastupdate, true));\n\t\trcu_assign_pointer(new->peer_lastupdate,\n\t\t\t\t   aa_get_label_rcu(&ctx->peer_lastupdate));\n\t}\n}\n\nstatic int unix_connect_perm(const struct cred *cred, struct aa_label *label,\n\t\t\t     struct sock *sk, struct sock *peer_sk)\n{\n\tstruct aa_sk_ctx *peer_ctx = aa_sock(peer_sk);\n\tint error;\n\n\terror = aa_unix_peer_perm(cred, label, OP_CONNECT,\n\t\t\t\t(AA_MAY_CONNECT | AA_MAY_SEND | AA_MAY_RECEIVE),\n\t\t\t\t  sk, peer_sk,\n\t\t\t\t  rcu_dereference_protected(peer_ctx->label,\n\t\t\t\t     lockdep_is_held(&unix_sk(peer_sk)->lock)));\n\tif (!is_unix_fs(peer_sk)) {\n\t\tlast_error(error,\n\t\t\t   aa_unix_peer_perm(cred,\n\t\t\t\trcu_dereference_protected(peer_ctx->label,\n\t\t\t\t     lockdep_is_held(&unix_sk(peer_sk)->lock)),\n\t\t\t\tOP_CONNECT,\n\t\t\t\t(AA_MAY_ACCEPT | AA_MAY_SEND | AA_MAY_RECEIVE),\n\t\t\t\t\t\t\t  peer_sk, sk, label));\n\t}\n\n\treturn error;\n}\n\n/* lockdep check in unix_connect_perm - push sks here to check */\nstatic void unix_connect_peers(struct aa_sk_ctx *sk_ctx,\n\t\t\t       struct aa_sk_ctx *peer_ctx)\n{\n\t/* Cross reference the peer labels for SO_PEERSEC */\n\tstruct aa_label *label = rcu_dereference_protected(sk_ctx->label, true);\n\n\taa_get_label(label);\n\taa_put_label(rcu_dereference_protected(peer_ctx->peer,\n\t\t\t\t\t     true));\n\trcu_assign_pointer(peer_ctx->peer, label);\t/* transfer cnt */\n\n\tlabel = aa_get_label(rcu_dereference_protected(peer_ctx->label,\n\t\t\t\t\t     true));\n\t//spin_unlock(&peer_ctx->lock);\n\n\t//spin_lock(&sk_ctx->lock);\n\taa_put_label(rcu_dereference_protected(sk_ctx->peer,\n\t\t\t\t\t       true));\n\taa_put_label(rcu_dereference_protected(sk_ctx->peer_lastupdate,\n\t\t\t\t\t       true));\n\n\trcu_assign_pointer(sk_ctx->peer, aa_get_label(label));\n\trcu_assign_pointer(sk_ctx->peer_lastupdate, label);     /* transfer cnt */\n\t//spin_unlock(&sk_ctx->lock);\n}\n\n/**\n * apparmor_unix_stream_connect - check perms before making unix domain conn\n * @sk: sk attempting to connect\n * @peer_sk: sk that is accepting the connection\n * @newsk: new sk created for this connection\n * peer is locked when this hook is called\n *\n * Return:\n *   0 if connection is permitted\n *   error code on denial or failure\n */\nstatic int apparmor_unix_stream_connect(struct sock *sk, struct sock *peer_sk,\n\t\t\t\t\tstruct sock *newsk)\n{\n\tstruct aa_sk_ctx *sk_ctx = aa_sock(sk);\n\tstruct aa_sk_ctx *peer_ctx = aa_sock(peer_sk);\n\tstruct aa_sk_ctx *new_ctx = aa_sock(newsk);\n\tstruct aa_label *label;\n\tint error;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\terror = unix_connect_perm(current_cred(), label, sk, peer_sk);\n\t__end_current_label_crit_section(label, needput);\n\n\tif (error)\n\t\treturn error;\n\n\t/* newsk doesn't go through post_create, but does go through\n\t * security_sk_alloc()\n\t */\n\trcu_assign_pointer(new_ctx->label,\n\t\t\t   aa_get_label(rcu_dereference_protected(peer_ctx->label,\n\t\t\t\t\t\t\t\t  true)));\n\n\t/* Cross reference the peer labels for SO_PEERSEC */\n\tunix_connect_peers(sk_ctx, new_ctx);\n\n\treturn 0;\n}\n\n/**\n * apparmor_unix_may_send - check perms before conn or sending unix dgrams\n * @sock: socket sending the message\n * @peer: socket message is being send to\n *\n * Performs bidirectional permission checks for Unix domain socket communication:\n * 1. Verifies sender has AA_MAY_SEND to target socket\n * 2. Verifies receiver has AA_MAY_RECEIVE from source socket\n *\n * sock and peer are locked when this hook is called\n * called by: dgram_connect peer setup but path not copied to newsk\n *\n * Return:\n *   0 if transmission is permitted\n *   error code on denial or failure\n */\nstatic int apparmor_unix_may_send(struct socket *sock, struct socket *peer)\n{\n\tstruct aa_sk_ctx *peer_ctx = aa_sock(peer->sk);\n\tstruct aa_label *label;\n\tint error;\n\tbool needput;\n\n\tlabel = __begin_current_label_crit_section(&needput);\n\terror = xcheck(aa_unix_peer_perm(current_cred(),\n\t\t\t\tlabel, OP_SENDMSG, AA_MAY_SEND,\n\t\t\t\tsock->sk, peer->sk,\n\t\t\t\trcu_dereference_protected(peer_ctx->label,\n\t\t\t\t\t\t\t  true)),\n\t\t       aa_unix_peer_perm(peer->file ? peer->file->f_cred : NULL,\n\t\t\t\trcu_dereference_protected(peer_ctx->label,\n\t\t\t\t\t\t\t  true),\n\t\t\t\tOP_SENDMSG, AA_MAY_RECEIVE, peer->sk,\n\t\t\t\tsock->sk, label));\n\t__end_current_label_crit_section(label, needput);\n\n\treturn error;\n}\n\nstatic int apparmor_socket_create(int family, int type, int protocol, int kern)\n{\n\tstruct aa_label *label;\n\tint error = 0;\n\n\tAA_BUG(in_interrupt());\n\n\tif (kern)\n\t\treturn 0;\n\n\tlabel = begin_current_label_crit_section();\n\tif (!unconfined(label)) {\n\t\tif (family == PF_UNIX)\n\t\t\terror = aa_unix_create_perm(label, family, type,\n\t\t\t\t\t\t    protocol);\n\t\telse\n\t\t\terror = aa_af_perm(current_cred(), label, OP_CREATE,\n\t\t\t\t\t   AA_MAY_CREATE, family, type,\n\t\t\t\t\t   protocol);\n\t}\n\tend_current_label_crit_section(label);\n\n\treturn error;\n}\n\n/**\n * apparmor_socket_post_create - setup the per-socket security struct\n * @sock: socket that is being setup\n * @family: family of socket being created\n * @type: type of the socket\n * @protocol: protocol of the socket\n * @kern: socket is a special kernel socket\n *\n * Note:\n * -   kernel sockets labeled kernel_t used to use unconfined\n * -   socket may not have sk here if created with sock_create_lite or\n *     sock_alloc. These should be accept cases which will be handled in\n *     sock_graft.\n */\nstatic int apparmor_socket_post_create(struct socket *sock, int family,\n\t\t\t\t       int type, int protocol, int kern)\n{\n\tstruct aa_label *label;\n\n\tif (kern) {\n\t\tlabel = aa_get_label(kernel_t);\n\t} else\n\t\tlabel = aa_get_current_label();\n\n\tif (sock->sk) {\n\t\tstruct aa_sk_ctx *ctx = aa_sock(sock->sk);\n\n\t\t/* still not live */\n\t\taa_put_label(rcu_dereference_protected(ctx->label, true));\n\t\trcu_assign_pointer(ctx->label, aa_get_label(label));\n\t}\n\taa_put_label(label);\n\n\treturn 0;\n}\n\nstatic int apparmor_socket_socketpair(struct socket *socka,\n\t\t\t\t      struct socket *sockb)\n{\n\tstruct aa_sk_ctx *a_ctx = aa_sock(socka->sk);\n\tstruct aa_sk_ctx *b_ctx = aa_sock(sockb->sk);\n\tstruct aa_label *label;\n\n\t/* socks not live yet - initial values set in sk_alloc */\n\tlabel = begin_current_label_crit_section();\n\tif (rcu_access_pointer(a_ctx->label) != label) {\n\t\tAA_BUG(\"a_ctx != label\");\n\t\taa_put_label(rcu_dereference_protected(a_ctx->label, true));\n\t\trcu_assign_pointer(a_ctx->label, aa_get_label(label));\n\t}\n\tif (rcu_access_pointer(b_ctx->label) != label) {\n\t\tAA_BUG(\"b_ctx != label\");\n\t\taa_put_label(rcu_dereference_protected(b_ctx->label, true));\n\t\trcu_assign_pointer(b_ctx->label, aa_get_label(label));\n\t}\n\n\tif (socka->sk->sk_family == PF_UNIX) {\n\t\t/* unix socket pairs by-pass unix_stream_connect */\n\t\tunix_connect_peers(a_ctx, b_ctx);\n\t}\n\tend_current_label_crit_section(label);\n\n\treturn 0;\n}\n\n/**\n * apparmor_socket_bind - check perms before bind addr to socket\n * @sock: socket to bind the address to (must be non-NULL)\n * @address: address that is being bound (must be non-NULL)\n * @addrlen: length of @address\n *\n * Performs security checks before allowing a socket to bind to an address.\n * Handles Unix domain sockets specially through aa_unix_bind_perm().\n * For other socket families, uses generic permission check via aa_sk_perm().\n *\n * Return:\n *   0 if binding is permitted\n *   error code on denial or invalid parameters\n */\nstatic int apparmor_socket_bind(struct socket *sock,\n\t\t\t\tstruct sockaddr *address, int addrlen)\n{\n\tAA_BUG(!sock);\n\tAA_BUG(!sock->sk);\n\tAA_BUG(!address);\n\tAA_BUG(in_interrupt());\n\n\tif (sock->sk->sk_family == PF_UNIX)\n\t\treturn aa_unix_bind_perm(sock, address, addrlen);\n\treturn aa_sk_perm(OP_BIND, AA_MAY_BIND, sock->sk);\n}\n\nstatic int apparmor_socket_connect(struct socket *sock,\n\t\t\t\t   struct sockaddr *address, int addrlen)\n{\n\tAA_BUG(!sock);\n\tAA_BUG(!sock->sk);\n\tAA_BUG(!address);\n\tAA_BUG(in_interrupt());\n\n\t/* PF_UNIX goes through unix_stream_connect && unix_may_send */\n\tif (sock->sk->sk_family == PF_UNIX)\n\t\treturn 0;\n\treturn aa_sk_perm(OP_CONNECT, AA_MAY_CONNECT, sock->sk);\n}\n\nstatic int apparmor_socket_listen(struct socket *sock, int backlog)\n{\n\tAA_BUG(!sock);\n\tAA_BUG(!sock->sk);\n\tAA_BUG(in_interrupt());\n\n\tif (sock->sk->sk_family == PF_UNIX)\n\t\treturn aa_unix_listen_perm(sock, backlog);\n\treturn aa_sk_perm(OP_LISTEN, AA_MAY_LISTEN, sock->sk);\n}\n\n/*\n * Note: while @newsock is created and has some information, the accept\n *       has not been done.\n */\nstatic int apparmor_socket_accept(struct socket *sock, struct socket *newsock)\n{\n\tAA_BUG(!sock);\n\tAA_BUG(!sock->sk);\n\tAA_BUG(!newsock);\n\tAA_BUG(in_interrupt());\n\n\tif (sock->sk->sk_family == PF_UNIX)\n\t\treturn aa_unix_accept_perm(sock, newsock);\n\treturn aa_sk_perm(OP_ACCEPT, AA_MAY_ACCEPT, sock->sk);\n}\n\nstatic int aa_sock_msg_perm(const char *op, u32 request, struct socket *sock,\n\t\t\t    struct msghdr *msg, int size)\n{\n\tAA_BUG(!sock);\n\tAA_BUG(!sock->sk);\n\tAA_BUG(!msg);\n\tAA_BUG(in_interrupt());\n\n\t/* PF_UNIX goes through unix_may_send */\n\tif (sock->sk->sk_family == PF_UNIX)\n\t\treturn 0;\n\treturn aa_sk_perm(op, request, sock->sk);\n}\n\nstatic int apparmor_socket_sendmsg(struct socket *sock,\n\t\t\t\t   struct msghdr *msg, int size)\n{\n\treturn aa_sock_msg_perm(OP_SENDMSG, AA_MAY_SEND, sock, msg, size);\n}\n\nstatic int apparmor_socket_recvmsg(struct socket *sock,\n\t\t\t\t   struct msghdr *msg, int size, int flags)\n{\n\treturn aa_sock_msg_perm(OP_RECVMSG, AA_MAY_RECEIVE, sock, msg, size);\n}\n\n/* revaliation, get/set attr, shutdown */\nstatic int aa_sock_perm(const char *op, u32 request, struct socket *sock)\n{\n\tAA_BUG(!sock);\n\tAA_BUG(!sock->sk);\n\tAA_BUG(in_interrupt());\n\n\tif (sock->sk->sk_family == PF_UNIX)\n\t\treturn aa_unix_sock_perm(op, request, sock);\n\treturn aa_sk_perm(op, request, sock->sk);\n}\n\nstatic int apparmor_socket_getsockname(struct socket *sock)\n{\n\treturn aa_sock_perm(OP_GETSOCKNAME, AA_MAY_GETATTR, sock);\n}\n\nstatic int apparmor_socket_getpeername(struct socket *sock)\n{\n\treturn aa_sock_perm(OP_GETPEERNAME, AA_MAY_GETATTR, sock);\n}\n\n/* revaliation, get/set attr, opt */\nstatic int aa_sock_opt_perm(const char *op, u32 request, struct socket *sock,\n\t\t\t    int level, int optname)\n{\n\tAA_BUG(!sock);\n\tAA_BUG(!sock->sk);\n\tAA_BUG(in_interrupt());\n\n\tif (sock->sk->sk_family == PF_UNIX)\n\t\treturn aa_unix_opt_perm(op, request, sock, level, optname);\n\treturn aa_sk_perm(op, request, sock->sk);\n}\n\nstatic int apparmor_socket_getsockopt(struct socket *sock, int level,\n\t\t\t\t      int optname)\n{\n\treturn aa_sock_opt_perm(OP_GETSOCKOPT, AA_MAY_GETOPT, sock,\n\t\t\t\tlevel, optname);\n}\n\nstatic int apparmor_socket_setsockopt(struct socket *sock, int level,\n\t\t\t\t      int optname)\n{\n\treturn aa_sock_opt_perm(OP_SETSOCKOPT, AA_MAY_SETOPT, sock,\n\t\t\t\tlevel, optname);\n}\n\nstatic int apparmor_socket_shutdown(struct socket *sock, int how)\n{\n\treturn aa_sock_perm(OP_SHUTDOWN, AA_MAY_SHUTDOWN, sock);\n}\n\n#ifdef CONFIG_NETWORK_SECMARK\n/**\n * apparmor_socket_sock_rcv_skb - check perms before associating skb to sk\n * @sk: sk to associate @skb with\n * @skb: skb to check for perms\n *\n * Note: can not sleep may be called with locks held\n *\n * dont want protocol specific in __skb_recv_datagram()\n * to deny an incoming connection  socket_sock_rcv_skb()\n */\nstatic int apparmor_socket_sock_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct aa_sk_ctx *ctx = aa_sock(sk);\n\tint error;\n\n\tif (!skb->secmark)\n\t\treturn 0;\n\n\t/*\n\t * If reach here before socket_post_create hook is called, in which\n\t * case label is null, drop the packet.\n\t */\n\tif (!rcu_access_pointer(ctx->label))\n\t\treturn -EACCES;\n\n\trcu_read_lock();\n\terror = apparmor_secmark_check(rcu_dereference(ctx->label), OP_RECVMSG,\n\t\t\t\t       AA_MAY_RECEIVE, skb->secmark, sk);\n\trcu_read_unlock();\n\n\treturn error;\n}\n#endif\n\n\nstatic struct aa_label *sk_peer_get_label(struct sock *sk)\n{\n\tstruct aa_sk_ctx *ctx = aa_sock(sk);\n\tstruct aa_label *label = ERR_PTR(-ENOPROTOOPT);\n\n\tif (rcu_access_pointer(ctx->peer))\n\t\treturn aa_get_label_rcu(&ctx->peer);\n\n\tif (sk->sk_family != PF_UNIX)\n\t\treturn ERR_PTR(-ENOPROTOOPT);\n\n\treturn label;\n}\n\n/**\n * apparmor_socket_getpeersec_stream - get security context of peer\n * @sock: socket that we are trying to get the peer context of\n * @optval: output - buffer to copy peer name to\n * @optlen: output - size of copied name in @optval\n * @len: size of @optval buffer\n * Returns: 0 on success, -errno of failure\n *\n * Note: for tcp only valid if using ipsec or cipso on lan\n */\nstatic int apparmor_socket_getpeersec_stream(struct socket *sock,\n\t\t\t\t\t     sockptr_t optval, sockptr_t optlen,\n\t\t\t\t\t     unsigned int len)\n{\n\tchar *name = NULL;\n\tint slen, error = 0;\n\tstruct aa_label *label;\n\tstruct aa_label *peer;\n\n\tpeer = sk_peer_get_label(sock->sk);\n\tif (IS_ERR(peer)) {\n\t\terror = PTR_ERR(peer);\n\t\tgoto done;\n\t}\n\tlabel = begin_current_label_crit_section();\n\tslen = aa_label_asxprint(&name, labels_ns(label), peer,\n\t\t\t\t FLAG_SHOW_MODE | FLAG_VIEW_SUBNS |\n\t\t\t\t FLAG_HIDDEN_UNCONFINED, GFP_KERNEL);\n\t/* don't include terminating \\0 in slen, it breaks some apps */\n\tif (slen < 0) {\n\t\terror = -ENOMEM;\n\t\tgoto done_put;\n\t}\n\tif (slen > len) {\n\t\terror = -ERANGE;\n\t\tgoto done_len;\n\t}\n\n\tif (copy_to_sockptr(optval, name, slen))\n\t\terror = -EFAULT;\ndone_len:\n\tif (copy_to_sockptr(optlen, &slen, sizeof(slen)))\n\t\terror = -EFAULT;\n\ndone_put:\n\tend_current_label_crit_section(label);\n\taa_put_label(peer);\ndone:\n\tkfree(name);\n\treturn error;\n}\n\n/**\n * apparmor_socket_getpeersec_dgram - get security label of packet\n * @sock: the peer socket\n * @skb: packet data\n * @secid: pointer to where to put the secid of the packet\n *\n * Sets the netlabel socket state on sk from parent\n */\nstatic int apparmor_socket_getpeersec_dgram(struct socket *sock,\n\t\t\t\t\t    struct sk_buff *skb, u32 *secid)\n\n{\n\t/* TODO: requires secid support */\n\treturn -ENOPROTOOPT;\n}\n\n/**\n * apparmor_sock_graft - Initialize newly created socket\n * @sk: child sock\n * @parent: parent socket\n *\n * Note: could set off of SOCK_CTX(parent) but need to track inode and we can\n *       just set sk security information off of current creating process label\n *       Labeling of sk for accept case - probably should be sock based\n *       instead of task, because of the case where an implicitly labeled\n *       socket is shared by different tasks.\n */\nstatic void apparmor_sock_graft(struct sock *sk, struct socket *parent)\n{\n\tstruct aa_sk_ctx *ctx = aa_sock(sk);\n\n\t/* setup - not live */\n\tif (!rcu_access_pointer(ctx->label))\n\t\trcu_assign_pointer(ctx->label, aa_get_current_label());\n}\n\n#ifdef CONFIG_NETWORK_SECMARK\nstatic int apparmor_inet_conn_request(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      struct request_sock *req)\n{\n\tstruct aa_sk_ctx *ctx = aa_sock(sk);\n\tint error;\n\n\tif (!skb->secmark)\n\t\treturn 0;\n\n\trcu_read_lock();\n\terror = apparmor_secmark_check(rcu_dereference(ctx->label), OP_CONNECT,\n\t\t\t\t       AA_MAY_CONNECT, skb->secmark, sk);\n\trcu_read_unlock();\n\n\treturn error;\n}\n#endif\n\n/*\n * The cred blob is a pointer to, not an instance of, an aa_label.\n */\nstruct lsm_blob_sizes apparmor_blob_sizes __ro_after_init = {\n\t.lbs_cred = sizeof(struct aa_label *),\n\t.lbs_file = sizeof(struct aa_file_ctx),\n\t.lbs_task = sizeof(struct aa_task_ctx),\n\t.lbs_sock = sizeof(struct aa_sk_ctx),\n};\n\nstatic const struct lsm_id apparmor_lsmid = {\n\t.name = \"apparmor\",\n\t.id = LSM_ID_APPARMOR,\n};\n\nstatic struct security_hook_list apparmor_hooks[] __ro_after_init = {\n\tLSM_HOOK_INIT(ptrace_access_check, apparmor_ptrace_access_check),\n\tLSM_HOOK_INIT(ptrace_traceme, apparmor_ptrace_traceme),\n\tLSM_HOOK_INIT(capget, apparmor_capget),\n\tLSM_HOOK_INIT(capable, apparmor_capable),\n\n\tLSM_HOOK_INIT(move_mount, apparmor_move_mount),\n\tLSM_HOOK_INIT(sb_mount, apparmor_sb_mount),\n\tLSM_HOOK_INIT(sb_umount, apparmor_sb_umount),\n\tLSM_HOOK_INIT(sb_pivotroot, apparmor_sb_pivotroot),\n\n\tLSM_HOOK_INIT(path_link, apparmor_path_link),\n\tLSM_HOOK_INIT(path_unlink, apparmor_path_unlink),\n\tLSM_HOOK_INIT(path_symlink, apparmor_path_symlink),\n\tLSM_HOOK_INIT(path_mkdir, apparmor_path_mkdir),\n\tLSM_HOOK_INIT(path_rmdir, apparmor_path_rmdir),\n\tLSM_HOOK_INIT(path_mknod, apparmor_path_mknod),\n\tLSM_HOOK_INIT(path_rename, apparmor_path_rename),\n\tLSM_HOOK_INIT(path_chmod, apparmor_path_chmod),\n\tLSM_HOOK_INIT(path_chown, apparmor_path_chown),\n\tLSM_HOOK_INIT(path_truncate, apparmor_path_truncate),\n\tLSM_HOOK_INIT(inode_getattr, apparmor_inode_getattr),\n\n\tLSM_HOOK_INIT(file_open, apparmor_file_open),\n\tLSM_HOOK_INIT(file_receive, apparmor_file_receive),\n\tLSM_HOOK_INIT(file_permission, apparmor_file_permission),\n\tLSM_HOOK_INIT(file_alloc_security, apparmor_file_alloc_security),\n\tLSM_HOOK_INIT(file_free_security, apparmor_file_free_security),\n\tLSM_HOOK_INIT(mmap_file, apparmor_mmap_file),\n\tLSM_HOOK_INIT(file_mprotect, apparmor_file_mprotect),\n\tLSM_HOOK_INIT(file_lock, apparmor_file_lock),\n\tLSM_HOOK_INIT(file_truncate, apparmor_file_truncate),\n\n\tLSM_HOOK_INIT(getselfattr, apparmor_getselfattr),\n\tLSM_HOOK_INIT(setselfattr, apparmor_setselfattr),\n\tLSM_HOOK_INIT(getprocattr, apparmor_getprocattr),\n\tLSM_HOOK_INIT(setprocattr, apparmor_setprocattr),\n\n\tLSM_HOOK_INIT(sk_alloc_security, apparmor_sk_alloc_security),\n\tLSM_HOOK_INIT(sk_free_security, apparmor_sk_free_security),\n\tLSM_HOOK_INIT(sk_clone_security, apparmor_sk_clone_security),\n\n\tLSM_HOOK_INIT(unix_stream_connect, apparmor_unix_stream_connect),\n\tLSM_HOOK_INIT(unix_may_send, apparmor_unix_may_send),\n\n\tLSM_HOOK_INIT(socket_create, apparmor_socket_create),\n\tLSM_HOOK_INIT(socket_post_create, apparmor_socket_post_create),\n\tLSM_HOOK_INIT(socket_socketpair, apparmor_socket_socketpair),\n\tLSM_HOOK_INIT(socket_bind, apparmor_socket_bind),\n\tLSM_HOOK_INIT(socket_connect, apparmor_socket_connect),\n\tLSM_HOOK_INIT(socket_listen, apparmor_socket_listen),\n\tLSM_HOOK_INIT(socket_accept, apparmor_socket_accept),\n\tLSM_HOOK_INIT(socket_sendmsg, apparmor_socket_sendmsg),\n\tLSM_HOOK_INIT(socket_recvmsg, apparmor_socket_recvmsg),\n\tLSM_HOOK_INIT(socket_getsockname, apparmor_socket_getsockname),\n\tLSM_HOOK_INIT(socket_getpeername, apparmor_socket_getpeername),\n\tLSM_HOOK_INIT(socket_getsockopt, apparmor_socket_getsockopt),\n\tLSM_HOOK_INIT(socket_setsockopt, apparmor_socket_setsockopt),\n\tLSM_HOOK_INIT(socket_shutdown, apparmor_socket_shutdown),\n#ifdef CONFIG_NETWORK_SECMARK\n\tLSM_HOOK_INIT(socket_sock_rcv_skb, apparmor_socket_sock_rcv_skb),\n#endif\n\tLSM_HOOK_INIT(socket_getpeersec_stream,\n\t\t      apparmor_socket_getpeersec_stream),\n\tLSM_HOOK_INIT(socket_getpeersec_dgram,\n\t\t      apparmor_socket_getpeersec_dgram),\n\tLSM_HOOK_INIT(sock_graft, apparmor_sock_graft),\n#ifdef CONFIG_NETWORK_SECMARK\n\tLSM_HOOK_INIT(inet_conn_request, apparmor_inet_conn_request),\n#endif\n\n\tLSM_HOOK_INIT(cred_alloc_blank, apparmor_cred_alloc_blank),\n\tLSM_HOOK_INIT(cred_free, apparmor_cred_free),\n\tLSM_HOOK_INIT(cred_prepare, apparmor_cred_prepare),\n\tLSM_HOOK_INIT(cred_transfer, apparmor_cred_transfer),\n\n\tLSM_HOOK_INIT(bprm_creds_for_exec, apparmor_bprm_creds_for_exec),\n\tLSM_HOOK_INIT(bprm_committing_creds, apparmor_bprm_committing_creds),\n\tLSM_HOOK_INIT(bprm_committed_creds, apparmor_bprm_committed_creds),\n\n\tLSM_HOOK_INIT(task_free, apparmor_task_free),\n\tLSM_HOOK_INIT(task_alloc, apparmor_task_alloc),\n\tLSM_HOOK_INIT(current_getlsmprop_subj,\n\t\t      apparmor_current_getlsmprop_subj),\n\tLSM_HOOK_INIT(task_getlsmprop_obj, apparmor_task_getlsmprop_obj),\n\tLSM_HOOK_INIT(task_setrlimit, apparmor_task_setrlimit),\n\tLSM_HOOK_INIT(task_kill, apparmor_task_kill),\n\tLSM_HOOK_INIT(userns_create, apparmor_userns_create),\n\n#ifdef CONFIG_AUDIT\n\tLSM_HOOK_INIT(audit_rule_init, aa_audit_rule_init),\n\tLSM_HOOK_INIT(audit_rule_known, aa_audit_rule_known),\n\tLSM_HOOK_INIT(audit_rule_match, aa_audit_rule_match),\n\tLSM_HOOK_INIT(audit_rule_free, aa_audit_rule_free),\n#endif\n\n\tLSM_HOOK_INIT(secid_to_secctx, apparmor_secid_to_secctx),\n\tLSM_HOOK_INIT(lsmprop_to_secctx, apparmor_lsmprop_to_secctx),\n\tLSM_HOOK_INIT(secctx_to_secid, apparmor_secctx_to_secid),\n\tLSM_HOOK_INIT(release_secctx, apparmor_release_secctx),\n\n#ifdef CONFIG_IO_URING\n\tLSM_HOOK_INIT(uring_override_creds, apparmor_uring_override_creds),\n\tLSM_HOOK_INIT(uring_sqpoll, apparmor_uring_sqpoll),\n#endif\n};\n\n/*\n * AppArmor sysfs module parameters\n */\n\nstatic int param_set_aabool(const char *val, const struct kernel_param *kp);\nstatic int param_get_aabool(char *buffer, const struct kernel_param *kp);\n#define param_check_aabool param_check_bool\nstatic const struct kernel_param_ops param_ops_aabool = {\n\t.flags = KERNEL_PARAM_OPS_FL_NOARG,\n\t.set = param_set_aabool,\n\t.get = param_get_aabool\n};\n\nstatic int param_set_aauint(const char *val, const struct kernel_param *kp);\nstatic int param_get_aauint(char *buffer, const struct kernel_param *kp);\n#define param_check_aauint param_check_uint\nstatic const struct kernel_param_ops param_ops_aauint = {\n\t.set = param_set_aauint,\n\t.get = param_get_aauint\n};\n\nstatic int param_set_aacompressionlevel(const char *val,\n\t\t\t\t\tconst struct kernel_param *kp);\nstatic int param_get_aacompressionlevel(char *buffer,\n\t\t\t\t\tconst struct kernel_param *kp);\n#define param_check_aacompressionlevel param_check_int\nstatic const struct kernel_param_ops param_ops_aacompressionlevel = {\n\t.set = param_set_aacompressionlevel,\n\t.get = param_get_aacompressionlevel\n};\n\nstatic int param_set_aalockpolicy(const char *val, const struct kernel_param *kp);\nstatic int param_get_aalockpolicy(char *buffer, const struct kernel_param *kp);\n#define param_check_aalockpolicy param_check_bool\nstatic const struct kernel_param_ops param_ops_aalockpolicy = {\n\t.flags = KERNEL_PARAM_OPS_FL_NOARG,\n\t.set = param_set_aalockpolicy,\n\t.get = param_get_aalockpolicy\n};\n\nstatic int param_set_debug(const char *val, const struct kernel_param *kp);\nstatic int param_get_debug(char *buffer, const struct kernel_param *kp);\n\nstatic int param_set_audit(const char *val, const struct kernel_param *kp);\nstatic int param_get_audit(char *buffer, const struct kernel_param *kp);\n\nstatic int param_set_mode(const char *val, const struct kernel_param *kp);\nstatic int param_get_mode(char *buffer, const struct kernel_param *kp);\n\n/* Flag values, also controllable via /sys/module/apparmor/parameters\n * We define special types as we want to do additional mediation.\n */\n\n/* AppArmor global enforcement switch - complain, enforce, kill */\nenum profile_mode aa_g_profile_mode = APPARMOR_ENFORCE;\nmodule_param_call(mode, param_set_mode, param_get_mode,\n\t\t  &aa_g_profile_mode, S_IRUSR | S_IWUSR);\n\n/* whether policy verification hashing is enabled */\nbool aa_g_hash_policy = IS_ENABLED(CONFIG_SECURITY_APPARMOR_HASH_DEFAULT);\n#ifdef CONFIG_SECURITY_APPARMOR_HASH\nmodule_param_named(hash_policy, aa_g_hash_policy, aabool, S_IRUSR | S_IWUSR);\n#endif\n\n/* whether policy exactly as loaded is retained for debug and checkpointing */\nbool aa_g_export_binary = IS_ENABLED(CONFIG_SECURITY_APPARMOR_EXPORT_BINARY);\n#ifdef CONFIG_SECURITY_APPARMOR_EXPORT_BINARY\nmodule_param_named(export_binary, aa_g_export_binary, aabool, 0600);\n#endif\n\n/* policy loaddata compression level */\nint aa_g_rawdata_compression_level = AA_DEFAULT_CLEVEL;\nmodule_param_named(rawdata_compression_level, aa_g_rawdata_compression_level,\n\t\t   aacompressionlevel, 0400);\n\n/* Debug mode */\nint aa_g_debug;\nmodule_param_call(debug, param_set_debug, param_get_debug,\n\t\t  &aa_g_debug, 0600);\n\n/* Audit mode */\nenum audit_mode aa_g_audit;\nmodule_param_call(audit, param_set_audit, param_get_audit,\n\t\t  &aa_g_audit, S_IRUSR | S_IWUSR);\n\n/* Determines if audit header is included in audited messages.  This\n * provides more context if the audit daemon is not running\n */\nbool aa_g_audit_header = true;\nmodule_param_named(audit_header, aa_g_audit_header, aabool,\n\t\t   S_IRUSR | S_IWUSR);\n\n/* lock out loading/removal of policy\n * TODO: add in at boot loading of policy, which is the only way to\n *       load policy, if lock_policy is set\n */\nbool aa_g_lock_policy;\nmodule_param_named(lock_policy, aa_g_lock_policy, aalockpolicy,\n\t\t   S_IRUSR | S_IWUSR);\n\n/* Syscall logging mode */\nbool aa_g_logsyscall;\nmodule_param_named(logsyscall, aa_g_logsyscall, aabool, S_IRUSR | S_IWUSR);\n\n/* Maximum pathname length before accesses will start getting rejected */\nunsigned int aa_g_path_max = 2 * PATH_MAX;\nmodule_param_named(path_max, aa_g_path_max, aauint, S_IRUSR);\n\n/* Determines how paranoid loading of policy is and how much verification\n * on the loaded policy is done.\n * DEPRECATED: read only as strict checking of load is always done now\n * that none root users (user namespaces) can load policy.\n */\nbool aa_g_paranoid_load = IS_ENABLED(CONFIG_SECURITY_APPARMOR_PARANOID_LOAD);\nmodule_param_named(paranoid_load, aa_g_paranoid_load, aabool, S_IRUGO);\n\nstatic int param_get_aaintbool(char *buffer, const struct kernel_param *kp);\nstatic int param_set_aaintbool(const char *val, const struct kernel_param *kp);\n#define param_check_aaintbool param_check_int\nstatic const struct kernel_param_ops param_ops_aaintbool = {\n\t.set = param_set_aaintbool,\n\t.get = param_get_aaintbool\n};\n/* Boot time disable flag */\nstatic int apparmor_enabled __ro_after_init = 1;\nmodule_param_named(enabled, apparmor_enabled, aaintbool, 0444);\n\nstatic int __init apparmor_enabled_setup(char *str)\n{\n\tunsigned long enabled;\n\tint error = kstrtoul(str, 0, &enabled);\n\tif (!error)\n\t\tapparmor_enabled = enabled ? 1 : 0;\n\treturn 1;\n}\n\n__setup(\"apparmor=\", apparmor_enabled_setup);\n\n/* set global flag turning off the ability to load policy */\nstatic int param_set_aalockpolicy(const char *val, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_admin_capable(NULL))\n\t\treturn -EPERM;\n\treturn param_set_bool(val, kp);\n}\n\nstatic int param_get_aalockpolicy(char *buffer, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_view_capable(NULL))\n\t\treturn -EPERM;\n\treturn param_get_bool(buffer, kp);\n}\n\nstatic int param_set_aabool(const char *val, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_admin_capable(NULL))\n\t\treturn -EPERM;\n\treturn param_set_bool(val, kp);\n}\n\nstatic int param_get_aabool(char *buffer, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_view_capable(NULL))\n\t\treturn -EPERM;\n\treturn param_get_bool(buffer, kp);\n}\n\nstatic int param_set_aauint(const char *val, const struct kernel_param *kp)\n{\n\tint error;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\t/* file is ro but enforce 2nd line check */\n\tif (apparmor_initialized)\n\t\treturn -EPERM;\n\n\terror = param_set_uint(val, kp);\n\taa_g_path_max = max_t(uint32_t, aa_g_path_max, sizeof(union aa_buffer));\n\tpr_info(\"AppArmor: buffer size set to %d bytes\\n\", aa_g_path_max);\n\n\treturn error;\n}\n\nstatic int param_get_aauint(char *buffer, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_view_capable(NULL))\n\t\treturn -EPERM;\n\treturn param_get_uint(buffer, kp);\n}\n\n/* Can only be set before AppArmor is initialized (i.e. on boot cmdline). */\nstatic int param_set_aaintbool(const char *val, const struct kernel_param *kp)\n{\n\tstruct kernel_param kp_local;\n\tbool value;\n\tint error;\n\n\tif (apparmor_initialized)\n\t\treturn -EPERM;\n\n\t/* Create local copy, with arg pointing to bool type. */\n\tvalue = !!*((int *)kp->arg);\n\tmemcpy(&kp_local, kp, sizeof(kp_local));\n\tkp_local.arg = &value;\n\n\terror = param_set_bool(val, &kp_local);\n\tif (!error)\n\t\t*((int *)kp->arg) = *((bool *)kp_local.arg);\n\treturn error;\n}\n\n/*\n * To avoid changing /sys/module/apparmor/parameters/enabled from Y/N to\n * 1/0, this converts the \"int that is actually bool\" back to bool for\n * display in the /sys filesystem, while keeping it \"int\" for the LSM\n * infrastructure.\n */\nstatic int param_get_aaintbool(char *buffer, const struct kernel_param *kp)\n{\n\tstruct kernel_param kp_local;\n\tbool value;\n\n\t/* Create local copy, with arg pointing to bool type. */\n\tvalue = !!*((int *)kp->arg);\n\tmemcpy(&kp_local, kp, sizeof(kp_local));\n\tkp_local.arg = &value;\n\n\treturn param_get_bool(buffer, &kp_local);\n}\n\nstatic int param_set_aacompressionlevel(const char *val,\n\t\t\t\t\tconst struct kernel_param *kp)\n{\n\tint error;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized)\n\t\treturn -EPERM;\n\n\terror = param_set_int(val, kp);\n\n\taa_g_rawdata_compression_level = clamp(aa_g_rawdata_compression_level,\n\t\t\t\t\t       AA_MIN_CLEVEL, AA_MAX_CLEVEL);\n\tpr_info(\"AppArmor: policy rawdata compression level set to %d\\n\",\n\t\taa_g_rawdata_compression_level);\n\n\treturn error;\n}\n\nstatic int param_get_aacompressionlevel(char *buffer,\n\t\t\t\t\tconst struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_view_capable(NULL))\n\t\treturn -EPERM;\n\treturn param_get_int(buffer, kp);\n}\n\nstatic int param_get_debug(char *buffer, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_view_capable(NULL))\n\t\treturn -EPERM;\n\treturn aa_print_debug_params(buffer);\n}\n\nstatic int param_set_debug(const char *val, const struct kernel_param *kp)\n{\n\tint i;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (!val)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_admin_capable(NULL))\n\t\treturn -EPERM;\n\n\ti = aa_parse_debug_params(val);\n\tif (i == DEBUG_PARSE_ERROR)\n\t\treturn -EINVAL;\n\n\taa_g_debug = i;\n\treturn 0;\n}\n\nstatic int param_get_audit(char *buffer, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_view_capable(NULL))\n\t\treturn -EPERM;\n\treturn sprintf(buffer, \"%s\", audit_mode_names[aa_g_audit]);\n}\n\nstatic int param_set_audit(const char *val, const struct kernel_param *kp)\n{\n\tint i;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (!val)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_admin_capable(NULL))\n\t\treturn -EPERM;\n\n\ti = match_string(audit_mode_names, AUDIT_MAX_INDEX, val);\n\tif (i < 0)\n\t\treturn -EINVAL;\n\n\taa_g_audit = i;\n\treturn 0;\n}\n\nstatic int param_get_mode(char *buffer, const struct kernel_param *kp)\n{\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_view_capable(NULL))\n\t\treturn -EPERM;\n\n\treturn sprintf(buffer, \"%s\", aa_profile_mode_names[aa_g_profile_mode]);\n}\n\nstatic int param_set_mode(const char *val, const struct kernel_param *kp)\n{\n\tint i;\n\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\tif (!val)\n\t\treturn -EINVAL;\n\tif (apparmor_initialized && !aa_current_policy_admin_capable(NULL))\n\t\treturn -EPERM;\n\n\ti = match_string(aa_profile_mode_names, APPARMOR_MODE_NAMES_MAX_INDEX,\n\t\t\t val);\n\tif (i < 0)\n\t\treturn -EINVAL;\n\n\taa_g_profile_mode = i;\n\treturn 0;\n}\n\nchar *aa_get_buffer(bool in_atomic)\n{\n\tunion aa_buffer *aa_buf;\n\tstruct aa_local_cache *cache;\n\tbool try_again = true;\n\tgfp_t flags = (GFP_KERNEL | __GFP_RETRY_MAYFAIL | __GFP_NOWARN);\n\n\t/* use per cpu cached buffers first */\n\tcache = get_cpu_ptr(&aa_local_buffers);\n\tif (!list_empty(&cache->head)) {\n\t\taa_buf = list_first_entry(&cache->head, union aa_buffer, list);\n\t\tlist_del(&aa_buf->list);\n\t\tcache->hold--;\n\t\tcache->count--;\n\t\tput_cpu_ptr(&aa_local_buffers);\n\t\treturn &aa_buf->buffer[0];\n\t}\n\tput_cpu_ptr(&aa_local_buffers);\n\n\tif (!spin_trylock(&aa_buffers_lock)) {\n\t\tcache = get_cpu_ptr(&aa_local_buffers);\n\t\tcache->hold += 1;\n\t\tput_cpu_ptr(&aa_local_buffers);\n\t\tspin_lock(&aa_buffers_lock);\n\t} else {\n\t\tcache = get_cpu_ptr(&aa_local_buffers);\n\t\tput_cpu_ptr(&aa_local_buffers);\n\t}\nretry:\n\tif (buffer_count > reserve_count ||\n\t    (in_atomic && !list_empty(&aa_global_buffers))) {\n\t\taa_buf = list_first_entry(&aa_global_buffers, union aa_buffer,\n\t\t\t\t\t  list);\n\t\tlist_del(&aa_buf->list);\n\t\tbuffer_count--;\n\t\tspin_unlock(&aa_buffers_lock);\n\t\treturn aa_buf->buffer;\n\t}\n\tif (in_atomic) {\n\t\t/*\n\t\t * out of reserve buffers and in atomic context so increase\n\t\t * how many buffers to keep in reserve\n\t\t */\n\t\treserve_count++;\n\t\tflags = GFP_ATOMIC;\n\t}\n\tspin_unlock(&aa_buffers_lock);\n\n\tif (!in_atomic)\n\t\tmight_sleep();\n\taa_buf = kmalloc(aa_g_path_max, flags);\n\tif (!aa_buf) {\n\t\tif (try_again) {\n\t\t\ttry_again = false;\n\t\t\tspin_lock(&aa_buffers_lock);\n\t\t\tgoto retry;\n\t\t}\n\t\tpr_warn_once(\"AppArmor: Failed to allocate a memory buffer.\\n\");\n\t\treturn NULL;\n\t}\n\treturn aa_buf->buffer;\n}\n\nvoid aa_put_buffer(char *buf)\n{\n\tunion aa_buffer *aa_buf;\n\tstruct aa_local_cache *cache;\n\n\tif (!buf)\n\t\treturn;\n\taa_buf = container_of(buf, union aa_buffer, buffer[0]);\n\n\tcache = get_cpu_ptr(&aa_local_buffers);\n\tif (!cache->hold) {\n\t\tput_cpu_ptr(&aa_local_buffers);\n\n\t\tif (spin_trylock(&aa_buffers_lock)) {\n\t\t\t/* put back on global list */\n\t\t\tlist_add(&aa_buf->list, &aa_global_buffers);\n\t\t\tbuffer_count++;\n\t\t\tspin_unlock(&aa_buffers_lock);\n\t\t\tcache = get_cpu_ptr(&aa_local_buffers);\n\t\t\tput_cpu_ptr(&aa_local_buffers);\n\t\t\treturn;\n\t\t}\n\t\t/* contention on global list, fallback to percpu */\n\t\tcache = get_cpu_ptr(&aa_local_buffers);\n\t\tcache->hold += 1;\n\t}\n\n\t/* cache in percpu list */\n\tlist_add(&aa_buf->list, &cache->head);\n\tcache->count++;\n\tput_cpu_ptr(&aa_local_buffers);\n}\n\n/*\n * AppArmor init functions\n */\n\n/**\n * set_init_ctx - set a task context and profile on the first task.\n *\n * TODO: allow setting an alternate profile than unconfined\n */\nstatic int __init set_init_ctx(void)\n{\n\tstruct cred *cred = (__force struct cred *)current->real_cred;\n\n\tset_cred_label(cred, aa_get_label(ns_unconfined(root_ns)));\n\n\treturn 0;\n}\n\nstatic void destroy_buffers(void)\n{\n\tunion aa_buffer *aa_buf;\n\n\tspin_lock(&aa_buffers_lock);\n\twhile (!list_empty(&aa_global_buffers)) {\n\t\taa_buf = list_first_entry(&aa_global_buffers, union aa_buffer,\n\t\t\t\t\t list);\n\t\tlist_del(&aa_buf->list);\n\t\tspin_unlock(&aa_buffers_lock);\n\t\tkfree(aa_buf);\n\t\tspin_lock(&aa_buffers_lock);\n\t}\n\tspin_unlock(&aa_buffers_lock);\n}\n\nstatic int __init alloc_buffers(void)\n{\n\tunion aa_buffer *aa_buf;\n\tint i, num;\n\n\t/*\n\t * per cpu set of cached allocated buffers used to help reduce\n\t * lock contention\n\t */\n\tfor_each_possible_cpu(i) {\n\t\tper_cpu(aa_local_buffers, i).hold = 0;\n\t\tper_cpu(aa_local_buffers, i).count = 0;\n\t\tINIT_LIST_HEAD(&per_cpu(aa_local_buffers, i).head);\n\t}\n\t/*\n\t * A function may require two buffers at once. Usually the buffers are\n\t * used for a short period of time and are shared. On UP kernel buffers\n\t * two should be enough, with more CPUs it is possible that more\n\t * buffers will be used simultaneously. The preallocated pool may grow.\n\t * This preallocation has also the side-effect that AppArmor will be\n\t * disabled early at boot if aa_g_path_max is extremely high.\n\t */\n\tif (num_online_cpus() > 1)\n\t\tnum = 4 + RESERVE_COUNT;\n\telse\n\t\tnum = 2 + RESERVE_COUNT;\n\n\tfor (i = 0; i < num; i++) {\n\n\t\taa_buf = kmalloc(aa_g_path_max, GFP_KERNEL |\n\t\t\t\t __GFP_RETRY_MAYFAIL | __GFP_NOWARN);\n\t\tif (!aa_buf) {\n\t\t\tdestroy_buffers();\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\taa_put_buffer(aa_buf->buffer);\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int apparmor_dointvec(const struct ctl_table *table, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tif (!aa_current_policy_admin_capable(NULL))\n\t\treturn -EPERM;\n\tif (!apparmor_enabled)\n\t\treturn -EINVAL;\n\n\treturn proc_dointvec(table, write, buffer, lenp, ppos);\n}\n\nstatic const struct ctl_table apparmor_sysctl_table[] = {\n#ifdef CONFIG_USER_NS\n\t{\n\t\t.procname       = \"unprivileged_userns_apparmor_policy\",\n\t\t.data           = &unprivileged_userns_apparmor_policy,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0600,\n\t\t.proc_handler   = apparmor_dointvec,\n\t},\n#endif /* CONFIG_USER_NS */\n\t{\n\t\t.procname       = \"apparmor_display_secid_mode\",\n\t\t.data           = &apparmor_display_secid_mode,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0600,\n\t\t.proc_handler   = apparmor_dointvec,\n\t},\n\t{\n\t\t.procname       = \"apparmor_restrict_unprivileged_unconfined\",\n\t\t.data           = &aa_unprivileged_unconfined_restricted,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0600,\n\t\t.proc_handler   = apparmor_dointvec,\n\t},\n};\n\nstatic int __init apparmor_init_sysctl(void)\n{\n\treturn register_sysctl(\"kernel\", apparmor_sysctl_table) ? 0 : -ENOMEM;\n}\n#else\nstatic inline int apparmor_init_sysctl(void)\n{\n\treturn 0;\n}\n#endif /* CONFIG_SYSCTL */\n\n#if defined(CONFIG_NETFILTER) && defined(CONFIG_NETWORK_SECMARK)\nstatic unsigned int apparmor_ip_postroute(void *priv,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  const struct nf_hook_state *state)\n{\n\tstruct aa_sk_ctx *ctx;\n\tstruct sock *sk;\n\tint error;\n\n\tif (!skb->secmark)\n\t\treturn NF_ACCEPT;\n\n\tsk = skb_to_full_sk(skb);\n\tif (sk == NULL)\n\t\treturn NF_ACCEPT;\n\n\tctx = aa_sock(sk);\n\trcu_read_lock();\n\terror = apparmor_secmark_check(rcu_dereference(ctx->label), OP_SENDMSG,\n\t\t\t\t       AA_MAY_SEND, skb->secmark, sk);\n\trcu_read_unlock();\n\tif (!error)\n\t\treturn NF_ACCEPT;\n\n\treturn NF_DROP_ERR(-ECONNREFUSED);\n\n}\n\nstatic const struct nf_hook_ops apparmor_nf_ops[] = {\n\t{\n\t\t.hook =         apparmor_ip_postroute,\n\t\t.pf =           NFPROTO_IPV4,\n\t\t.hooknum =      NF_INET_POST_ROUTING,\n\t\t.priority =     NF_IP_PRI_SELINUX_FIRST,\n\t},\n#if IS_ENABLED(CONFIG_IPV6)\n\t{\n\t\t.hook =         apparmor_ip_postroute,\n\t\t.pf =           NFPROTO_IPV6,\n\t\t.hooknum =      NF_INET_POST_ROUTING,\n\t\t.priority =     NF_IP6_PRI_SELINUX_FIRST,\n\t},\n#endif\n};\n\nstatic int __net_init apparmor_nf_register(struct net *net)\n{\n\treturn nf_register_net_hooks(net, apparmor_nf_ops,\n\t\t\t\t    ARRAY_SIZE(apparmor_nf_ops));\n}\n\nstatic void __net_exit apparmor_nf_unregister(struct net *net)\n{\n\tnf_unregister_net_hooks(net, apparmor_nf_ops,\n\t\t\t\tARRAY_SIZE(apparmor_nf_ops));\n}\n\nstatic struct pernet_operations apparmor_net_ops = {\n\t.init = apparmor_nf_register,\n\t.exit = apparmor_nf_unregister,\n};\n\nstatic int __init apparmor_nf_ip_init(void)\n{\n\tint err;\n\n\tif (!apparmor_enabled)\n\t\treturn 0;\n\n\terr = register_pernet_subsys(&apparmor_net_ops);\n\tif (err)\n\t\tpanic(\"Apparmor: register_pernet_subsys: error %d\\n\", err);\n\n\treturn 0;\n}\n__initcall(apparmor_nf_ip_init);\n#endif\n\nstatic char nulldfa_src[] __aligned(8) = {\n\t#include \"nulldfa.in\"\n};\nstatic struct aa_dfa *nulldfa;\n\nstatic char stacksplitdfa_src[] __aligned(8) = {\n\t#include \"stacksplitdfa.in\"\n};\nstruct aa_dfa *stacksplitdfa;\nstruct aa_policydb *nullpdb;\n\nstatic int __init aa_setup_dfa_engine(void)\n{\n\tint error = -ENOMEM;\n\n\tnullpdb = aa_alloc_pdb(GFP_KERNEL);\n\tif (!nullpdb)\n\t\treturn -ENOMEM;\n\n\tnulldfa = aa_dfa_unpack(nulldfa_src, sizeof(nulldfa_src),\n\t\t\t    TO_ACCEPT1_FLAG(YYTD_DATA32) |\n\t\t\t    TO_ACCEPT2_FLAG(YYTD_DATA32));\n\tif (IS_ERR(nulldfa)) {\n\t\terror = PTR_ERR(nulldfa);\n\t\tgoto fail;\n\t}\n\tnullpdb->dfa = aa_get_dfa(nulldfa);\n\tnullpdb->perms = kcalloc(2, sizeof(struct aa_perms), GFP_KERNEL);\n\tif (!nullpdb->perms)\n\t\tgoto fail;\n\tnullpdb->size = 2;\n\n\tstacksplitdfa = aa_dfa_unpack(stacksplitdfa_src,\n\t\t\t\t      sizeof(stacksplitdfa_src),\n\t\t\t\t      TO_ACCEPT1_FLAG(YYTD_DATA32) |\n\t\t\t\t      TO_ACCEPT2_FLAG(YYTD_DATA32));\n\tif (IS_ERR(stacksplitdfa)) {\n\t\terror = PTR_ERR(stacksplitdfa);\n\t\tgoto fail;\n\t}\n\n\treturn 0;\n\nfail:\n\taa_put_pdb(nullpdb);\n\taa_put_dfa(nulldfa);\n\tnullpdb = NULL;\n\tnulldfa = NULL;\n\tstacksplitdfa = NULL;\n\n\treturn error;\n}\n\nstatic void __init aa_teardown_dfa_engine(void)\n{\n\taa_put_dfa(stacksplitdfa);\n\taa_put_dfa(nulldfa);\n\taa_put_pdb(nullpdb);\n\tnullpdb = NULL;\n\tstacksplitdfa = NULL;\n\tnulldfa = NULL;\n}\n\nstatic int __init apparmor_init(void)\n{\n\tint error;\n\n\terror = aa_setup_dfa_engine();\n\tif (error) {\n\t\tAA_ERROR(\"Unable to setup dfa engine\\n\");\n\t\tgoto alloc_out;\n\t}\n\n\terror = aa_alloc_root_ns();\n\tif (error) {\n\t\tAA_ERROR(\"Unable to allocate default profile namespace\\n\");\n\t\tgoto alloc_out;\n\t}\n\n\terror = apparmor_init_sysctl();\n\tif (error) {\n\t\tAA_ERROR(\"Unable to register sysctls\\n\");\n\t\tgoto alloc_out;\n\n\t}\n\n\terror = alloc_buffers();\n\tif (error) {\n\t\tAA_ERROR(\"Unable to allocate work buffers\\n\");\n\t\tgoto alloc_out;\n\t}\n\n\terror = set_init_ctx();\n\tif (error) {\n\t\tAA_ERROR(\"Failed to set context on init task\\n\");\n\t\taa_free_root_ns();\n\t\tgoto buffers_out;\n\t}\n\tsecurity_add_hooks(apparmor_hooks, ARRAY_SIZE(apparmor_hooks),\n\t\t\t\t&apparmor_lsmid);\n\n\t/* Report that AppArmor successfully initialized */\n\tapparmor_initialized = 1;\n\tif (aa_g_profile_mode == APPARMOR_COMPLAIN)\n\t\taa_info_message(\"AppArmor initialized: complain mode enabled\");\n\telse if (aa_g_profile_mode == APPARMOR_KILL)\n\t\taa_info_message(\"AppArmor initialized: kill mode enabled\");\n\telse\n\t\taa_info_message(\"AppArmor initialized\");\n\n\treturn error;\n\nbuffers_out:\n\tdestroy_buffers();\nalloc_out:\n\taa_destroy_aafs();\n\taa_teardown_dfa_engine();\n\n\tapparmor_enabled = false;\n\treturn error;\n}\n\nDEFINE_LSM(apparmor) = {\n\t.name = \"apparmor\",\n\t.flags = LSM_FLAG_LEGACY_MAJOR | LSM_FLAG_EXCLUSIVE,\n\t.enabled = &apparmor_enabled,\n\t.blobs = &apparmor_blob_sizes,\n\t.init = apparmor_init,\n};\n", "patch": "@@ -500,34 +500,34 @@ static int apparmor_setprocattr(struct task_struct *task, char *name,\n {\n \tstruct common_audit_data sa;\n \tstruct apparmor_audit_data aad = {0,};\n-\tchar *command, *args = value;\n+\tchar *command, *largs = NULL, *args = value;\n \tsize_t arg_size;\n \tint error;\n \n \tif (size == 0)\n \t\treturn -EINVAL;\n-\t/* args points to a PAGE_SIZE buffer, AppArmor requires that\n-\t * the buffer must be null terminated or have size <= PAGE_SIZE -1\n-\t * so that AppArmor can null terminate them\n-\t */\n-\tif (args[size - 1] != '\\0') {\n-\t\tif (size == PAGE_SIZE)\n-\t\t\treturn -EINVAL;\n-\t\targs[size] = '\\0';\n-\t}\n-\n \t/* task can only write its own attributes */\n \tif (current != task)\n \t\treturn -EACCES;\n \n-\targs = value;\n+\t/* AppArmor requires that the buffer must be null terminated atm */\n+\tif (args[size - 1] != '\\0') {\n+\t\t/* null terminate */\n+\t\tlargs = args = kmalloc(size + 1, GFP_KERNEL);\n+\t\tif (!args)\n+\t\t\treturn -ENOMEM;\n+\t\tmemcpy(args, value, size);\n+\t\targs[size] = '\\0';\n+\t}\n+\n+\terror = -EINVAL;\n \targs = strim(args);\n \tcommand = strsep(&args, \" \");\n \tif (!args)\n-\t\treturn -EINVAL;\n+\t\tgoto out;\n \targs = skip_spaces(args);\n \tif (!*args)\n-\t\treturn -EINVAL;\n+\t\tgoto out;\n \n \targ_size = size - (args - (char *) value);\n \tif (strcmp(name, \"current\") == 0) {\n@@ -553,10 +553,12 @@ static int apparmor_setprocattr(struct task_struct *task, char *name,\n \t\t\tgoto fail;\n \t} else\n \t\t/* only support the \"current\" and \"exec\" process attributes */\n-\t\treturn -EINVAL;\n+\t\tgoto fail;\n \n \tif (!error)\n \t\terror = size;\n+out:\n+\tkfree(largs);\n \treturn error;\n \n fail:\n@@ -565,9 +567,9 @@ static int apparmor_setprocattr(struct task_struct *task, char *name,\n \taad.profile = aa_current_profile();\n \taad.op = OP_SETPROCATTR;\n \taad.info = name;\n-\taad.error = -EINVAL;\n+\taad.error = error = -EINVAL;\n \taa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);\n-\treturn -EINVAL;\n+\tgoto out;\n }\n \n static int apparmor_task_setrlimit(struct task_struct *task,", "file_path": "files/2016_8\\96", "file_language": "c", "file_name": "security/apparmor/lsm.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 36, "cve_id": "CVE-2016-6198", "cwe_id": ["CWE-284"], "cve_language": "C", "cve_description": "The filesystem layer in the Linux kernel before 4.5.5 proceeds with post-rename operations after an OverlayFS file is renamed to a self-hardlink, which allows local users to cause a denial of service (system crash) via a rename system call, related to fs/namei.c and fs/open.c.", "cvss": "5.5", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "NONE", "A": "HIGH", "commit_id": "54d5ca871e72f2bb172ec9323497f01cd5091ec7", "commit_message": "vfs: add vfs_select_inode() helper\n\nSigned-off-by: Miklos Szeredi <mszeredi@redhat.com>\nCc: <stable@vger.kernel.org> # v4.2+", "commit_date": "2016-05-11T03:55:01Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/54d5ca871e72f2bb172ec9323497f01cd5091ec7", "html_url": "https://github.com/torvalds/linux/commit/54d5ca871e72f2bb172ec9323497f01cd5091ec7", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "44549e8f5eea4e0a41b487b63e616cb089922b99", "url_before": "https://api.github.com/repos/torvalds/linux/commits/44549e8f5eea4e0a41b487b63e616cb089922b99", "html_url_before": "https://github.com/torvalds/linux/commit/44549e8f5eea4e0a41b487b63e616cb089922b99"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/54d5ca871e72f2bb172ec9323497f01cd5091ec7/fs/open.c", "code": "/*\n *  linux/fs/open.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/fsnotify.h>\n#include <linux/module.h>\n#include <linux/tty.h>\n#include <linux/namei.h>\n#include <linux/backing-dev.h>\n#include <linux/capability.h>\n#include <linux/securebits.h>\n#include <linux/security.h>\n#include <linux/mount.h>\n#include <linux/fcntl.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/fs.h>\n#include <linux/personality.h>\n#include <linux/pagemap.h>\n#include <linux/syscalls.h>\n#include <linux/rcupdate.h>\n#include <linux/audit.h>\n#include <linux/falloc.h>\n#include <linux/fs_struct.h>\n#include <linux/ima.h>\n#include <linux/dnotify.h>\n#include <linux/compat.h>\n\n#include \"internal.h\"\n\nint do_truncate(struct dentry *dentry, loff_t length, unsigned int time_attrs,\n\tstruct file *filp)\n{\n\tint ret;\n\tstruct iattr newattrs;\n\n\t/* Not pretty: \"inode->i_size\" shouldn't really be signed. But it is. */\n\tif (length < 0)\n\t\treturn -EINVAL;\n\n\tnewattrs.ia_size = length;\n\tnewattrs.ia_valid = ATTR_SIZE | time_attrs;\n\tif (filp) {\n\t\tnewattrs.ia_file = filp;\n\t\tnewattrs.ia_valid |= ATTR_FILE;\n\t}\n\n\t/* Remove suid, sgid, and file capabilities on truncate too */\n\tret = dentry_needs_remove_privs(dentry);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret)\n\t\tnewattrs.ia_valid |= ret | ATTR_FORCE;\n\n\tinode_lock(dentry->d_inode);\n\t/* Note any delegations or leases have already been broken: */\n\tret = notify_change(dentry, &newattrs, NULL);\n\tinode_unlock(dentry->d_inode);\n\treturn ret;\n}\n\nlong vfs_truncate(struct path *path, loff_t length)\n{\n\tstruct inode *inode;\n\tlong error;\n\n\tinode = path->dentry->d_inode;\n\n\t/* For directories it's -EISDIR, for other non-regulars - -EINVAL */\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn -EISDIR;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\terror = mnt_want_write(path->mnt);\n\tif (error)\n\t\tgoto out;\n\n\terror = inode_permission(inode, MAY_WRITE);\n\tif (error)\n\t\tgoto mnt_drop_write_and_out;\n\n\terror = -EPERM;\n\tif (IS_APPEND(inode))\n\t\tgoto mnt_drop_write_and_out;\n\n\terror = get_write_access(inode);\n\tif (error)\n\t\tgoto mnt_drop_write_and_out;\n\n\t/*\n\t * Make sure that there are no leases.  get_write_access() protects\n\t * against the truncate racing with a lease-granting setlease().\n\t */\n\terror = break_lease(inode, O_WRONLY);\n\tif (error)\n\t\tgoto put_write_and_out;\n\n\terror = locks_verify_truncate(inode, NULL, length);\n\tif (!error)\n\t\terror = security_path_truncate(path);\n\tif (!error)\n\t\terror = do_truncate(path->dentry, length, 0, NULL);\n\nput_write_and_out:\n\tput_write_access(inode);\nmnt_drop_write_and_out:\n\tmnt_drop_write(path->mnt);\nout:\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(vfs_truncate);\n\nstatic long do_sys_truncate(const char __user *pathname, loff_t length)\n{\n\tunsigned int lookup_flags = LOOKUP_FOLLOW;\n\tstruct path path;\n\tint error;\n\n\tif (length < 0)\t/* sorry, but loff_t says... */\n\t\treturn -EINVAL;\n\nretry:\n\terror = user_path_at(AT_FDCWD, pathname, lookup_flags, &path);\n\tif (!error) {\n\t\terror = vfs_truncate(&path, length);\n\t\tpath_put(&path);\n\t}\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(truncate, const char __user *, path, long, length)\n{\n\treturn do_sys_truncate(path, length);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(truncate, const char __user *, path, compat_off_t, length)\n{\n\treturn do_sys_truncate(path, length);\n}\n#endif\n\nstatic long do_sys_ftruncate(unsigned int fd, loff_t length, int small)\n{\n\tstruct inode *inode;\n\tstruct dentry *dentry;\n\tstruct fd f;\n\tint error;\n\n\terror = -EINVAL;\n\tif (length < 0)\n\t\tgoto out;\n\terror = -EBADF;\n\tf = fdget(fd);\n\tif (!f.file)\n\t\tgoto out;\n\n\t/* explicitly opened as large or we are on 64-bit box */\n\tif (f.file->f_flags & O_LARGEFILE)\n\t\tsmall = 0;\n\n\tdentry = f.file->f_path.dentry;\n\tinode = dentry->d_inode;\n\terror = -EINVAL;\n\tif (!S_ISREG(inode->i_mode) || !(f.file->f_mode & FMODE_WRITE))\n\t\tgoto out_putf;\n\n\terror = -EINVAL;\n\t/* Cannot ftruncate over 2^31 bytes without large file support */\n\tif (small && length > MAX_NON_LFS)\n\t\tgoto out_putf;\n\n\terror = -EPERM;\n\tif (IS_APPEND(inode))\n\t\tgoto out_putf;\n\n\tsb_start_write(inode->i_sb);\n\terror = locks_verify_truncate(inode, f.file, length);\n\tif (!error)\n\t\terror = security_path_truncate(&f.file->f_path);\n\tif (!error)\n\t\terror = do_truncate(dentry, length, ATTR_MTIME|ATTR_CTIME, f.file);\n\tsb_end_write(inode->i_sb);\nout_putf:\n\tfdput(f);\nout:\n\treturn error;\n}\n\nSYSCALL_DEFINE2(ftruncate, unsigned int, fd, unsigned long, length)\n{\n\treturn do_sys_ftruncate(fd, length, 1);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(ftruncate, unsigned int, fd, compat_ulong_t, length)\n{\n\treturn do_sys_ftruncate(fd, length, 1);\n}\n#endif\n\n/* LFS versions of truncate are only needed on 32 bit machines */\n#if BITS_PER_LONG == 32\nSYSCALL_DEFINE2(truncate64, const char __user *, path, loff_t, length)\n{\n\treturn do_sys_truncate(path, length);\n}\n\nSYSCALL_DEFINE2(ftruncate64, unsigned int, fd, loff_t, length)\n{\n\treturn do_sys_ftruncate(fd, length, 0);\n}\n#endif /* BITS_PER_LONG == 32 */\n\n\nint vfs_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file_inode(file);\n\tlong ret;\n\n\tif (offset < 0 || len <= 0)\n\t\treturn -EINVAL;\n\n\t/* Return error if mode is not supported */\n\tif (mode & ~FALLOC_FL_SUPPORTED_MASK)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Punch hole and zero range are mutually exclusive */\n\tif ((mode & (FALLOC_FL_PUNCH_HOLE | FALLOC_FL_ZERO_RANGE)) ==\n\t    (FALLOC_FL_PUNCH_HOLE | FALLOC_FL_ZERO_RANGE))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Punch hole must have keep size set */\n\tif ((mode & FALLOC_FL_PUNCH_HOLE) &&\n\t    !(mode & FALLOC_FL_KEEP_SIZE))\n\t\treturn -EOPNOTSUPP;\n\n\t/* Collapse range should only be used exclusively. */\n\tif ((mode & FALLOC_FL_COLLAPSE_RANGE) &&\n\t    (mode & ~FALLOC_FL_COLLAPSE_RANGE))\n\t\treturn -EINVAL;\n\n\t/* Insert range should only be used exclusively. */\n\tif ((mode & FALLOC_FL_INSERT_RANGE) &&\n\t    (mode & ~FALLOC_FL_INSERT_RANGE))\n\t\treturn -EINVAL;\n\n\tif (!(file->f_mode & FMODE_WRITE))\n\t\treturn -EBADF;\n\n\t/*\n\t * We can only allow pure fallocate on append only files\n\t */\n\tif ((mode & ~FALLOC_FL_KEEP_SIZE) && IS_APPEND(inode))\n\t\treturn -EPERM;\n\n\tif (IS_IMMUTABLE(inode))\n\t\treturn -EPERM;\n\n\t/*\n\t * We cannot allow any fallocate operation on an active swapfile\n\t */\n\tif (IS_SWAPFILE(inode))\n\t\treturn -ETXTBSY;\n\n\t/*\n\t * Revalidate the write permissions, in case security policy has\n\t * changed since the files were opened.\n\t */\n\tret = security_file_permission(file, MAY_WRITE);\n\tif (ret)\n\t\treturn ret;\n\n\tif (S_ISFIFO(inode->i_mode))\n\t\treturn -ESPIPE;\n\n\t/*\n\t * Let individual file system decide if it supports preallocation\n\t * for directories or not.\n\t */\n\tif (!S_ISREG(inode->i_mode) && !S_ISDIR(inode->i_mode))\n\t\treturn -ENODEV;\n\n\t/* Check for wrap through zero too */\n\tif (((offset + len) > inode->i_sb->s_maxbytes) || ((offset + len) < 0))\n\t\treturn -EFBIG;\n\n\tif (!file->f_op->fallocate)\n\t\treturn -EOPNOTSUPP;\n\n\tsb_start_write(inode->i_sb);\n\tret = file->f_op->fallocate(file, mode, offset, len);\n\n\t/*\n\t * Create inotify and fanotify events.\n\t *\n\t * To keep the logic simple always create events if fallocate succeeds.\n\t * This implies that events are even created if the file size remains\n\t * unchanged, e.g. when using flag FALLOC_FL_KEEP_SIZE.\n\t */\n\tif (ret == 0)\n\t\tfsnotify_modify(file);\n\n\tsb_end_write(inode->i_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(vfs_fallocate);\n\nSYSCALL_DEFINE4(fallocate, int, fd, int, mode, loff_t, offset, loff_t, len)\n{\n\tstruct fd f = fdget(fd);\n\tint error = -EBADF;\n\n\tif (f.file) {\n\t\terror = vfs_fallocate(f.file, mode, offset, len);\n\t\tfdput(f);\n\t}\n\treturn error;\n}\n\n/*\n * access() needs to use the real uid/gid, not the effective uid/gid.\n * We do this by temporarily clearing all FS-related capabilities and\n * switching the fsuid/fsgid around to the real ones.\n */\nSYSCALL_DEFINE3(faccessat, int, dfd, const char __user *, filename, int, mode)\n{\n\tconst struct cred *old_cred;\n\tstruct cred *override_cred;\n\tstruct path path;\n\tstruct inode *inode;\n\tint res;\n\tunsigned int lookup_flags = LOOKUP_FOLLOW;\n\n\tif (mode & ~S_IRWXO)\t/* where's F_OK, X_OK, W_OK, R_OK? */\n\t\treturn -EINVAL;\n\n\toverride_cred = prepare_creds();\n\tif (!override_cred)\n\t\treturn -ENOMEM;\n\n\toverride_cred->fsuid = override_cred->uid;\n\toverride_cred->fsgid = override_cred->gid;\n\n\tif (!issecure(SECURE_NO_SETUID_FIXUP)) {\n\t\t/* Clear the capabilities if we switch to a non-root user */\n\t\tkuid_t root_uid = make_kuid(override_cred->user_ns, 0);\n\t\tif (!uid_eq(override_cred->uid, root_uid))\n\t\t\tcap_clear(override_cred->cap_effective);\n\t\telse\n\t\t\toverride_cred->cap_effective =\n\t\t\t\toverride_cred->cap_permitted;\n\t}\n\n\told_cred = override_creds(override_cred);\nretry:\n\tres = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (res)\n\t\tgoto out;\n\n\tinode = d_backing_inode(path.dentry);\n\n\tif ((mode & MAY_EXEC) && S_ISREG(inode->i_mode)) {\n\t\t/*\n\t\t * MAY_EXEC on regular files is denied if the fs is mounted\n\t\t * with the \"noexec\" flag.\n\t\t */\n\t\tres = -EACCES;\n\t\tif (path_noexec(&path))\n\t\t\tgoto out_path_release;\n\t}\n\n\tres = inode_permission(inode, mode | MAY_ACCESS);\n\t/* SuS v2 requires we report a read only fs too */\n\tif (res || !(mode & S_IWOTH) || special_file(inode->i_mode))\n\t\tgoto out_path_release;\n\t/*\n\t * This is a rare case where using __mnt_is_readonly()\n\t * is OK without a mnt_want/drop_write() pair.  Since\n\t * no actual write to the fs is performed here, we do\n\t * not need to telegraph to that to anyone.\n\t *\n\t * By doing this, we accept that this access is\n\t * inherently racy and know that the fs may change\n\t * state before we even see this result.\n\t */\n\tif (__mnt_is_readonly(path.mnt))\n\t\tres = -EROFS;\n\nout_path_release:\n\tpath_put(&path);\n\tif (retry_estale(res, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\trevert_creds(old_cred);\n\tput_cred(override_cred);\n\treturn res;\n}\n\nSYSCALL_DEFINE2(access, const char __user *, filename, int, mode)\n{\n\treturn sys_faccessat(AT_FDCWD, filename, mode);\n}\n\nSYSCALL_DEFINE1(chdir, const char __user *, filename)\n{\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\nretry:\n\terror = user_path_at(AT_FDCWD, filename, lookup_flags, &path);\n\tif (error)\n\t\tgoto out;\n\n\terror = inode_permission(path.dentry->d_inode, MAY_EXEC | MAY_CHDIR);\n\tif (error)\n\t\tgoto dput_and_out;\n\n\tset_fs_pwd(current->fs, &path);\n\ndput_and_out:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\treturn error;\n}\n\nSYSCALL_DEFINE1(fchdir, unsigned int, fd)\n{\n\tstruct fd f = fdget_raw(fd);\n\tstruct inode *inode;\n\tint error = -EBADF;\n\n\terror = -EBADF;\n\tif (!f.file)\n\t\tgoto out;\n\n\tinode = file_inode(f.file);\n\n\terror = -ENOTDIR;\n\tif (!S_ISDIR(inode->i_mode))\n\t\tgoto out_putf;\n\n\terror = inode_permission(inode, MAY_EXEC | MAY_CHDIR);\n\tif (!error)\n\t\tset_fs_pwd(current->fs, &f.file->f_path);\nout_putf:\n\tfdput(f);\nout:\n\treturn error;\n}\n\nSYSCALL_DEFINE1(chroot, const char __user *, filename)\n{\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\nretry:\n\terror = user_path_at(AT_FDCWD, filename, lookup_flags, &path);\n\tif (error)\n\t\tgoto out;\n\n\terror = inode_permission(path.dentry->d_inode, MAY_EXEC | MAY_CHDIR);\n\tif (error)\n\t\tgoto dput_and_out;\n\n\terror = -EPERM;\n\tif (!ns_capable(current_user_ns(), CAP_SYS_CHROOT))\n\t\tgoto dput_and_out;\n\terror = security_path_chroot(&path);\n\tif (error)\n\t\tgoto dput_and_out;\n\n\tset_fs_root(current->fs, &path);\n\terror = 0;\ndput_and_out:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\treturn error;\n}\n\nstatic int chmod_common(struct path *path, umode_t mode)\n{\n\tstruct inode *inode = path->dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tstruct iattr newattrs;\n\tint error;\n\n\terror = mnt_want_write(path->mnt);\n\tif (error)\n\t\treturn error;\nretry_deleg:\n\tinode_lock(inode);\n\terror = security_path_chmod(path, mode);\n\tif (error)\n\t\tgoto out_unlock;\n\tnewattrs.ia_mode = (mode & S_IALLUGO) | (inode->i_mode & ~S_IALLUGO);\n\tnewattrs.ia_valid = ATTR_MODE | ATTR_CTIME;\n\terror = notify_change(path->dentry, &newattrs, &delegated_inode);\nout_unlock:\n\tinode_unlock(inode);\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\tmnt_drop_write(path->mnt);\n\treturn error;\n}\n\nSYSCALL_DEFINE2(fchmod, unsigned int, fd, umode_t, mode)\n{\n\tstruct fd f = fdget(fd);\n\tint err = -EBADF;\n\n\tif (f.file) {\n\t\taudit_file(f.file);\n\t\terr = chmod_common(&f.file->f_path, mode);\n\t\tfdput(f);\n\t}\n\treturn err;\n}\n\nSYSCALL_DEFINE3(fchmodat, int, dfd, const char __user *, filename, umode_t, mode)\n{\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = LOOKUP_FOLLOW;\nretry:\n\terror = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (!error) {\n\t\terror = chmod_common(&path, mode);\n\t\tpath_put(&path);\n\t\tif (retry_estale(error, lookup_flags)) {\n\t\t\tlookup_flags |= LOOKUP_REVAL;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(chmod, const char __user *, filename, umode_t, mode)\n{\n\treturn sys_fchmodat(AT_FDCWD, filename, mode);\n}\n\nstatic int chown_common(struct path *path, uid_t user, gid_t group)\n{\n\tstruct inode *inode = path->dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tint error;\n\tstruct iattr newattrs;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tuid = make_kuid(current_user_ns(), user);\n\tgid = make_kgid(current_user_ns(), group);\n\nretry_deleg:\n\tnewattrs.ia_valid =  ATTR_CTIME;\n\tif (user != (uid_t) -1) {\n\t\tif (!uid_valid(uid))\n\t\t\treturn -EINVAL;\n\t\tnewattrs.ia_valid |= ATTR_UID;\n\t\tnewattrs.ia_uid = uid;\n\t}\n\tif (group != (gid_t) -1) {\n\t\tif (!gid_valid(gid))\n\t\t\treturn -EINVAL;\n\t\tnewattrs.ia_valid |= ATTR_GID;\n\t\tnewattrs.ia_gid = gid;\n\t}\n\tif (!S_ISDIR(inode->i_mode))\n\t\tnewattrs.ia_valid |=\n\t\t\tATTR_KILL_SUID | ATTR_KILL_SGID | ATTR_KILL_PRIV;\n\tinode_lock(inode);\n\terror = security_path_chown(path, uid, gid);\n\tif (!error)\n\t\terror = notify_change(path->dentry, &newattrs, &delegated_inode);\n\tinode_unlock(inode);\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE5(fchownat, int, dfd, const char __user *, filename, uid_t, user,\n\t\tgid_t, group, int, flag)\n{\n\tstruct path path;\n\tint error = -EINVAL;\n\tint lookup_flags;\n\n\tif ((flag & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)) != 0)\n\t\tgoto out;\n\n\tlookup_flags = (flag & AT_SYMLINK_NOFOLLOW) ? 0 : LOOKUP_FOLLOW;\n\tif (flag & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\nretry:\n\terror = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (error)\n\t\tgoto out;\n\terror = mnt_want_write(path.mnt);\n\tif (error)\n\t\tgoto out_release;\n\terror = chown_common(&path, user, group);\n\tmnt_drop_write(path.mnt);\nout_release:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\treturn error;\n}\n\nSYSCALL_DEFINE3(chown, const char __user *, filename, uid_t, user, gid_t, group)\n{\n\treturn sys_fchownat(AT_FDCWD, filename, user, group, 0);\n}\n\nSYSCALL_DEFINE3(lchown, const char __user *, filename, uid_t, user, gid_t, group)\n{\n\treturn sys_fchownat(AT_FDCWD, filename, user, group,\n\t\t\t    AT_SYMLINK_NOFOLLOW);\n}\n\nSYSCALL_DEFINE3(fchown, unsigned int, fd, uid_t, user, gid_t, group)\n{\n\tstruct fd f = fdget(fd);\n\tint error = -EBADF;\n\n\tif (!f.file)\n\t\tgoto out;\n\n\terror = mnt_want_write_file(f.file);\n\tif (error)\n\t\tgoto out_fput;\n\taudit_file(f.file);\n\terror = chown_common(&f.file->f_path, user, group);\n\tmnt_drop_write_file(f.file);\nout_fput:\n\tfdput(f);\nout:\n\treturn error;\n}\n\nint open_check_o_direct(struct file *f)\n{\n\t/* NB: we're sure to have correct a_ops only after f_op->open */\n\tif (f->f_flags & O_DIRECT) {\n\t\tif (!f->f_mapping->a_ops || !f->f_mapping->a_ops->direct_IO)\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int do_dentry_open(struct file *f,\n\t\t\t  struct inode *inode,\n\t\t\t  int (*open)(struct inode *, struct file *),\n\t\t\t  const struct cred *cred)\n{\n\tstatic const struct file_operations empty_fops = {};\n\tint error;\n\n\tf->f_mode = OPEN_FMODE(f->f_flags) | FMODE_LSEEK |\n\t\t\t\tFMODE_PREAD | FMODE_PWRITE;\n\n\tpath_get(&f->f_path);\n\tf->f_inode = inode;\n\tf->f_mapping = inode->i_mapping;\n\n\tif (unlikely(f->f_flags & O_PATH)) {\n\t\tf->f_mode = FMODE_PATH;\n\t\tf->f_op = &empty_fops;\n\t\treturn 0;\n\t}\n\n\tif (f->f_mode & FMODE_WRITE && !special_file(inode->i_mode)) {\n\t\terror = get_write_access(inode);\n\t\tif (unlikely(error))\n\t\t\tgoto cleanup_file;\n\t\terror = __mnt_want_write(f->f_path.mnt);\n\t\tif (unlikely(error)) {\n\t\t\tput_write_access(inode);\n\t\t\tgoto cleanup_file;\n\t\t}\n\t\tf->f_mode |= FMODE_WRITER;\n\t}\n\n\t/* POSIX.1-2008/SUSv4 Section XSI 2.9.7 */\n\tif (S_ISREG(inode->i_mode))\n\t\tf->f_mode |= FMODE_ATOMIC_POS;\n\n\tf->f_op = fops_get(inode->i_fop);\n\tif (unlikely(WARN_ON(!f->f_op))) {\n\t\terror = -ENODEV;\n\t\tgoto cleanup_all;\n\t}\n\n\terror = security_file_open(f, cred);\n\tif (error)\n\t\tgoto cleanup_all;\n\n\terror = break_lease(inode, f->f_flags);\n\tif (error)\n\t\tgoto cleanup_all;\n\n\tif (!open)\n\t\topen = f->f_op->open;\n\tif (open) {\n\t\terror = open(inode, f);\n\t\tif (error)\n\t\t\tgoto cleanup_all;\n\t}\n\tif ((f->f_mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ)\n\t\ti_readcount_inc(inode);\n\tif ((f->f_mode & FMODE_READ) &&\n\t     likely(f->f_op->read || f->f_op->read_iter))\n\t\tf->f_mode |= FMODE_CAN_READ;\n\tif ((f->f_mode & FMODE_WRITE) &&\n\t     likely(f->f_op->write || f->f_op->write_iter))\n\t\tf->f_mode |= FMODE_CAN_WRITE;\n\n\tf->f_flags &= ~(O_CREAT | O_EXCL | O_NOCTTY | O_TRUNC);\n\n\tfile_ra_state_init(&f->f_ra, f->f_mapping->host->i_mapping);\n\n\treturn 0;\n\ncleanup_all:\n\tfops_put(f->f_op);\n\tif (f->f_mode & FMODE_WRITER) {\n\t\tput_write_access(inode);\n\t\t__mnt_drop_write(f->f_path.mnt);\n\t}\ncleanup_file:\n\tpath_put(&f->f_path);\n\tf->f_path.mnt = NULL;\n\tf->f_path.dentry = NULL;\n\tf->f_inode = NULL;\n\treturn error;\n}\n\n/**\n * finish_open - finish opening a file\n * @file: file pointer\n * @dentry: pointer to dentry\n * @open: open callback\n * @opened: state of open\n *\n * This can be used to finish opening a file passed to i_op->atomic_open().\n *\n * If the open callback is set to NULL, then the standard f_op->open()\n * filesystem callback is substituted.\n *\n * NB: the dentry reference is _not_ consumed.  If, for example, the dentry is\n * the return value of d_splice_alias(), then the caller needs to perform dput()\n * on it after finish_open().\n *\n * On successful return @file is a fully instantiated open file.  After this, if\n * an error occurs in ->atomic_open(), it needs to clean up with fput().\n *\n * Returns zero on success or -errno if the open failed.\n */\nint finish_open(struct file *file, struct dentry *dentry,\n\t\tint (*open)(struct inode *, struct file *),\n\t\tint *opened)\n{\n\tint error;\n\tBUG_ON(*opened & FILE_OPENED); /* once it's opened, it's opened */\n\n\tfile->f_path.dentry = dentry;\n\terror = do_dentry_open(file, d_backing_inode(dentry), open,\n\t\t\t       current_cred());\n\tif (!error)\n\t\t*opened |= FILE_OPENED;\n\n\treturn error;\n}\nEXPORT_SYMBOL(finish_open);\n\n/**\n * finish_no_open - finish ->atomic_open() without opening the file\n *\n * @file: file pointer\n * @dentry: dentry or NULL (as returned from ->lookup())\n *\n * This can be used to set the result of a successful lookup in ->atomic_open().\n *\n * NB: unlike finish_open() this function does consume the dentry reference and\n * the caller need not dput() it.\n *\n * Returns \"1\" which must be the return value of ->atomic_open() after having\n * called this function.\n */\nint finish_no_open(struct file *file, struct dentry *dentry)\n{\n\tfile->f_path.dentry = dentry;\n\treturn 1;\n}\nEXPORT_SYMBOL(finish_no_open);\n\nchar *file_path(struct file *filp, char *buf, int buflen)\n{\n\treturn d_path(&filp->f_path, buf, buflen);\n}\nEXPORT_SYMBOL(file_path);\n\n/**\n * vfs_open - open the file at the given path\n * @path: path to open\n * @file: newly allocated file with f_flag initialized\n * @cred: credentials to use\n */\nint vfs_open(const struct path *path, struct file *file,\n\t     const struct cred *cred)\n{\n\tstruct inode *inode = vfs_select_inode(path->dentry, file->f_flags);\n\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\n\tfile->f_path = *path;\n\treturn do_dentry_open(file, inode, NULL, cred);\n}\n\nstruct file *dentry_open(const struct path *path, int flags,\n\t\t\t const struct cred *cred)\n{\n\tint error;\n\tstruct file *f;\n\n\tvalidate_creds(cred);\n\n\t/* We must always pass in a valid mount pointer. */\n\tBUG_ON(!path->mnt);\n\n\tf = get_empty_filp();\n\tif (!IS_ERR(f)) {\n\t\tf->f_flags = flags;\n\t\terror = vfs_open(path, f, cred);\n\t\tif (!error) {\n\t\t\t/* from now on we need fput() to dispose of f */\n\t\t\terror = open_check_o_direct(f);\n\t\t\tif (error) {\n\t\t\t\tfput(f);\n\t\t\t\tf = ERR_PTR(error);\n\t\t\t}\n\t\t} else { \n\t\t\tput_filp(f);\n\t\t\tf = ERR_PTR(error);\n\t\t}\n\t}\n\treturn f;\n}\nEXPORT_SYMBOL(dentry_open);\n\nstatic inline int build_open_flags(int flags, umode_t mode, struct open_flags *op)\n{\n\tint lookup_flags = 0;\n\tint acc_mode = ACC_MODE(flags);\n\n\tif (flags & (O_CREAT | __O_TMPFILE))\n\t\top->mode = (mode & S_IALLUGO) | S_IFREG;\n\telse\n\t\top->mode = 0;\n\n\t/* Must never be set by userspace */\n\tflags &= ~FMODE_NONOTIFY & ~O_CLOEXEC;\n\n\t/*\n\t * O_SYNC is implemented as __O_SYNC|O_DSYNC.  As many places only\n\t * check for O_DSYNC if the need any syncing at all we enforce it's\n\t * always set instead of having to deal with possibly weird behaviour\n\t * for malicious applications setting only __O_SYNC.\n\t */\n\tif (flags & __O_SYNC)\n\t\tflags |= O_DSYNC;\n\n\tif (flags & __O_TMPFILE) {\n\t\tif ((flags & O_TMPFILE_MASK) != O_TMPFILE)\n\t\t\treturn -EINVAL;\n\t\tif (!(acc_mode & MAY_WRITE))\n\t\t\treturn -EINVAL;\n\t} else if (flags & O_PATH) {\n\t\t/*\n\t\t * If we have O_PATH in the open flag. Then we\n\t\t * cannot have anything other than the below set of flags\n\t\t */\n\t\tflags &= O_DIRECTORY | O_NOFOLLOW | O_PATH;\n\t\tacc_mode = 0;\n\t}\n\n\top->open_flag = flags;\n\n\t/* O_TRUNC implies we need access checks for write permissions */\n\tif (flags & O_TRUNC)\n\t\tacc_mode |= MAY_WRITE;\n\n\t/* Allow the LSM permission hook to distinguish append\n\t   access from general write access. */\n\tif (flags & O_APPEND)\n\t\tacc_mode |= MAY_APPEND;\n\n\top->acc_mode = acc_mode;\n\n\top->intent = flags & O_PATH ? 0 : LOOKUP_OPEN;\n\n\tif (flags & O_CREAT) {\n\t\top->intent |= LOOKUP_CREATE;\n\t\tif (flags & O_EXCL)\n\t\t\top->intent |= LOOKUP_EXCL;\n\t}\n\n\tif (flags & O_DIRECTORY)\n\t\tlookup_flags |= LOOKUP_DIRECTORY;\n\tif (!(flags & O_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\top->lookup_flags = lookup_flags;\n\treturn 0;\n}\n\n/**\n * file_open_name - open file and return file pointer\n *\n * @name:\tstruct filename containing path to open\n * @flags:\topen flags as per the open(2) second argument\n * @mode:\tmode for the new file if O_CREAT is set, else ignored\n *\n * This is the helper to open a file from kernelspace if you really\n * have to.  But in generally you should not do this, so please move\n * along, nothing to see here..\n */\nstruct file *file_open_name(struct filename *name, int flags, umode_t mode)\n{\n\tstruct open_flags op;\n\tint err = build_open_flags(flags, mode, &op);\n\treturn err ? ERR_PTR(err) : do_filp_open(AT_FDCWD, name, &op);\n}\n\n/**\n * filp_open - open file and return file pointer\n *\n * @filename:\tpath to open\n * @flags:\topen flags as per the open(2) second argument\n * @mode:\tmode for the new file if O_CREAT is set, else ignored\n *\n * This is the helper to open a file from kernelspace if you really\n * have to.  But in generally you should not do this, so please move\n * along, nothing to see here..\n */\nstruct file *filp_open(const char *filename, int flags, umode_t mode)\n{\n\tstruct filename *name = getname_kernel(filename);\n\tstruct file *file = ERR_CAST(name);\n\t\n\tif (!IS_ERR(name)) {\n\t\tfile = file_open_name(name, flags, mode);\n\t\tputname(name);\n\t}\n\treturn file;\n}\nEXPORT_SYMBOL(filp_open);\n\nstruct file *file_open_root(struct dentry *dentry, struct vfsmount *mnt,\n\t\t\t    const char *filename, int flags, umode_t mode)\n{\n\tstruct open_flags op;\n\tint err = build_open_flags(flags, mode, &op);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\treturn do_file_open_root(dentry, mnt, filename, &op);\n}\nEXPORT_SYMBOL(file_open_root);\n\nlong do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)\n{\n\tstruct open_flags op;\n\tint fd = build_open_flags(flags, mode, &op);\n\tstruct filename *tmp;\n\n\tif (fd)\n\t\treturn fd;\n\n\ttmp = getname(filename);\n\tif (IS_ERR(tmp))\n\t\treturn PTR_ERR(tmp);\n\n\tfd = get_unused_fd_flags(flags);\n\tif (fd >= 0) {\n\t\tstruct file *f = do_filp_open(dfd, tmp, &op);\n\t\tif (IS_ERR(f)) {\n\t\t\tput_unused_fd(fd);\n\t\t\tfd = PTR_ERR(f);\n\t\t} else {\n\t\t\tfsnotify_open(f);\n\t\t\tfd_install(fd, f);\n\t\t}\n\t}\n\tputname(tmp);\n\treturn fd;\n}\n\nSYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)\n{\n\tif (force_o_largefile())\n\t\tflags |= O_LARGEFILE;\n\n\treturn do_sys_open(AT_FDCWD, filename, flags, mode);\n}\n\nSYSCALL_DEFINE4(openat, int, dfd, const char __user *, filename, int, flags,\n\t\tumode_t, mode)\n{\n\tif (force_o_largefile())\n\t\tflags |= O_LARGEFILE;\n\n\treturn do_sys_open(dfd, filename, flags, mode);\n}\n\n#ifndef __alpha__\n\n/*\n * For backward compatibility?  Maybe this should be moved\n * into arch/i386 instead?\n */\nSYSCALL_DEFINE2(creat, const char __user *, pathname, umode_t, mode)\n{\n\treturn sys_open(pathname, O_CREAT | O_WRONLY | O_TRUNC, mode);\n}\n\n#endif\n\n/*\n * \"id\" is the POSIX thread ID. We use the\n * files pointer for this..\n */\nint filp_close(struct file *filp, fl_owner_t id)\n{\n\tint retval = 0;\n\n\tif (!file_count(filp)) {\n\t\tprintk(KERN_ERR \"VFS: Close: file count is 0\\n\");\n\t\treturn 0;\n\t}\n\n\tif (filp->f_op->flush)\n\t\tretval = filp->f_op->flush(filp, id);\n\n\tif (likely(!(filp->f_mode & FMODE_PATH))) {\n\t\tdnotify_flush(filp, id);\n\t\tlocks_remove_posix(filp, id);\n\t}\n\tfput(filp);\n\treturn retval;\n}\n\nEXPORT_SYMBOL(filp_close);\n\n/*\n * Careful here! We test whether the file pointer is NULL before\n * releasing the fd. This ensures that one clone task can't release\n * an fd while another clone is opening it.\n */\nSYSCALL_DEFINE1(close, unsigned int, fd)\n{\n\tint retval = __close_fd(current->files, fd);\n\n\t/* can't restart close syscall because file table entry was cleared */\n\tif (unlikely(retval == -ERESTARTSYS ||\n\t\t     retval == -ERESTARTNOINTR ||\n\t\t     retval == -ERESTARTNOHAND ||\n\t\t     retval == -ERESTART_RESTARTBLOCK))\n\t\tretval = -EINTR;\n\n\treturn retval;\n}\nEXPORT_SYMBOL(sys_close);\n\n/*\n * This routine simulates a hangup on the tty, to arrange that users\n * are given clean terminals at login time.\n */\nSYSCALL_DEFINE0(vhangup)\n{\n\tif (capable(CAP_SYS_TTY_CONFIG)) {\n\t\ttty_vhangup_self();\n\t\treturn 0;\n\t}\n\treturn -EPERM;\n}\n\n/*\n * Called when an inode is about to be open.\n * We use this to disallow opening large files on 32bit systems if\n * the caller didn't specify O_LARGEFILE.  On 64bit systems we force\n * on this flag in sys_open.\n */\nint generic_file_open(struct inode * inode, struct file * filp)\n{\n\tif (!(filp->f_flags & O_LARGEFILE) && i_size_read(inode) > MAX_NON_LFS)\n\t\treturn -EOVERFLOW;\n\treturn 0;\n}\n\nEXPORT_SYMBOL(generic_file_open);\n\n/*\n * This is used by subsystems that don't want seekable\n * file descriptors. The function is not supposed to ever fail, the only\n * reason it returns an 'int' and not 'void' is so that it can be plugged\n * directly into file_operations structure.\n */\nint nonseekable_open(struct inode *inode, struct file *filp)\n{\n\tfilp->f_mode &= ~(FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE);\n\treturn 0;\n}\n\nEXPORT_SYMBOL(nonseekable_open);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/fs/open.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/fsnotify.h>\n#include <linux/module.h>\n#include <linux/tty.h>\n#include <linux/namei.h>\n#include <linux/backing-dev.h>\n#include <linux/capability.h>\n#include <linux/securebits.h>\n#include <linux/security.h>\n#include <linux/mount.h>\n#include <linux/fcntl.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/fs.h>\n#include <linux/personality.h>\n#include <linux/pagemap.h>\n#include <linux/syscalls.h>\n#include <linux/rcupdate.h>\n#include <linux/audit.h>\n#include <linux/falloc.h>\n#include <linux/fs_struct.h>\n#include <linux/dnotify.h>\n#include <linux/compat.h>\n#include <linux/mnt_idmapping.h>\n#include <linux/filelock.h>\n\n#include \"internal.h\"\n\nint do_truncate(struct mnt_idmap *idmap, struct dentry *dentry,\n\t\tloff_t length, unsigned int time_attrs, struct file *filp)\n{\n\tint ret;\n\tstruct iattr newattrs;\n\n\t/* Not pretty: \"inode->i_size\" shouldn't really be signed. But it is. */\n\tif (length < 0)\n\t\treturn -EINVAL;\n\n\tnewattrs.ia_size = length;\n\tnewattrs.ia_valid = ATTR_SIZE | time_attrs;\n\tif (filp) {\n\t\tnewattrs.ia_file = filp;\n\t\tnewattrs.ia_valid |= ATTR_FILE;\n\t}\n\n\t/* Remove suid, sgid, and file capabilities on truncate too */\n\tret = dentry_needs_remove_privs(idmap, dentry);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret)\n\t\tnewattrs.ia_valid |= ret | ATTR_FORCE;\n\n\tret = inode_lock_killable(dentry->d_inode);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Note any delegations or leases have already been broken: */\n\tret = notify_change(idmap, dentry, &newattrs, NULL);\n\tinode_unlock(dentry->d_inode);\n\treturn ret;\n}\n\nint vfs_truncate(const struct path *path, loff_t length)\n{\n\tstruct mnt_idmap *idmap;\n\tstruct inode *inode;\n\tint error;\n\n\tinode = path->dentry->d_inode;\n\n\t/* For directories it's -EISDIR, for other non-regulars - -EINVAL */\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn -EISDIR;\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\tidmap = mnt_idmap(path->mnt);\n\terror = inode_permission(idmap, inode, MAY_WRITE);\n\tif (error)\n\t\treturn error;\n\n\terror = fsnotify_truncate_perm(path, length);\n\tif (error)\n\t\treturn error;\n\n\terror = mnt_want_write(path->mnt);\n\tif (error)\n\t\treturn error;\n\n\terror = -EPERM;\n\tif (IS_APPEND(inode))\n\t\tgoto mnt_drop_write_and_out;\n\n\terror = get_write_access(inode);\n\tif (error)\n\t\tgoto mnt_drop_write_and_out;\n\n\t/*\n\t * Make sure that there are no leases.  get_write_access() protects\n\t * against the truncate racing with a lease-granting setlease().\n\t */\n\terror = break_lease(inode, O_WRONLY);\n\tif (error)\n\t\tgoto put_write_and_out;\n\n\terror = security_path_truncate(path);\n\tif (!error)\n\t\terror = do_truncate(idmap, path->dentry, length, 0, NULL);\n\nput_write_and_out:\n\tput_write_access(inode);\nmnt_drop_write_and_out:\n\tmnt_drop_write(path->mnt);\n\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(vfs_truncate);\n\nint do_sys_truncate(const char __user *pathname, loff_t length)\n{\n\tunsigned int lookup_flags = LOOKUP_FOLLOW;\n\tstruct path path;\n\tint error;\n\n\tif (length < 0)\t/* sorry, but loff_t says... */\n\t\treturn -EINVAL;\n\nretry:\n\terror = user_path_at(AT_FDCWD, pathname, lookup_flags, &path);\n\tif (!error) {\n\t\terror = vfs_truncate(&path, length);\n\t\tpath_put(&path);\n\t}\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(truncate, const char __user *, path, long, length)\n{\n\treturn do_sys_truncate(path, length);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(truncate, const char __user *, path, compat_off_t, length)\n{\n\treturn do_sys_truncate(path, length);\n}\n#endif\n\nint do_ftruncate(struct file *file, loff_t length, int small)\n{\n\tstruct inode *inode;\n\tstruct dentry *dentry;\n\tint error;\n\n\t/* explicitly opened as large or we are on 64-bit box */\n\tif (file->f_flags & O_LARGEFILE)\n\t\tsmall = 0;\n\n\tdentry = file->f_path.dentry;\n\tinode = dentry->d_inode;\n\tif (!S_ISREG(inode->i_mode) || !(file->f_mode & FMODE_WRITE))\n\t\treturn -EINVAL;\n\n\t/* Cannot ftruncate over 2^31 bytes without large file support */\n\tif (small && length > MAX_NON_LFS)\n\t\treturn -EINVAL;\n\n\t/* Check IS_APPEND on real upper inode */\n\tif (IS_APPEND(file_inode(file)))\n\t\treturn -EPERM;\n\n\terror = security_file_truncate(file);\n\tif (error)\n\t\treturn error;\n\n\terror = fsnotify_truncate_perm(&file->f_path, length);\n\tif (error)\n\t\treturn error;\n\n\tsb_start_write(inode->i_sb);\n\terror = do_truncate(file_mnt_idmap(file), dentry, length,\n\t\t\t    ATTR_MTIME | ATTR_CTIME, file);\n\tsb_end_write(inode->i_sb);\n\n\treturn error;\n}\n\nint do_sys_ftruncate(unsigned int fd, loff_t length, int small)\n{\n\tif (length < 0)\n\t\treturn -EINVAL;\n\tCLASS(fd, f)(fd);\n\tif (fd_empty(f))\n\t\treturn -EBADF;\n\n\treturn do_ftruncate(fd_file(f), length, small);\n}\n\nSYSCALL_DEFINE2(ftruncate, unsigned int, fd, off_t, length)\n{\n\treturn do_sys_ftruncate(fd, length, 1);\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(ftruncate, unsigned int, fd, compat_off_t, length)\n{\n\treturn do_sys_ftruncate(fd, length, 1);\n}\n#endif\n\n/* LFS versions of truncate are only needed on 32 bit machines */\n#if BITS_PER_LONG == 32\nSYSCALL_DEFINE2(truncate64, const char __user *, path, loff_t, length)\n{\n\treturn do_sys_truncate(path, length);\n}\n\nSYSCALL_DEFINE2(ftruncate64, unsigned int, fd, loff_t, length)\n{\n\treturn do_sys_ftruncate(fd, length, 0);\n}\n#endif /* BITS_PER_LONG == 32 */\n\n#if defined(CONFIG_COMPAT) && defined(__ARCH_WANT_COMPAT_TRUNCATE64)\nCOMPAT_SYSCALL_DEFINE3(truncate64, const char __user *, pathname,\n\t\t       compat_arg_u64_dual(length))\n{\n\treturn ksys_truncate(pathname, compat_arg_u64_glue(length));\n}\n#endif\n\n#if defined(CONFIG_COMPAT) && defined(__ARCH_WANT_COMPAT_FTRUNCATE64)\nCOMPAT_SYSCALL_DEFINE3(ftruncate64, unsigned int, fd,\n\t\t       compat_arg_u64_dual(length))\n{\n\treturn ksys_ftruncate(fd, compat_arg_u64_glue(length));\n}\n#endif\n\nint vfs_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file_inode(file);\n\tint ret;\n\tloff_t sum;\n\n\tif (offset < 0 || len <= 0)\n\t\treturn -EINVAL;\n\n\tif (mode & ~(FALLOC_FL_MODE_MASK | FALLOC_FL_KEEP_SIZE))\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * Modes are exclusive, even if that is not obvious from the encoding\n\t * as bit masks and the mix with the flag in the same namespace.\n\t *\n\t * To make things even more complicated, FALLOC_FL_ALLOCATE_RANGE is\n\t * encoded as no bit set.\n\t */\n\tswitch (mode & FALLOC_FL_MODE_MASK) {\n\tcase FALLOC_FL_ALLOCATE_RANGE:\n\tcase FALLOC_FL_UNSHARE_RANGE:\n\tcase FALLOC_FL_ZERO_RANGE:\n\t\tbreak;\n\tcase FALLOC_FL_PUNCH_HOLE:\n\t\tif (!(mode & FALLOC_FL_KEEP_SIZE))\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\tcase FALLOC_FL_COLLAPSE_RANGE:\n\tcase FALLOC_FL_INSERT_RANGE:\n\tcase FALLOC_FL_WRITE_ZEROES:\n\t\tif (mode & FALLOC_FL_KEEP_SIZE)\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!(file->f_mode & FMODE_WRITE))\n\t\treturn -EBADF;\n\n\t/*\n\t * On append-only files only space preallocation is supported.\n\t */\n\tif ((mode & ~FALLOC_FL_KEEP_SIZE) && IS_APPEND(inode))\n\t\treturn -EPERM;\n\n\tif (IS_IMMUTABLE(inode))\n\t\treturn -EPERM;\n\n\t/*\n\t * We cannot allow any fallocate operation on an active swapfile\n\t */\n\tif (IS_SWAPFILE(inode))\n\t\treturn -ETXTBSY;\n\n\t/*\n\t * Revalidate the write permissions, in case security policy has\n\t * changed since the files were opened.\n\t */\n\tret = security_file_permission(file, MAY_WRITE);\n\tif (ret)\n\t\treturn ret;\n\n\tret = fsnotify_file_area_perm(file, MAY_WRITE, &offset, len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (S_ISFIFO(inode->i_mode))\n\t\treturn -ESPIPE;\n\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn -EISDIR;\n\n\tif (!S_ISREG(inode->i_mode) && !S_ISBLK(inode->i_mode))\n\t\treturn -ENODEV;\n\n\t/* Check for wraparound */\n\tif (check_add_overflow(offset, len, &sum))\n\t\treturn -EFBIG;\n\n\tif (sum > inode->i_sb->s_maxbytes)\n\t\treturn -EFBIG;\n\n\tif (!file->f_op->fallocate)\n\t\treturn -EOPNOTSUPP;\n\n\tfile_start_write(file);\n\tret = file->f_op->fallocate(file, mode, offset, len);\n\n\t/*\n\t * Create inotify and fanotify events.\n\t *\n\t * To keep the logic simple always create events if fallocate succeeds.\n\t * This implies that events are even created if the file size remains\n\t * unchanged, e.g. when using flag FALLOC_FL_KEEP_SIZE.\n\t */\n\tif (ret == 0)\n\t\tfsnotify_modify(file);\n\n\tfile_end_write(file);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(vfs_fallocate);\n\nint ksys_fallocate(int fd, int mode, loff_t offset, loff_t len)\n{\n\tCLASS(fd, f)(fd);\n\n\tif (fd_empty(f))\n\t\treturn -EBADF;\n\n\treturn vfs_fallocate(fd_file(f), mode, offset, len);\n}\n\nSYSCALL_DEFINE4(fallocate, int, fd, int, mode, loff_t, offset, loff_t, len)\n{\n\treturn ksys_fallocate(fd, mode, offset, len);\n}\n\n#if defined(CONFIG_COMPAT) && defined(__ARCH_WANT_COMPAT_FALLOCATE)\nCOMPAT_SYSCALL_DEFINE6(fallocate, int, fd, int, mode, compat_arg_u64_dual(offset),\n\t\t       compat_arg_u64_dual(len))\n{\n\treturn ksys_fallocate(fd, mode, compat_arg_u64_glue(offset),\n\t\t\t      compat_arg_u64_glue(len));\n}\n#endif\n\n/*\n * access() needs to use the real uid/gid, not the effective uid/gid.\n * We do this by temporarily clearing all FS-related capabilities and\n * switching the fsuid/fsgid around to the real ones.\n *\n * Creating new credentials is expensive, so we try to skip doing it,\n * which we can if the result would match what we already got.\n */\nstatic bool access_need_override_creds(int flags)\n{\n\tconst struct cred *cred;\n\n\tif (flags & AT_EACCESS)\n\t\treturn false;\n\n\tcred = current_cred();\n\tif (!uid_eq(cred->fsuid, cred->uid) ||\n\t    !gid_eq(cred->fsgid, cred->gid))\n\t\treturn true;\n\n\tif (!issecure(SECURE_NO_SETUID_FIXUP)) {\n\t\tkuid_t root_uid = make_kuid(cred->user_ns, 0);\n\t\tif (!uid_eq(cred->uid, root_uid)) {\n\t\t\tif (!cap_isclear(cred->cap_effective))\n\t\t\t\treturn true;\n\t\t} else {\n\t\t\tif (!cap_isidentical(cred->cap_effective,\n\t\t\t    cred->cap_permitted))\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic const struct cred *access_override_creds(void)\n{\n\tstruct cred *override_cred;\n\n\toverride_cred = prepare_creds();\n\tif (!override_cred)\n\t\treturn NULL;\n\n\t/*\n\t * XXX access_need_override_creds performs checks in hopes of skipping\n\t * this work. Make sure it stays in sync if making any changes in this\n\t * routine.\n\t */\n\n\toverride_cred->fsuid = override_cred->uid;\n\toverride_cred->fsgid = override_cred->gid;\n\n\tif (!issecure(SECURE_NO_SETUID_FIXUP)) {\n\t\t/* Clear the capabilities if we switch to a non-root user */\n\t\tkuid_t root_uid = make_kuid(override_cred->user_ns, 0);\n\t\tif (!uid_eq(override_cred->uid, root_uid))\n\t\t\tcap_clear(override_cred->cap_effective);\n\t\telse\n\t\t\toverride_cred->cap_effective =\n\t\t\t\toverride_cred->cap_permitted;\n\t}\n\n\t/*\n\t * The new set of credentials can *only* be used in\n\t * task-synchronous circumstances, and does not need\n\t * RCU freeing, unless somebody then takes a separate\n\t * reference to it.\n\t *\n\t * NOTE! This is _only_ true because this credential\n\t * is used purely for override_creds() that installs\n\t * it as the subjective cred. Other threads will be\n\t * accessing ->real_cred, not the subjective cred.\n\t *\n\t * If somebody _does_ make a copy of this (using the\n\t * 'get_current_cred()' function), that will clear the\n\t * non_rcu field, because now that other user may be\n\t * expecting RCU freeing. But normal thread-synchronous\n\t * cred accesses will keep things non-racy to avoid RCU\n\t * freeing.\n\t */\n\toverride_cred->non_rcu = 1;\n\treturn override_creds(override_cred);\n}\n\nstatic int do_faccessat(int dfd, const char __user *filename, int mode, int flags)\n{\n\tstruct path path;\n\tstruct inode *inode;\n\tint res;\n\tunsigned int lookup_flags = LOOKUP_FOLLOW;\n\tconst struct cred *old_cred = NULL;\n\n\tif (mode & ~S_IRWXO)\t/* where's F_OK, X_OK, W_OK, R_OK? */\n\t\treturn -EINVAL;\n\n\tif (flags & ~(AT_EACCESS | AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH))\n\t\treturn -EINVAL;\n\n\tif (flags & AT_SYMLINK_NOFOLLOW)\n\t\tlookup_flags &= ~LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\n\n\tif (access_need_override_creds(flags)) {\n\t\told_cred = access_override_creds();\n\t\tif (!old_cred)\n\t\t\treturn -ENOMEM;\n\t}\n\nretry:\n\tres = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (res)\n\t\tgoto out;\n\n\tinode = d_backing_inode(path.dentry);\n\n\tif ((mode & MAY_EXEC) && S_ISREG(inode->i_mode)) {\n\t\t/*\n\t\t * MAY_EXEC on regular files is denied if the fs is mounted\n\t\t * with the \"noexec\" flag.\n\t\t */\n\t\tres = -EACCES;\n\t\tif (path_noexec(&path))\n\t\t\tgoto out_path_release;\n\t}\n\n\tres = inode_permission(mnt_idmap(path.mnt), inode, mode | MAY_ACCESS);\n\t/* SuS v2 requires we report a read only fs too */\n\tif (res || !(mode & S_IWOTH) || special_file(inode->i_mode))\n\t\tgoto out_path_release;\n\t/*\n\t * This is a rare case where using __mnt_is_readonly()\n\t * is OK without a mnt_want/drop_write() pair.  Since\n\t * no actual write to the fs is performed here, we do\n\t * not need to telegraph to that to anyone.\n\t *\n\t * By doing this, we accept that this access is\n\t * inherently racy and know that the fs may change\n\t * state before we even see this result.\n\t */\n\tif (__mnt_is_readonly(path.mnt))\n\t\tres = -EROFS;\n\nout_path_release:\n\tpath_put(&path);\n\tif (retry_estale(res, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\tif (old_cred)\n\t\tput_cred(revert_creds(old_cred));\n\n\treturn res;\n}\n\nSYSCALL_DEFINE3(faccessat, int, dfd, const char __user *, filename, int, mode)\n{\n\treturn do_faccessat(dfd, filename, mode, 0);\n}\n\nSYSCALL_DEFINE4(faccessat2, int, dfd, const char __user *, filename, int, mode,\n\t\tint, flags)\n{\n\treturn do_faccessat(dfd, filename, mode, flags);\n}\n\nSYSCALL_DEFINE2(access, const char __user *, filename, int, mode)\n{\n\treturn do_faccessat(AT_FDCWD, filename, mode, 0);\n}\n\nSYSCALL_DEFINE1(chdir, const char __user *, filename)\n{\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\nretry:\n\terror = user_path_at(AT_FDCWD, filename, lookup_flags, &path);\n\tif (error)\n\t\tgoto out;\n\n\terror = path_permission(&path, MAY_EXEC | MAY_CHDIR);\n\tif (error)\n\t\tgoto dput_and_out;\n\n\tset_fs_pwd(current->fs, &path);\n\ndput_and_out:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\treturn error;\n}\n\nSYSCALL_DEFINE1(fchdir, unsigned int, fd)\n{\n\tCLASS(fd_raw, f)(fd);\n\tint error;\n\n\tif (fd_empty(f))\n\t\treturn -EBADF;\n\n\tif (!d_can_lookup(fd_file(f)->f_path.dentry))\n\t\treturn -ENOTDIR;\n\n\terror = file_permission(fd_file(f), MAY_EXEC | MAY_CHDIR);\n\tif (!error)\n\t\tset_fs_pwd(current->fs, &fd_file(f)->f_path);\n\treturn error;\n}\n\nSYSCALL_DEFINE1(chroot, const char __user *, filename)\n{\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\nretry:\n\terror = user_path_at(AT_FDCWD, filename, lookup_flags, &path);\n\tif (error)\n\t\tgoto out;\n\n\terror = path_permission(&path, MAY_EXEC | MAY_CHDIR);\n\tif (error)\n\t\tgoto dput_and_out;\n\n\terror = -EPERM;\n\tif (!ns_capable(current_user_ns(), CAP_SYS_CHROOT))\n\t\tgoto dput_and_out;\n\terror = security_path_chroot(&path);\n\tif (error)\n\t\tgoto dput_and_out;\n\n\tset_fs_root(current->fs, &path);\n\terror = 0;\ndput_and_out:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\treturn error;\n}\n\nint chmod_common(const struct path *path, umode_t mode)\n{\n\tstruct inode *inode = path->dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tstruct iattr newattrs;\n\tint error;\n\n\terror = mnt_want_write(path->mnt);\n\tif (error)\n\t\treturn error;\nretry_deleg:\n\terror = inode_lock_killable(inode);\n\tif (error)\n\t\tgoto out_mnt_unlock;\n\terror = security_path_chmod(path, mode);\n\tif (error)\n\t\tgoto out_unlock;\n\tnewattrs.ia_mode = (mode & S_IALLUGO) | (inode->i_mode & ~S_IALLUGO);\n\tnewattrs.ia_valid = ATTR_MODE | ATTR_CTIME;\n\terror = notify_change(mnt_idmap(path->mnt), path->dentry,\n\t\t\t      &newattrs, &delegated_inode);\nout_unlock:\n\tinode_unlock(inode);\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\nout_mnt_unlock:\n\tmnt_drop_write(path->mnt);\n\treturn error;\n}\n\nint vfs_fchmod(struct file *file, umode_t mode)\n{\n\taudit_file(file);\n\treturn chmod_common(&file->f_path, mode);\n}\n\nSYSCALL_DEFINE2(fchmod, unsigned int, fd, umode_t, mode)\n{\n\tCLASS(fd, f)(fd);\n\n\tif (fd_empty(f))\n\t\treturn -EBADF;\n\n\treturn vfs_fchmod(fd_file(f), mode);\n}\n\nstatic int do_fchmodat(int dfd, const char __user *filename, umode_t mode,\n\t\t       unsigned int flags)\n{\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags;\n\n\tif (unlikely(flags & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)))\n\t\treturn -EINVAL;\n\n\tlookup_flags = (flags & AT_SYMLINK_NOFOLLOW) ? 0 : LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\n\nretry:\n\terror = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (!error) {\n\t\terror = chmod_common(&path, mode);\n\t\tpath_put(&path);\n\t\tif (retry_estale(error, lookup_flags)) {\n\t\t\tlookup_flags |= LOOKUP_REVAL;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE4(fchmodat2, int, dfd, const char __user *, filename,\n\t\tumode_t, mode, unsigned int, flags)\n{\n\treturn do_fchmodat(dfd, filename, mode, flags);\n}\n\nSYSCALL_DEFINE3(fchmodat, int, dfd, const char __user *, filename,\n\t\tumode_t, mode)\n{\n\treturn do_fchmodat(dfd, filename, mode, 0);\n}\n\nSYSCALL_DEFINE2(chmod, const char __user *, filename, umode_t, mode)\n{\n\treturn do_fchmodat(AT_FDCWD, filename, mode, 0);\n}\n\n/*\n * Check whether @kuid is valid and if so generate and set vfsuid_t in\n * ia_vfsuid.\n *\n * Return: true if @kuid is valid, false if not.\n */\nstatic inline bool setattr_vfsuid(struct iattr *attr, kuid_t kuid)\n{\n\tif (!uid_valid(kuid))\n\t\treturn false;\n\tattr->ia_valid |= ATTR_UID;\n\tattr->ia_vfsuid = VFSUIDT_INIT(kuid);\n\treturn true;\n}\n\n/*\n * Check whether @kgid is valid and if so generate and set vfsgid_t in\n * ia_vfsgid.\n *\n * Return: true if @kgid is valid, false if not.\n */\nstatic inline bool setattr_vfsgid(struct iattr *attr, kgid_t kgid)\n{\n\tif (!gid_valid(kgid))\n\t\treturn false;\n\tattr->ia_valid |= ATTR_GID;\n\tattr->ia_vfsgid = VFSGIDT_INIT(kgid);\n\treturn true;\n}\n\nint chown_common(const struct path *path, uid_t user, gid_t group)\n{\n\tstruct mnt_idmap *idmap;\n\tstruct user_namespace *fs_userns;\n\tstruct inode *inode = path->dentry->d_inode;\n\tstruct inode *delegated_inode = NULL;\n\tint error;\n\tstruct iattr newattrs;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tuid = make_kuid(current_user_ns(), user);\n\tgid = make_kgid(current_user_ns(), group);\n\n\tidmap = mnt_idmap(path->mnt);\n\tfs_userns = i_user_ns(inode);\n\nretry_deleg:\n\tnewattrs.ia_vfsuid = INVALID_VFSUID;\n\tnewattrs.ia_vfsgid = INVALID_VFSGID;\n\tnewattrs.ia_valid =  ATTR_CTIME;\n\tif ((user != (uid_t)-1) && !setattr_vfsuid(&newattrs, uid))\n\t\treturn -EINVAL;\n\tif ((group != (gid_t)-1) && !setattr_vfsgid(&newattrs, gid))\n\t\treturn -EINVAL;\n\terror = inode_lock_killable(inode);\n\tif (error)\n\t\treturn error;\n\tif (!S_ISDIR(inode->i_mode))\n\t\tnewattrs.ia_valid |= ATTR_KILL_SUID | ATTR_KILL_PRIV |\n\t\t\t\t     setattr_should_drop_sgid(idmap, inode);\n\t/* Continue to send actual fs values, not the mount values. */\n\terror = security_path_chown(\n\t\tpath,\n\t\tfrom_vfsuid(idmap, fs_userns, newattrs.ia_vfsuid),\n\t\tfrom_vfsgid(idmap, fs_userns, newattrs.ia_vfsgid));\n\tif (!error)\n\t\terror = notify_change(idmap, path->dentry, &newattrs,\n\t\t\t\t      &delegated_inode);\n\tinode_unlock(inode);\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\treturn error;\n}\n\nint do_fchownat(int dfd, const char __user *filename, uid_t user, gid_t group,\n\t\tint flag)\n{\n\tstruct path path;\n\tint error = -EINVAL;\n\tint lookup_flags;\n\n\tif ((flag & ~(AT_SYMLINK_NOFOLLOW | AT_EMPTY_PATH)) != 0)\n\t\tgoto out;\n\n\tlookup_flags = (flag & AT_SYMLINK_NOFOLLOW) ? 0 : LOOKUP_FOLLOW;\n\tif (flag & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\nretry:\n\terror = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (error)\n\t\tgoto out;\n\terror = mnt_want_write(path.mnt);\n\tif (error)\n\t\tgoto out_release;\n\terror = chown_common(&path, user, group);\n\tmnt_drop_write(path.mnt);\nout_release:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\treturn error;\n}\n\nSYSCALL_DEFINE5(fchownat, int, dfd, const char __user *, filename, uid_t, user,\n\t\tgid_t, group, int, flag)\n{\n\treturn do_fchownat(dfd, filename, user, group, flag);\n}\n\nSYSCALL_DEFINE3(chown, const char __user *, filename, uid_t, user, gid_t, group)\n{\n\treturn do_fchownat(AT_FDCWD, filename, user, group, 0);\n}\n\nSYSCALL_DEFINE3(lchown, const char __user *, filename, uid_t, user, gid_t, group)\n{\n\treturn do_fchownat(AT_FDCWD, filename, user, group,\n\t\t\t   AT_SYMLINK_NOFOLLOW);\n}\n\nint vfs_fchown(struct file *file, uid_t user, gid_t group)\n{\n\tint error;\n\n\terror = mnt_want_write_file(file);\n\tif (error)\n\t\treturn error;\n\taudit_file(file);\n\terror = chown_common(&file->f_path, user, group);\n\tmnt_drop_write_file(file);\n\treturn error;\n}\n\nint ksys_fchown(unsigned int fd, uid_t user, gid_t group)\n{\n\tCLASS(fd, f)(fd);\n\n\tif (fd_empty(f))\n\t\treturn -EBADF;\n\n\treturn vfs_fchown(fd_file(f), user, group);\n}\n\nSYSCALL_DEFINE3(fchown, unsigned int, fd, uid_t, user, gid_t, group)\n{\n\treturn ksys_fchown(fd, user, group);\n}\n\nstatic inline int file_get_write_access(struct file *f)\n{\n\tint error;\n\n\terror = get_write_access(f->f_inode);\n\tif (unlikely(error))\n\t\treturn error;\n\terror = mnt_get_write_access(f->f_path.mnt);\n\tif (unlikely(error))\n\t\tgoto cleanup_inode;\n\tif (unlikely(f->f_mode & FMODE_BACKING)) {\n\t\terror = mnt_get_write_access(backing_file_user_path(f)->mnt);\n\t\tif (unlikely(error))\n\t\t\tgoto cleanup_mnt;\n\t}\n\treturn 0;\n\ncleanup_mnt:\n\tmnt_put_write_access(f->f_path.mnt);\ncleanup_inode:\n\tput_write_access(f->f_inode);\n\treturn error;\n}\n\nstatic int do_dentry_open(struct file *f,\n\t\t\t  int (*open)(struct inode *, struct file *))\n{\n\tstatic const struct file_operations empty_fops = {};\n\tstruct inode *inode = f->f_path.dentry->d_inode;\n\tint error;\n\n\tpath_get(&f->f_path);\n\tf->f_inode = inode;\n\tf->f_mapping = inode->i_mapping;\n\tf->f_wb_err = filemap_sample_wb_err(f->f_mapping);\n\tf->f_sb_err = file_sample_sb_err(f);\n\n\tif (unlikely(f->f_flags & O_PATH)) {\n\t\tf->f_mode = FMODE_PATH | FMODE_OPENED;\n\t\tfile_set_fsnotify_mode(f, FMODE_NONOTIFY);\n\t\tf->f_op = &empty_fops;\n\t\treturn 0;\n\t}\n\n\tif ((f->f_mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ) {\n\t\ti_readcount_inc(inode);\n\t} else if (f->f_mode & FMODE_WRITE && !special_file(inode->i_mode)) {\n\t\terror = file_get_write_access(f);\n\t\tif (unlikely(error))\n\t\t\tgoto cleanup_file;\n\t\tf->f_mode |= FMODE_WRITER;\n\t}\n\n\t/* POSIX.1-2008/SUSv4 Section XSI 2.9.7 */\n\tif (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode))\n\t\tf->f_mode |= FMODE_ATOMIC_POS;\n\n\tf->f_op = fops_get(inode->i_fop);\n\tif (WARN_ON(!f->f_op)) {\n\t\terror = -ENODEV;\n\t\tgoto cleanup_all;\n\t}\n\n\terror = security_file_open(f);\n\tif (error)\n\t\tgoto cleanup_all;\n\n\t/*\n\t * Call fsnotify open permission hook and set FMODE_NONOTIFY_* bits\n\t * according to existing permission watches.\n\t * If FMODE_NONOTIFY mode was already set for an fanotify fd or for a\n\t * pseudo file, this call will not change the mode.\n\t */\n\terror = fsnotify_open_perm_and_set_mode(f);\n\tif (error)\n\t\tgoto cleanup_all;\n\n\terror = break_lease(file_inode(f), f->f_flags);\n\tif (error)\n\t\tgoto cleanup_all;\n\n\t/* normally all 3 are set; ->open() can clear them if needed */\n\tf->f_mode |= FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE;\n\tif (!open)\n\t\topen = f->f_op->open;\n\tif (open) {\n\t\terror = open(inode, f);\n\t\tif (error)\n\t\t\tgoto cleanup_all;\n\t}\n\tf->f_mode |= FMODE_OPENED;\n\tif ((f->f_mode & FMODE_READ) &&\n\t     likely(f->f_op->read || f->f_op->read_iter))\n\t\tf->f_mode |= FMODE_CAN_READ;\n\tif ((f->f_mode & FMODE_WRITE) &&\n\t     likely(f->f_op->write || f->f_op->write_iter))\n\t\tf->f_mode |= FMODE_CAN_WRITE;\n\tif ((f->f_mode & FMODE_LSEEK) && !f->f_op->llseek)\n\t\tf->f_mode &= ~FMODE_LSEEK;\n\tif (f->f_mapping->a_ops && f->f_mapping->a_ops->direct_IO)\n\t\tf->f_mode |= FMODE_CAN_ODIRECT;\n\n\tf->f_flags &= ~(O_CREAT | O_EXCL | O_NOCTTY | O_TRUNC);\n\tf->f_iocb_flags = iocb_flags(f);\n\n\tfile_ra_state_init(&f->f_ra, f->f_mapping->host->i_mapping);\n\n\tif ((f->f_flags & O_DIRECT) && !(f->f_mode & FMODE_CAN_ODIRECT))\n\t\treturn -EINVAL;\n\n\t/*\n\t * XXX: Huge page cache doesn't support writing yet. Drop all page\n\t * cache for this file before processing writes.\n\t */\n\tif (f->f_mode & FMODE_WRITE) {\n\t\t/*\n\t\t * Depends on full fence from get_write_access() to synchronize\n\t\t * against collapse_file() regarding i_writecount and nr_thps\n\t\t * updates. Ensures subsequent insertion of THPs into the page\n\t\t * cache will fail.\n\t\t */\n\t\tif (filemap_nr_thps(inode->i_mapping)) {\n\t\t\tstruct address_space *mapping = inode->i_mapping;\n\n\t\t\tfilemap_invalidate_lock(inode->i_mapping);\n\t\t\t/*\n\t\t\t * unmap_mapping_range just need to be called once\n\t\t\t * here, because the private pages is not need to be\n\t\t\t * unmapped mapping (e.g. data segment of dynamic\n\t\t\t * shared libraries here).\n\t\t\t */\n\t\t\tunmap_mapping_range(mapping, 0, 0, 0);\n\t\t\ttruncate_inode_pages(mapping, 0);\n\t\t\tfilemap_invalidate_unlock(inode->i_mapping);\n\t\t}\n\t}\n\n\treturn 0;\n\ncleanup_all:\n\tif (WARN_ON_ONCE(error > 0))\n\t\terror = -EINVAL;\n\tfops_put(f->f_op);\n\tput_file_access(f);\ncleanup_file:\n\tpath_put(&f->f_path);\n\tf->__f_path.mnt = NULL;\n\tf->__f_path.dentry = NULL;\n\tf->f_inode = NULL;\n\treturn error;\n}\n\n/**\n * finish_open - finish opening a file\n * @file: file pointer\n * @dentry: pointer to dentry\n * @open: open callback\n *\n * This can be used to finish opening a file passed to i_op->atomic_open().\n *\n * If the open callback is set to NULL, then the standard f_op->open()\n * filesystem callback is substituted.\n *\n * NB: the dentry reference is _not_ consumed.  If, for example, the dentry is\n * the return value of d_splice_alias(), then the caller needs to perform dput()\n * on it after finish_open().\n *\n * Returns zero on success or -errno if the open failed.\n */\nint finish_open(struct file *file, struct dentry *dentry,\n\t\tint (*open)(struct inode *, struct file *))\n{\n\tBUG_ON(file->f_mode & FMODE_OPENED); /* once it's opened, it's opened */\n\n\tfile->__f_path.dentry = dentry;\n\treturn do_dentry_open(file, open);\n}\nEXPORT_SYMBOL(finish_open);\n\n/**\n * finish_no_open - finish ->atomic_open() without opening the file\n *\n * @file: file pointer\n * @dentry: dentry or NULL (as returned from ->lookup())\n *\n * This can be used to set the result of a successful lookup in ->atomic_open().\n *\n * NB: unlike finish_open() this function does consume the dentry reference and\n * the caller need not dput() it.\n *\n * Returns \"0\" which must be the return value of ->atomic_open() after having\n * called this function.\n */\nint finish_no_open(struct file *file, struct dentry *dentry)\n{\n\tfile->__f_path.dentry = dentry;\n\treturn 0;\n}\nEXPORT_SYMBOL(finish_no_open);\n\nchar *file_path(struct file *filp, char *buf, int buflen)\n{\n\treturn d_path(&filp->f_path, buf, buflen);\n}\nEXPORT_SYMBOL(file_path);\n\n/**\n * vfs_open - open the file at the given path\n * @path: path to open\n * @file: newly allocated file with f_flag initialized\n */\nint vfs_open(const struct path *path, struct file *file)\n{\n\tint ret;\n\n\tfile->__f_path = *path;\n\tret = do_dentry_open(file, NULL);\n\tif (!ret) {\n\t\t/*\n\t\t * Once we return a file with FMODE_OPENED, __fput() will call\n\t\t * fsnotify_close(), so we need fsnotify_open() here for\n\t\t * symmetry.\n\t\t */\n\t\tfsnotify_open(file);\n\t}\n\treturn ret;\n}\n\nstruct file *dentry_open(const struct path *path, int flags,\n\t\t\t const struct cred *cred)\n{\n\tint error;\n\tstruct file *f;\n\n\t/* We must always pass in a valid mount pointer. */\n\tBUG_ON(!path->mnt);\n\n\tf = alloc_empty_file(flags, cred);\n\tif (!IS_ERR(f)) {\n\t\terror = vfs_open(path, f);\n\t\tif (error) {\n\t\t\tfput(f);\n\t\t\tf = ERR_PTR(error);\n\t\t}\n\t}\n\treturn f;\n}\nEXPORT_SYMBOL(dentry_open);\n\nstruct file *dentry_open_nonotify(const struct path *path, int flags,\n\t\t\t\t  const struct cred *cred)\n{\n\tstruct file *f = alloc_empty_file(flags, cred);\n\tif (!IS_ERR(f)) {\n\t\tint error;\n\n\t\tfile_set_fsnotify_mode(f, FMODE_NONOTIFY);\n\t\terror = vfs_open(path, f);\n\t\tif (error) {\n\t\t\tfput(f);\n\t\t\tf = ERR_PTR(error);\n\t\t}\n\t}\n\treturn f;\n}\n\n/**\n * dentry_create - Create and open a file\n * @path: path to create\n * @flags: O_ flags\n * @mode: mode bits for new file\n * @cred: credentials to use\n *\n * Caller must hold the parent directory's lock, and have prepared\n * a negative dentry, placed in @path->dentry, for the new file.\n *\n * Caller sets @path->mnt to the vfsmount of the filesystem where\n * the new file is to be created. The parent directory and the\n * negative dentry must reside on the same filesystem instance.\n *\n * On success, returns a \"struct file *\". Otherwise a ERR_PTR\n * is returned.\n */\nstruct file *dentry_create(const struct path *path, int flags, umode_t mode,\n\t\t\t   const struct cred *cred)\n{\n\tstruct file *f;\n\tint error;\n\n\tf = alloc_empty_file(flags, cred);\n\tif (IS_ERR(f))\n\t\treturn f;\n\n\terror = vfs_create(mnt_idmap(path->mnt),\n\t\t\t   d_inode(path->dentry->d_parent),\n\t\t\t   path->dentry, mode, true);\n\tif (!error)\n\t\terror = vfs_open(path, f);\n\n\tif (unlikely(error)) {\n\t\tfput(f);\n\t\treturn ERR_PTR(error);\n\t}\n\treturn f;\n}\nEXPORT_SYMBOL(dentry_create);\n\n/**\n * kernel_file_open - open a file for kernel internal use\n * @path:\tpath of the file to open\n * @flags:\topen flags\n * @cred:\tcredentials for open\n *\n * Open a file for use by in-kernel consumers. The file is not accounted\n * against nr_files and must not be installed into the file descriptor\n * table.\n *\n * Return: Opened file on success, an error pointer on failure.\n */\nstruct file *kernel_file_open(const struct path *path, int flags,\n\t\t\t\tconst struct cred *cred)\n{\n\tstruct file *f;\n\tint error;\n\n\tf = alloc_empty_file_noaccount(flags, cred);\n\tif (IS_ERR(f))\n\t\treturn f;\n\n\terror = vfs_open(path, f);\n\tif (error) {\n\t\tfput(f);\n\t\treturn ERR_PTR(error);\n\t}\n\treturn f;\n}\nEXPORT_SYMBOL_GPL(kernel_file_open);\n\n#define WILL_CREATE(flags)\t(flags & (O_CREAT | __O_TMPFILE))\n#define O_PATH_FLAGS\t\t(O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC)\n\ninline struct open_how build_open_how(int flags, umode_t mode)\n{\n\tstruct open_how how = {\n\t\t.flags = flags & VALID_OPEN_FLAGS,\n\t\t.mode = mode & S_IALLUGO,\n\t};\n\n\t/* O_PATH beats everything else. */\n\tif (how.flags & O_PATH)\n\t\thow.flags &= O_PATH_FLAGS;\n\t/* Modes should only be set for create-like flags. */\n\tif (!WILL_CREATE(how.flags))\n\t\thow.mode = 0;\n\treturn how;\n}\n\ninline int build_open_flags(const struct open_how *how, struct open_flags *op)\n{\n\tu64 flags = how->flags;\n\tu64 strip = O_CLOEXEC;\n\tint lookup_flags = 0;\n\tint acc_mode = ACC_MODE(flags);\n\n\tBUILD_BUG_ON_MSG(upper_32_bits(VALID_OPEN_FLAGS),\n\t\t\t \"struct open_flags doesn't yet handle flags > 32 bits\");\n\n\t/*\n\t * Strip flags that aren't relevant in determining struct open_flags.\n\t */\n\tflags &= ~strip;\n\n\t/*\n\t * Older syscalls implicitly clear all of the invalid flags or argument\n\t * values before calling build_open_flags(), but openat2(2) checks all\n\t * of its arguments.\n\t */\n\tif (flags & ~VALID_OPEN_FLAGS)\n\t\treturn -EINVAL;\n\tif (how->resolve & ~VALID_RESOLVE_FLAGS)\n\t\treturn -EINVAL;\n\n\t/* Scoping flags are mutually exclusive. */\n\tif ((how->resolve & RESOLVE_BENEATH) && (how->resolve & RESOLVE_IN_ROOT))\n\t\treturn -EINVAL;\n\n\t/* Deal with the mode. */\n\tif (WILL_CREATE(flags)) {\n\t\tif (how->mode & ~S_IALLUGO)\n\t\t\treturn -EINVAL;\n\t\top->mode = how->mode | S_IFREG;\n\t} else {\n\t\tif (how->mode != 0)\n\t\t\treturn -EINVAL;\n\t\top->mode = 0;\n\t}\n\n\t/*\n\t * Block bugs where O_DIRECTORY | O_CREAT created regular files.\n\t * Note, that blocking O_DIRECTORY | O_CREAT here also protects\n\t * O_TMPFILE below which requires O_DIRECTORY being raised.\n\t */\n\tif ((flags & (O_DIRECTORY | O_CREAT)) == (O_DIRECTORY | O_CREAT))\n\t\treturn -EINVAL;\n\n\t/* Now handle the creative implementation of O_TMPFILE. */\n\tif (flags & __O_TMPFILE) {\n\t\t/*\n\t\t * In order to ensure programs get explicit errors when trying\n\t\t * to use O_TMPFILE on old kernels we enforce that O_DIRECTORY\n\t\t * is raised alongside __O_TMPFILE.\n\t\t */\n\t\tif (!(flags & O_DIRECTORY))\n\t\t\treturn -EINVAL;\n\t\tif (!(acc_mode & MAY_WRITE))\n\t\t\treturn -EINVAL;\n\t}\n\tif (flags & O_PATH) {\n\t\t/* O_PATH only permits certain other flags to be set. */\n\t\tif (flags & ~O_PATH_FLAGS)\n\t\t\treturn -EINVAL;\n\t\tacc_mode = 0;\n\t}\n\n\t/*\n\t * O_SYNC is implemented as __O_SYNC|O_DSYNC.  As many places only\n\t * check for O_DSYNC if the need any syncing at all we enforce it's\n\t * always set instead of having to deal with possibly weird behaviour\n\t * for malicious applications setting only __O_SYNC.\n\t */\n\tif (flags & __O_SYNC)\n\t\tflags |= O_DSYNC;\n\n\top->open_flag = flags;\n\n\t/* O_TRUNC implies we need access checks for write permissions */\n\tif (flags & O_TRUNC)\n\t\tacc_mode |= MAY_WRITE;\n\n\t/* Allow the LSM permission hook to distinguish append\n\t   access from general write access. */\n\tif (flags & O_APPEND)\n\t\tacc_mode |= MAY_APPEND;\n\n\top->acc_mode = acc_mode;\n\n\top->intent = flags & O_PATH ? 0 : LOOKUP_OPEN;\n\n\tif (flags & O_CREAT) {\n\t\top->intent |= LOOKUP_CREATE;\n\t\tif (flags & O_EXCL) {\n\t\t\top->intent |= LOOKUP_EXCL;\n\t\t\tflags |= O_NOFOLLOW;\n\t\t}\n\t}\n\n\tif (flags & O_DIRECTORY)\n\t\tlookup_flags |= LOOKUP_DIRECTORY;\n\tif (!(flags & O_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\n\tif (how->resolve & RESOLVE_NO_XDEV)\n\t\tlookup_flags |= LOOKUP_NO_XDEV;\n\tif (how->resolve & RESOLVE_NO_MAGICLINKS)\n\t\tlookup_flags |= LOOKUP_NO_MAGICLINKS;\n\tif (how->resolve & RESOLVE_NO_SYMLINKS)\n\t\tlookup_flags |= LOOKUP_NO_SYMLINKS;\n\tif (how->resolve & RESOLVE_BENEATH)\n\t\tlookup_flags |= LOOKUP_BENEATH;\n\tif (how->resolve & RESOLVE_IN_ROOT)\n\t\tlookup_flags |= LOOKUP_IN_ROOT;\n\tif (how->resolve & RESOLVE_CACHED) {\n\t\t/* Don't bother even trying for create/truncate/tmpfile open */\n\t\tif (flags & (O_TRUNC | O_CREAT | __O_TMPFILE))\n\t\t\treturn -EAGAIN;\n\t\tlookup_flags |= LOOKUP_CACHED;\n\t}\n\n\top->lookup_flags = lookup_flags;\n\treturn 0;\n}\n\n/**\n * file_open_name - open file and return file pointer\n *\n * @name:\tstruct filename containing path to open\n * @flags:\topen flags as per the open(2) second argument\n * @mode:\tmode for the new file if O_CREAT is set, else ignored\n *\n * This is the helper to open a file from kernelspace if you really\n * have to.  But in generally you should not do this, so please move\n * along, nothing to see here..\n */\nstruct file *file_open_name(struct filename *name, int flags, umode_t mode)\n{\n\tstruct open_flags op;\n\tstruct open_how how = build_open_how(flags, mode);\n\tint err = build_open_flags(&how, &op);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\treturn do_filp_open(AT_FDCWD, name, &op);\n}\n\n/**\n * filp_open - open file and return file pointer\n *\n * @filename:\tpath to open\n * @flags:\topen flags as per the open(2) second argument\n * @mode:\tmode for the new file if O_CREAT is set, else ignored\n *\n * This is the helper to open a file from kernelspace if you really\n * have to.  But in generally you should not do this, so please move\n * along, nothing to see here..\n */\nstruct file *filp_open(const char *filename, int flags, umode_t mode)\n{\n\tstruct filename *name = getname_kernel(filename);\n\tstruct file *file = ERR_CAST(name);\n\n\tif (!IS_ERR(name)) {\n\t\tfile = file_open_name(name, flags, mode);\n\t\tputname(name);\n\t}\n\treturn file;\n}\nEXPORT_SYMBOL(filp_open);\n\nstruct file *file_open_root(const struct path *root,\n\t\t\t    const char *filename, int flags, umode_t mode)\n{\n\tstruct open_flags op;\n\tstruct open_how how = build_open_how(flags, mode);\n\tint err = build_open_flags(&how, &op);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\treturn do_file_open_root(root, filename, &op);\n}\nEXPORT_SYMBOL(file_open_root);\n\nstatic int do_sys_openat2(int dfd, const char __user *filename,\n\t\t\t  struct open_how *how)\n{\n\tstruct open_flags op;\n\tstruct filename *tmp;\n\tint err, fd;\n\n\terr = build_open_flags(how, &op);\n\tif (unlikely(err))\n\t\treturn err;\n\n\ttmp = getname(filename);\n\tif (IS_ERR(tmp))\n\t\treturn PTR_ERR(tmp);\n\n\tfd = get_unused_fd_flags(how->flags);\n\tif (likely(fd >= 0)) {\n\t\tstruct file *f = do_filp_open(dfd, tmp, &op);\n\t\tif (IS_ERR(f)) {\n\t\t\tput_unused_fd(fd);\n\t\t\tfd = PTR_ERR(f);\n\t\t} else {\n\t\t\tfd_install(fd, f);\n\t\t}\n\t}\n\tputname(tmp);\n\treturn fd;\n}\n\nint do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)\n{\n\tstruct open_how how = build_open_how(flags, mode);\n\treturn do_sys_openat2(dfd, filename, &how);\n}\n\n\nSYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)\n{\n\tif (force_o_largefile())\n\t\tflags |= O_LARGEFILE;\n\treturn do_sys_open(AT_FDCWD, filename, flags, mode);\n}\n\nSYSCALL_DEFINE4(openat, int, dfd, const char __user *, filename, int, flags,\n\t\tumode_t, mode)\n{\n\tif (force_o_largefile())\n\t\tflags |= O_LARGEFILE;\n\treturn do_sys_open(dfd, filename, flags, mode);\n}\n\nSYSCALL_DEFINE4(openat2, int, dfd, const char __user *, filename,\n\t\tstruct open_how __user *, how, size_t, usize)\n{\n\tint err;\n\tstruct open_how tmp;\n\n\tBUILD_BUG_ON(sizeof(struct open_how) < OPEN_HOW_SIZE_VER0);\n\tBUILD_BUG_ON(sizeof(struct open_how) != OPEN_HOW_SIZE_LATEST);\n\n\tif (unlikely(usize < OPEN_HOW_SIZE_VER0))\n\t\treturn -EINVAL;\n\tif (unlikely(usize > PAGE_SIZE))\n\t\treturn -E2BIG;\n\n\terr = copy_struct_from_user(&tmp, sizeof(tmp), how, usize);\n\tif (err)\n\t\treturn err;\n\n\taudit_openat2_how(&tmp);\n\n\t/* O_LARGEFILE is only allowed for non-O_PATH. */\n\tif (!(tmp.flags & O_PATH) && force_o_largefile())\n\t\ttmp.flags |= O_LARGEFILE;\n\n\treturn do_sys_openat2(dfd, filename, &tmp);\n}\n\n#ifdef CONFIG_COMPAT\n/*\n * Exactly like sys_open(), except that it doesn't set the\n * O_LARGEFILE flag.\n */\nCOMPAT_SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)\n{\n\treturn do_sys_open(AT_FDCWD, filename, flags, mode);\n}\n\n/*\n * Exactly like sys_openat(), except that it doesn't set the\n * O_LARGEFILE flag.\n */\nCOMPAT_SYSCALL_DEFINE4(openat, int, dfd, const char __user *, filename, int, flags, umode_t, mode)\n{\n\treturn do_sys_open(dfd, filename, flags, mode);\n}\n#endif\n\n#ifndef __alpha__\n\n/*\n * For backward compatibility?  Maybe this should be moved\n * into arch/i386 instead?\n */\nSYSCALL_DEFINE2(creat, const char __user *, pathname, umode_t, mode)\n{\n\tint flags = O_CREAT | O_WRONLY | O_TRUNC;\n\n\tif (force_o_largefile())\n\t\tflags |= O_LARGEFILE;\n\treturn do_sys_open(AT_FDCWD, pathname, flags, mode);\n}\n#endif\n\n/*\n * \"id\" is the POSIX thread ID. We use the\n * files pointer for this..\n */\nstatic int filp_flush(struct file *filp, fl_owner_t id)\n{\n\tint retval = 0;\n\n\tif (CHECK_DATA_CORRUPTION(file_count(filp) == 0, filp,\n\t\t\t\"VFS: Close: file count is 0 (f_op=%ps)\",\n\t\t\tfilp->f_op)) {\n\t\treturn 0;\n\t}\n\n\tif (filp->f_op->flush)\n\t\tretval = filp->f_op->flush(filp, id);\n\n\tif (likely(!(filp->f_mode & FMODE_PATH))) {\n\t\tdnotify_flush(filp, id);\n\t\tlocks_remove_posix(filp, id);\n\t}\n\treturn retval;\n}\n\nint filp_close(struct file *filp, fl_owner_t id)\n{\n\tint retval;\n\n\tretval = filp_flush(filp, id);\n\tfput_close(filp);\n\n\treturn retval;\n}\nEXPORT_SYMBOL(filp_close);\n\n/*\n * Careful here! We test whether the file pointer is NULL before\n * releasing the fd. This ensures that one clone task can't release\n * an fd while another clone is opening it.\n */\nSYSCALL_DEFINE1(close, unsigned int, fd)\n{\n\tint retval;\n\tstruct file *file;\n\n\tfile = file_close_fd(fd);\n\tif (!file)\n\t\treturn -EBADF;\n\n\tretval = filp_flush(file, current->files);\n\n\t/*\n\t * We're returning to user space. Don't bother\n\t * with any delayed fput() cases.\n\t */\n\tfput_close_sync(file);\n\n\tif (likely(retval == 0))\n\t\treturn 0;\n\n\t/* can't restart close syscall because file table entry was cleared */\n\tif (retval == -ERESTARTSYS ||\n\t    retval == -ERESTARTNOINTR ||\n\t    retval == -ERESTARTNOHAND ||\n\t    retval == -ERESTART_RESTARTBLOCK)\n\t\tretval = -EINTR;\n\n\treturn retval;\n}\n\n/*\n * This routine simulates a hangup on the tty, to arrange that users\n * are given clean terminals at login time.\n */\nSYSCALL_DEFINE0(vhangup)\n{\n\tif (capable(CAP_SYS_TTY_CONFIG)) {\n\t\ttty_vhangup_self();\n\t\treturn 0;\n\t}\n\treturn -EPERM;\n}\n\n/*\n * Called when an inode is about to be open.\n * We use this to disallow opening large files on 32bit systems if\n * the caller didn't specify O_LARGEFILE.  On 64bit systems we force\n * on this flag in sys_open.\n */\nint generic_file_open(struct inode * inode, struct file * filp)\n{\n\tif (!(filp->f_flags & O_LARGEFILE) && i_size_read(inode) > MAX_NON_LFS)\n\t\treturn -EOVERFLOW;\n\treturn 0;\n}\n\nEXPORT_SYMBOL(generic_file_open);\n\n/*\n * This is used by subsystems that don't want seekable\n * file descriptors. The function is not supposed to ever fail, the only\n * reason it returns an 'int' and not 'void' is so that it can be plugged\n * directly into file_operations structure.\n */\nint nonseekable_open(struct inode *inode, struct file *filp)\n{\n\tfilp->f_mode &= ~(FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE);\n\treturn 0;\n}\n\nEXPORT_SYMBOL(nonseekable_open);\n\n/*\n * stream_open is used by subsystems that want stream-like file descriptors.\n * Such file descriptors are not seekable and don't have notion of position\n * (file.f_pos is always 0 and ppos passed to .read()/.write() is always NULL).\n * Contrary to file descriptors of other regular files, .read() and .write()\n * can run simultaneously.\n *\n * stream_open never fails and is marked to return int so that it could be\n * directly used as file_operations.open .\n */\nint stream_open(struct inode *inode, struct file *filp)\n{\n\tfilp->f_mode &= ~(FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE | FMODE_ATOMIC_POS);\n\tfilp->f_mode |= FMODE_STREAM;\n\treturn 0;\n}\n\nEXPORT_SYMBOL(stream_open);\n", "patch": "@@ -840,16 +840,12 @@ EXPORT_SYMBOL(file_path);\n int vfs_open(const struct path *path, struct file *file,\n \t     const struct cred *cred)\n {\n-\tstruct dentry *dentry = path->dentry;\n-\tstruct inode *inode = dentry->d_inode;\n+\tstruct inode *inode = vfs_select_inode(path->dentry, file->f_flags);\n \n-\tfile->f_path = *path;\n-\tif (dentry->d_flags & DCACHE_OP_SELECT_INODE) {\n-\t\tinode = dentry->d_op->d_select_inode(dentry, file->f_flags);\n-\t\tif (IS_ERR(inode))\n-\t\t\treturn PTR_ERR(inode);\n-\t}\n+\tif (IS_ERR(inode))\n+\t\treturn PTR_ERR(inode);\n \n+\tfile->f_path = *path;\n \treturn do_dentry_open(file, inode, NULL, cred);\n }\n ", "file_path": "files/2016_8\\97", "file_language": "c", "file_name": "fs/open.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}, {"raw_url": "https://github.com/torvalds/linux/raw/54d5ca871e72f2bb172ec9323497f01cd5091ec7/include/linux/dcache.h", "code": "#ifndef __LINUX_DCACHE_H\n#define __LINUX_DCACHE_H\n\n#include <linux/atomic.h>\n#include <linux/list.h>\n#include <linux/rculist.h>\n#include <linux/rculist_bl.h>\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/cache.h>\n#include <linux/rcupdate.h>\n#include <linux/lockref.h>\n\nstruct path;\nstruct vfsmount;\n\n/*\n * linux/include/linux/dcache.h\n *\n * Dirent cache data structures\n *\n * (C) Copyright 1997 Thomas Schoebel-Theuer,\n * with heavy changes by Linus Torvalds\n */\n\n#define IS_ROOT(x) ((x) == (x)->d_parent)\n\n/* The hash is always the low bits of hash_len */\n#ifdef __LITTLE_ENDIAN\n #define HASH_LEN_DECLARE u32 hash; u32 len\n #define bytemask_from_count(cnt)\t(~(~0ul << (cnt)*8))\n#else\n #define HASH_LEN_DECLARE u32 len; u32 hash\n #define bytemask_from_count(cnt)\t(~(~0ul >> (cnt)*8))\n#endif\n\n/*\n * \"quick string\" -- eases parameter passing, but more importantly\n * saves \"metadata\" about the string (ie length and the hash).\n *\n * hash comes first so it snuggles against d_parent in the\n * dentry.\n */\nstruct qstr {\n\tunion {\n\t\tstruct {\n\t\t\tHASH_LEN_DECLARE;\n\t\t};\n\t\tu64 hash_len;\n\t};\n\tconst unsigned char *name;\n};\n\n#define QSTR_INIT(n,l) { { { .len = l } }, .name = n }\n#define hashlen_hash(hashlen) ((u32) (hashlen))\n#define hashlen_len(hashlen)  ((u32)((hashlen) >> 32))\n#define hashlen_create(hash,len) (((u64)(len)<<32)|(u32)(hash))\n\nstruct dentry_stat_t {\n\tlong nr_dentry;\n\tlong nr_unused;\n\tlong age_limit;          /* age in seconds */\n\tlong want_pages;         /* pages requested by system */\n\tlong dummy[2];\n};\nextern struct dentry_stat_t dentry_stat;\n\n/* Name hashing routines. Initial hash value */\n/* Hash courtesy of the R5 hash in reiserfs modulo sign bits */\n#define init_name_hash()\t\t0\n\n/* partial hash update function. Assume roughly 4 bits per character */\nstatic inline unsigned long\npartial_name_hash(unsigned long c, unsigned long prevhash)\n{\n\treturn (prevhash + (c << 4) + (c >> 4)) * 11;\n}\n\n/*\n * Finally: cut down the number of bits to a int value (and try to avoid\n * losing bits)\n */\nstatic inline unsigned long end_name_hash(unsigned long hash)\n{\n\treturn (unsigned int) hash;\n}\n\n/* Compute the hash for a name string. */\nextern unsigned int full_name_hash(const unsigned char *, unsigned int);\n\n/*\n * Try to keep struct dentry aligned on 64 byte cachelines (this will\n * give reasonable cacheline footprint with larger lines without the\n * large memory footprint increase).\n */\n#ifdef CONFIG_64BIT\n# define DNAME_INLINE_LEN 32 /* 192 bytes */\n#else\n# ifdef CONFIG_SMP\n#  define DNAME_INLINE_LEN 36 /* 128 bytes */\n# else\n#  define DNAME_INLINE_LEN 40 /* 128 bytes */\n# endif\n#endif\n\n#define d_lock\td_lockref.lock\n\nstruct dentry {\n\t/* RCU lookup touched fields */\n\tunsigned int d_flags;\t\t/* protected by d_lock */\n\tseqcount_t d_seq;\t\t/* per dentry seqlock */\n\tstruct hlist_bl_node d_hash;\t/* lookup hash list */\n\tstruct dentry *d_parent;\t/* parent directory */\n\tstruct qstr d_name;\n\tstruct inode *d_inode;\t\t/* Where the name belongs to - NULL is\n\t\t\t\t\t * negative */\n\tunsigned char d_iname[DNAME_INLINE_LEN];\t/* small names */\n\n\t/* Ref lookup also touches following */\n\tstruct lockref d_lockref;\t/* per-dentry lock and refcount */\n\tconst struct dentry_operations *d_op;\n\tstruct super_block *d_sb;\t/* The root of the dentry tree */\n\tunsigned long d_time;\t\t/* used by d_revalidate */\n\tvoid *d_fsdata;\t\t\t/* fs-specific data */\n\n\tstruct list_head d_lru;\t\t/* LRU list */\n\tstruct list_head d_child;\t/* child of parent list */\n\tstruct list_head d_subdirs;\t/* our children */\n\t/*\n\t * d_alias and d_rcu can share memory\n\t */\n\tunion {\n\t\tstruct hlist_node d_alias;\t/* inode alias list */\n\t \tstruct rcu_head d_rcu;\n\t} d_u;\n};\n\n/*\n * dentry->d_lock spinlock nesting subclasses:\n *\n * 0: normal\n * 1: nested\n */\nenum dentry_d_lock_class\n{\n\tDENTRY_D_LOCK_NORMAL, /* implicitly used by plain spin_lock() APIs. */\n\tDENTRY_D_LOCK_NESTED\n};\n\nstruct dentry_operations {\n\tint (*d_revalidate)(struct dentry *, unsigned int);\n\tint (*d_weak_revalidate)(struct dentry *, unsigned int);\n\tint (*d_hash)(const struct dentry *, struct qstr *);\n\tint (*d_compare)(const struct dentry *, const struct dentry *,\n\t\t\tunsigned int, const char *, const struct qstr *);\n\tint (*d_delete)(const struct dentry *);\n\tvoid (*d_release)(struct dentry *);\n\tvoid (*d_prune)(struct dentry *);\n\tvoid (*d_iput)(struct dentry *, struct inode *);\n\tchar *(*d_dname)(struct dentry *, char *, int);\n\tstruct vfsmount *(*d_automount)(struct path *);\n\tint (*d_manage)(struct dentry *, bool);\n\tstruct inode *(*d_select_inode)(struct dentry *, unsigned);\n\tstruct dentry *(*d_real)(struct dentry *, struct inode *);\n} ____cacheline_aligned;\n\n/*\n * Locking rules for dentry_operations callbacks are to be found in\n * Documentation/filesystems/Locking. Keep it updated!\n *\n * FUrther descriptions are found in Documentation/filesystems/vfs.txt.\n * Keep it updated too!\n */\n\n/* d_flags entries */\n#define DCACHE_OP_HASH\t\t\t0x00000001\n#define DCACHE_OP_COMPARE\t\t0x00000002\n#define DCACHE_OP_REVALIDATE\t\t0x00000004\n#define DCACHE_OP_DELETE\t\t0x00000008\n#define DCACHE_OP_PRUNE\t\t\t0x00000010\n\n#define\tDCACHE_DISCONNECTED\t\t0x00000020\n     /* This dentry is possibly not currently connected to the dcache tree, in\n      * which case its parent will either be itself, or will have this flag as\n      * well.  nfsd will not use a dentry with this bit set, but will first\n      * endeavour to clear the bit either by discovering that it is connected,\n      * or by performing lookup operations.   Any filesystem which supports\n      * nfsd_operations MUST have a lookup function which, if it finds a\n      * directory inode with a DCACHE_DISCONNECTED dentry, will d_move that\n      * dentry into place and return that dentry rather than the passed one,\n      * typically using d_splice_alias. */\n\n#define DCACHE_REFERENCED\t\t0x00000040 /* Recently used, don't discard. */\n#define DCACHE_RCUACCESS\t\t0x00000080 /* Entry has ever been RCU-visible */\n\n#define DCACHE_CANT_MOUNT\t\t0x00000100\n#define DCACHE_GENOCIDE\t\t\t0x00000200\n#define DCACHE_SHRINK_LIST\t\t0x00000400\n\n#define DCACHE_OP_WEAK_REVALIDATE\t0x00000800\n\n#define DCACHE_NFSFS_RENAMED\t\t0x00001000\n     /* this dentry has been \"silly renamed\" and has to be deleted on the last\n      * dput() */\n#define DCACHE_COOKIE\t\t\t0x00002000 /* For use by dcookie subsystem */\n#define DCACHE_FSNOTIFY_PARENT_WATCHED\t0x00004000\n     /* Parent inode is watched by some fsnotify listener */\n\n#define DCACHE_DENTRY_KILLED\t\t0x00008000\n\n#define DCACHE_MOUNTED\t\t\t0x00010000 /* is a mountpoint */\n#define DCACHE_NEED_AUTOMOUNT\t\t0x00020000 /* handle automount on this dir */\n#define DCACHE_MANAGE_TRANSIT\t\t0x00040000 /* manage transit from this dirent */\n#define DCACHE_MANAGED_DENTRY \\\n\t(DCACHE_MOUNTED|DCACHE_NEED_AUTOMOUNT|DCACHE_MANAGE_TRANSIT)\n\n#define DCACHE_LRU_LIST\t\t\t0x00080000\n\n#define DCACHE_ENTRY_TYPE\t\t0x00700000\n#define DCACHE_MISS_TYPE\t\t0x00000000 /* Negative dentry (maybe fallthru to nowhere) */\n#define DCACHE_WHITEOUT_TYPE\t\t0x00100000 /* Whiteout dentry (stop pathwalk) */\n#define DCACHE_DIRECTORY_TYPE\t\t0x00200000 /* Normal directory */\n#define DCACHE_AUTODIR_TYPE\t\t0x00300000 /* Lookupless directory (presumed automount) */\n#define DCACHE_REGULAR_TYPE\t\t0x00400000 /* Regular file type (or fallthru to such) */\n#define DCACHE_SPECIAL_TYPE\t\t0x00500000 /* Other file type (or fallthru to such) */\n#define DCACHE_SYMLINK_TYPE\t\t0x00600000 /* Symlink (or fallthru to such) */\n\n#define DCACHE_MAY_FREE\t\t\t0x00800000\n#define DCACHE_FALLTHRU\t\t\t0x01000000 /* Fall through to lower layer */\n#define DCACHE_OP_SELECT_INODE\t\t0x02000000 /* Unioned entry: dcache op selects inode */\n\n#define DCACHE_ENCRYPTED_WITH_KEY\t0x04000000 /* dir is encrypted with a valid key */\n#define DCACHE_OP_REAL\t\t\t0x08000000\n\nextern seqlock_t rename_lock;\n\n/*\n * These are the low-level FS interfaces to the dcache..\n */\nextern void d_instantiate(struct dentry *, struct inode *);\nextern struct dentry * d_instantiate_unique(struct dentry *, struct inode *);\nextern int d_instantiate_no_diralias(struct dentry *, struct inode *);\nextern void __d_drop(struct dentry *dentry);\nextern void d_drop(struct dentry *dentry);\nextern void d_delete(struct dentry *);\nextern void d_set_d_op(struct dentry *dentry, const struct dentry_operations *op);\n\n/* allocate/de-allocate */\nextern struct dentry * d_alloc(struct dentry *, const struct qstr *);\nextern struct dentry * d_alloc_pseudo(struct super_block *, const struct qstr *);\nextern struct dentry * d_splice_alias(struct inode *, struct dentry *);\nextern struct dentry * d_add_ci(struct dentry *, struct inode *, struct qstr *);\nextern struct dentry * d_exact_alias(struct dentry *, struct inode *);\nextern struct dentry *d_find_any_alias(struct inode *inode);\nextern struct dentry * d_obtain_alias(struct inode *);\nextern struct dentry * d_obtain_root(struct inode *);\nextern void shrink_dcache_sb(struct super_block *);\nextern void shrink_dcache_parent(struct dentry *);\nextern void shrink_dcache_for_umount(struct super_block *);\nextern void d_invalidate(struct dentry *);\n\n/* only used at mount-time */\nextern struct dentry * d_make_root(struct inode *);\n\n/* <clickety>-<click> the ramfs-type tree */\nextern void d_genocide(struct dentry *);\n\nextern void d_tmpfile(struct dentry *, struct inode *);\n\nextern struct dentry *d_find_alias(struct inode *);\nextern void d_prune_aliases(struct inode *);\n\n/* test whether we have any submounts in a subdir tree */\nextern int have_submounts(struct dentry *);\n\n/*\n * This adds the entry to the hash queues.\n */\nextern void d_rehash(struct dentry *);\n \nextern void d_add(struct dentry *, struct inode *);\n\nextern void dentry_update_name_case(struct dentry *, struct qstr *);\n\n/* used for rename() and baskets */\nextern void d_move(struct dentry *, struct dentry *);\nextern void d_exchange(struct dentry *, struct dentry *);\nextern struct dentry *d_ancestor(struct dentry *, struct dentry *);\n\n/* appendix may either be NULL or be used for transname suffixes */\nextern struct dentry *d_lookup(const struct dentry *, const struct qstr *);\nextern struct dentry *d_hash_and_lookup(struct dentry *, struct qstr *);\nextern struct dentry *__d_lookup(const struct dentry *, const struct qstr *);\nextern struct dentry *__d_lookup_rcu(const struct dentry *parent,\n\t\t\t\tconst struct qstr *name, unsigned *seq);\n\nstatic inline unsigned d_count(const struct dentry *dentry)\n{\n\treturn dentry->d_lockref.count;\n}\n\n/*\n * helper function for dentry_operations.d_dname() members\n */\nextern __printf(4, 5)\nchar *dynamic_dname(struct dentry *, char *, int, const char *, ...);\nextern char *simple_dname(struct dentry *, char *, int);\n\nextern char *__d_path(const struct path *, const struct path *, char *, int);\nextern char *d_absolute_path(const struct path *, char *, int);\nextern char *d_path(const struct path *, char *, int);\nextern char *dentry_path_raw(struct dentry *, char *, int);\nextern char *dentry_path(struct dentry *, char *, int);\n\n/* Allocation counts.. */\n\n/**\n *\tdget, dget_dlock -\tget a reference to a dentry\n *\t@dentry: dentry to get a reference to\n *\n *\tGiven a dentry or %NULL pointer increment the reference count\n *\tif appropriate and return the dentry. A dentry will not be \n *\tdestroyed when it has references.\n */\nstatic inline struct dentry *dget_dlock(struct dentry *dentry)\n{\n\tif (dentry)\n\t\tdentry->d_lockref.count++;\n\treturn dentry;\n}\n\nstatic inline struct dentry *dget(struct dentry *dentry)\n{\n\tif (dentry)\n\t\tlockref_get(&dentry->d_lockref);\n\treturn dentry;\n}\n\nextern struct dentry *dget_parent(struct dentry *dentry);\n\n/**\n *\td_unhashed -\tis dentry hashed\n *\t@dentry: entry to check\n *\n *\tReturns true if the dentry passed is not currently hashed.\n */\n \nstatic inline int d_unhashed(const struct dentry *dentry)\n{\n\treturn hlist_bl_unhashed(&dentry->d_hash);\n}\n\nstatic inline int d_unlinked(const struct dentry *dentry)\n{\n\treturn d_unhashed(dentry) && !IS_ROOT(dentry);\n}\n\nstatic inline int cant_mount(const struct dentry *dentry)\n{\n\treturn (dentry->d_flags & DCACHE_CANT_MOUNT);\n}\n\nstatic inline void dont_mount(struct dentry *dentry)\n{\n\tspin_lock(&dentry->d_lock);\n\tdentry->d_flags |= DCACHE_CANT_MOUNT;\n\tspin_unlock(&dentry->d_lock);\n}\n\nextern void dput(struct dentry *);\n\nstatic inline bool d_managed(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_MANAGED_DENTRY;\n}\n\nstatic inline bool d_mountpoint(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_MOUNTED;\n}\n\n/*\n * Directory cache entry type accessor functions.\n */\nstatic inline unsigned __d_entry_type(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_ENTRY_TYPE;\n}\n\nstatic inline bool d_is_miss(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_MISS_TYPE;\n}\n\nstatic inline bool d_is_whiteout(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_WHITEOUT_TYPE;\n}\n\nstatic inline bool d_can_lookup(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_DIRECTORY_TYPE;\n}\n\nstatic inline bool d_is_autodir(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_AUTODIR_TYPE;\n}\n\nstatic inline bool d_is_dir(const struct dentry *dentry)\n{\n\treturn d_can_lookup(dentry) || d_is_autodir(dentry);\n}\n\nstatic inline bool d_is_symlink(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_SYMLINK_TYPE;\n}\n\nstatic inline bool d_is_reg(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_REGULAR_TYPE;\n}\n\nstatic inline bool d_is_special(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_SPECIAL_TYPE;\n}\n\nstatic inline bool d_is_file(const struct dentry *dentry)\n{\n\treturn d_is_reg(dentry) || d_is_special(dentry);\n}\n\nstatic inline bool d_is_negative(const struct dentry *dentry)\n{\n\t// TODO: check d_is_whiteout(dentry) also.\n\treturn d_is_miss(dentry);\n}\n\nstatic inline bool d_is_positive(const struct dentry *dentry)\n{\n\treturn !d_is_negative(dentry);\n}\n\n/**\n * d_really_is_negative - Determine if a dentry is really negative (ignoring fallthroughs)\n * @dentry: The dentry in question\n *\n * Returns true if the dentry represents either an absent name or a name that\n * doesn't map to an inode (ie. ->d_inode is NULL).  The dentry could represent\n * a true miss, a whiteout that isn't represented by a 0,0 chardev or a\n * fallthrough marker in an opaque directory.\n *\n * Note!  (1) This should be used *only* by a filesystem to examine its own\n * dentries.  It should not be used to look at some other filesystem's\n * dentries.  (2) It should also be used in combination with d_inode() to get\n * the inode.  (3) The dentry may have something attached to ->d_lower and the\n * type field of the flags may be set to something other than miss or whiteout.\n */\nstatic inline bool d_really_is_negative(const struct dentry *dentry)\n{\n\treturn dentry->d_inode == NULL;\n}\n\n/**\n * d_really_is_positive - Determine if a dentry is really positive (ignoring fallthroughs)\n * @dentry: The dentry in question\n *\n * Returns true if the dentry represents a name that maps to an inode\n * (ie. ->d_inode is not NULL).  The dentry might still represent a whiteout if\n * that is represented on medium as a 0,0 chardev.\n *\n * Note!  (1) This should be used *only* by a filesystem to examine its own\n * dentries.  It should not be used to look at some other filesystem's\n * dentries.  (2) It should also be used in combination with d_inode() to get\n * the inode.\n */\nstatic inline bool d_really_is_positive(const struct dentry *dentry)\n{\n\treturn dentry->d_inode != NULL;\n}\n\nstatic inline int simple_positive(struct dentry *dentry)\n{\n\treturn d_really_is_positive(dentry) && !d_unhashed(dentry);\n}\n\nextern void d_set_fallthru(struct dentry *dentry);\n\nstatic inline bool d_is_fallthru(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_FALLTHRU;\n}\n\n\nextern int sysctl_vfs_cache_pressure;\n\nstatic inline unsigned long vfs_pressure_ratio(unsigned long val)\n{\n\treturn mult_frac(val, sysctl_vfs_cache_pressure, 100);\n}\n\n/**\n * d_inode - Get the actual inode of this dentry\n * @dentry: The dentry to query\n *\n * This is the helper normal filesystems should use to get at their own inodes\n * in their own dentries and ignore the layering superimposed upon them.\n */\nstatic inline struct inode *d_inode(const struct dentry *dentry)\n{\n\treturn dentry->d_inode;\n}\n\n/**\n * d_inode_rcu - Get the actual inode of this dentry with ACCESS_ONCE()\n * @dentry: The dentry to query\n *\n * This is the helper normal filesystems should use to get at their own inodes\n * in their own dentries and ignore the layering superimposed upon them.\n */\nstatic inline struct inode *d_inode_rcu(const struct dentry *dentry)\n{\n\treturn ACCESS_ONCE(dentry->d_inode);\n}\n\n/**\n * d_backing_inode - Get upper or lower inode we should be using\n * @upper: The upper layer\n *\n * This is the helper that should be used to get at the inode that will be used\n * if this dentry were to be opened as a file.  The inode may be on the upper\n * dentry or it may be on a lower dentry pinned by the upper.\n *\n * Normal filesystems should not use this to access their own inodes.\n */\nstatic inline struct inode *d_backing_inode(const struct dentry *upper)\n{\n\tstruct inode *inode = upper->d_inode;\n\n\treturn inode;\n}\n\n/**\n * d_backing_dentry - Get upper or lower dentry we should be using\n * @upper: The upper layer\n *\n * This is the helper that should be used to get the dentry of the inode that\n * will be used if this dentry were opened as a file.  It may be the upper\n * dentry or it may be a lower dentry pinned by the upper.\n *\n * Normal filesystems should not use this to access their own dentries.\n */\nstatic inline struct dentry *d_backing_dentry(struct dentry *upper)\n{\n\treturn upper;\n}\n\nstatic inline struct dentry *d_real(struct dentry *dentry)\n{\n\tif (unlikely(dentry->d_flags & DCACHE_OP_REAL))\n\t\treturn dentry->d_op->d_real(dentry, NULL);\n\telse\n\t\treturn dentry;\n}\n\nstatic inline struct inode *vfs_select_inode(struct dentry *dentry,\n\t\t\t\t\t     unsigned open_flags)\n{\n\tstruct inode *inode = d_inode(dentry);\n\n\tif (inode && unlikely(dentry->d_flags & DCACHE_OP_SELECT_INODE))\n\t\tinode = dentry->d_op->d_select_inode(dentry, open_flags);\n\n\treturn inode;\n}\n\n\n#endif\t/* __LINUX_DCACHE_H */\n", "code_before": "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __LINUX_DCACHE_H\n#define __LINUX_DCACHE_H\n\n#include <linux/atomic.h>\n#include <linux/list.h>\n#include <linux/math.h>\n#include <linux/rculist.h>\n#include <linux/rculist_bl.h>\n#include <linux/spinlock.h>\n#include <linux/seqlock.h>\n#include <linux/cache.h>\n#include <linux/rcupdate.h>\n#include <linux/lockref.h>\n#include <linux/stringhash.h>\n#include <linux/wait.h>\n\nstruct path;\nstruct file;\nstruct vfsmount;\n\n/*\n * linux/include/linux/dcache.h\n *\n * Dirent cache data structures\n *\n * (C) Copyright 1997 Thomas Schoebel-Theuer,\n * with heavy changes by Linus Torvalds\n */\n\n#define IS_ROOT(x) ((x) == (x)->d_parent)\n\n/* The hash is always the low bits of hash_len */\n#ifdef __LITTLE_ENDIAN\n #define HASH_LEN_DECLARE u32 hash; u32 len\n #define bytemask_from_count(cnt)\t(~(~0ul << (cnt)*8))\n#else\n #define HASH_LEN_DECLARE u32 len; u32 hash\n #define bytemask_from_count(cnt)\t(~(~0ul >> (cnt)*8))\n#endif\n\n/*\n * \"quick string\" -- eases parameter passing, but more importantly\n * saves \"metadata\" about the string (ie length and the hash).\n *\n * hash comes first so it snuggles against d_parent in the\n * dentry.\n */\nstruct qstr {\n\tunion {\n\t\tstruct {\n\t\t\tHASH_LEN_DECLARE;\n\t\t};\n\t\tu64 hash_len;\n\t};\n\tconst unsigned char *name;\n};\n\n#define QSTR_INIT(n,l) { { { .len = l } }, .name = n }\n#define QSTR_LEN(n,l) (struct qstr)QSTR_INIT(n,l)\n#define QSTR(n) QSTR_LEN(n, strlen(n))\n\nextern const struct qstr empty_name;\nextern const struct qstr slash_name;\nextern const struct qstr dotdot_name;\n\n/*\n * Try to keep struct dentry aligned on 64 byte cachelines (this will\n * give reasonable cacheline footprint with larger lines without the\n * large memory footprint increase).\n */\n#ifdef CONFIG_64BIT\n# define DNAME_INLINE_WORDS 5 /* 192 bytes */\n#else\n# ifdef CONFIG_SMP\n#  define DNAME_INLINE_WORDS 9 /* 128 bytes */\n# else\n#  define DNAME_INLINE_WORDS 11 /* 128 bytes */\n# endif\n#endif\n\n#define DNAME_INLINE_LEN (DNAME_INLINE_WORDS*sizeof(unsigned long))\n\nunion shortname_store {\n\tunsigned char string[DNAME_INLINE_LEN];\n\tunsigned long words[DNAME_INLINE_WORDS];\n};\n\n#define d_lock\td_lockref.lock\n#define d_iname d_shortname.string\n\nstruct dentry {\n\t/* RCU lookup touched fields */\n\tunsigned int d_flags;\t\t/* protected by d_lock */\n\tseqcount_spinlock_t d_seq;\t/* per dentry seqlock */\n\tstruct hlist_bl_node d_hash;\t/* lookup hash list */\n\tstruct dentry *d_parent;\t/* parent directory */\n\tstruct qstr d_name;\n\tstruct inode *d_inode;\t\t/* Where the name belongs to - NULL is\n\t\t\t\t\t * negative */\n\tunion shortname_store d_shortname;\n\t/* --- cacheline 1 boundary (64 bytes) was 32 bytes ago --- */\n\n\t/* Ref lookup also touches following */\n\tconst struct dentry_operations *d_op;\n\tstruct super_block *d_sb;\t/* The root of the dentry tree */\n\tunsigned long d_time;\t\t/* used by d_revalidate */\n\tvoid *d_fsdata;\t\t\t/* fs-specific data */\n\t/* --- cacheline 2 boundary (128 bytes) --- */\n\tstruct lockref d_lockref;\t/* per-dentry lock and refcount\n\t\t\t\t\t * keep separate from RCU lookup area if\n\t\t\t\t\t * possible!\n\t\t\t\t\t */\n\n\tunion {\n\t\tstruct list_head d_lru;\t\t/* LRU list */\n\t\twait_queue_head_t *d_wait;\t/* in-lookup ones only */\n\t};\n\tstruct hlist_node d_sib;\t/* child of parent list */\n\tstruct hlist_head d_children;\t/* our children */\n\t/*\n\t * d_alias and d_rcu can share memory\n\t */\n\tunion {\n\t\tstruct hlist_node d_alias;\t/* inode alias list */\n\t\tstruct hlist_bl_node d_in_lookup_hash;\t/* only for in-lookup ones */\n\t \tstruct rcu_head d_rcu;\n\t} d_u;\n};\n\n/*\n * dentry->d_lock spinlock nesting subclasses:\n *\n * 0: normal\n * 1: nested\n */\nenum dentry_d_lock_class\n{\n\tDENTRY_D_LOCK_NORMAL, /* implicitly used by plain spin_lock() APIs. */\n\tDENTRY_D_LOCK_NESTED\n};\n\nenum d_real_type {\n\tD_REAL_DATA,\n\tD_REAL_METADATA,\n};\n\nstruct dentry_operations {\n\tint (*d_revalidate)(struct inode *, const struct qstr *,\n\t\t\t    struct dentry *, unsigned int);\n\tint (*d_weak_revalidate)(struct dentry *, unsigned int);\n\tint (*d_hash)(const struct dentry *, struct qstr *);\n\tint (*d_compare)(const struct dentry *,\n\t\t\tunsigned int, const char *, const struct qstr *);\n\tint (*d_delete)(const struct dentry *);\n\tint (*d_init)(struct dentry *);\n\tvoid (*d_release)(struct dentry *);\n\tvoid (*d_prune)(struct dentry *);\n\tvoid (*d_iput)(struct dentry *, struct inode *);\n\tchar *(*d_dname)(struct dentry *, char *, int);\n\tstruct vfsmount *(*d_automount)(struct path *);\n\tint (*d_manage)(const struct path *, bool);\n\tstruct dentry *(*d_real)(struct dentry *, enum d_real_type type);\n\tbool (*d_unalias_trylock)(const struct dentry *);\n\tvoid (*d_unalias_unlock)(const struct dentry *);\n} ____cacheline_aligned;\n\n/*\n * Locking rules for dentry_operations callbacks are to be found in\n * Documentation/filesystems/locking.rst. Keep it updated!\n *\n * FUrther descriptions are found in Documentation/filesystems/vfs.rst.\n * Keep it updated too!\n */\n\n/* d_flags entries */\nenum dentry_flags {\n\tDCACHE_OP_HASH\t\t\t= BIT(0),\n\tDCACHE_OP_COMPARE\t\t= BIT(1),\n\tDCACHE_OP_REVALIDATE\t\t= BIT(2),\n\tDCACHE_OP_DELETE\t\t= BIT(3),\n\tDCACHE_OP_PRUNE\t\t\t= BIT(4),\n\t/*\n\t * This dentry is possibly not currently connected to the dcache tree,\n\t * in which case its parent will either be itself, or will have this\n\t * flag as well.  nfsd will not use a dentry with this bit set, but will\n\t * first endeavour to clear the bit either by discovering that it is\n\t * connected, or by performing lookup operations.  Any filesystem which\n\t * supports nfsd_operations MUST have a lookup function which, if it\n\t * finds a directory inode with a DCACHE_DISCONNECTED dentry, will\n\t * d_move that dentry into place and return that dentry rather than the\n\t * passed one, typically using d_splice_alias.\n\t */\n\tDCACHE_DISCONNECTED\t\t= BIT(5),\n\tDCACHE_REFERENCED\t\t= BIT(6),\t/* Recently used, don't discard. */\n\tDCACHE_DONTCACHE\t\t= BIT(7),\t/* Purge from memory on final dput() */\n\tDCACHE_CANT_MOUNT\t\t= BIT(8),\n\tDCACHE_GENOCIDE\t\t\t= BIT(9),\n\tDCACHE_SHRINK_LIST\t\t= BIT(10),\n\tDCACHE_OP_WEAK_REVALIDATE\t= BIT(11),\n\t/*\n\t * this dentry has been \"silly renamed\" and has to be deleted on the\n\t * last dput()\n\t */\n\tDCACHE_NFSFS_RENAMED\t\t= BIT(12),\n\tDCACHE_FSNOTIFY_PARENT_WATCHED\t= BIT(13),\t/* Parent inode is watched by some fsnotify listener */\n\tDCACHE_DENTRY_KILLED\t\t= BIT(14),\n\tDCACHE_MOUNTED\t\t\t= BIT(15),\t/* is a mountpoint */\n\tDCACHE_NEED_AUTOMOUNT\t\t= BIT(16),\t/* handle automount on this dir */\n\tDCACHE_MANAGE_TRANSIT\t\t= BIT(17),\t/* manage transit from this dirent */\n\tDCACHE_LRU_LIST\t\t\t= BIT(18),\n\tDCACHE_ENTRY_TYPE\t\t= (7 << 19),\t/* bits 19..21 are for storing type: */\n\tDCACHE_MISS_TYPE\t\t= (0 << 19),\t/* Negative dentry */\n\tDCACHE_WHITEOUT_TYPE\t\t= (1 << 19),\t/* Whiteout dentry (stop pathwalk) */\n\tDCACHE_DIRECTORY_TYPE\t\t= (2 << 19),\t/* Normal directory */\n\tDCACHE_AUTODIR_TYPE\t\t= (3 << 19),\t/* Lookupless directory (presumed automount) */\n\tDCACHE_REGULAR_TYPE\t\t= (4 << 19),\t/* Regular file type */\n\tDCACHE_SPECIAL_TYPE\t\t= (5 << 19),\t/* Other file type */\n\tDCACHE_SYMLINK_TYPE\t\t= (6 << 19),\t/* Symlink */\n\tDCACHE_NOKEY_NAME\t\t= BIT(22),\t/* Encrypted name encoded without key */\n\tDCACHE_OP_REAL\t\t\t= BIT(23),\n\tDCACHE_PAR_LOOKUP\t\t= BIT(24),\t/* being looked up (with parent locked shared) */\n\tDCACHE_DENTRY_CURSOR\t\t= BIT(25),\n\tDCACHE_NORCU\t\t\t= BIT(26),\t/* No RCU delay for freeing */\n};\n\n#define DCACHE_MANAGED_DENTRY \\\n\t(DCACHE_MOUNTED|DCACHE_NEED_AUTOMOUNT|DCACHE_MANAGE_TRANSIT)\n\nextern seqlock_t rename_lock;\n\n/*\n * These are the low-level FS interfaces to the dcache..\n */\nextern void d_instantiate(struct dentry *, struct inode *);\nextern void d_instantiate_new(struct dentry *, struct inode *);\nextern void __d_drop(struct dentry *dentry);\nextern void d_drop(struct dentry *dentry);\nextern void d_delete(struct dentry *);\n\n/* allocate/de-allocate */\nextern struct dentry * d_alloc(struct dentry *, const struct qstr *);\nextern struct dentry * d_alloc_anon(struct super_block *);\nextern struct dentry * d_alloc_parallel(struct dentry *, const struct qstr *,\n\t\t\t\t\twait_queue_head_t *);\nextern struct dentry * d_splice_alias(struct inode *, struct dentry *);\n/* weird procfs mess; *NOT* exported */\nextern struct dentry * d_splice_alias_ops(struct inode *, struct dentry *,\n\t\t\t\t\t  const struct dentry_operations *);\nextern struct dentry * d_add_ci(struct dentry *, struct inode *, struct qstr *);\nextern bool d_same_name(const struct dentry *dentry, const struct dentry *parent,\n\t\t\tconst struct qstr *name);\nextern struct dentry *d_find_any_alias(struct inode *inode);\nextern struct dentry * d_obtain_alias(struct inode *);\nextern struct dentry * d_obtain_root(struct inode *);\nextern void shrink_dcache_sb(struct super_block *);\nextern void shrink_dcache_parent(struct dentry *);\nextern void d_invalidate(struct dentry *);\n\n/* only used at mount-time */\nextern struct dentry * d_make_root(struct inode *);\n\nextern void d_mark_tmpfile(struct file *, struct inode *);\nextern void d_tmpfile(struct file *, struct inode *);\n\nextern struct dentry *d_find_alias(struct inode *);\nextern void d_prune_aliases(struct inode *);\n\nextern struct dentry *d_find_alias_rcu(struct inode *);\n\n/* test whether we have any submounts in a subdir tree */\nextern int path_has_submounts(const struct path *);\n\n/*\n * This adds the entry to the hash queues.\n */\nextern void d_rehash(struct dentry *);\n \nextern void d_add(struct dentry *, struct inode *);\n\n/* used for rename() and baskets */\nextern void d_move(struct dentry *, struct dentry *);\nextern void d_exchange(struct dentry *, struct dentry *);\nextern struct dentry *d_ancestor(struct dentry *, struct dentry *);\n\nextern struct dentry *d_lookup(const struct dentry *, const struct qstr *);\n\nstatic inline unsigned d_count(const struct dentry *dentry)\n{\n\treturn dentry->d_lockref.count;\n}\n\nino_t d_parent_ino(struct dentry *dentry);\n\n/*\n * helper function for dentry_operations.d_dname() members\n */\nextern __printf(3, 4)\nchar *dynamic_dname(char *, int, const char *, ...);\n\nextern char *__d_path(const struct path *, const struct path *, char *, int);\nextern char *d_absolute_path(const struct path *, char *, int);\nextern char *d_path(const struct path *, char *, int);\nextern char *dentry_path_raw(const struct dentry *, char *, int);\nextern char *dentry_path(const struct dentry *, char *, int);\n\n/* Allocation counts.. */\n\n/**\n * dget_dlock -\tget a reference to a dentry\n * @dentry: dentry to get a reference to\n *\n * Given a live dentry, increment the reference count and return the dentry.\n * Caller must hold @dentry->d_lock.  Making sure that dentry is alive is\n * caller's resonsibility.  There are many conditions sufficient to guarantee\n * that; e.g. anything with non-negative refcount is alive, so's anything\n * hashed, anything positive, anyone's parent, etc.\n */\nstatic inline struct dentry *dget_dlock(struct dentry *dentry)\n{\n\tdentry->d_lockref.count++;\n\treturn dentry;\n}\n\n\n/**\n * dget - get a reference to a dentry\n * @dentry: dentry to get a reference to\n *\n * Given a dentry or %NULL pointer increment the reference count\n * if appropriate and return the dentry.  A dentry will not be\n * destroyed when it has references.  Conversely, a dentry with\n * no references can disappear for any number of reasons, starting\n * with memory pressure.  In other words, that primitive is\n * used to clone an existing reference; using it on something with\n * zero refcount is a bug.\n *\n * NOTE: it will spin if @dentry->d_lock is held.  From the deadlock\n * avoidance point of view it is equivalent to spin_lock()/increment\n * refcount/spin_unlock(), so calling it under @dentry->d_lock is\n * always a bug; so's calling it under ->d_lock on any of its descendents.\n *\n */\nstatic inline struct dentry *dget(struct dentry *dentry)\n{\n\tif (dentry)\n\t\tlockref_get(&dentry->d_lockref);\n\treturn dentry;\n}\n\nextern struct dentry *dget_parent(struct dentry *dentry);\n\n/**\n * d_unhashed - is dentry hashed\n * @dentry: entry to check\n *\n * Returns true if the dentry passed is not currently hashed.\n */\nstatic inline int d_unhashed(const struct dentry *dentry)\n{\n\treturn hlist_bl_unhashed(&dentry->d_hash);\n}\n\nstatic inline int d_unlinked(const struct dentry *dentry)\n{\n\treturn d_unhashed(dentry) && !IS_ROOT(dentry);\n}\n\nstatic inline int cant_mount(const struct dentry *dentry)\n{\n\treturn (dentry->d_flags & DCACHE_CANT_MOUNT);\n}\n\nstatic inline void dont_mount(struct dentry *dentry)\n{\n\tspin_lock(&dentry->d_lock);\n\tdentry->d_flags |= DCACHE_CANT_MOUNT;\n\tspin_unlock(&dentry->d_lock);\n}\n\nextern void __d_lookup_unhash_wake(struct dentry *dentry);\n\nstatic inline int d_in_lookup(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_PAR_LOOKUP;\n}\n\nstatic inline void d_lookup_done(struct dentry *dentry)\n{\n\tif (unlikely(d_in_lookup(dentry)))\n\t\t__d_lookup_unhash_wake(dentry);\n}\n\nextern void dput(struct dentry *);\n\nstatic inline bool d_managed(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_MANAGED_DENTRY;\n}\n\nstatic inline bool d_mountpoint(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_MOUNTED;\n}\n\n/*\n * Directory cache entry type accessor functions.\n */\nstatic inline unsigned __d_entry_type(const struct dentry *dentry)\n{\n\treturn dentry->d_flags & DCACHE_ENTRY_TYPE;\n}\n\nstatic inline bool d_is_miss(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_MISS_TYPE;\n}\n\nstatic inline bool d_is_whiteout(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_WHITEOUT_TYPE;\n}\n\nstatic inline bool d_can_lookup(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_DIRECTORY_TYPE;\n}\n\nstatic inline bool d_is_autodir(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_AUTODIR_TYPE;\n}\n\nstatic inline bool d_is_dir(const struct dentry *dentry)\n{\n\treturn d_can_lookup(dentry) || d_is_autodir(dentry);\n}\n\nstatic inline bool d_is_symlink(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_SYMLINK_TYPE;\n}\n\nstatic inline bool d_is_reg(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_REGULAR_TYPE;\n}\n\nstatic inline bool d_is_special(const struct dentry *dentry)\n{\n\treturn __d_entry_type(dentry) == DCACHE_SPECIAL_TYPE;\n}\n\nstatic inline bool d_is_file(const struct dentry *dentry)\n{\n\treturn d_is_reg(dentry) || d_is_special(dentry);\n}\n\nstatic inline bool d_is_negative(const struct dentry *dentry)\n{\n\t// TODO: check d_is_whiteout(dentry) also.\n\treturn d_is_miss(dentry);\n}\n\nstatic inline bool d_flags_negative(unsigned flags)\n{\n\treturn (flags & DCACHE_ENTRY_TYPE) == DCACHE_MISS_TYPE;\n}\n\nstatic inline bool d_is_positive(const struct dentry *dentry)\n{\n\treturn !d_is_negative(dentry);\n}\n\n/**\n * d_really_is_negative - Determine if a dentry is really negative (ignoring fallthroughs)\n * @dentry: The dentry in question\n *\n * Returns true if the dentry represents either an absent name or a name that\n * doesn't map to an inode (ie. ->d_inode is NULL).  The dentry could represent\n * a true miss, a whiteout that isn't represented by a 0,0 chardev or a\n * fallthrough marker in an opaque directory.\n *\n * Note!  (1) This should be used *only* by a filesystem to examine its own\n * dentries.  It should not be used to look at some other filesystem's\n * dentries.  (2) It should also be used in combination with d_inode() to get\n * the inode.  (3) The dentry may have something attached to ->d_lower and the\n * type field of the flags may be set to something other than miss or whiteout.\n */\nstatic inline bool d_really_is_negative(const struct dentry *dentry)\n{\n\treturn dentry->d_inode == NULL;\n}\n\n/**\n * d_really_is_positive - Determine if a dentry is really positive (ignoring fallthroughs)\n * @dentry: The dentry in question\n *\n * Returns true if the dentry represents a name that maps to an inode\n * (ie. ->d_inode is not NULL).  The dentry might still represent a whiteout if\n * that is represented on medium as a 0,0 chardev.\n *\n * Note!  (1) This should be used *only* by a filesystem to examine its own\n * dentries.  It should not be used to look at some other filesystem's\n * dentries.  (2) It should also be used in combination with d_inode() to get\n * the inode.\n */\nstatic inline bool d_really_is_positive(const struct dentry *dentry)\n{\n\treturn dentry->d_inode != NULL;\n}\n\nstatic inline int simple_positive(const struct dentry *dentry)\n{\n\treturn d_really_is_positive(dentry) && !d_unhashed(dentry);\n}\n\nunsigned long vfs_pressure_ratio(unsigned long val);\n\n/**\n * d_inode - Get the actual inode of this dentry\n * @dentry: The dentry to query\n *\n * This is the helper normal filesystems should use to get at their own inodes\n * in their own dentries and ignore the layering superimposed upon them.\n */\nstatic inline struct inode *d_inode(const struct dentry *dentry)\n{\n\treturn dentry->d_inode;\n}\n\n/**\n * d_inode_rcu - Get the actual inode of this dentry with READ_ONCE()\n * @dentry: The dentry to query\n *\n * This is the helper normal filesystems should use to get at their own inodes\n * in their own dentries and ignore the layering superimposed upon them.\n */\nstatic inline struct inode *d_inode_rcu(const struct dentry *dentry)\n{\n\treturn READ_ONCE(dentry->d_inode);\n}\n\n/**\n * d_backing_inode - Get upper or lower inode we should be using\n * @upper: The upper layer\n *\n * This is the helper that should be used to get at the inode that will be used\n * if this dentry were to be opened as a file.  The inode may be on the upper\n * dentry or it may be on a lower dentry pinned by the upper.\n *\n * Normal filesystems should not use this to access their own inodes.\n */\nstatic inline struct inode *d_backing_inode(const struct dentry *upper)\n{\n\tstruct inode *inode = upper->d_inode;\n\n\treturn inode;\n}\n\n/**\n * d_real - Return the real dentry\n * @dentry: the dentry to query\n * @type: the type of real dentry (data or metadata)\n *\n * If dentry is on a union/overlay, then return the underlying, real dentry.\n * Otherwise return the dentry itself.\n *\n * See also: Documentation/filesystems/vfs.rst\n */\nstatic inline struct dentry *d_real(struct dentry *dentry, enum d_real_type type)\n{\n\tif (unlikely(dentry->d_flags & DCACHE_OP_REAL))\n\t\treturn dentry->d_op->d_real(dentry, type);\n\telse\n\t\treturn dentry;\n}\n\n/**\n * d_real_inode - Return the real inode hosting the data\n * @dentry: The dentry to query\n *\n * If dentry is on a union/overlay, then return the underlying, real inode.\n * Otherwise return d_inode().\n */\nstatic inline struct inode *d_real_inode(const struct dentry *dentry)\n{\n\t/* This usage of d_real() results in const dentry */\n\treturn d_inode(d_real((struct dentry *) dentry, D_REAL_DATA));\n}\n\nstruct name_snapshot {\n\tstruct qstr name;\n\tunion shortname_store inline_name;\n};\nvoid take_dentry_name_snapshot(struct name_snapshot *, struct dentry *);\nvoid release_dentry_name_snapshot(struct name_snapshot *);\n\nstatic inline struct dentry *d_first_child(const struct dentry *dentry)\n{\n\treturn hlist_entry_safe(dentry->d_children.first, struct dentry, d_sib);\n}\n\nstatic inline struct dentry *d_next_sibling(const struct dentry *dentry)\n{\n\treturn hlist_entry_safe(dentry->d_sib.next, struct dentry, d_sib);\n}\n\nvoid set_default_d_op(struct super_block *, const struct dentry_operations *);\n\n#endif\t/* __LINUX_DCACHE_H */\n", "patch": "@@ -565,4 +565,16 @@ static inline struct dentry *d_real(struct dentry *dentry)\n \t\treturn dentry;\n }\n \n+static inline struct inode *vfs_select_inode(struct dentry *dentry,\n+\t\t\t\t\t     unsigned open_flags)\n+{\n+\tstruct inode *inode = d_inode(dentry);\n+\n+\tif (inode && unlikely(dentry->d_flags & DCACHE_OP_SELECT_INODE))\n+\t\tinode = dentry->d_op->d_select_inode(dentry, open_flags);\n+\n+\treturn inode;\n+}\n+\n+\n #endif\t/* __LINUX_DCACHE_H */", "file_path": "files/2016_8\\98", "file_language": "h", "file_name": "include/linux/dcache.h", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 37, "cve_id": "CVE-2016-6198", "cwe_id": ["CWE-284"], "cve_language": "C", "cve_description": "The filesystem layer in the Linux kernel before 4.5.5 proceeds with post-rename operations after an OverlayFS file is renamed to a self-hardlink, which allows local users to cause a denial of service (system crash) via a rename system call, related to fs/namei.c and fs/open.c.", "cvss": "5.5", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "NONE", "A": "HIGH", "commit_id": "9409e22acdfc9153f88d9b1ed2bd2a5b34d2d3ca", "commit_message": "vfs: rename: check backing inode being equal\n\nIf a file is renamed to a hardlink of itself POSIX specifies that rename(2)\nshould do nothing and return success.\n\nThis condition is checked in vfs_rename().  However it won't detect hard\nlinks on overlayfs where these are given separate inodes on the overlayfs\nlayer.\n\nOverlayfs itself detects this condition and returns success without doing\nanything, but then vfs_rename() will proceed as if this was a successful\nrename (detach_mounts(), d_move()).\n\nThe correct thing to do is to detect this condition before even calling\ninto overlayfs.  This patch does this by calling vfs_select_inode() to get\nthe underlying inodes.\n\nSigned-off-by: Miklos Szeredi <mszeredi@redhat.com>\nCc: <stable@vger.kernel.org> # v4.2+", "commit_date": "2016-05-11T03:55:43Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/9409e22acdfc9153f88d9b1ed2bd2a5b34d2d3ca", "html_url": "https://github.com/torvalds/linux/commit/9409e22acdfc9153f88d9b1ed2bd2a5b34d2d3ca", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "54d5ca871e72f2bb172ec9323497f01cd5091ec7", "url_before": "https://api.github.com/repos/torvalds/linux/commits/54d5ca871e72f2bb172ec9323497f01cd5091ec7", "html_url_before": "https://github.com/torvalds/linux/commit/54d5ca871e72f2bb172ec9323497f01cd5091ec7"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/9409e22acdfc9153f88d9b1ed2bd2a5b34d2d3ca/fs/namei.c", "code": "/*\n *  linux/fs/namei.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n * Some corrections by tytso.\n */\n\n/* [Feb 1997 T. Schoebel-Theuer] Complete rewrite of the pathname\n * lookup logic.\n */\n/* [Feb-Apr 2000, AV] Rewrite to the new namespace architecture.\n */\n\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/pagemap.h>\n#include <linux/fsnotify.h>\n#include <linux/personality.h>\n#include <linux/security.h>\n#include <linux/ima.h>\n#include <linux/syscalls.h>\n#include <linux/mount.h>\n#include <linux/audit.h>\n#include <linux/capability.h>\n#include <linux/file.h>\n#include <linux/fcntl.h>\n#include <linux/device_cgroup.h>\n#include <linux/fs_struct.h>\n#include <linux/posix_acl.h>\n#include <linux/hash.h>\n#include <asm/uaccess.h>\n\n#include \"internal.h\"\n#include \"mount.h\"\n\n/* [Feb-1997 T. Schoebel-Theuer]\n * Fundamental changes in the pathname lookup mechanisms (namei)\n * were necessary because of omirr.  The reason is that omirr needs\n * to know the _real_ pathname, not the user-supplied one, in case\n * of symlinks (and also when transname replacements occur).\n *\n * The new code replaces the old recursive symlink resolution with\n * an iterative one (in case of non-nested symlink chains).  It does\n * this with calls to <fs>_follow_link().\n * As a side effect, dir_namei(), _namei() and follow_link() are now \n * replaced with a single function lookup_dentry() that can handle all \n * the special cases of the former code.\n *\n * With the new dcache, the pathname is stored at each inode, at least as\n * long as the refcount of the inode is positive.  As a side effect, the\n * size of the dcache depends on the inode cache and thus is dynamic.\n *\n * [29-Apr-1998 C. Scott Ananian] Updated above description of symlink\n * resolution to correspond with current state of the code.\n *\n * Note that the symlink resolution is not *completely* iterative.\n * There is still a significant amount of tail- and mid- recursion in\n * the algorithm.  Also, note that <fs>_readlink() is not used in\n * lookup_dentry(): lookup_dentry() on the result of <fs>_readlink()\n * may return different results than <fs>_follow_link().  Many virtual\n * filesystems (including /proc) exhibit this behavior.\n */\n\n/* [24-Feb-97 T. Schoebel-Theuer] Side effects caused by new implementation:\n * New symlink semantics: when open() is called with flags O_CREAT | O_EXCL\n * and the name already exists in form of a symlink, try to create the new\n * name indicated by the symlink. The old code always complained that the\n * name already exists, due to not following the symlink even if its target\n * is nonexistent.  The new semantics affects also mknod() and link() when\n * the name is a symlink pointing to a non-existent name.\n *\n * I don't know which semantics is the right one, since I have no access\n * to standards. But I found by trial that HP-UX 9.0 has the full \"new\"\n * semantics implemented, while SunOS 4.1.1 and Solaris (SunOS 5.4) have the\n * \"old\" one. Personally, I think the new semantics is much more logical.\n * Note that \"ln old new\" where \"new\" is a symlink pointing to a non-existing\n * file does succeed in both HP-UX and SunOs, but not in Solaris\n * and in the old Linux semantics.\n */\n\n/* [16-Dec-97 Kevin Buhr] For security reasons, we change some symlink\n * semantics.  See the comments in \"open_namei\" and \"do_link\" below.\n *\n * [10-Sep-98 Alan Modra] Another symlink change.\n */\n\n/* [Feb-Apr 2000 AV] Complete rewrite. Rules for symlinks:\n *\tinside the path - always follow.\n *\tin the last component in creation/removal/renaming - never follow.\n *\tif LOOKUP_FOLLOW passed - follow.\n *\tif the pathname has trailing slashes - follow.\n *\totherwise - don't follow.\n * (applied in that order).\n *\n * [Jun 2000 AV] Inconsistent behaviour of open() in case if flags==O_CREAT\n * restored for 2.4. This is the last surviving part of old 4.2BSD bug.\n * During the 2.4 we need to fix the userland stuff depending on it -\n * hopefully we will be able to get rid of that wart in 2.5. So far only\n * XEmacs seems to be relying on it...\n */\n/*\n * [Sep 2001 AV] Single-semaphore locking scheme (kudos to David Holland)\n * implemented.  Let's see if raised priority of ->s_vfs_rename_mutex gives\n * any extra contention...\n */\n\n/* In order to reduce some races, while at the same time doing additional\n * checking and hopefully speeding things up, we copy filenames to the\n * kernel data space before using them..\n *\n * POSIX.1 2.4: an empty pathname is invalid (ENOENT).\n * PATH_MAX includes the nul terminator --RR.\n */\n\n#define EMBEDDED_NAME_MAX\t(PATH_MAX - offsetof(struct filename, iname))\n\nstruct filename *\ngetname_flags(const char __user *filename, int flags, int *empty)\n{\n\tstruct filename *result;\n\tchar *kname;\n\tint len;\n\n\tresult = audit_reusename(filename);\n\tif (result)\n\t\treturn result;\n\n\tresult = __getname();\n\tif (unlikely(!result))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * First, try to embed the struct filename inside the names_cache\n\t * allocation\n\t */\n\tkname = (char *)result->iname;\n\tresult->name = kname;\n\n\tlen = strncpy_from_user(kname, filename, EMBEDDED_NAME_MAX);\n\tif (unlikely(len < 0)) {\n\t\t__putname(result);\n\t\treturn ERR_PTR(len);\n\t}\n\n\t/*\n\t * Uh-oh. We have a name that's approaching PATH_MAX. Allocate a\n\t * separate struct filename so we can dedicate the entire\n\t * names_cache allocation for the pathname, and re-do the copy from\n\t * userland.\n\t */\n\tif (unlikely(len == EMBEDDED_NAME_MAX)) {\n\t\tconst size_t size = offsetof(struct filename, iname[1]);\n\t\tkname = (char *)result;\n\n\t\t/*\n\t\t * size is chosen that way we to guarantee that\n\t\t * result->iname[0] is within the same object and that\n\t\t * kname can't be equal to result->iname, no matter what.\n\t\t */\n\t\tresult = kzalloc(size, GFP_KERNEL);\n\t\tif (unlikely(!result)) {\n\t\t\t__putname(kname);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tresult->name = kname;\n\t\tlen = strncpy_from_user(kname, filename, PATH_MAX);\n\t\tif (unlikely(len < 0)) {\n\t\t\t__putname(kname);\n\t\t\tkfree(result);\n\t\t\treturn ERR_PTR(len);\n\t\t}\n\t\tif (unlikely(len == PATH_MAX)) {\n\t\t\t__putname(kname);\n\t\t\tkfree(result);\n\t\t\treturn ERR_PTR(-ENAMETOOLONG);\n\t\t}\n\t}\n\n\tresult->refcnt = 1;\n\t/* The empty path is special. */\n\tif (unlikely(!len)) {\n\t\tif (empty)\n\t\t\t*empty = 1;\n\t\tif (!(flags & LOOKUP_EMPTY)) {\n\t\t\tputname(result);\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\t}\n\t}\n\n\tresult->uptr = filename;\n\tresult->aname = NULL;\n\taudit_getname(result);\n\treturn result;\n}\n\nstruct filename *\ngetname(const char __user * filename)\n{\n\treturn getname_flags(filename, 0, NULL);\n}\n\nstruct filename *\ngetname_kernel(const char * filename)\n{\n\tstruct filename *result;\n\tint len = strlen(filename) + 1;\n\n\tresult = __getname();\n\tif (unlikely(!result))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (len <= EMBEDDED_NAME_MAX) {\n\t\tresult->name = (char *)result->iname;\n\t} else if (len <= PATH_MAX) {\n\t\tstruct filename *tmp;\n\n\t\ttmp = kmalloc(sizeof(*tmp), GFP_KERNEL);\n\t\tif (unlikely(!tmp)) {\n\t\t\t__putname(result);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\ttmp->name = (char *)result;\n\t\tresult = tmp;\n\t} else {\n\t\t__putname(result);\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\t}\n\tmemcpy((char *)result->name, filename, len);\n\tresult->uptr = NULL;\n\tresult->aname = NULL;\n\tresult->refcnt = 1;\n\taudit_getname(result);\n\n\treturn result;\n}\n\nvoid putname(struct filename *name)\n{\n\tBUG_ON(name->refcnt <= 0);\n\n\tif (--name->refcnt > 0)\n\t\treturn;\n\n\tif (name->name != name->iname) {\n\t\t__putname(name->name);\n\t\tkfree(name);\n\t} else\n\t\t__putname(name);\n}\n\nstatic int check_acl(struct inode *inode, int mask)\n{\n#ifdef CONFIG_FS_POSIX_ACL\n\tstruct posix_acl *acl;\n\n\tif (mask & MAY_NOT_BLOCK) {\n\t\tacl = get_cached_acl_rcu(inode, ACL_TYPE_ACCESS);\n\t        if (!acl)\n\t                return -EAGAIN;\n\t\t/* no ->get_acl() calls in RCU mode... */\n\t\tif (acl == ACL_NOT_CACHED)\n\t\t\treturn -ECHILD;\n\t        return posix_acl_permission(inode, acl, mask & ~MAY_NOT_BLOCK);\n\t}\n\n\tacl = get_acl(inode, ACL_TYPE_ACCESS);\n\tif (IS_ERR(acl))\n\t\treturn PTR_ERR(acl);\n\tif (acl) {\n\t        int error = posix_acl_permission(inode, acl, mask);\n\t        posix_acl_release(acl);\n\t        return error;\n\t}\n#endif\n\n\treturn -EAGAIN;\n}\n\n/*\n * This does the basic permission checking\n */\nstatic int acl_permission_check(struct inode *inode, int mask)\n{\n\tunsigned int mode = inode->i_mode;\n\n\tif (likely(uid_eq(current_fsuid(), inode->i_uid)))\n\t\tmode >>= 6;\n\telse {\n\t\tif (IS_POSIXACL(inode) && (mode & S_IRWXG)) {\n\t\t\tint error = check_acl(inode, mask);\n\t\t\tif (error != -EAGAIN)\n\t\t\t\treturn error;\n\t\t}\n\n\t\tif (in_group_p(inode->i_gid))\n\t\t\tmode >>= 3;\n\t}\n\n\t/*\n\t * If the DACs are ok we don't need any capability check.\n\t */\n\tif ((mask & ~mode & (MAY_READ | MAY_WRITE | MAY_EXEC)) == 0)\n\t\treturn 0;\n\treturn -EACCES;\n}\n\n/**\n * generic_permission -  check for access rights on a Posix-like filesystem\n * @inode:\tinode to check access rights for\n * @mask:\tright to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC, ...)\n *\n * Used to check for read/write/execute permissions on a file.\n * We use \"fsuid\" for this, letting us set arbitrary permissions\n * for filesystem access without changing the \"normal\" uids which\n * are used for other things.\n *\n * generic_permission is rcu-walk aware. It returns -ECHILD in case an rcu-walk\n * request cannot be satisfied (eg. requires blocking or too much complexity).\n * It would then be called again in ref-walk mode.\n */\nint generic_permission(struct inode *inode, int mask)\n{\n\tint ret;\n\n\t/*\n\t * Do the basic permission checks.\n\t */\n\tret = acl_permission_check(inode, mask);\n\tif (ret != -EACCES)\n\t\treturn ret;\n\n\tif (S_ISDIR(inode->i_mode)) {\n\t\t/* DACs are overridable for directories */\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\t\tif (!(mask & MAY_WRITE))\n\t\t\tif (capable_wrt_inode_uidgid(inode,\n\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))\n\t\t\t\treturn 0;\n\t\treturn -EACCES;\n\t}\n\t/*\n\t * Read/write DACs are always overridable.\n\t * Executable DACs are overridable when there is\n\t * at least one exec bit set.\n\t */\n\tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\n\t/*\n\t * Searching includes executable on directories, else just read.\n\t */\n\tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n\tif (mask == MAY_READ)\n\t\tif (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))\n\t\t\treturn 0;\n\n\treturn -EACCES;\n}\nEXPORT_SYMBOL(generic_permission);\n\n/*\n * We _really_ want to just do \"generic_permission()\" without\n * even looking at the inode->i_op values. So we keep a cache\n * flag in inode->i_opflags, that says \"this has not special\n * permission function, use the fast case\".\n */\nstatic inline int do_inode_permission(struct inode *inode, int mask)\n{\n\tif (unlikely(!(inode->i_opflags & IOP_FASTPERM))) {\n\t\tif (likely(inode->i_op->permission))\n\t\t\treturn inode->i_op->permission(inode, mask);\n\n\t\t/* This gets set once for the inode lifetime */\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_opflags |= IOP_FASTPERM;\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\treturn generic_permission(inode, mask);\n}\n\n/**\n * __inode_permission - Check for access rights to a given inode\n * @inode: Inode to check permission on\n * @mask: Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)\n *\n * Check for read/write/execute permissions on an inode.\n *\n * When checking for MAY_APPEND, MAY_WRITE must also be set in @mask.\n *\n * This does not check for a read-only file system.  You probably want\n * inode_permission().\n */\nint __inode_permission(struct inode *inode, int mask)\n{\n\tint retval;\n\n\tif (unlikely(mask & MAY_WRITE)) {\n\t\t/*\n\t\t * Nobody gets write access to an immutable file.\n\t\t */\n\t\tif (IS_IMMUTABLE(inode))\n\t\t\treturn -EACCES;\n\t}\n\n\tretval = do_inode_permission(inode, mask);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = devcgroup_inode_permission(inode, mask);\n\tif (retval)\n\t\treturn retval;\n\n\treturn security_inode_permission(inode, mask);\n}\nEXPORT_SYMBOL(__inode_permission);\n\n/**\n * sb_permission - Check superblock-level permissions\n * @sb: Superblock of inode to check permission on\n * @inode: Inode to check permission on\n * @mask: Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)\n *\n * Separate out file-system wide checks from inode-specific permission checks.\n */\nstatic int sb_permission(struct super_block *sb, struct inode *inode, int mask)\n{\n\tif (unlikely(mask & MAY_WRITE)) {\n\t\tumode_t mode = inode->i_mode;\n\n\t\t/* Nobody gets write access to a read-only fs. */\n\t\tif ((sb->s_flags & MS_RDONLY) &&\n\t\t    (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)))\n\t\t\treturn -EROFS;\n\t}\n\treturn 0;\n}\n\n/**\n * inode_permission - Check for access rights to a given inode\n * @inode: Inode to check permission on\n * @mask: Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)\n *\n * Check for read/write/execute permissions on an inode.  We use fs[ug]id for\n * this, letting us set arbitrary permissions for filesystem access without\n * changing the \"normal\" UIDs which are used for other things.\n *\n * When checking for MAY_APPEND, MAY_WRITE must also be set in @mask.\n */\nint inode_permission(struct inode *inode, int mask)\n{\n\tint retval;\n\n\tretval = sb_permission(inode->i_sb, inode, mask);\n\tif (retval)\n\t\treturn retval;\n\treturn __inode_permission(inode, mask);\n}\nEXPORT_SYMBOL(inode_permission);\n\n/**\n * path_get - get a reference to a path\n * @path: path to get the reference to\n *\n * Given a path increment the reference count to the dentry and the vfsmount.\n */\nvoid path_get(const struct path *path)\n{\n\tmntget(path->mnt);\n\tdget(path->dentry);\n}\nEXPORT_SYMBOL(path_get);\n\n/**\n * path_put - put a reference to a path\n * @path: path to put the reference to\n *\n * Given a path decrement the reference count to the dentry and the vfsmount.\n */\nvoid path_put(const struct path *path)\n{\n\tdput(path->dentry);\n\tmntput(path->mnt);\n}\nEXPORT_SYMBOL(path_put);\n\n#define EMBEDDED_LEVELS 2\nstruct nameidata {\n\tstruct path\tpath;\n\tstruct qstr\tlast;\n\tstruct path\troot;\n\tstruct inode\t*inode; /* path.dentry.d_inode */\n\tunsigned int\tflags;\n\tunsigned\tseq, m_seq;\n\tint\t\tlast_type;\n\tunsigned\tdepth;\n\tint\t\ttotal_link_count;\n\tstruct saved {\n\t\tstruct path link;\n\t\tstruct delayed_call done;\n\t\tconst char *name;\n\t\tunsigned seq;\n\t} *stack, internal[EMBEDDED_LEVELS];\n\tstruct filename\t*name;\n\tstruct nameidata *saved;\n\tstruct inode\t*link_inode;\n\tunsigned\troot_seq;\n\tint\t\tdfd;\n};\n\nstatic void set_nameidata(struct nameidata *p, int dfd, struct filename *name)\n{\n\tstruct nameidata *old = current->nameidata;\n\tp->stack = p->internal;\n\tp->dfd = dfd;\n\tp->name = name;\n\tp->total_link_count = old ? old->total_link_count : 0;\n\tp->saved = old;\n\tcurrent->nameidata = p;\n}\n\nstatic void restore_nameidata(void)\n{\n\tstruct nameidata *now = current->nameidata, *old = now->saved;\n\n\tcurrent->nameidata = old;\n\tif (old)\n\t\told->total_link_count = now->total_link_count;\n\tif (now->stack != now->internal)\n\t\tkfree(now->stack);\n}\n\nstatic int __nd_alloc_stack(struct nameidata *nd)\n{\n\tstruct saved *p;\n\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tp= kmalloc(MAXSYMLINKS * sizeof(struct saved),\n\t\t\t\t  GFP_ATOMIC);\n\t\tif (unlikely(!p))\n\t\t\treturn -ECHILD;\n\t} else {\n\t\tp= kmalloc(MAXSYMLINKS * sizeof(struct saved),\n\t\t\t\t  GFP_KERNEL);\n\t\tif (unlikely(!p))\n\t\t\treturn -ENOMEM;\n\t}\n\tmemcpy(p, nd->internal, sizeof(nd->internal));\n\tnd->stack = p;\n\treturn 0;\n}\n\n/**\n * path_connected - Verify that a path->dentry is below path->mnt.mnt_root\n * @path: nameidate to verify\n *\n * Rename can sometimes move a file or directory outside of a bind\n * mount, path_connected allows those cases to be detected.\n */\nstatic bool path_connected(const struct path *path)\n{\n\tstruct vfsmount *mnt = path->mnt;\n\n\t/* Only bind mounts can have disconnected paths */\n\tif (mnt->mnt_root == mnt->mnt_sb->s_root)\n\t\treturn true;\n\n\treturn is_subdir(path->dentry, mnt->mnt_root);\n}\n\nstatic inline int nd_alloc_stack(struct nameidata *nd)\n{\n\tif (likely(nd->depth != EMBEDDED_LEVELS))\n\t\treturn 0;\n\tif (likely(nd->stack != nd->internal))\n\t\treturn 0;\n\treturn __nd_alloc_stack(nd);\n}\n\nstatic void drop_links(struct nameidata *nd)\n{\n\tint i = nd->depth;\n\twhile (i--) {\n\t\tstruct saved *last = nd->stack + i;\n\t\tdo_delayed_call(&last->done);\n\t\tclear_delayed_call(&last->done);\n\t}\n}\n\nstatic void terminate_walk(struct nameidata *nd)\n{\n\tdrop_links(nd);\n\tif (!(nd->flags & LOOKUP_RCU)) {\n\t\tint i;\n\t\tpath_put(&nd->path);\n\t\tfor (i = 0; i < nd->depth; i++)\n\t\t\tpath_put(&nd->stack[i].link);\n\t\tif (nd->root.mnt && !(nd->flags & LOOKUP_ROOT)) {\n\t\t\tpath_put(&nd->root);\n\t\t\tnd->root.mnt = NULL;\n\t\t}\n\t} else {\n\t\tnd->flags &= ~LOOKUP_RCU;\n\t\tif (!(nd->flags & LOOKUP_ROOT))\n\t\t\tnd->root.mnt = NULL;\n\t\trcu_read_unlock();\n\t}\n\tnd->depth = 0;\n}\n\n/* path_put is needed afterwards regardless of success or failure */\nstatic bool legitimize_path(struct nameidata *nd,\n\t\t\t    struct path *path, unsigned seq)\n{\n\tint res = __legitimize_mnt(path->mnt, nd->m_seq);\n\tif (unlikely(res)) {\n\t\tif (res > 0)\n\t\t\tpath->mnt = NULL;\n\t\tpath->dentry = NULL;\n\t\treturn false;\n\t}\n\tif (unlikely(!lockref_get_not_dead(&path->dentry->d_lockref))) {\n\t\tpath->dentry = NULL;\n\t\treturn false;\n\t}\n\treturn !read_seqcount_retry(&path->dentry->d_seq, seq);\n}\n\nstatic bool legitimize_links(struct nameidata *nd)\n{\n\tint i;\n\tfor (i = 0; i < nd->depth; i++) {\n\t\tstruct saved *last = nd->stack + i;\n\t\tif (unlikely(!legitimize_path(nd, &last->link, last->seq))) {\n\t\t\tdrop_links(nd);\n\t\t\tnd->depth = i + 1;\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\n/*\n * Path walking has 2 modes, rcu-walk and ref-walk (see\n * Documentation/filesystems/path-lookup.txt).  In situations when we can't\n * continue in RCU mode, we attempt to drop out of rcu-walk mode and grab\n * normal reference counts on dentries and vfsmounts to transition to ref-walk\n * mode.  Refcounts are grabbed at the last known good point before rcu-walk\n * got stuck, so ref-walk may continue from there. If this is not successful\n * (eg. a seqcount has changed), then failure is returned and it's up to caller\n * to restart the path walk from the beginning in ref-walk mode.\n */\n\n/**\n * unlazy_walk - try to switch to ref-walk mode.\n * @nd: nameidata pathwalk data\n * @dentry: child of nd->path.dentry or NULL\n * @seq: seq number to check dentry against\n * Returns: 0 on success, -ECHILD on failure\n *\n * unlazy_walk attempts to legitimize the current nd->path, nd->root and dentry\n * for ref-walk mode.  @dentry must be a path found by a do_lookup call on\n * @nd or NULL.  Must be called from rcu-walk context.\n * Nothing should touch nameidata between unlazy_walk() failure and\n * terminate_walk().\n */\nstatic int unlazy_walk(struct nameidata *nd, struct dentry *dentry, unsigned seq)\n{\n\tstruct dentry *parent = nd->path.dentry;\n\n\tBUG_ON(!(nd->flags & LOOKUP_RCU));\n\n\tnd->flags &= ~LOOKUP_RCU;\n\tif (unlikely(!legitimize_links(nd)))\n\t\tgoto out2;\n\tif (unlikely(!legitimize_mnt(nd->path.mnt, nd->m_seq)))\n\t\tgoto out2;\n\tif (unlikely(!lockref_get_not_dead(&parent->d_lockref)))\n\t\tgoto out1;\n\n\t/*\n\t * For a negative lookup, the lookup sequence point is the parents\n\t * sequence point, and it only needs to revalidate the parent dentry.\n\t *\n\t * For a positive lookup, we need to move both the parent and the\n\t * dentry from the RCU domain to be properly refcounted. And the\n\t * sequence number in the dentry validates *both* dentry counters,\n\t * since we checked the sequence number of the parent after we got\n\t * the child sequence number. So we know the parent must still\n\t * be valid if the child sequence number is still valid.\n\t */\n\tif (!dentry) {\n\t\tif (read_seqcount_retry(&parent->d_seq, nd->seq))\n\t\t\tgoto out;\n\t\tBUG_ON(nd->inode != parent->d_inode);\n\t} else {\n\t\tif (!lockref_get_not_dead(&dentry->d_lockref))\n\t\t\tgoto out;\n\t\tif (read_seqcount_retry(&dentry->d_seq, seq))\n\t\t\tgoto drop_dentry;\n\t}\n\n\t/*\n\t * Sequence counts matched. Now make sure that the root is\n\t * still valid and get it if required.\n\t */\n\tif (nd->root.mnt && !(nd->flags & LOOKUP_ROOT)) {\n\t\tif (unlikely(!legitimize_path(nd, &nd->root, nd->root_seq))) {\n\t\t\trcu_read_unlock();\n\t\t\tdput(dentry);\n\t\t\treturn -ECHILD;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn 0;\n\ndrop_dentry:\n\trcu_read_unlock();\n\tdput(dentry);\n\tgoto drop_root_mnt;\nout2:\n\tnd->path.mnt = NULL;\nout1:\n\tnd->path.dentry = NULL;\nout:\n\trcu_read_unlock();\ndrop_root_mnt:\n\tif (!(nd->flags & LOOKUP_ROOT))\n\t\tnd->root.mnt = NULL;\n\treturn -ECHILD;\n}\n\nstatic int unlazy_link(struct nameidata *nd, struct path *link, unsigned seq)\n{\n\tif (unlikely(!legitimize_path(nd, link, seq))) {\n\t\tdrop_links(nd);\n\t\tnd->depth = 0;\n\t\tnd->flags &= ~LOOKUP_RCU;\n\t\tnd->path.mnt = NULL;\n\t\tnd->path.dentry = NULL;\n\t\tif (!(nd->flags & LOOKUP_ROOT))\n\t\t\tnd->root.mnt = NULL;\n\t\trcu_read_unlock();\n\t} else if (likely(unlazy_walk(nd, NULL, 0)) == 0) {\n\t\treturn 0;\n\t}\n\tpath_put(link);\n\treturn -ECHILD;\n}\n\nstatic inline int d_revalidate(struct dentry *dentry, unsigned int flags)\n{\n\treturn dentry->d_op->d_revalidate(dentry, flags);\n}\n\n/**\n * complete_walk - successful completion of path walk\n * @nd:  pointer nameidata\n *\n * If we had been in RCU mode, drop out of it and legitimize nd->path.\n * Revalidate the final result, unless we'd already done that during\n * the path walk or the filesystem doesn't ask for it.  Return 0 on\n * success, -error on failure.  In case of failure caller does not\n * need to drop nd->path.\n */\nstatic int complete_walk(struct nameidata *nd)\n{\n\tstruct dentry *dentry = nd->path.dentry;\n\tint status;\n\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tif (!(nd->flags & LOOKUP_ROOT))\n\t\t\tnd->root.mnt = NULL;\n\t\tif (unlikely(unlazy_walk(nd, NULL, 0)))\n\t\t\treturn -ECHILD;\n\t}\n\n\tif (likely(!(nd->flags & LOOKUP_JUMPED)))\n\t\treturn 0;\n\n\tif (likely(!(dentry->d_flags & DCACHE_OP_WEAK_REVALIDATE)))\n\t\treturn 0;\n\n\tstatus = dentry->d_op->d_weak_revalidate(dentry, nd->flags);\n\tif (status > 0)\n\t\treturn 0;\n\n\tif (!status)\n\t\tstatus = -ESTALE;\n\n\treturn status;\n}\n\nstatic void set_root(struct nameidata *nd)\n{\n\tstruct fs_struct *fs = current->fs;\n\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tunsigned seq;\n\n\t\tdo {\n\t\t\tseq = read_seqcount_begin(&fs->seq);\n\t\t\tnd->root = fs->root;\n\t\t\tnd->root_seq = __read_seqcount_begin(&nd->root.dentry->d_seq);\n\t\t} while (read_seqcount_retry(&fs->seq, seq));\n\t} else {\n\t\tget_fs_root(fs, &nd->root);\n\t}\n}\n\nstatic void path_put_conditional(struct path *path, struct nameidata *nd)\n{\n\tdput(path->dentry);\n\tif (path->mnt != nd->path.mnt)\n\t\tmntput(path->mnt);\n}\n\nstatic inline void path_to_nameidata(const struct path *path,\n\t\t\t\t\tstruct nameidata *nd)\n{\n\tif (!(nd->flags & LOOKUP_RCU)) {\n\t\tdput(nd->path.dentry);\n\t\tif (nd->path.mnt != path->mnt)\n\t\t\tmntput(nd->path.mnt);\n\t}\n\tnd->path.mnt = path->mnt;\n\tnd->path.dentry = path->dentry;\n}\n\nstatic int nd_jump_root(struct nameidata *nd)\n{\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tstruct dentry *d;\n\t\tnd->path = nd->root;\n\t\td = nd->path.dentry;\n\t\tnd->inode = d->d_inode;\n\t\tnd->seq = nd->root_seq;\n\t\tif (unlikely(read_seqcount_retry(&d->d_seq, nd->seq)))\n\t\t\treturn -ECHILD;\n\t} else {\n\t\tpath_put(&nd->path);\n\t\tnd->path = nd->root;\n\t\tpath_get(&nd->path);\n\t\tnd->inode = nd->path.dentry->d_inode;\n\t}\n\tnd->flags |= LOOKUP_JUMPED;\n\treturn 0;\n}\n\n/*\n * Helper to directly jump to a known parsed path from ->get_link,\n * caller must have taken a reference to path beforehand.\n */\nvoid nd_jump_link(struct path *path)\n{\n\tstruct nameidata *nd = current->nameidata;\n\tpath_put(&nd->path);\n\n\tnd->path = *path;\n\tnd->inode = nd->path.dentry->d_inode;\n\tnd->flags |= LOOKUP_JUMPED;\n}\n\nstatic inline void put_link(struct nameidata *nd)\n{\n\tstruct saved *last = nd->stack + --nd->depth;\n\tdo_delayed_call(&last->done);\n\tif (!(nd->flags & LOOKUP_RCU))\n\t\tpath_put(&last->link);\n}\n\nint sysctl_protected_symlinks __read_mostly = 0;\nint sysctl_protected_hardlinks __read_mostly = 0;\n\n/**\n * may_follow_link - Check symlink following for unsafe situations\n * @nd: nameidata pathwalk data\n *\n * In the case of the sysctl_protected_symlinks sysctl being enabled,\n * CAP_DAC_OVERRIDE needs to be specifically ignored if the symlink is\n * in a sticky world-writable directory. This is to protect privileged\n * processes from failing races against path names that may change out\n * from under them by way of other users creating malicious symlinks.\n * It will permit symlinks to be followed only when outside a sticky\n * world-writable directory, or when the uid of the symlink and follower\n * match, or when the directory owner matches the symlink's owner.\n *\n * Returns 0 if following the symlink is allowed, -ve on error.\n */\nstatic inline int may_follow_link(struct nameidata *nd)\n{\n\tconst struct inode *inode;\n\tconst struct inode *parent;\n\n\tif (!sysctl_protected_symlinks)\n\t\treturn 0;\n\n\t/* Allowed if owner and follower match. */\n\tinode = nd->link_inode;\n\tif (uid_eq(current_cred()->fsuid, inode->i_uid))\n\t\treturn 0;\n\n\t/* Allowed if parent directory not sticky and world-writable. */\n\tparent = nd->inode;\n\tif ((parent->i_mode & (S_ISVTX|S_IWOTH)) != (S_ISVTX|S_IWOTH))\n\t\treturn 0;\n\n\t/* Allowed if parent directory and link owner match. */\n\tif (uid_eq(parent->i_uid, inode->i_uid))\n\t\treturn 0;\n\n\tif (nd->flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\taudit_log_link_denied(\"follow_link\", &nd->stack[0].link);\n\treturn -EACCES;\n}\n\n/**\n * safe_hardlink_source - Check for safe hardlink conditions\n * @inode: the source inode to hardlink from\n *\n * Return false if at least one of the following conditions:\n *    - inode is not a regular file\n *    - inode is setuid\n *    - inode is setgid and group-exec\n *    - access failure for read and write\n *\n * Otherwise returns true.\n */\nstatic bool safe_hardlink_source(struct inode *inode)\n{\n\tumode_t mode = inode->i_mode;\n\n\t/* Special files should not get pinned to the filesystem. */\n\tif (!S_ISREG(mode))\n\t\treturn false;\n\n\t/* Setuid files should not get pinned to the filesystem. */\n\tif (mode & S_ISUID)\n\t\treturn false;\n\n\t/* Executable setgid files should not get pinned to the filesystem. */\n\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP))\n\t\treturn false;\n\n\t/* Hardlinking to unreadable or unwritable sources is dangerous. */\n\tif (inode_permission(inode, MAY_READ | MAY_WRITE))\n\t\treturn false;\n\n\treturn true;\n}\n\n/**\n * may_linkat - Check permissions for creating a hardlink\n * @link: the source to hardlink from\n *\n * Block hardlink when all of:\n *  - sysctl_protected_hardlinks enabled\n *  - fsuid does not match inode\n *  - hardlink source is unsafe (see safe_hardlink_source() above)\n *  - not CAP_FOWNER in a namespace with the inode owner uid mapped\n *\n * Returns 0 if successful, -ve on error.\n */\nstatic int may_linkat(struct path *link)\n{\n\tstruct inode *inode;\n\n\tif (!sysctl_protected_hardlinks)\n\t\treturn 0;\n\n\tinode = link->dentry->d_inode;\n\n\t/* Source inode owner (or CAP_FOWNER) can hardlink all they like,\n\t * otherwise, it must be a safe source.\n\t */\n\tif (inode_owner_or_capable(inode) || safe_hardlink_source(inode))\n\t\treturn 0;\n\n\taudit_log_link_denied(\"linkat\", link);\n\treturn -EPERM;\n}\n\nstatic __always_inline\nconst char *get_link(struct nameidata *nd)\n{\n\tstruct saved *last = nd->stack + nd->depth - 1;\n\tstruct dentry *dentry = last->link.dentry;\n\tstruct inode *inode = nd->link_inode;\n\tint error;\n\tconst char *res;\n\n\tif (!(nd->flags & LOOKUP_RCU)) {\n\t\ttouch_atime(&last->link);\n\t\tcond_resched();\n\t} else if (atime_needs_update(&last->link, inode)) {\n\t\tif (unlikely(unlazy_walk(nd, NULL, 0)))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\ttouch_atime(&last->link);\n\t}\n\n\terror = security_inode_follow_link(dentry, inode,\n\t\t\t\t\t   nd->flags & LOOKUP_RCU);\n\tif (unlikely(error))\n\t\treturn ERR_PTR(error);\n\n\tnd->last_type = LAST_BIND;\n\tres = inode->i_link;\n\tif (!res) {\n\t\tconst char * (*get)(struct dentry *, struct inode *,\n\t\t\t\tstruct delayed_call *);\n\t\tget = inode->i_op->get_link;\n\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\tres = get(NULL, inode, &last->done);\n\t\t\tif (res == ERR_PTR(-ECHILD)) {\n\t\t\t\tif (unlikely(unlazy_walk(nd, NULL, 0)))\n\t\t\t\t\treturn ERR_PTR(-ECHILD);\n\t\t\t\tres = get(dentry, inode, &last->done);\n\t\t\t}\n\t\t} else {\n\t\t\tres = get(dentry, inode, &last->done);\n\t\t}\n\t\tif (IS_ERR_OR_NULL(res))\n\t\t\treturn res;\n\t}\n\tif (*res == '/') {\n\t\tif (!nd->root.mnt)\n\t\t\tset_root(nd);\n\t\tif (unlikely(nd_jump_root(nd)))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\twhile (unlikely(*++res == '/'))\n\t\t\t;\n\t}\n\tif (!*res)\n\t\tres = NULL;\n\treturn res;\n}\n\n/*\n * follow_up - Find the mountpoint of path's vfsmount\n *\n * Given a path, find the mountpoint of its source file system.\n * Replace @path with the path of the mountpoint in the parent mount.\n * Up is towards /.\n *\n * Return 1 if we went up a level and 0 if we were already at the\n * root.\n */\nint follow_up(struct path *path)\n{\n\tstruct mount *mnt = real_mount(path->mnt);\n\tstruct mount *parent;\n\tstruct dentry *mountpoint;\n\n\tread_seqlock_excl(&mount_lock);\n\tparent = mnt->mnt_parent;\n\tif (parent == mnt) {\n\t\tread_sequnlock_excl(&mount_lock);\n\t\treturn 0;\n\t}\n\tmntget(&parent->mnt);\n\tmountpoint = dget(mnt->mnt_mountpoint);\n\tread_sequnlock_excl(&mount_lock);\n\tdput(path->dentry);\n\tpath->dentry = mountpoint;\n\tmntput(path->mnt);\n\tpath->mnt = &parent->mnt;\n\treturn 1;\n}\nEXPORT_SYMBOL(follow_up);\n\n/*\n * Perform an automount\n * - return -EISDIR to tell follow_managed() to stop and return the path we\n *   were called with.\n */\nstatic int follow_automount(struct path *path, struct nameidata *nd,\n\t\t\t    bool *need_mntput)\n{\n\tstruct vfsmount *mnt;\n\tint err;\n\n\tif (!path->dentry->d_op || !path->dentry->d_op->d_automount)\n\t\treturn -EREMOTE;\n\n\t/* We don't want to mount if someone's just doing a stat -\n\t * unless they're stat'ing a directory and appended a '/' to\n\t * the name.\n\t *\n\t * We do, however, want to mount if someone wants to open or\n\t * create a file of any type under the mountpoint, wants to\n\t * traverse through the mountpoint or wants to open the\n\t * mounted directory.  Also, autofs may mark negative dentries\n\t * as being automount points.  These will need the attentions\n\t * of the daemon to instantiate them before they can be used.\n\t */\n\tif (!(nd->flags & (LOOKUP_PARENT | LOOKUP_DIRECTORY |\n\t\t\t   LOOKUP_OPEN | LOOKUP_CREATE | LOOKUP_AUTOMOUNT)) &&\n\t    path->dentry->d_inode)\n\t\treturn -EISDIR;\n\n\tnd->total_link_count++;\n\tif (nd->total_link_count >= 40)\n\t\treturn -ELOOP;\n\n\tmnt = path->dentry->d_op->d_automount(path);\n\tif (IS_ERR(mnt)) {\n\t\t/*\n\t\t * The filesystem is allowed to return -EISDIR here to indicate\n\t\t * it doesn't want to automount.  For instance, autofs would do\n\t\t * this so that its userspace daemon can mount on this dentry.\n\t\t *\n\t\t * However, we can only permit this if it's a terminal point in\n\t\t * the path being looked up; if it wasn't then the remainder of\n\t\t * the path is inaccessible and we should say so.\n\t\t */\n\t\tif (PTR_ERR(mnt) == -EISDIR && (nd->flags & LOOKUP_PARENT))\n\t\t\treturn -EREMOTE;\n\t\treturn PTR_ERR(mnt);\n\t}\n\n\tif (!mnt) /* mount collision */\n\t\treturn 0;\n\n\tif (!*need_mntput) {\n\t\t/* lock_mount() may release path->mnt on error */\n\t\tmntget(path->mnt);\n\t\t*need_mntput = true;\n\t}\n\terr = finish_automount(mnt, path);\n\n\tswitch (err) {\n\tcase -EBUSY:\n\t\t/* Someone else made a mount here whilst we were busy */\n\t\treturn 0;\n\tcase 0:\n\t\tpath_put(path);\n\t\tpath->mnt = mnt;\n\t\tpath->dentry = dget(mnt->mnt_root);\n\t\treturn 0;\n\tdefault:\n\t\treturn err;\n\t}\n\n}\n\n/*\n * Handle a dentry that is managed in some way.\n * - Flagged for transit management (autofs)\n * - Flagged as mountpoint\n * - Flagged as automount point\n *\n * This may only be called in refwalk mode.\n *\n * Serialization is taken care of in namespace.c\n */\nstatic int follow_managed(struct path *path, struct nameidata *nd)\n{\n\tstruct vfsmount *mnt = path->mnt; /* held by caller, must be left alone */\n\tunsigned managed;\n\tbool need_mntput = false;\n\tint ret = 0;\n\n\t/* Given that we're not holding a lock here, we retain the value in a\n\t * local variable for each dentry as we look at it so that we don't see\n\t * the components of that value change under us */\n\twhile (managed = ACCESS_ONCE(path->dentry->d_flags),\n\t       managed &= DCACHE_MANAGED_DENTRY,\n\t       unlikely(managed != 0)) {\n\t\t/* Allow the filesystem to manage the transit without i_mutex\n\t\t * being held. */\n\t\tif (managed & DCACHE_MANAGE_TRANSIT) {\n\t\t\tBUG_ON(!path->dentry->d_op);\n\t\t\tBUG_ON(!path->dentry->d_op->d_manage);\n\t\t\tret = path->dentry->d_op->d_manage(path->dentry, false);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/* Transit to a mounted filesystem. */\n\t\tif (managed & DCACHE_MOUNTED) {\n\t\t\tstruct vfsmount *mounted = lookup_mnt(path);\n\t\t\tif (mounted) {\n\t\t\t\tdput(path->dentry);\n\t\t\t\tif (need_mntput)\n\t\t\t\t\tmntput(path->mnt);\n\t\t\t\tpath->mnt = mounted;\n\t\t\t\tpath->dentry = dget(mounted->mnt_root);\n\t\t\t\tneed_mntput = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* Something is mounted on this dentry in another\n\t\t\t * namespace and/or whatever was mounted there in this\n\t\t\t * namespace got unmounted before lookup_mnt() could\n\t\t\t * get it */\n\t\t}\n\n\t\t/* Handle an automount point */\n\t\tif (managed & DCACHE_NEED_AUTOMOUNT) {\n\t\t\tret = follow_automount(path, nd, &need_mntput);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* We didn't change the current path point */\n\t\tbreak;\n\t}\n\n\tif (need_mntput && path->mnt == mnt)\n\t\tmntput(path->mnt);\n\tif (ret == -EISDIR || !ret)\n\t\tret = 1;\n\tif (need_mntput)\n\t\tnd->flags |= LOOKUP_JUMPED;\n\tif (unlikely(ret < 0))\n\t\tpath_put_conditional(path, nd);\n\treturn ret;\n}\n\nint follow_down_one(struct path *path)\n{\n\tstruct vfsmount *mounted;\n\n\tmounted = lookup_mnt(path);\n\tif (mounted) {\n\t\tdput(path->dentry);\n\t\tmntput(path->mnt);\n\t\tpath->mnt = mounted;\n\t\tpath->dentry = dget(mounted->mnt_root);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(follow_down_one);\n\nstatic inline int managed_dentry_rcu(struct dentry *dentry)\n{\n\treturn (dentry->d_flags & DCACHE_MANAGE_TRANSIT) ?\n\t\tdentry->d_op->d_manage(dentry, true) : 0;\n}\n\n/*\n * Try to skip to top of mountpoint pile in rcuwalk mode.  Fail if\n * we meet a managed dentry that would need blocking.\n */\nstatic bool __follow_mount_rcu(struct nameidata *nd, struct path *path,\n\t\t\t       struct inode **inode, unsigned *seqp)\n{\n\tfor (;;) {\n\t\tstruct mount *mounted;\n\t\t/*\n\t\t * Don't forget we might have a non-mountpoint managed dentry\n\t\t * that wants to block transit.\n\t\t */\n\t\tswitch (managed_dentry_rcu(path->dentry)) {\n\t\tcase -ECHILD:\n\t\tdefault:\n\t\t\treturn false;\n\t\tcase -EISDIR:\n\t\t\treturn true;\n\t\tcase 0:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!d_mountpoint(path->dentry))\n\t\t\treturn !(path->dentry->d_flags & DCACHE_NEED_AUTOMOUNT);\n\n\t\tmounted = __lookup_mnt(path->mnt, path->dentry);\n\t\tif (!mounted)\n\t\t\tbreak;\n\t\tpath->mnt = &mounted->mnt;\n\t\tpath->dentry = mounted->mnt.mnt_root;\n\t\tnd->flags |= LOOKUP_JUMPED;\n\t\t*seqp = read_seqcount_begin(&path->dentry->d_seq);\n\t\t/*\n\t\t * Update the inode too. We don't need to re-check the\n\t\t * dentry sequence number here after this d_inode read,\n\t\t * because a mount-point is always pinned.\n\t\t */\n\t\t*inode = path->dentry->d_inode;\n\t}\n\treturn !read_seqretry(&mount_lock, nd->m_seq) &&\n\t\t!(path->dentry->d_flags & DCACHE_NEED_AUTOMOUNT);\n}\n\nstatic int follow_dotdot_rcu(struct nameidata *nd)\n{\n\tstruct inode *inode = nd->inode;\n\n\twhile (1) {\n\t\tif (path_equal(&nd->path, &nd->root))\n\t\t\tbreak;\n\t\tif (nd->path.dentry != nd->path.mnt->mnt_root) {\n\t\t\tstruct dentry *old = nd->path.dentry;\n\t\t\tstruct dentry *parent = old->d_parent;\n\t\t\tunsigned seq;\n\n\t\t\tinode = parent->d_inode;\n\t\t\tseq = read_seqcount_begin(&parent->d_seq);\n\t\t\tif (unlikely(read_seqcount_retry(&old->d_seq, nd->seq)))\n\t\t\t\treturn -ECHILD;\n\t\t\tnd->path.dentry = parent;\n\t\t\tnd->seq = seq;\n\t\t\tif (unlikely(!path_connected(&nd->path)))\n\t\t\t\treturn -ENOENT;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct mount *mnt = real_mount(nd->path.mnt);\n\t\t\tstruct mount *mparent = mnt->mnt_parent;\n\t\t\tstruct dentry *mountpoint = mnt->mnt_mountpoint;\n\t\t\tstruct inode *inode2 = mountpoint->d_inode;\n\t\t\tunsigned seq = read_seqcount_begin(&mountpoint->d_seq);\n\t\t\tif (unlikely(read_seqretry(&mount_lock, nd->m_seq)))\n\t\t\t\treturn -ECHILD;\n\t\t\tif (&mparent->mnt == nd->path.mnt)\n\t\t\t\tbreak;\n\t\t\t/* we know that mountpoint was pinned */\n\t\t\tnd->path.dentry = mountpoint;\n\t\t\tnd->path.mnt = &mparent->mnt;\n\t\t\tinode = inode2;\n\t\t\tnd->seq = seq;\n\t\t}\n\t}\n\twhile (unlikely(d_mountpoint(nd->path.dentry))) {\n\t\tstruct mount *mounted;\n\t\tmounted = __lookup_mnt(nd->path.mnt, nd->path.dentry);\n\t\tif (unlikely(read_seqretry(&mount_lock, nd->m_seq)))\n\t\t\treturn -ECHILD;\n\t\tif (!mounted)\n\t\t\tbreak;\n\t\tnd->path.mnt = &mounted->mnt;\n\t\tnd->path.dentry = mounted->mnt.mnt_root;\n\t\tinode = nd->path.dentry->d_inode;\n\t\tnd->seq = read_seqcount_begin(&nd->path.dentry->d_seq);\n\t}\n\tnd->inode = inode;\n\treturn 0;\n}\n\n/*\n * Follow down to the covering mount currently visible to userspace.  At each\n * point, the filesystem owning that dentry may be queried as to whether the\n * caller is permitted to proceed or not.\n */\nint follow_down(struct path *path)\n{\n\tunsigned managed;\n\tint ret;\n\n\twhile (managed = ACCESS_ONCE(path->dentry->d_flags),\n\t       unlikely(managed & DCACHE_MANAGED_DENTRY)) {\n\t\t/* Allow the filesystem to manage the transit without i_mutex\n\t\t * being held.\n\t\t *\n\t\t * We indicate to the filesystem if someone is trying to mount\n\t\t * something here.  This gives autofs the chance to deny anyone\n\t\t * other than its daemon the right to mount on its\n\t\t * superstructure.\n\t\t *\n\t\t * The filesystem may sleep at this point.\n\t\t */\n\t\tif (managed & DCACHE_MANAGE_TRANSIT) {\n\t\t\tBUG_ON(!path->dentry->d_op);\n\t\t\tBUG_ON(!path->dentry->d_op->d_manage);\n\t\t\tret = path->dentry->d_op->d_manage(\n\t\t\t\tpath->dentry, false);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret == -EISDIR ? 0 : ret;\n\t\t}\n\n\t\t/* Transit to a mounted filesystem. */\n\t\tif (managed & DCACHE_MOUNTED) {\n\t\t\tstruct vfsmount *mounted = lookup_mnt(path);\n\t\t\tif (!mounted)\n\t\t\t\tbreak;\n\t\t\tdput(path->dentry);\n\t\t\tmntput(path->mnt);\n\t\t\tpath->mnt = mounted;\n\t\t\tpath->dentry = dget(mounted->mnt_root);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Don't handle automount points here */\n\t\tbreak;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(follow_down);\n\n/*\n * Skip to top of mountpoint pile in refwalk mode for follow_dotdot()\n */\nstatic void follow_mount(struct path *path)\n{\n\twhile (d_mountpoint(path->dentry)) {\n\t\tstruct vfsmount *mounted = lookup_mnt(path);\n\t\tif (!mounted)\n\t\t\tbreak;\n\t\tdput(path->dentry);\n\t\tmntput(path->mnt);\n\t\tpath->mnt = mounted;\n\t\tpath->dentry = dget(mounted->mnt_root);\n\t}\n}\n\nstatic int follow_dotdot(struct nameidata *nd)\n{\n\twhile(1) {\n\t\tstruct dentry *old = nd->path.dentry;\n\n\t\tif (nd->path.dentry == nd->root.dentry &&\n\t\t    nd->path.mnt == nd->root.mnt) {\n\t\t\tbreak;\n\t\t}\n\t\tif (nd->path.dentry != nd->path.mnt->mnt_root) {\n\t\t\t/* rare case of legitimate dget_parent()... */\n\t\t\tnd->path.dentry = dget_parent(nd->path.dentry);\n\t\t\tdput(old);\n\t\t\tif (unlikely(!path_connected(&nd->path)))\n\t\t\t\treturn -ENOENT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!follow_up(&nd->path))\n\t\t\tbreak;\n\t}\n\tfollow_mount(&nd->path);\n\tnd->inode = nd->path.dentry->d_inode;\n\treturn 0;\n}\n\n/*\n * This looks up the name in dcache, possibly revalidates the old dentry and\n * allocates a new one if not found or not valid.  In the need_lookup argument\n * returns whether i_op->lookup is necessary.\n */\nstatic struct dentry *lookup_dcache(const struct qstr *name,\n\t\t\t\t    struct dentry *dir,\n\t\t\t\t    unsigned int flags)\n{\n\tstruct dentry *dentry;\n\tint error;\n\n\tdentry = d_lookup(dir, name);\n\tif (dentry) {\n\t\tif (dentry->d_flags & DCACHE_OP_REVALIDATE) {\n\t\t\terror = d_revalidate(dentry, flags);\n\t\t\tif (unlikely(error <= 0)) {\n\t\t\t\tif (!error)\n\t\t\t\t\td_invalidate(dentry);\n\t\t\t\tdput(dentry);\n\t\t\t\treturn ERR_PTR(error);\n\t\t\t}\n\t\t}\n\t}\n\treturn dentry;\n}\n\n/*\n * Call i_op->lookup on the dentry.  The dentry must be negative and\n * unhashed.\n *\n * dir->d_inode->i_mutex must be held\n */\nstatic struct dentry *lookup_real(struct inode *dir, struct dentry *dentry,\n\t\t\t\t  unsigned int flags)\n{\n\tstruct dentry *old;\n\n\t/* Don't create child dentry for a dead directory. */\n\tif (unlikely(IS_DEADDIR(dir))) {\n\t\tdput(dentry);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\told = dir->i_op->lookup(dir, dentry, flags);\n\tif (unlikely(old)) {\n\t\tdput(dentry);\n\t\tdentry = old;\n\t}\n\treturn dentry;\n}\n\nstatic struct dentry *__lookup_hash(const struct qstr *name,\n\t\tstruct dentry *base, unsigned int flags)\n{\n\tstruct dentry *dentry = lookup_dcache(name, base, flags);\n\n\tif (dentry)\n\t\treturn dentry;\n\n\tdentry = d_alloc(base, name);\n\tif (unlikely(!dentry))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn lookup_real(base->d_inode, dentry, flags);\n}\n\nstatic int lookup_fast(struct nameidata *nd,\n\t\t       struct path *path, struct inode **inode,\n\t\t       unsigned *seqp)\n{\n\tstruct vfsmount *mnt = nd->path.mnt;\n\tstruct dentry *dentry, *parent = nd->path.dentry;\n\tint status = 1;\n\tint err;\n\n\t/*\n\t * Rename seqlock is not required here because in the off chance\n\t * of a false negative due to a concurrent rename, the caller is\n\t * going to fall back to non-racy lookup.\n\t */\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tunsigned seq;\n\t\tbool negative;\n\t\tdentry = __d_lookup_rcu(parent, &nd->last, &seq);\n\t\tif (unlikely(!dentry)) {\n\t\t\tif (unlazy_walk(nd, NULL, 0))\n\t\t\t\treturn -ECHILD;\n\t\t\treturn 0;\n\t\t}\n\n\t\t/*\n\t\t * This sequence count validates that the inode matches\n\t\t * the dentry name information from lookup.\n\t\t */\n\t\t*inode = d_backing_inode(dentry);\n\t\tnegative = d_is_negative(dentry);\n\t\tif (unlikely(read_seqcount_retry(&dentry->d_seq, seq)))\n\t\t\treturn -ECHILD;\n\n\t\t/*\n\t\t * This sequence count validates that the parent had no\n\t\t * changes while we did the lookup of the dentry above.\n\t\t *\n\t\t * The memory barrier in read_seqcount_begin of child is\n\t\t *  enough, we can use __read_seqcount_retry here.\n\t\t */\n\t\tif (unlikely(__read_seqcount_retry(&parent->d_seq, nd->seq)))\n\t\t\treturn -ECHILD;\n\n\t\t*seqp = seq;\n\t\tif (unlikely(dentry->d_flags & DCACHE_OP_REVALIDATE))\n\t\t\tstatus = d_revalidate(dentry, nd->flags);\n\t\tif (unlikely(status <= 0)) {\n\t\t\tif (unlazy_walk(nd, dentry, seq))\n\t\t\t\treturn -ECHILD;\n\t\t\tif (status == -ECHILD)\n\t\t\t\tstatus = d_revalidate(dentry, nd->flags);\n\t\t} else {\n\t\t\t/*\n\t\t\t * Note: do negative dentry check after revalidation in\n\t\t\t * case that drops it.\n\t\t\t */\n\t\t\tif (unlikely(negative))\n\t\t\t\treturn -ENOENT;\n\t\t\tpath->mnt = mnt;\n\t\t\tpath->dentry = dentry;\n\t\t\tif (likely(__follow_mount_rcu(nd, path, inode, seqp)))\n\t\t\t\treturn 1;\n\t\t\tif (unlazy_walk(nd, dentry, seq))\n\t\t\t\treturn -ECHILD;\n\t\t}\n\t} else {\n\t\tdentry = __d_lookup(parent, &nd->last);\n\t\tif (unlikely(!dentry))\n\t\t\treturn 0;\n\t\tif (unlikely(dentry->d_flags & DCACHE_OP_REVALIDATE))\n\t\t\tstatus = d_revalidate(dentry, nd->flags);\n\t}\n\tif (unlikely(status <= 0)) {\n\t\tif (!status)\n\t\t\td_invalidate(dentry);\n\t\tdput(dentry);\n\t\treturn status;\n\t}\n\tif (unlikely(d_is_negative(dentry))) {\n\t\tdput(dentry);\n\t\treturn -ENOENT;\n\t}\n\n\tpath->mnt = mnt;\n\tpath->dentry = dentry;\n\terr = follow_managed(path, nd);\n\tif (likely(err > 0))\n\t\t*inode = d_backing_inode(path->dentry);\n\treturn err;\n}\n\n/* Fast lookup failed, do it the slow way */\nstatic struct dentry *lookup_slow(const struct qstr *name,\n\t\t\t\t  struct dentry *dir,\n\t\t\t\t  unsigned int flags)\n{\n\tstruct dentry *dentry;\n\tinode_lock(dir->d_inode);\n\tdentry = d_lookup(dir, name);\n\tif (unlikely(dentry)) {\n\t\tif ((dentry->d_flags & DCACHE_OP_REVALIDATE) &&\n\t\t    !(flags & LOOKUP_NO_REVAL)) {\n\t\t\tint error = d_revalidate(dentry, flags);\n\t\t\tif (unlikely(error <= 0)) {\n\t\t\t\tif (!error)\n\t\t\t\t\td_invalidate(dentry);\n\t\t\t\tdput(dentry);\n\t\t\t\tdentry = ERR_PTR(error);\n\t\t\t}\n\t\t}\n\t\tif (dentry) {\n\t\t\tinode_unlock(dir->d_inode);\n\t\t\treturn dentry;\n\t\t}\n\t}\n\tdentry = d_alloc(dir, name);\n\tif (unlikely(!dentry)) {\n\t\tinode_unlock(dir->d_inode);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tdentry = lookup_real(dir->d_inode, dentry, flags);\n\tinode_unlock(dir->d_inode);\n\treturn dentry;\n}\n\nstatic inline int may_lookup(struct nameidata *nd)\n{\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tint err = inode_permission(nd->inode, MAY_EXEC|MAY_NOT_BLOCK);\n\t\tif (err != -ECHILD)\n\t\t\treturn err;\n\t\tif (unlazy_walk(nd, NULL, 0))\n\t\t\treturn -ECHILD;\n\t}\n\treturn inode_permission(nd->inode, MAY_EXEC);\n}\n\nstatic inline int handle_dots(struct nameidata *nd, int type)\n{\n\tif (type == LAST_DOTDOT) {\n\t\tif (!nd->root.mnt)\n\t\t\tset_root(nd);\n\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\treturn follow_dotdot_rcu(nd);\n\t\t} else\n\t\t\treturn follow_dotdot(nd);\n\t}\n\treturn 0;\n}\n\nstatic int pick_link(struct nameidata *nd, struct path *link,\n\t\t     struct inode *inode, unsigned seq)\n{\n\tint error;\n\tstruct saved *last;\n\tif (unlikely(nd->total_link_count++ >= MAXSYMLINKS)) {\n\t\tpath_to_nameidata(link, nd);\n\t\treturn -ELOOP;\n\t}\n\tif (!(nd->flags & LOOKUP_RCU)) {\n\t\tif (link->mnt == nd->path.mnt)\n\t\t\tmntget(link->mnt);\n\t}\n\terror = nd_alloc_stack(nd);\n\tif (unlikely(error)) {\n\t\tif (error == -ECHILD) {\n\t\t\tif (unlikely(unlazy_link(nd, link, seq)))\n\t\t\t\treturn -ECHILD;\n\t\t\terror = nd_alloc_stack(nd);\n\t\t}\n\t\tif (error) {\n\t\t\tpath_put(link);\n\t\t\treturn error;\n\t\t}\n\t}\n\n\tlast = nd->stack + nd->depth++;\n\tlast->link = *link;\n\tclear_delayed_call(&last->done);\n\tnd->link_inode = inode;\n\tlast->seq = seq;\n\treturn 1;\n}\n\n/*\n * Do we need to follow links? We _really_ want to be able\n * to do this check without having to look at inode->i_op,\n * so we keep a cache of \"no, this doesn't need follow_link\"\n * for the common case.\n */\nstatic inline int should_follow_link(struct nameidata *nd, struct path *link,\n\t\t\t\t     int follow,\n\t\t\t\t     struct inode *inode, unsigned seq)\n{\n\tif (likely(!d_is_symlink(link->dentry)))\n\t\treturn 0;\n\tif (!follow)\n\t\treturn 0;\n\t/* make sure that d_is_symlink above matches inode */\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tif (read_seqcount_retry(&link->dentry->d_seq, seq))\n\t\t\treturn -ECHILD;\n\t}\n\treturn pick_link(nd, link, inode, seq);\n}\n\nenum {WALK_GET = 1, WALK_PUT = 2};\n\nstatic int walk_component(struct nameidata *nd, int flags)\n{\n\tstruct path path;\n\tstruct inode *inode;\n\tunsigned seq;\n\tint err;\n\t/*\n\t * \".\" and \"..\" are special - \"..\" especially so because it has\n\t * to be able to know about the current root directory and\n\t * parent relationships.\n\t */\n\tif (unlikely(nd->last_type != LAST_NORM)) {\n\t\terr = handle_dots(nd, nd->last_type);\n\t\tif (flags & WALK_PUT)\n\t\t\tput_link(nd);\n\t\treturn err;\n\t}\n\terr = lookup_fast(nd, &path, &inode, &seq);\n\tif (unlikely(err <= 0)) {\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tpath.dentry = lookup_slow(&nd->last, nd->path.dentry,\n\t\t\t\t\t  nd->flags);\n\t\tif (IS_ERR(path.dentry))\n\t\t\treturn PTR_ERR(path.dentry);\n\n\t\tpath.mnt = nd->path.mnt;\n\t\terr = follow_managed(&path, nd);\n\t\tif (unlikely(err < 0))\n\t\t\treturn err;\n\n\t\tif (unlikely(d_is_negative(path.dentry))) {\n\t\t\tpath_to_nameidata(&path, nd);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tseq = 0;\t/* we are already out of RCU mode */\n\t\tinode = d_backing_inode(path.dentry);\n\t}\n\n\tif (flags & WALK_PUT)\n\t\tput_link(nd);\n\terr = should_follow_link(nd, &path, flags & WALK_GET, inode, seq);\n\tif (unlikely(err))\n\t\treturn err;\n\tpath_to_nameidata(&path, nd);\n\tnd->inode = inode;\n\tnd->seq = seq;\n\treturn 0;\n}\n\n/*\n * We can do the critical dentry name comparison and hashing\n * operations one word at a time, but we are limited to:\n *\n * - Architectures with fast unaligned word accesses. We could\n *   do a \"get_unaligned()\" if this helps and is sufficiently\n *   fast.\n *\n * - non-CONFIG_DEBUG_PAGEALLOC configurations (so that we\n *   do not trap on the (extremely unlikely) case of a page\n *   crossing operation.\n *\n * - Furthermore, we need an efficient 64-bit compile for the\n *   64-bit case in order to generate the \"number of bytes in\n *   the final mask\". Again, that could be replaced with a\n *   efficient population count instruction or similar.\n */\n#ifdef CONFIG_DCACHE_WORD_ACCESS\n\n#include <asm/word-at-a-time.h>\n\n#ifdef CONFIG_64BIT\n\nstatic inline unsigned int fold_hash(unsigned long hash)\n{\n\treturn hash_64(hash, 32);\n}\n\n#else\t/* 32-bit case */\n\n#define fold_hash(x) (x)\n\n#endif\n\nunsigned int full_name_hash(const unsigned char *name, unsigned int len)\n{\n\tunsigned long a, mask;\n\tunsigned long hash = 0;\n\n\tfor (;;) {\n\t\ta = load_unaligned_zeropad(name);\n\t\tif (len < sizeof(unsigned long))\n\t\t\tbreak;\n\t\thash += a;\n\t\thash *= 9;\n\t\tname += sizeof(unsigned long);\n\t\tlen -= sizeof(unsigned long);\n\t\tif (!len)\n\t\t\tgoto done;\n\t}\n\tmask = bytemask_from_count(len);\n\thash += mask & a;\ndone:\n\treturn fold_hash(hash);\n}\nEXPORT_SYMBOL(full_name_hash);\n\n/*\n * Calculate the length and hash of the path component, and\n * return the \"hash_len\" as the result.\n */\nstatic inline u64 hash_name(const char *name)\n{\n\tunsigned long a, b, adata, bdata, mask, hash, len;\n\tconst struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;\n\n\thash = a = 0;\n\tlen = -sizeof(unsigned long);\n\tdo {\n\t\thash = (hash + a) * 9;\n\t\tlen += sizeof(unsigned long);\n\t\ta = load_unaligned_zeropad(name+len);\n\t\tb = a ^ REPEAT_BYTE('/');\n\t} while (!(has_zero(a, &adata, &constants) | has_zero(b, &bdata, &constants)));\n\n\tadata = prep_zero_mask(a, adata, &constants);\n\tbdata = prep_zero_mask(b, bdata, &constants);\n\n\tmask = create_zero_mask(adata | bdata);\n\n\thash += a & zero_bytemask(mask);\n\tlen += find_zero(mask);\n\treturn hashlen_create(fold_hash(hash), len);\n}\n\n#else\n\nunsigned int full_name_hash(const unsigned char *name, unsigned int len)\n{\n\tunsigned long hash = init_name_hash();\n\twhile (len--)\n\t\thash = partial_name_hash(*name++, hash);\n\treturn end_name_hash(hash);\n}\nEXPORT_SYMBOL(full_name_hash);\n\n/*\n * We know there's a real path component here of at least\n * one character.\n */\nstatic inline u64 hash_name(const char *name)\n{\n\tunsigned long hash = init_name_hash();\n\tunsigned long len = 0, c;\n\n\tc = (unsigned char)*name;\n\tdo {\n\t\tlen++;\n\t\thash = partial_name_hash(c, hash);\n\t\tc = (unsigned char)name[len];\n\t} while (c && c != '/');\n\treturn hashlen_create(end_name_hash(hash), len);\n}\n\n#endif\n\n/*\n * Name resolution.\n * This is the basic name resolution function, turning a pathname into\n * the final dentry. We expect 'base' to be positive and a directory.\n *\n * Returns 0 and nd will have valid dentry and mnt on success.\n * Returns error and drops reference to input namei data on failure.\n */\nstatic int link_path_walk(const char *name, struct nameidata *nd)\n{\n\tint err;\n\n\twhile (*name=='/')\n\t\tname++;\n\tif (!*name)\n\t\treturn 0;\n\n\t/* At this point we know we have a real path component. */\n\tfor(;;) {\n\t\tu64 hash_len;\n\t\tint type;\n\n\t\terr = may_lookup(nd);\n \t\tif (err)\n\t\t\treturn err;\n\n\t\thash_len = hash_name(name);\n\n\t\ttype = LAST_NORM;\n\t\tif (name[0] == '.') switch (hashlen_len(hash_len)) {\n\t\t\tcase 2:\n\t\t\t\tif (name[1] == '.') {\n\t\t\t\t\ttype = LAST_DOTDOT;\n\t\t\t\t\tnd->flags |= LOOKUP_JUMPED;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase 1:\n\t\t\t\ttype = LAST_DOT;\n\t\t}\n\t\tif (likely(type == LAST_NORM)) {\n\t\t\tstruct dentry *parent = nd->path.dentry;\n\t\t\tnd->flags &= ~LOOKUP_JUMPED;\n\t\t\tif (unlikely(parent->d_flags & DCACHE_OP_HASH)) {\n\t\t\t\tstruct qstr this = { { .hash_len = hash_len }, .name = name };\n\t\t\t\terr = parent->d_op->d_hash(parent, &this);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t\thash_len = this.hash_len;\n\t\t\t\tname = this.name;\n\t\t\t}\n\t\t}\n\n\t\tnd->last.hash_len = hash_len;\n\t\tnd->last.name = name;\n\t\tnd->last_type = type;\n\n\t\tname += hashlen_len(hash_len);\n\t\tif (!*name)\n\t\t\tgoto OK;\n\t\t/*\n\t\t * If it wasn't NUL, we know it was '/'. Skip that\n\t\t * slash, and continue until no more slashes.\n\t\t */\n\t\tdo {\n\t\t\tname++;\n\t\t} while (unlikely(*name == '/'));\n\t\tif (unlikely(!*name)) {\nOK:\n\t\t\t/* pathname body, done */\n\t\t\tif (!nd->depth)\n\t\t\t\treturn 0;\n\t\t\tname = nd->stack[nd->depth - 1].name;\n\t\t\t/* trailing symlink, done */\n\t\t\tif (!name)\n\t\t\t\treturn 0;\n\t\t\t/* last component of nested symlink */\n\t\t\terr = walk_component(nd, WALK_GET | WALK_PUT);\n\t\t} else {\n\t\t\terr = walk_component(nd, WALK_GET);\n\t\t}\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (err) {\n\t\t\tconst char *s = get_link(nd);\n\n\t\t\tif (IS_ERR(s))\n\t\t\t\treturn PTR_ERR(s);\n\t\t\terr = 0;\n\t\t\tif (unlikely(!s)) {\n\t\t\t\t/* jumped */\n\t\t\t\tput_link(nd);\n\t\t\t} else {\n\t\t\t\tnd->stack[nd->depth - 1].name = name;\n\t\t\t\tname = s;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (unlikely(!d_can_lookup(nd->path.dentry))) {\n\t\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\t\tif (unlazy_walk(nd, NULL, 0))\n\t\t\t\t\treturn -ECHILD;\n\t\t\t}\n\t\t\treturn -ENOTDIR;\n\t\t}\n\t}\n}\n\nstatic const char *path_init(struct nameidata *nd, unsigned flags)\n{\n\tint retval = 0;\n\tconst char *s = nd->name->name;\n\n\tnd->last_type = LAST_ROOT; /* if there are only slashes... */\n\tnd->flags = flags | LOOKUP_JUMPED | LOOKUP_PARENT;\n\tnd->depth = 0;\n\tif (flags & LOOKUP_ROOT) {\n\t\tstruct dentry *root = nd->root.dentry;\n\t\tstruct inode *inode = root->d_inode;\n\t\tif (*s) {\n\t\t\tif (!d_can_lookup(root))\n\t\t\t\treturn ERR_PTR(-ENOTDIR);\n\t\t\tretval = inode_permission(inode, MAY_EXEC);\n\t\t\tif (retval)\n\t\t\t\treturn ERR_PTR(retval);\n\t\t}\n\t\tnd->path = nd->root;\n\t\tnd->inode = inode;\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\trcu_read_lock();\n\t\t\tnd->seq = __read_seqcount_begin(&nd->path.dentry->d_seq);\n\t\t\tnd->root_seq = nd->seq;\n\t\t\tnd->m_seq = read_seqbegin(&mount_lock);\n\t\t} else {\n\t\t\tpath_get(&nd->path);\n\t\t}\n\t\treturn s;\n\t}\n\n\tnd->root.mnt = NULL;\n\tnd->path.mnt = NULL;\n\tnd->path.dentry = NULL;\n\n\tnd->m_seq = read_seqbegin(&mount_lock);\n\tif (*s == '/') {\n\t\tif (flags & LOOKUP_RCU)\n\t\t\trcu_read_lock();\n\t\tset_root(nd);\n\t\tif (likely(!nd_jump_root(nd)))\n\t\t\treturn s;\n\t\tnd->root.mnt = NULL;\n\t\trcu_read_unlock();\n\t\treturn ERR_PTR(-ECHILD);\n\t} else if (nd->dfd == AT_FDCWD) {\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\tstruct fs_struct *fs = current->fs;\n\t\t\tunsigned seq;\n\n\t\t\trcu_read_lock();\n\n\t\t\tdo {\n\t\t\t\tseq = read_seqcount_begin(&fs->seq);\n\t\t\t\tnd->path = fs->pwd;\n\t\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t\t\tnd->seq = __read_seqcount_begin(&nd->path.dentry->d_seq);\n\t\t\t} while (read_seqcount_retry(&fs->seq, seq));\n\t\t} else {\n\t\t\tget_fs_pwd(current->fs, &nd->path);\n\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t}\n\t\treturn s;\n\t} else {\n\t\t/* Caller must check execute permissions on the starting path component */\n\t\tstruct fd f = fdget_raw(nd->dfd);\n\t\tstruct dentry *dentry;\n\n\t\tif (!f.file)\n\t\t\treturn ERR_PTR(-EBADF);\n\n\t\tdentry = f.file->f_path.dentry;\n\n\t\tif (*s) {\n\t\t\tif (!d_can_lookup(dentry)) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn ERR_PTR(-ENOTDIR);\n\t\t\t}\n\t\t}\n\n\t\tnd->path = f.file->f_path;\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\trcu_read_lock();\n\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t\tnd->seq = read_seqcount_begin(&nd->path.dentry->d_seq);\n\t\t} else {\n\t\t\tpath_get(&nd->path);\n\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t}\n\t\tfdput(f);\n\t\treturn s;\n\t}\n}\n\nstatic const char *trailing_symlink(struct nameidata *nd)\n{\n\tconst char *s;\n\tint error = may_follow_link(nd);\n\tif (unlikely(error))\n\t\treturn ERR_PTR(error);\n\tnd->flags |= LOOKUP_PARENT;\n\tnd->stack[0].name = NULL;\n\ts = get_link(nd);\n\treturn s ? s : \"\";\n}\n\nstatic inline int lookup_last(struct nameidata *nd)\n{\n\tif (nd->last_type == LAST_NORM && nd->last.name[nd->last.len])\n\t\tnd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\n\n\tnd->flags &= ~LOOKUP_PARENT;\n\treturn walk_component(nd,\n\t\t\tnd->flags & LOOKUP_FOLLOW\n\t\t\t\t? nd->depth\n\t\t\t\t\t? WALK_PUT | WALK_GET\n\t\t\t\t\t: WALK_GET\n\t\t\t\t: 0);\n}\n\n/* Returns 0 and nd will be valid on success; Retuns error, otherwise. */\nstatic int path_lookupat(struct nameidata *nd, unsigned flags, struct path *path)\n{\n\tconst char *s = path_init(nd, flags);\n\tint err;\n\n\tif (IS_ERR(s))\n\t\treturn PTR_ERR(s);\n\twhile (!(err = link_path_walk(s, nd))\n\t\t&& ((err = lookup_last(nd)) > 0)) {\n\t\ts = trailing_symlink(nd);\n\t\tif (IS_ERR(s)) {\n\t\t\terr = PTR_ERR(s);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!err)\n\t\terr = complete_walk(nd);\n\n\tif (!err && nd->flags & LOOKUP_DIRECTORY)\n\t\tif (!d_can_lookup(nd->path.dentry))\n\t\t\terr = -ENOTDIR;\n\tif (!err) {\n\t\t*path = nd->path;\n\t\tnd->path.mnt = NULL;\n\t\tnd->path.dentry = NULL;\n\t}\n\tterminate_walk(nd);\n\treturn err;\n}\n\nstatic int filename_lookup(int dfd, struct filename *name, unsigned flags,\n\t\t\t   struct path *path, struct path *root)\n{\n\tint retval;\n\tstruct nameidata nd;\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\tif (unlikely(root)) {\n\t\tnd.root = *root;\n\t\tflags |= LOOKUP_ROOT;\n\t}\n\tset_nameidata(&nd, dfd, name);\n\tretval = path_lookupat(&nd, flags | LOOKUP_RCU, path);\n\tif (unlikely(retval == -ECHILD))\n\t\tretval = path_lookupat(&nd, flags, path);\n\tif (unlikely(retval == -ESTALE))\n\t\tretval = path_lookupat(&nd, flags | LOOKUP_REVAL, path);\n\n\tif (likely(!retval))\n\t\taudit_inode(name, path->dentry, flags & LOOKUP_PARENT);\n\trestore_nameidata();\n\tputname(name);\n\treturn retval;\n}\n\n/* Returns 0 and nd will be valid on success; Retuns error, otherwise. */\nstatic int path_parentat(struct nameidata *nd, unsigned flags,\n\t\t\t\tstruct path *parent)\n{\n\tconst char *s = path_init(nd, flags);\n\tint err;\n\tif (IS_ERR(s))\n\t\treturn PTR_ERR(s);\n\terr = link_path_walk(s, nd);\n\tif (!err)\n\t\terr = complete_walk(nd);\n\tif (!err) {\n\t\t*parent = nd->path;\n\t\tnd->path.mnt = NULL;\n\t\tnd->path.dentry = NULL;\n\t}\n\tterminate_walk(nd);\n\treturn err;\n}\n\nstatic struct filename *filename_parentat(int dfd, struct filename *name,\n\t\t\t\tunsigned int flags, struct path *parent,\n\t\t\t\tstruct qstr *last, int *type)\n{\n\tint retval;\n\tstruct nameidata nd;\n\n\tif (IS_ERR(name))\n\t\treturn name;\n\tset_nameidata(&nd, dfd, name);\n\tretval = path_parentat(&nd, flags | LOOKUP_RCU, parent);\n\tif (unlikely(retval == -ECHILD))\n\t\tretval = path_parentat(&nd, flags, parent);\n\tif (unlikely(retval == -ESTALE))\n\t\tretval = path_parentat(&nd, flags | LOOKUP_REVAL, parent);\n\tif (likely(!retval)) {\n\t\t*last = nd.last;\n\t\t*type = nd.last_type;\n\t\taudit_inode(name, parent->dentry, LOOKUP_PARENT);\n\t} else {\n\t\tputname(name);\n\t\tname = ERR_PTR(retval);\n\t}\n\trestore_nameidata();\n\treturn name;\n}\n\n/* does lookup, returns the object with parent locked */\nstruct dentry *kern_path_locked(const char *name, struct path *path)\n{\n\tstruct filename *filename;\n\tstruct dentry *d;\n\tstruct qstr last;\n\tint type;\n\n\tfilename = filename_parentat(AT_FDCWD, getname_kernel(name), 0, path,\n\t\t\t\t    &last, &type);\n\tif (IS_ERR(filename))\n\t\treturn ERR_CAST(filename);\n\tif (unlikely(type != LAST_NORM)) {\n\t\tpath_put(path);\n\t\tputname(filename);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tinode_lock_nested(path->dentry->d_inode, I_MUTEX_PARENT);\n\td = __lookup_hash(&last, path->dentry, 0);\n\tif (IS_ERR(d)) {\n\t\tinode_unlock(path->dentry->d_inode);\n\t\tpath_put(path);\n\t}\n\tputname(filename);\n\treturn d;\n}\n\nint kern_path(const char *name, unsigned int flags, struct path *path)\n{\n\treturn filename_lookup(AT_FDCWD, getname_kernel(name),\n\t\t\t       flags, path, NULL);\n}\nEXPORT_SYMBOL(kern_path);\n\n/**\n * vfs_path_lookup - lookup a file path relative to a dentry-vfsmount pair\n * @dentry:  pointer to dentry of the base directory\n * @mnt: pointer to vfs mount of the base directory\n * @name: pointer to file name\n * @flags: lookup flags\n * @path: pointer to struct path to fill\n */\nint vfs_path_lookup(struct dentry *dentry, struct vfsmount *mnt,\n\t\t    const char *name, unsigned int flags,\n\t\t    struct path *path)\n{\n\tstruct path root = {.mnt = mnt, .dentry = dentry};\n\t/* the first argument of filename_lookup() is ignored with root */\n\treturn filename_lookup(AT_FDCWD, getname_kernel(name),\n\t\t\t       flags , path, &root);\n}\nEXPORT_SYMBOL(vfs_path_lookup);\n\n/**\n * lookup_one_len - filesystem helper to lookup single pathname component\n * @name:\tpathname component to lookup\n * @base:\tbase directory to lookup from\n * @len:\tmaximum length @len should be interpreted to\n *\n * Note that this routine is purely a helper for filesystem usage and should\n * not be called by generic code.\n *\n * The caller must hold base->i_mutex.\n */\nstruct dentry *lookup_one_len(const char *name, struct dentry *base, int len)\n{\n\tstruct qstr this;\n\tunsigned int c;\n\tint err;\n\n\tWARN_ON_ONCE(!inode_is_locked(base->d_inode));\n\n\tthis.name = name;\n\tthis.len = len;\n\tthis.hash = full_name_hash(name, len);\n\tif (!len)\n\t\treturn ERR_PTR(-EACCES);\n\n\tif (unlikely(name[0] == '.')) {\n\t\tif (len < 2 || (len == 2 && name[1] == '.'))\n\t\t\treturn ERR_PTR(-EACCES);\n\t}\n\n\twhile (len--) {\n\t\tc = *(const unsigned char *)name++;\n\t\tif (c == '/' || c == '\\0')\n\t\t\treturn ERR_PTR(-EACCES);\n\t}\n\t/*\n\t * See if the low-level filesystem might want\n\t * to use its own hash..\n\t */\n\tif (base->d_flags & DCACHE_OP_HASH) {\n\t\tint err = base->d_op->d_hash(base, &this);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\terr = inode_permission(base->d_inode, MAY_EXEC);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn __lookup_hash(&this, base, 0);\n}\nEXPORT_SYMBOL(lookup_one_len);\n\n/**\n * lookup_one_len_unlocked - filesystem helper to lookup single pathname component\n * @name:\tpathname component to lookup\n * @base:\tbase directory to lookup from\n * @len:\tmaximum length @len should be interpreted to\n *\n * Note that this routine is purely a helper for filesystem usage and should\n * not be called by generic code.\n *\n * Unlike lookup_one_len, it should be called without the parent\n * i_mutex held, and will take the i_mutex itself if necessary.\n */\nstruct dentry *lookup_one_len_unlocked(const char *name,\n\t\t\t\t       struct dentry *base, int len)\n{\n\tstruct qstr this;\n\tunsigned int c;\n\tint err;\n\tstruct dentry *ret;\n\n\tthis.name = name;\n\tthis.len = len;\n\tthis.hash = full_name_hash(name, len);\n\tif (!len)\n\t\treturn ERR_PTR(-EACCES);\n\n\tif (unlikely(name[0] == '.')) {\n\t\tif (len < 2 || (len == 2 && name[1] == '.'))\n\t\t\treturn ERR_PTR(-EACCES);\n\t}\n\n\twhile (len--) {\n\t\tc = *(const unsigned char *)name++;\n\t\tif (c == '/' || c == '\\0')\n\t\t\treturn ERR_PTR(-EACCES);\n\t}\n\t/*\n\t * See if the low-level filesystem might want\n\t * to use its own hash..\n\t */\n\tif (base->d_flags & DCACHE_OP_HASH) {\n\t\tint err = base->d_op->d_hash(base, &this);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\terr = inode_permission(base->d_inode, MAY_EXEC);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tret = lookup_dcache(&this, base, 0);\n\tif (!ret)\n\t\tret = lookup_slow(&this, base, 0);\n\treturn ret;\n}\nEXPORT_SYMBOL(lookup_one_len_unlocked);\n\nint user_path_at_empty(int dfd, const char __user *name, unsigned flags,\n\t\t struct path *path, int *empty)\n{\n\treturn filename_lookup(dfd, getname_flags(name, flags, empty),\n\t\t\t       flags, path, NULL);\n}\nEXPORT_SYMBOL(user_path_at_empty);\n\n/*\n * NB: most callers don't do anything directly with the reference to the\n *     to struct filename, but the nd->last pointer points into the name string\n *     allocated by getname. So we must hold the reference to it until all\n *     path-walking is complete.\n */\nstatic inline struct filename *\nuser_path_parent(int dfd, const char __user *path,\n\t\t struct path *parent,\n\t\t struct qstr *last,\n\t\t int *type,\n\t\t unsigned int flags)\n{\n\t/* only LOOKUP_REVAL is allowed in extra flags */\n\treturn filename_parentat(dfd, getname(path), flags & LOOKUP_REVAL,\n\t\t\t\t parent, last, type);\n}\n\n/**\n * mountpoint_last - look up last component for umount\n * @nd:   pathwalk nameidata - currently pointing at parent directory of \"last\"\n * @path: pointer to container for result\n *\n * This is a special lookup_last function just for umount. In this case, we\n * need to resolve the path without doing any revalidation.\n *\n * The nameidata should be the result of doing a LOOKUP_PARENT pathwalk. Since\n * mountpoints are always pinned in the dcache, their ancestors are too. Thus,\n * in almost all cases, this lookup will be served out of the dcache. The only\n * cases where it won't are if nd->last refers to a symlink or the path is\n * bogus and it doesn't exist.\n *\n * Returns:\n * -error: if there was an error during lookup. This includes -ENOENT if the\n *         lookup found a negative dentry. The nd->path reference will also be\n *         put in this case.\n *\n * 0:      if we successfully resolved nd->path and found it to not to be a\n *         symlink that needs to be followed. \"path\" will also be populated.\n *         The nd->path reference will also be put.\n *\n * 1:      if we successfully resolved nd->last and found it to be a symlink\n *         that needs to be followed. \"path\" will be populated with the path\n *         to the link, and nd->path will *not* be put.\n */\nstatic int\nmountpoint_last(struct nameidata *nd, struct path *path)\n{\n\tint error = 0;\n\tstruct dentry *dentry;\n\tstruct dentry *dir = nd->path.dentry;\n\n\t/* If we're in rcuwalk, drop out of it to handle last component */\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tif (unlazy_walk(nd, NULL, 0))\n\t\t\treturn -ECHILD;\n\t}\n\n\tnd->flags &= ~LOOKUP_PARENT;\n\n\tif (unlikely(nd->last_type != LAST_NORM)) {\n\t\terror = handle_dots(nd, nd->last_type);\n\t\tif (error)\n\t\t\treturn error;\n\t\tdentry = dget(nd->path.dentry);\n\t} else {\n\t\tdentry = d_lookup(dir, &nd->last);\n\t\tif (!dentry) {\n\t\t\t/*\n\t\t\t * No cached dentry. Mounted dentries are pinned in the\n\t\t\t * cache, so that means that this dentry is probably\n\t\t\t * a symlink or the path doesn't actually point\n\t\t\t * to a mounted dentry.\n\t\t\t */\n\t\t\tdentry = lookup_slow(&nd->last, dir,\n\t\t\t\t\t     nd->flags | LOOKUP_NO_REVAL);\n\t\t\tif (IS_ERR(dentry))\n\t\t\t\treturn PTR_ERR(dentry);\n\t\t}\n\t}\n\tif (d_is_negative(dentry)) {\n\t\tdput(dentry);\n\t\treturn -ENOENT;\n\t}\n\tif (nd->depth)\n\t\tput_link(nd);\n\tpath->dentry = dentry;\n\tpath->mnt = nd->path.mnt;\n\terror = should_follow_link(nd, path, nd->flags & LOOKUP_FOLLOW,\n\t\t\t\t   d_backing_inode(dentry), 0);\n\tif (unlikely(error))\n\t\treturn error;\n\tmntget(path->mnt);\n\tfollow_mount(path);\n\treturn 0;\n}\n\n/**\n * path_mountpoint - look up a path to be umounted\n * @nd:\t\tlookup context\n * @flags:\tlookup flags\n * @path:\tpointer to container for result\n *\n * Look up the given name, but don't attempt to revalidate the last component.\n * Returns 0 and \"path\" will be valid on success; Returns error otherwise.\n */\nstatic int\npath_mountpoint(struct nameidata *nd, unsigned flags, struct path *path)\n{\n\tconst char *s = path_init(nd, flags);\n\tint err;\n\tif (IS_ERR(s))\n\t\treturn PTR_ERR(s);\n\twhile (!(err = link_path_walk(s, nd)) &&\n\t\t(err = mountpoint_last(nd, path)) > 0) {\n\t\ts = trailing_symlink(nd);\n\t\tif (IS_ERR(s)) {\n\t\t\terr = PTR_ERR(s);\n\t\t\tbreak;\n\t\t}\n\t}\n\tterminate_walk(nd);\n\treturn err;\n}\n\nstatic int\nfilename_mountpoint(int dfd, struct filename *name, struct path *path,\n\t\t\tunsigned int flags)\n{\n\tstruct nameidata nd;\n\tint error;\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\tset_nameidata(&nd, dfd, name);\n\terror = path_mountpoint(&nd, flags | LOOKUP_RCU, path);\n\tif (unlikely(error == -ECHILD))\n\t\terror = path_mountpoint(&nd, flags, path);\n\tif (unlikely(error == -ESTALE))\n\t\terror = path_mountpoint(&nd, flags | LOOKUP_REVAL, path);\n\tif (likely(!error))\n\t\taudit_inode(name, path->dentry, 0);\n\trestore_nameidata();\n\tputname(name);\n\treturn error;\n}\n\n/**\n * user_path_mountpoint_at - lookup a path from userland in order to umount it\n * @dfd:\tdirectory file descriptor\n * @name:\tpathname from userland\n * @flags:\tlookup flags\n * @path:\tpointer to container to hold result\n *\n * A umount is a special case for path walking. We're not actually interested\n * in the inode in this situation, and ESTALE errors can be a problem. We\n * simply want track down the dentry and vfsmount attached at the mountpoint\n * and avoid revalidating the last component.\n *\n * Returns 0 and populates \"path\" on success.\n */\nint\nuser_path_mountpoint_at(int dfd, const char __user *name, unsigned int flags,\n\t\t\tstruct path *path)\n{\n\treturn filename_mountpoint(dfd, getname(name), path, flags);\n}\n\nint\nkern_path_mountpoint(int dfd, const char *name, struct path *path,\n\t\t\tunsigned int flags)\n{\n\treturn filename_mountpoint(dfd, getname_kernel(name), path, flags);\n}\nEXPORT_SYMBOL(kern_path_mountpoint);\n\nint __check_sticky(struct inode *dir, struct inode *inode)\n{\n\tkuid_t fsuid = current_fsuid();\n\n\tif (uid_eq(inode->i_uid, fsuid))\n\t\treturn 0;\n\tif (uid_eq(dir->i_uid, fsuid))\n\t\treturn 0;\n\treturn !capable_wrt_inode_uidgid(inode, CAP_FOWNER);\n}\nEXPORT_SYMBOL(__check_sticky);\n\n/*\n *\tCheck whether we can remove a link victim from directory dir, check\n *  whether the type of victim is right.\n *  1. We can't do it if dir is read-only (done in permission())\n *  2. We should have write and exec permissions on dir\n *  3. We can't remove anything from append-only dir\n *  4. We can't do anything with immutable dir (done in permission())\n *  5. If the sticky bit on dir is set we should either\n *\ta. be owner of dir, or\n *\tb. be owner of victim, or\n *\tc. have CAP_FOWNER capability\n *  6. If the victim is append-only or immutable we can't do antyhing with\n *     links pointing to it.\n *  7. If we were asked to remove a directory and victim isn't one - ENOTDIR.\n *  8. If we were asked to remove a non-directory and victim isn't one - EISDIR.\n *  9. We can't remove a root or mountpoint.\n * 10. We don't allow removal of NFS sillyrenamed files; it's handled by\n *     nfs_async_unlink().\n */\nstatic int may_delete(struct inode *dir, struct dentry *victim, bool isdir)\n{\n\tstruct inode *inode = d_backing_inode(victim);\n\tint error;\n\n\tif (d_is_negative(victim))\n\t\treturn -ENOENT;\n\tBUG_ON(!inode);\n\n\tBUG_ON(victim->d_parent->d_inode != dir);\n\taudit_inode_child(dir, victim, AUDIT_TYPE_CHILD_DELETE);\n\n\terror = inode_permission(dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\tif (IS_APPEND(dir))\n\t\treturn -EPERM;\n\n\tif (check_sticky(dir, inode) || IS_APPEND(inode) ||\n\t    IS_IMMUTABLE(inode) || IS_SWAPFILE(inode))\n\t\treturn -EPERM;\n\tif (isdir) {\n\t\tif (!d_is_dir(victim))\n\t\t\treturn -ENOTDIR;\n\t\tif (IS_ROOT(victim))\n\t\t\treturn -EBUSY;\n\t} else if (d_is_dir(victim))\n\t\treturn -EISDIR;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\tif (victim->d_flags & DCACHE_NFSFS_RENAMED)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n/*\tCheck whether we can create an object with dentry child in directory\n *  dir.\n *  1. We can't do it if child already exists (open has special treatment for\n *     this case, but since we are inlined it's OK)\n *  2. We can't do it if dir is read-only (done in permission())\n *  3. We should have write and exec permissions on dir\n *  4. We can't do it if dir is immutable (done in permission())\n */\nstatic inline int may_create(struct inode *dir, struct dentry *child)\n{\n\taudit_inode_child(dir, child, AUDIT_TYPE_CHILD_CREATE);\n\tif (child->d_inode)\n\t\treturn -EEXIST;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\treturn inode_permission(dir, MAY_WRITE | MAY_EXEC);\n}\n\n/*\n * p1 and p2 should be directories on the same fs.\n */\nstruct dentry *lock_rename(struct dentry *p1, struct dentry *p2)\n{\n\tstruct dentry *p;\n\n\tif (p1 == p2) {\n\t\tinode_lock_nested(p1->d_inode, I_MUTEX_PARENT);\n\t\treturn NULL;\n\t}\n\n\tmutex_lock(&p1->d_inode->i_sb->s_vfs_rename_mutex);\n\n\tp = d_ancestor(p2, p1);\n\tif (p) {\n\t\tinode_lock_nested(p2->d_inode, I_MUTEX_PARENT);\n\t\tinode_lock_nested(p1->d_inode, I_MUTEX_CHILD);\n\t\treturn p;\n\t}\n\n\tp = d_ancestor(p1, p2);\n\tif (p) {\n\t\tinode_lock_nested(p1->d_inode, I_MUTEX_PARENT);\n\t\tinode_lock_nested(p2->d_inode, I_MUTEX_CHILD);\n\t\treturn p;\n\t}\n\n\tinode_lock_nested(p1->d_inode, I_MUTEX_PARENT);\n\tinode_lock_nested(p2->d_inode, I_MUTEX_PARENT2);\n\treturn NULL;\n}\nEXPORT_SYMBOL(lock_rename);\n\nvoid unlock_rename(struct dentry *p1, struct dentry *p2)\n{\n\tinode_unlock(p1->d_inode);\n\tif (p1 != p2) {\n\t\tinode_unlock(p2->d_inode);\n\t\tmutex_unlock(&p1->d_inode->i_sb->s_vfs_rename_mutex);\n\t}\n}\nEXPORT_SYMBOL(unlock_rename);\n\nint vfs_create(struct inode *dir, struct dentry *dentry, umode_t mode,\n\t\tbool want_excl)\n{\n\tint error = may_create(dir, dentry);\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->create)\n\t\treturn -EACCES;\t/* shouldn't it be ENOSYS? */\n\tmode &= S_IALLUGO;\n\tmode |= S_IFREG;\n\terror = security_inode_create(dir, dentry, mode);\n\tif (error)\n\t\treturn error;\n\terror = dir->i_op->create(dir, dentry, mode, want_excl);\n\tif (!error)\n\t\tfsnotify_create(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_create);\n\nstatic int may_open(struct path *path, int acc_mode, int flag)\n{\n\tstruct dentry *dentry = path->dentry;\n\tstruct inode *inode = dentry->d_inode;\n\tint error;\n\n\tif (!inode)\n\t\treturn -ENOENT;\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFLNK:\n\t\treturn -ELOOP;\n\tcase S_IFDIR:\n\t\tif (acc_mode & MAY_WRITE)\n\t\t\treturn -EISDIR;\n\t\tbreak;\n\tcase S_IFBLK:\n\tcase S_IFCHR:\n\t\tif (path->mnt->mnt_flags & MNT_NODEV)\n\t\t\treturn -EACCES;\n\t\t/*FALLTHRU*/\n\tcase S_IFIFO:\n\tcase S_IFSOCK:\n\t\tflag &= ~O_TRUNC;\n\t\tbreak;\n\t}\n\n\terror = inode_permission(inode, MAY_OPEN | acc_mode);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * An append-only file must be opened in append mode for writing.\n\t */\n\tif (IS_APPEND(inode)) {\n\t\tif  ((flag & O_ACCMODE) != O_RDONLY && !(flag & O_APPEND))\n\t\t\treturn -EPERM;\n\t\tif (flag & O_TRUNC)\n\t\t\treturn -EPERM;\n\t}\n\n\t/* O_NOATIME can only be set by the owner or superuser */\n\tif (flag & O_NOATIME && !inode_owner_or_capable(inode))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int handle_truncate(struct file *filp)\n{\n\tstruct path *path = &filp->f_path;\n\tstruct inode *inode = path->dentry->d_inode;\n\tint error = get_write_access(inode);\n\tif (error)\n\t\treturn error;\n\t/*\n\t * Refuse to truncate files with mandatory locks held on them.\n\t */\n\terror = locks_verify_locked(filp);\n\tif (!error)\n\t\terror = security_path_truncate(path);\n\tif (!error) {\n\t\terror = do_truncate(path->dentry, 0,\n\t\t\t\t    ATTR_MTIME|ATTR_CTIME|ATTR_OPEN,\n\t\t\t\t    filp);\n\t}\n\tput_write_access(inode);\n\treturn error;\n}\n\nstatic inline int open_to_namei_flags(int flag)\n{\n\tif ((flag & O_ACCMODE) == 3)\n\t\tflag--;\n\treturn flag;\n}\n\nstatic int may_o_create(struct path *dir, struct dentry *dentry, umode_t mode)\n{\n\tint error = security_path_mknod(dir, dentry, mode, 0);\n\tif (error)\n\t\treturn error;\n\n\terror = inode_permission(dir->dentry->d_inode, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\n\treturn security_inode_create(dir->dentry->d_inode, dentry, mode);\n}\n\n/*\n * Attempt to atomically look up, create and open a file from a negative\n * dentry.\n *\n * Returns 0 if successful.  The file will have been created and attached to\n * @file by the filesystem calling finish_open().\n *\n * Returns 1 if the file was looked up only or didn't need creating.  The\n * caller will need to perform the open themselves.  @path will have been\n * updated to point to the new dentry.  This may be negative.\n *\n * Returns an error code otherwise.\n */\nstatic int atomic_open(struct nameidata *nd, struct dentry *dentry,\n\t\t\tstruct path *path, struct file *file,\n\t\t\tconst struct open_flags *op,\n\t\t\tbool got_write, bool need_lookup,\n\t\t\tint *opened)\n{\n\tstruct inode *dir =  nd->path.dentry->d_inode;\n\tunsigned open_flag = open_to_namei_flags(op->open_flag);\n\tumode_t mode;\n\tint error;\n\tint acc_mode;\n\tint create_error = 0;\n\tstruct dentry *const DENTRY_NOT_SET = (void *) -1UL;\n\tbool excl;\n\n\tBUG_ON(dentry->d_inode);\n\n\t/* Don't create child dentry for a dead directory. */\n\tif (unlikely(IS_DEADDIR(dir))) {\n\t\terror = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tmode = op->mode;\n\tif ((open_flag & O_CREAT) && !IS_POSIXACL(dir))\n\t\tmode &= ~current_umask();\n\n\texcl = (open_flag & (O_EXCL | O_CREAT)) == (O_EXCL | O_CREAT);\n\tif (excl)\n\t\topen_flag &= ~O_TRUNC;\n\n\t/*\n\t * Checking write permission is tricky, bacuse we don't know if we are\n\t * going to actually need it: O_CREAT opens should work as long as the\n\t * file exists.  But checking existence breaks atomicity.  The trick is\n\t * to check access and if not granted clear O_CREAT from the flags.\n\t *\n\t * Another problem is returing the \"right\" error value (e.g. for an\n\t * O_EXCL open we want to return EEXIST not EROFS).\n\t */\n\tif (((open_flag & (O_CREAT | O_TRUNC)) ||\n\t    (open_flag & O_ACCMODE) != O_RDONLY) && unlikely(!got_write)) {\n\t\tif (!(open_flag & O_CREAT)) {\n\t\t\t/*\n\t\t\t * No O_CREATE -> atomicity not a requirement -> fall\n\t\t\t * back to lookup + open\n\t\t\t */\n\t\t\tgoto no_open;\n\t\t} else if (open_flag & (O_EXCL | O_TRUNC)) {\n\t\t\t/* Fall back and fail with the right error */\n\t\t\tcreate_error = -EROFS;\n\t\t\tgoto no_open;\n\t\t} else {\n\t\t\t/* No side effects, safe to clear O_CREAT */\n\t\t\tcreate_error = -EROFS;\n\t\t\topen_flag &= ~O_CREAT;\n\t\t}\n\t}\n\n\tif (open_flag & O_CREAT) {\n\t\terror = may_o_create(&nd->path, dentry, mode);\n\t\tif (error) {\n\t\t\tcreate_error = error;\n\t\t\tif (open_flag & O_EXCL)\n\t\t\t\tgoto no_open;\n\t\t\topen_flag &= ~O_CREAT;\n\t\t}\n\t}\n\n\tif (nd->flags & LOOKUP_DIRECTORY)\n\t\topen_flag |= O_DIRECTORY;\n\n\tfile->f_path.dentry = DENTRY_NOT_SET;\n\tfile->f_path.mnt = nd->path.mnt;\n\terror = dir->i_op->atomic_open(dir, dentry, file, open_flag, mode,\n\t\t\t\t      opened);\n\tif (error < 0) {\n\t\tif (create_error && error == -ENOENT)\n\t\t\terror = create_error;\n\t\tgoto out;\n\t}\n\n\tif (error) {\t/* returned 1, that is */\n\t\tif (WARN_ON(file->f_path.dentry == DENTRY_NOT_SET)) {\n\t\t\terror = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (file->f_path.dentry) {\n\t\t\tdput(dentry);\n\t\t\tdentry = file->f_path.dentry;\n\t\t}\n\t\tif (*opened & FILE_CREATED)\n\t\t\tfsnotify_create(dir, dentry);\n\t\tif (!dentry->d_inode) {\n\t\t\tWARN_ON(*opened & FILE_CREATED);\n\t\t\tif (create_error) {\n\t\t\t\terror = create_error;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tif (excl && !(*opened & FILE_CREATED)) {\n\t\t\t\terror = -EEXIST;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tgoto looked_up;\n\t}\n\n\t/*\n\t * We didn't have the inode before the open, so check open permission\n\t * here.\n\t */\n\tacc_mode = op->acc_mode;\n\tif (*opened & FILE_CREATED) {\n\t\tWARN_ON(!(open_flag & O_CREAT));\n\t\tfsnotify_create(dir, dentry);\n\t\tacc_mode = 0;\n\t}\n\terror = may_open(&file->f_path, acc_mode, open_flag);\n\tif (error)\n\t\tfput(file);\n\nout:\n\tdput(dentry);\n\treturn error;\n\nno_open:\n\tif (need_lookup) {\n\t\tdentry = lookup_real(dir, dentry, nd->flags);\n\t\tif (IS_ERR(dentry))\n\t\t\treturn PTR_ERR(dentry);\n\n\t\tif (create_error) {\n\t\t\tint open_flag = op->open_flag;\n\n\t\t\terror = create_error;\n\t\t\tif ((open_flag & O_EXCL)) {\n\t\t\t\tif (!dentry->d_inode)\n\t\t\t\t\tgoto out;\n\t\t\t} else if (!dentry->d_inode) {\n\t\t\t\tgoto out;\n\t\t\t} else if ((open_flag & O_TRUNC) &&\n\t\t\t\t   d_is_reg(dentry)) {\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/* will fail later, go on to get the right error */\n\t\t}\n\t}\nlooked_up:\n\tpath->dentry = dentry;\n\tpath->mnt = nd->path.mnt;\n\treturn 1;\n}\n\n/*\n * Look up and maybe create and open the last component.\n *\n * Must be called with i_mutex held on parent.\n *\n * Returns 0 if the file was successfully atomically created (if necessary) and\n * opened.  In this case the file will be returned attached to @file.\n *\n * Returns 1 if the file was not completely opened at this time, though lookups\n * and creations will have been performed and the dentry returned in @path will\n * be positive upon return if O_CREAT was specified.  If O_CREAT wasn't\n * specified then a negative dentry may be returned.\n *\n * An error code is returned otherwise.\n *\n * FILE_CREATE will be set in @*opened if the dentry was created and will be\n * cleared otherwise prior to returning.\n */\nstatic int lookup_open(struct nameidata *nd, struct path *path,\n\t\t\tstruct file *file,\n\t\t\tconst struct open_flags *op,\n\t\t\tbool got_write, int *opened)\n{\n\tstruct dentry *dir = nd->path.dentry;\n\tstruct inode *dir_inode = dir->d_inode;\n\tstruct dentry *dentry;\n\tint error;\n\tbool need_lookup = false;\n\n\t*opened &= ~FILE_CREATED;\n\tdentry = lookup_dcache(&nd->last, dir, nd->flags);\n\tif (IS_ERR(dentry))\n\t\treturn PTR_ERR(dentry);\n\n\tif (!dentry) {\n\t\tdentry = d_alloc(dir, &nd->last);\n\t\tif (unlikely(!dentry))\n\t\t\treturn -ENOMEM;\n\t\tneed_lookup = true;\n\t} else if (dentry->d_inode) {\n\t\t/* Cached positive dentry: will open in f_op->open */\n\t\tgoto out_no_open;\n\t}\n\n\tif ((nd->flags & LOOKUP_OPEN) && dir_inode->i_op->atomic_open) {\n\t\treturn atomic_open(nd, dentry, path, file, op, got_write,\n\t\t\t\t   need_lookup, opened);\n\t}\n\n\tif (need_lookup) {\n\t\tBUG_ON(dentry->d_inode);\n\n\t\tdentry = lookup_real(dir_inode, dentry, nd->flags);\n\t\tif (IS_ERR(dentry))\n\t\t\treturn PTR_ERR(dentry);\n\t}\n\n\t/* Negative dentry, just create the file */\n\tif (!dentry->d_inode && (op->open_flag & O_CREAT)) {\n\t\tumode_t mode = op->mode;\n\t\tif (!IS_POSIXACL(dir->d_inode))\n\t\t\tmode &= ~current_umask();\n\t\t/*\n\t\t * This write is needed to ensure that a\n\t\t * rw->ro transition does not occur between\n\t\t * the time when the file is created and when\n\t\t * a permanent write count is taken through\n\t\t * the 'struct file' in finish_open().\n\t\t */\n\t\tif (!got_write) {\n\t\t\terror = -EROFS;\n\t\t\tgoto out_dput;\n\t\t}\n\t\t*opened |= FILE_CREATED;\n\t\terror = security_path_mknod(&nd->path, dentry, mode, 0);\n\t\tif (error)\n\t\t\tgoto out_dput;\n\t\terror = vfs_create(dir->d_inode, dentry, mode,\n\t\t\t\t   nd->flags & LOOKUP_EXCL);\n\t\tif (error)\n\t\t\tgoto out_dput;\n\t}\nout_no_open:\n\tpath->dentry = dentry;\n\tpath->mnt = nd->path.mnt;\n\treturn 1;\n\nout_dput:\n\tdput(dentry);\n\treturn error;\n}\n\n/*\n * Handle the last step of open()\n */\nstatic int do_last(struct nameidata *nd,\n\t\t   struct file *file, const struct open_flags *op,\n\t\t   int *opened)\n{\n\tstruct dentry *dir = nd->path.dentry;\n\tint open_flag = op->open_flag;\n\tbool will_truncate = (open_flag & O_TRUNC) != 0;\n\tbool got_write = false;\n\tint acc_mode = op->acc_mode;\n\tunsigned seq;\n\tstruct inode *inode;\n\tstruct path save_parent = { .dentry = NULL, .mnt = NULL };\n\tstruct path path;\n\tbool retried = false;\n\tint error;\n\n\tnd->flags &= ~LOOKUP_PARENT;\n\tnd->flags |= op->intent;\n\n\tif (nd->last_type != LAST_NORM) {\n\t\terror = handle_dots(nd, nd->last_type);\n\t\tif (unlikely(error))\n\t\t\treturn error;\n\t\tgoto finish_open;\n\t}\n\n\tif (!(open_flag & O_CREAT)) {\n\t\tif (nd->last.name[nd->last.len])\n\t\t\tnd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\n\t\t/* we _can_ be in RCU mode here */\n\t\terror = lookup_fast(nd, &path, &inode, &seq);\n\t\tif (likely(error > 0))\n\t\t\tgoto finish_lookup;\n\n\t\tif (error < 0)\n\t\t\treturn error;\n\n\t\tBUG_ON(nd->inode != dir->d_inode);\n\t\tBUG_ON(nd->flags & LOOKUP_RCU);\n\t} else {\n\t\t/* create side of things */\n\t\t/*\n\t\t * This will *only* deal with leaving RCU mode - LOOKUP_JUMPED\n\t\t * has been cleared when we got to the last component we are\n\t\t * about to look up\n\t\t */\n\t\terror = complete_walk(nd);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\taudit_inode(nd->name, dir, LOOKUP_PARENT);\n\t\t/* trailing slashes? */\n\t\tif (unlikely(nd->last.name[nd->last.len]))\n\t\t\treturn -EISDIR;\n\t}\n\nretry_lookup:\n\tif (op->open_flag & (O_CREAT | O_TRUNC | O_WRONLY | O_RDWR)) {\n\t\terror = mnt_want_write(nd->path.mnt);\n\t\tif (!error)\n\t\t\tgot_write = true;\n\t\t/*\n\t\t * do _not_ fail yet - we might not need that or fail with\n\t\t * a different error; let lookup_open() decide; we'll be\n\t\t * dropping this one anyway.\n\t\t */\n\t}\n\tinode_lock(dir->d_inode);\n\terror = lookup_open(nd, &path, file, op, got_write, opened);\n\tinode_unlock(dir->d_inode);\n\n\tif (error <= 0) {\n\t\tif (error)\n\t\t\tgoto out;\n\n\t\tif ((*opened & FILE_CREATED) ||\n\t\t    !S_ISREG(file_inode(file)->i_mode))\n\t\t\twill_truncate = false;\n\n\t\taudit_inode(nd->name, file->f_path.dentry, 0);\n\t\tgoto opened;\n\t}\n\n\tif (*opened & FILE_CREATED) {\n\t\t/* Don't check for write permission, don't truncate */\n\t\topen_flag &= ~O_TRUNC;\n\t\twill_truncate = false;\n\t\tacc_mode = 0;\n\t\tpath_to_nameidata(&path, nd);\n\t\tgoto finish_open_created;\n\t}\n\n\t/*\n\t * If atomic_open() acquired write access it is dropped now due to\n\t * possible mount and symlink following (this might be optimized away if\n\t * necessary...)\n\t */\n\tif (got_write) {\n\t\tmnt_drop_write(nd->path.mnt);\n\t\tgot_write = false;\n\t}\n\n\tif (unlikely(d_is_negative(path.dentry))) {\n\t\tpath_to_nameidata(&path, nd);\n\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * create/update audit record if it already exists.\n\t */\n\taudit_inode(nd->name, path.dentry, 0);\n\n\tif (unlikely((open_flag & (O_EXCL | O_CREAT)) == (O_EXCL | O_CREAT))) {\n\t\tpath_to_nameidata(&path, nd);\n\t\treturn -EEXIST;\n\t}\n\n\terror = follow_managed(&path, nd);\n\tif (unlikely(error < 0))\n\t\treturn error;\n\n\tseq = 0;\t/* out of RCU mode, so the value doesn't matter */\n\tinode = d_backing_inode(path.dentry);\nfinish_lookup:\n\tif (nd->depth)\n\t\tput_link(nd);\n\terror = should_follow_link(nd, &path, nd->flags & LOOKUP_FOLLOW,\n\t\t\t\t   inode, seq);\n\tif (unlikely(error))\n\t\treturn error;\n\n\tif ((nd->flags & LOOKUP_RCU) || nd->path.mnt != path.mnt) {\n\t\tpath_to_nameidata(&path, nd);\n\t} else {\n\t\tsave_parent.dentry = nd->path.dentry;\n\t\tsave_parent.mnt = mntget(path.mnt);\n\t\tnd->path.dentry = path.dentry;\n\n\t}\n\tnd->inode = inode;\n\tnd->seq = seq;\n\t/* Why this, you ask?  _Now_ we might have grown LOOKUP_JUMPED... */\nfinish_open:\n\terror = complete_walk(nd);\n\tif (error) {\n\t\tpath_put(&save_parent);\n\t\treturn error;\n\t}\n\taudit_inode(nd->name, nd->path.dentry, 0);\n\tif (unlikely(d_is_symlink(nd->path.dentry)) && !(open_flag & O_PATH)) {\n\t\terror = -ELOOP;\n\t\tgoto out;\n\t}\n\terror = -EISDIR;\n\tif ((open_flag & O_CREAT) && d_is_dir(nd->path.dentry))\n\t\tgoto out;\n\terror = -ENOTDIR;\n\tif ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry))\n\t\tgoto out;\n\tif (!d_is_reg(nd->path.dentry))\n\t\twill_truncate = false;\n\n\tif (will_truncate) {\n\t\terror = mnt_want_write(nd->path.mnt);\n\t\tif (error)\n\t\t\tgoto out;\n\t\tgot_write = true;\n\t}\nfinish_open_created:\n\tif (likely(!(open_flag & O_PATH))) {\n\t\terror = may_open(&nd->path, acc_mode, open_flag);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tBUG_ON(*opened & FILE_OPENED); /* once it's opened, it's opened */\n\terror = vfs_open(&nd->path, file, current_cred());\n\tif (!error) {\n\t\t*opened |= FILE_OPENED;\n\t} else {\n\t\tif (error == -EOPENSTALE)\n\t\t\tgoto stale_open;\n\t\tgoto out;\n\t}\nopened:\n\terror = open_check_o_direct(file);\n\tif (error)\n\t\tgoto exit_fput;\n\terror = ima_file_check(file, op->acc_mode, *opened);\n\tif (error)\n\t\tgoto exit_fput;\n\n\tif (will_truncate) {\n\t\terror = handle_truncate(file);\n\t\tif (error)\n\t\t\tgoto exit_fput;\n\t}\nout:\n\tif (unlikely(error > 0)) {\n\t\tWARN_ON(1);\n\t\terror = -EINVAL;\n\t}\n\tif (got_write)\n\t\tmnt_drop_write(nd->path.mnt);\n\tpath_put(&save_parent);\n\treturn error;\n\nexit_fput:\n\tfput(file);\n\tgoto out;\n\nstale_open:\n\t/* If no saved parent or already retried then can't retry */\n\tif (!save_parent.dentry || retried)\n\t\tgoto out;\n\n\tBUG_ON(save_parent.dentry != dir);\n\tpath_put(&nd->path);\n\tnd->path = save_parent;\n\tnd->inode = dir->d_inode;\n\tsave_parent.mnt = NULL;\n\tsave_parent.dentry = NULL;\n\tif (got_write) {\n\t\tmnt_drop_write(nd->path.mnt);\n\t\tgot_write = false;\n\t}\n\tretried = true;\n\tgoto retry_lookup;\n}\n\nstatic int do_tmpfile(struct nameidata *nd, unsigned flags,\n\t\tconst struct open_flags *op,\n\t\tstruct file *file, int *opened)\n{\n\tstatic const struct qstr name = QSTR_INIT(\"/\", 1);\n\tstruct dentry *child;\n\tstruct inode *dir;\n\tstruct path path;\n\tint error = path_lookupat(nd, flags | LOOKUP_DIRECTORY, &path);\n\tif (unlikely(error))\n\t\treturn error;\n\terror = mnt_want_write(path.mnt);\n\tif (unlikely(error))\n\t\tgoto out;\n\tdir = path.dentry->d_inode;\n\t/* we want directory to be writable */\n\terror = inode_permission(dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\tgoto out2;\n\tif (!dir->i_op->tmpfile) {\n\t\terror = -EOPNOTSUPP;\n\t\tgoto out2;\n\t}\n\tchild = d_alloc(path.dentry, &name);\n\tif (unlikely(!child)) {\n\t\terror = -ENOMEM;\n\t\tgoto out2;\n\t}\n\tdput(path.dentry);\n\tpath.dentry = child;\n\terror = dir->i_op->tmpfile(dir, child, op->mode);\n\tif (error)\n\t\tgoto out2;\n\taudit_inode(nd->name, child, 0);\n\t/* Don't check for other permissions, the inode was just created */\n\terror = may_open(&path, 0, op->open_flag);\n\tif (error)\n\t\tgoto out2;\n\tfile->f_path.mnt = path.mnt;\n\terror = finish_open(file, child, NULL, opened);\n\tif (error)\n\t\tgoto out2;\n\terror = open_check_o_direct(file);\n\tif (error) {\n\t\tfput(file);\n\t} else if (!(op->open_flag & O_EXCL)) {\n\t\tstruct inode *inode = file_inode(file);\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_state |= I_LINKABLE;\n\t\tspin_unlock(&inode->i_lock);\n\t}\nout2:\n\tmnt_drop_write(path.mnt);\nout:\n\tpath_put(&path);\n\treturn error;\n}\n\nstatic struct file *path_openat(struct nameidata *nd,\n\t\t\tconst struct open_flags *op, unsigned flags)\n{\n\tconst char *s;\n\tstruct file *file;\n\tint opened = 0;\n\tint error;\n\n\tfile = get_empty_filp();\n\tif (IS_ERR(file))\n\t\treturn file;\n\n\tfile->f_flags = op->open_flag;\n\n\tif (unlikely(file->f_flags & __O_TMPFILE)) {\n\t\terror = do_tmpfile(nd, flags, op, file, &opened);\n\t\tgoto out2;\n\t}\n\n\ts = path_init(nd, flags);\n\tif (IS_ERR(s)) {\n\t\tput_filp(file);\n\t\treturn ERR_CAST(s);\n\t}\n\twhile (!(error = link_path_walk(s, nd)) &&\n\t\t(error = do_last(nd, file, op, &opened)) > 0) {\n\t\tnd->flags &= ~(LOOKUP_OPEN|LOOKUP_CREATE|LOOKUP_EXCL);\n\t\ts = trailing_symlink(nd);\n\t\tif (IS_ERR(s)) {\n\t\t\terror = PTR_ERR(s);\n\t\t\tbreak;\n\t\t}\n\t}\n\tterminate_walk(nd);\nout2:\n\tif (!(opened & FILE_OPENED)) {\n\t\tBUG_ON(!error);\n\t\tput_filp(file);\n\t}\n\tif (unlikely(error)) {\n\t\tif (error == -EOPENSTALE) {\n\t\t\tif (flags & LOOKUP_RCU)\n\t\t\t\terror = -ECHILD;\n\t\t\telse\n\t\t\t\terror = -ESTALE;\n\t\t}\n\t\tfile = ERR_PTR(error);\n\t}\n\treturn file;\n}\n\nstruct file *do_filp_open(int dfd, struct filename *pathname,\n\t\tconst struct open_flags *op)\n{\n\tstruct nameidata nd;\n\tint flags = op->lookup_flags;\n\tstruct file *filp;\n\n\tset_nameidata(&nd, dfd, pathname);\n\tfilp = path_openat(&nd, op, flags | LOOKUP_RCU);\n\tif (unlikely(filp == ERR_PTR(-ECHILD)))\n\t\tfilp = path_openat(&nd, op, flags);\n\tif (unlikely(filp == ERR_PTR(-ESTALE)))\n\t\tfilp = path_openat(&nd, op, flags | LOOKUP_REVAL);\n\trestore_nameidata();\n\treturn filp;\n}\n\nstruct file *do_file_open_root(struct dentry *dentry, struct vfsmount *mnt,\n\t\tconst char *name, const struct open_flags *op)\n{\n\tstruct nameidata nd;\n\tstruct file *file;\n\tstruct filename *filename;\n\tint flags = op->lookup_flags | LOOKUP_ROOT;\n\n\tnd.root.mnt = mnt;\n\tnd.root.dentry = dentry;\n\n\tif (d_is_symlink(dentry) && op->intent & LOOKUP_OPEN)\n\t\treturn ERR_PTR(-ELOOP);\n\n\tfilename = getname_kernel(name);\n\tif (IS_ERR(filename))\n\t\treturn ERR_CAST(filename);\n\n\tset_nameidata(&nd, -1, filename);\n\tfile = path_openat(&nd, op, flags | LOOKUP_RCU);\n\tif (unlikely(file == ERR_PTR(-ECHILD)))\n\t\tfile = path_openat(&nd, op, flags);\n\tif (unlikely(file == ERR_PTR(-ESTALE)))\n\t\tfile = path_openat(&nd, op, flags | LOOKUP_REVAL);\n\trestore_nameidata();\n\tputname(filename);\n\treturn file;\n}\n\nstatic struct dentry *filename_create(int dfd, struct filename *name,\n\t\t\t\tstruct path *path, unsigned int lookup_flags)\n{\n\tstruct dentry *dentry = ERR_PTR(-EEXIST);\n\tstruct qstr last;\n\tint type;\n\tint err2;\n\tint error;\n\tbool is_dir = (lookup_flags & LOOKUP_DIRECTORY);\n\n\t/*\n\t * Note that only LOOKUP_REVAL and LOOKUP_DIRECTORY matter here. Any\n\t * other flags passed in are ignored!\n\t */\n\tlookup_flags &= LOOKUP_REVAL;\n\n\tname = filename_parentat(dfd, name, lookup_flags, path, &last, &type);\n\tif (IS_ERR(name))\n\t\treturn ERR_CAST(name);\n\n\t/*\n\t * Yucky last component or no last component at all?\n\t * (foo/., foo/.., /////)\n\t */\n\tif (unlikely(type != LAST_NORM))\n\t\tgoto out;\n\n\t/* don't fail immediately if it's r/o, at least try to report other errors */\n\terr2 = mnt_want_write(path->mnt);\n\t/*\n\t * Do the final lookup.\n\t */\n\tlookup_flags |= LOOKUP_CREATE | LOOKUP_EXCL;\n\tinode_lock_nested(path->dentry->d_inode, I_MUTEX_PARENT);\n\tdentry = __lookup_hash(&last, path->dentry, lookup_flags);\n\tif (IS_ERR(dentry))\n\t\tgoto unlock;\n\n\terror = -EEXIST;\n\tif (d_is_positive(dentry))\n\t\tgoto fail;\n\n\t/*\n\t * Special case - lookup gave negative, but... we had foo/bar/\n\t * From the vfs_mknod() POV we just have a negative dentry -\n\t * all is fine. Let's be bastards - you had / on the end, you've\n\t * been asking for (non-existent) directory. -ENOENT for you.\n\t */\n\tif (unlikely(!is_dir && last.name[last.len])) {\n\t\terror = -ENOENT;\n\t\tgoto fail;\n\t}\n\tif (unlikely(err2)) {\n\t\terror = err2;\n\t\tgoto fail;\n\t}\n\tputname(name);\n\treturn dentry;\nfail:\n\tdput(dentry);\n\tdentry = ERR_PTR(error);\nunlock:\n\tinode_unlock(path->dentry->d_inode);\n\tif (!err2)\n\t\tmnt_drop_write(path->mnt);\nout:\n\tpath_put(path);\n\tputname(name);\n\treturn dentry;\n}\n\nstruct dentry *kern_path_create(int dfd, const char *pathname,\n\t\t\t\tstruct path *path, unsigned int lookup_flags)\n{\n\treturn filename_create(dfd, getname_kernel(pathname),\n\t\t\t\tpath, lookup_flags);\n}\nEXPORT_SYMBOL(kern_path_create);\n\nvoid done_path_create(struct path *path, struct dentry *dentry)\n{\n\tdput(dentry);\n\tinode_unlock(path->dentry->d_inode);\n\tmnt_drop_write(path->mnt);\n\tpath_put(path);\n}\nEXPORT_SYMBOL(done_path_create);\n\ninline struct dentry *user_path_create(int dfd, const char __user *pathname,\n\t\t\t\tstruct path *path, unsigned int lookup_flags)\n{\n\treturn filename_create(dfd, getname(pathname), path, lookup_flags);\n}\nEXPORT_SYMBOL(user_path_create);\n\nint vfs_mknod(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev)\n{\n\tint error = may_create(dir, dentry);\n\n\tif (error)\n\t\treturn error;\n\n\tif ((S_ISCHR(mode) || S_ISBLK(mode)) && !capable(CAP_MKNOD))\n\t\treturn -EPERM;\n\n\tif (!dir->i_op->mknod)\n\t\treturn -EPERM;\n\n\terror = devcgroup_inode_mknod(mode, dev);\n\tif (error)\n\t\treturn error;\n\n\terror = security_inode_mknod(dir, dentry, mode, dev);\n\tif (error)\n\t\treturn error;\n\n\terror = dir->i_op->mknod(dir, dentry, mode, dev);\n\tif (!error)\n\t\tfsnotify_create(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_mknod);\n\nstatic int may_mknod(umode_t mode)\n{\n\tswitch (mode & S_IFMT) {\n\tcase S_IFREG:\n\tcase S_IFCHR:\n\tcase S_IFBLK:\n\tcase S_IFIFO:\n\tcase S_IFSOCK:\n\tcase 0: /* zero mode translates to S_IFREG */\n\t\treturn 0;\n\tcase S_IFDIR:\n\t\treturn -EPERM;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nSYSCALL_DEFINE4(mknodat, int, dfd, const char __user *, filename, umode_t, mode,\n\t\tunsigned, dev)\n{\n\tstruct dentry *dentry;\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = 0;\n\n\terror = may_mknod(mode);\n\tif (error)\n\t\treturn error;\nretry:\n\tdentry = user_path_create(dfd, filename, &path, lookup_flags);\n\tif (IS_ERR(dentry))\n\t\treturn PTR_ERR(dentry);\n\n\tif (!IS_POSIXACL(path.dentry->d_inode))\n\t\tmode &= ~current_umask();\n\terror = security_path_mknod(&path, dentry, mode, dev);\n\tif (error)\n\t\tgoto out;\n\tswitch (mode & S_IFMT) {\n\t\tcase 0: case S_IFREG:\n\t\t\terror = vfs_create(path.dentry->d_inode,dentry,mode,true);\n\t\t\tbreak;\n\t\tcase S_IFCHR: case S_IFBLK:\n\t\t\terror = vfs_mknod(path.dentry->d_inode,dentry,mode,\n\t\t\t\t\tnew_decode_dev(dev));\n\t\t\tbreak;\n\t\tcase S_IFIFO: case S_IFSOCK:\n\t\t\terror = vfs_mknod(path.dentry->d_inode,dentry,mode,0);\n\t\t\tbreak;\n\t}\nout:\n\tdone_path_create(&path, dentry);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE3(mknod, const char __user *, filename, umode_t, mode, unsigned, dev)\n{\n\treturn sys_mknodat(AT_FDCWD, filename, mode, dev);\n}\n\nint vfs_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)\n{\n\tint error = may_create(dir, dentry);\n\tunsigned max_links = dir->i_sb->s_max_links;\n\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->mkdir)\n\t\treturn -EPERM;\n\n\tmode &= (S_IRWXUGO|S_ISVTX);\n\terror = security_inode_mkdir(dir, dentry, mode);\n\tif (error)\n\t\treturn error;\n\n\tif (max_links && dir->i_nlink >= max_links)\n\t\treturn -EMLINK;\n\n\terror = dir->i_op->mkdir(dir, dentry, mode);\n\tif (!error)\n\t\tfsnotify_mkdir(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_mkdir);\n\nSYSCALL_DEFINE3(mkdirat, int, dfd, const char __user *, pathname, umode_t, mode)\n{\n\tstruct dentry *dentry;\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = LOOKUP_DIRECTORY;\n\nretry:\n\tdentry = user_path_create(dfd, pathname, &path, lookup_flags);\n\tif (IS_ERR(dentry))\n\t\treturn PTR_ERR(dentry);\n\n\tif (!IS_POSIXACL(path.dentry->d_inode))\n\t\tmode &= ~current_umask();\n\terror = security_path_mkdir(&path, dentry, mode);\n\tif (!error)\n\t\terror = vfs_mkdir(path.dentry->d_inode, dentry, mode);\n\tdone_path_create(&path, dentry);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(mkdir, const char __user *, pathname, umode_t, mode)\n{\n\treturn sys_mkdirat(AT_FDCWD, pathname, mode);\n}\n\nint vfs_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\tint error = may_delete(dir, dentry, 1);\n\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->rmdir)\n\t\treturn -EPERM;\n\n\tdget(dentry);\n\tinode_lock(dentry->d_inode);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(dentry))\n\t\tgoto out;\n\n\terror = security_inode_rmdir(dir, dentry);\n\tif (error)\n\t\tgoto out;\n\n\tshrink_dcache_parent(dentry);\n\terror = dir->i_op->rmdir(dir, dentry);\n\tif (error)\n\t\tgoto out;\n\n\tdentry->d_inode->i_flags |= S_DEAD;\n\tdont_mount(dentry);\n\tdetach_mounts(dentry);\n\nout:\n\tinode_unlock(dentry->d_inode);\n\tdput(dentry);\n\tif (!error)\n\t\td_delete(dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_rmdir);\n\nstatic long do_rmdir(int dfd, const char __user *pathname)\n{\n\tint error = 0;\n\tstruct filename *name;\n\tstruct dentry *dentry;\n\tstruct path path;\n\tstruct qstr last;\n\tint type;\n\tunsigned int lookup_flags = 0;\nretry:\n\tname = user_path_parent(dfd, pathname,\n\t\t\t\t&path, &last, &type, lookup_flags);\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\n\tswitch (type) {\n\tcase LAST_DOTDOT:\n\t\terror = -ENOTEMPTY;\n\t\tgoto exit1;\n\tcase LAST_DOT:\n\t\terror = -EINVAL;\n\t\tgoto exit1;\n\tcase LAST_ROOT:\n\t\terror = -EBUSY;\n\t\tgoto exit1;\n\t}\n\n\terror = mnt_want_write(path.mnt);\n\tif (error)\n\t\tgoto exit1;\n\n\tinode_lock_nested(path.dentry->d_inode, I_MUTEX_PARENT);\n\tdentry = __lookup_hash(&last, path.dentry, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto exit2;\n\tif (!dentry->d_inode) {\n\t\terror = -ENOENT;\n\t\tgoto exit3;\n\t}\n\terror = security_path_rmdir(&path, dentry);\n\tif (error)\n\t\tgoto exit3;\n\terror = vfs_rmdir(path.dentry->d_inode, dentry);\nexit3:\n\tdput(dentry);\nexit2:\n\tinode_unlock(path.dentry->d_inode);\n\tmnt_drop_write(path.mnt);\nexit1:\n\tpath_put(&path);\n\tputname(name);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE1(rmdir, const char __user *, pathname)\n{\n\treturn do_rmdir(AT_FDCWD, pathname);\n}\n\n/**\n * vfs_unlink - unlink a filesystem object\n * @dir:\tparent directory\n * @dentry:\tvictim\n * @delegated_inode: returns victim inode, if the inode is delegated.\n *\n * The caller must hold dir->i_mutex.\n *\n * If vfs_unlink discovers a delegation, it will return -EWOULDBLOCK and\n * return a reference to the inode in delegated_inode.  The caller\n * should then break the delegation on that inode and retry.  Because\n * breaking a delegation may take a long time, the caller should drop\n * dir->i_mutex before doing so.\n *\n * Alternatively, a caller may pass NULL for delegated_inode.  This may\n * be appropriate for callers that expect the underlying filesystem not\n * to be NFS exported.\n */\nint vfs_unlink(struct inode *dir, struct dentry *dentry, struct inode **delegated_inode)\n{\n\tstruct inode *target = dentry->d_inode;\n\tint error = may_delete(dir, dentry, 0);\n\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->unlink)\n\t\treturn -EPERM;\n\n\tinode_lock(target);\n\tif (is_local_mountpoint(dentry))\n\t\terror = -EBUSY;\n\telse {\n\t\terror = security_inode_unlink(dir, dentry);\n\t\tif (!error) {\n\t\t\terror = try_break_deleg(target, delegated_inode);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t\terror = dir->i_op->unlink(dir, dentry);\n\t\t\tif (!error) {\n\t\t\t\tdont_mount(dentry);\n\t\t\t\tdetach_mounts(dentry);\n\t\t\t}\n\t\t}\n\t}\nout:\n\tinode_unlock(target);\n\n\t/* We don't d_delete() NFS sillyrenamed files--they still exist. */\n\tif (!error && !(dentry->d_flags & DCACHE_NFSFS_RENAMED)) {\n\t\tfsnotify_link_count(target);\n\t\td_delete(dentry);\n\t}\n\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_unlink);\n\n/*\n * Make sure that the actual truncation of the file will occur outside its\n * directory's i_mutex.  Truncate can take a long time if there is a lot of\n * writeout happening, and we don't want to prevent access to the directory\n * while waiting on the I/O.\n */\nstatic long do_unlinkat(int dfd, const char __user *pathname)\n{\n\tint error;\n\tstruct filename *name;\n\tstruct dentry *dentry;\n\tstruct path path;\n\tstruct qstr last;\n\tint type;\n\tstruct inode *inode = NULL;\n\tstruct inode *delegated_inode = NULL;\n\tunsigned int lookup_flags = 0;\nretry:\n\tname = user_path_parent(dfd, pathname,\n\t\t\t\t&path, &last, &type, lookup_flags);\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\n\terror = -EISDIR;\n\tif (type != LAST_NORM)\n\t\tgoto exit1;\n\n\terror = mnt_want_write(path.mnt);\n\tif (error)\n\t\tgoto exit1;\nretry_deleg:\n\tinode_lock_nested(path.dentry->d_inode, I_MUTEX_PARENT);\n\tdentry = __lookup_hash(&last, path.dentry, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (!IS_ERR(dentry)) {\n\t\t/* Why not before? Because we want correct error value */\n\t\tif (last.name[last.len])\n\t\t\tgoto slashes;\n\t\tinode = dentry->d_inode;\n\t\tif (d_is_negative(dentry))\n\t\t\tgoto slashes;\n\t\tihold(inode);\n\t\terror = security_path_unlink(&path, dentry);\n\t\tif (error)\n\t\t\tgoto exit2;\n\t\terror = vfs_unlink(path.dentry->d_inode, dentry, &delegated_inode);\nexit2:\n\t\tdput(dentry);\n\t}\n\tinode_unlock(path.dentry->d_inode);\n\tif (inode)\n\t\tiput(inode);\t/* truncate the inode here */\n\tinode = NULL;\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\tmnt_drop_write(path.mnt);\nexit1:\n\tpath_put(&path);\n\tputname(name);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tinode = NULL;\n\t\tgoto retry;\n\t}\n\treturn error;\n\nslashes:\n\tif (d_is_negative(dentry))\n\t\terror = -ENOENT;\n\telse if (d_is_dir(dentry))\n\t\terror = -EISDIR;\n\telse\n\t\terror = -ENOTDIR;\n\tgoto exit2;\n}\n\nSYSCALL_DEFINE3(unlinkat, int, dfd, const char __user *, pathname, int, flag)\n{\n\tif ((flag & ~AT_REMOVEDIR) != 0)\n\t\treturn -EINVAL;\n\n\tif (flag & AT_REMOVEDIR)\n\t\treturn do_rmdir(dfd, pathname);\n\n\treturn do_unlinkat(dfd, pathname);\n}\n\nSYSCALL_DEFINE1(unlink, const char __user *, pathname)\n{\n\treturn do_unlinkat(AT_FDCWD, pathname);\n}\n\nint vfs_symlink(struct inode *dir, struct dentry *dentry, const char *oldname)\n{\n\tint error = may_create(dir, dentry);\n\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->symlink)\n\t\treturn -EPERM;\n\n\terror = security_inode_symlink(dir, dentry, oldname);\n\tif (error)\n\t\treturn error;\n\n\terror = dir->i_op->symlink(dir, dentry, oldname);\n\tif (!error)\n\t\tfsnotify_create(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_symlink);\n\nSYSCALL_DEFINE3(symlinkat, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname)\n{\n\tint error;\n\tstruct filename *from;\n\tstruct dentry *dentry;\n\tstruct path path;\n\tunsigned int lookup_flags = 0;\n\n\tfrom = getname(oldname);\n\tif (IS_ERR(from))\n\t\treturn PTR_ERR(from);\nretry:\n\tdentry = user_path_create(newdfd, newname, &path, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out_putname;\n\n\terror = security_path_symlink(&path, dentry, from->name);\n\tif (!error)\n\t\terror = vfs_symlink(path.dentry->d_inode, dentry, from->name);\n\tdone_path_create(&path, dentry);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout_putname:\n\tputname(from);\n\treturn error;\n}\n\nSYSCALL_DEFINE2(symlink, const char __user *, oldname, const char __user *, newname)\n{\n\treturn sys_symlinkat(oldname, AT_FDCWD, newname);\n}\n\n/**\n * vfs_link - create a new link\n * @old_dentry:\tobject to be linked\n * @dir:\tnew parent\n * @new_dentry:\twhere to create the new link\n * @delegated_inode: returns inode needing a delegation break\n *\n * The caller must hold dir->i_mutex\n *\n * If vfs_link discovers a delegation on the to-be-linked file in need\n * of breaking, it will return -EWOULDBLOCK and return a reference to the\n * inode in delegated_inode.  The caller should then break the delegation\n * and retry.  Because breaking a delegation may take a long time, the\n * caller should drop the i_mutex before doing so.\n *\n * Alternatively, a caller may pass NULL for delegated_inode.  This may\n * be appropriate for callers that expect the underlying filesystem not\n * to be NFS exported.\n */\nint vfs_link(struct dentry *old_dentry, struct inode *dir, struct dentry *new_dentry, struct inode **delegated_inode)\n{\n\tstruct inode *inode = old_dentry->d_inode;\n\tunsigned max_links = dir->i_sb->s_max_links;\n\tint error;\n\n\tif (!inode)\n\t\treturn -ENOENT;\n\n\terror = may_create(dir, new_dentry);\n\tif (error)\n\t\treturn error;\n\n\tif (dir->i_sb != inode->i_sb)\n\t\treturn -EXDEV;\n\n\t/*\n\t * A link to an append-only or immutable file cannot be created.\n\t */\n\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode))\n\t\treturn -EPERM;\n\tif (!dir->i_op->link)\n\t\treturn -EPERM;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn -EPERM;\n\n\terror = security_inode_link(old_dentry, dir, new_dentry);\n\tif (error)\n\t\treturn error;\n\n\tinode_lock(inode);\n\t/* Make sure we don't allow creating hardlink to an unlinked file */\n\tif (inode->i_nlink == 0 && !(inode->i_state & I_LINKABLE))\n\t\terror =  -ENOENT;\n\telse if (max_links && inode->i_nlink >= max_links)\n\t\terror = -EMLINK;\n\telse {\n\t\terror = try_break_deleg(inode, delegated_inode);\n\t\tif (!error)\n\t\t\terror = dir->i_op->link(old_dentry, dir, new_dentry);\n\t}\n\n\tif (!error && (inode->i_state & I_LINKABLE)) {\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_state &= ~I_LINKABLE;\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\tinode_unlock(inode);\n\tif (!error)\n\t\tfsnotify_link(dir, inode, new_dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_link);\n\n/*\n * Hardlinks are often used in delicate situations.  We avoid\n * security-related surprises by not following symlinks on the\n * newname.  --KAB\n *\n * We don't follow them on the oldname either to be compatible\n * with linux 2.0, and to avoid hard-linking to directories\n * and other special files.  --ADM\n */\nSYSCALL_DEFINE5(linkat, int, olddfd, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname, int, flags)\n{\n\tstruct dentry *new_dentry;\n\tstruct path old_path, new_path;\n\tstruct inode *delegated_inode = NULL;\n\tint how = 0;\n\tint error;\n\n\tif ((flags & ~(AT_SYMLINK_FOLLOW | AT_EMPTY_PATH)) != 0)\n\t\treturn -EINVAL;\n\t/*\n\t * To use null names we require CAP_DAC_READ_SEARCH\n\t * This ensures that not everyone will be able to create\n\t * handlink using the passed filedescriptor.\n\t */\n\tif (flags & AT_EMPTY_PATH) {\n\t\tif (!capable(CAP_DAC_READ_SEARCH))\n\t\t\treturn -ENOENT;\n\t\thow = LOOKUP_EMPTY;\n\t}\n\n\tif (flags & AT_SYMLINK_FOLLOW)\n\t\thow |= LOOKUP_FOLLOW;\nretry:\n\terror = user_path_at(olddfd, oldname, how, &old_path);\n\tif (error)\n\t\treturn error;\n\n\tnew_dentry = user_path_create(newdfd, newname, &new_path,\n\t\t\t\t\t(how & LOOKUP_REVAL));\n\terror = PTR_ERR(new_dentry);\n\tif (IS_ERR(new_dentry))\n\t\tgoto out;\n\n\terror = -EXDEV;\n\tif (old_path.mnt != new_path.mnt)\n\t\tgoto out_dput;\n\terror = may_linkat(&old_path);\n\tif (unlikely(error))\n\t\tgoto out_dput;\n\terror = security_path_link(old_path.dentry, &new_path, new_dentry);\n\tif (error)\n\t\tgoto out_dput;\n\terror = vfs_link(old_path.dentry, new_path.dentry->d_inode, new_dentry, &delegated_inode);\nout_dput:\n\tdone_path_create(&new_path, new_dentry);\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error) {\n\t\t\tpath_put(&old_path);\n\t\t\tgoto retry;\n\t\t}\n\t}\n\tif (retry_estale(error, how)) {\n\t\tpath_put(&old_path);\n\t\thow |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout:\n\tpath_put(&old_path);\n\n\treturn error;\n}\n\nSYSCALL_DEFINE2(link, const char __user *, oldname, const char __user *, newname)\n{\n\treturn sys_linkat(AT_FDCWD, oldname, AT_FDCWD, newname, 0);\n}\n\n/**\n * vfs_rename - rename a filesystem object\n * @old_dir:\tparent of source\n * @old_dentry:\tsource\n * @new_dir:\tparent of destination\n * @new_dentry:\tdestination\n * @delegated_inode: returns an inode needing a delegation break\n * @flags:\trename flags\n *\n * The caller must hold multiple mutexes--see lock_rename()).\n *\n * If vfs_rename discovers a delegation in need of breaking at either\n * the source or destination, it will return -EWOULDBLOCK and return a\n * reference to the inode in delegated_inode.  The caller should then\n * break the delegation and retry.  Because breaking a delegation may\n * take a long time, the caller should drop all locks before doing\n * so.\n *\n * Alternatively, a caller may pass NULL for delegated_inode.  This may\n * be appropriate for callers that expect the underlying filesystem not\n * to be NFS exported.\n *\n * The worst of all namespace operations - renaming directory. \"Perverted\"\n * doesn't even start to describe it. Somebody in UCB had a heck of a trip...\n * Problems:\n *\ta) we can get into loop creation.\n *\tb) race potential - two innocent renames can create a loop together.\n *\t   That's where 4.4 screws up. Current fix: serialization on\n *\t   sb->s_vfs_rename_mutex. We might be more accurate, but that's another\n *\t   story.\n *\tc) we have to lock _four_ objects - parents and victim (if it exists),\n *\t   and source (if it is not a directory).\n *\t   And that - after we got ->i_mutex on parents (until then we don't know\n *\t   whether the target exists).  Solution: try to be smart with locking\n *\t   order for inodes.  We rely on the fact that tree topology may change\n *\t   only under ->s_vfs_rename_mutex _and_ that parent of the object we\n *\t   move will be locked.  Thus we can rank directories by the tree\n *\t   (ancestors first) and rank all non-directories after them.\n *\t   That works since everybody except rename does \"lock parent, lookup,\n *\t   lock child\" and rename is under ->s_vfs_rename_mutex.\n *\t   HOWEVER, it relies on the assumption that any object with ->lookup()\n *\t   has no more than 1 dentry.  If \"hybrid\" objects will ever appear,\n *\t   we'd better make sure that there's no link(2) for them.\n *\td) conversion from fhandle to dentry may come in the wrong moment - when\n *\t   we are removing the target. Solution: we will have to grab ->i_mutex\n *\t   in the fhandle_to_dentry code. [FIXME - current nfsfh.c relies on\n *\t   ->i_mutex on parents, which works but leads to some truly excessive\n *\t   locking].\n */\nint vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n\t       struct inode *new_dir, struct dentry *new_dentry,\n\t       struct inode **delegated_inode, unsigned int flags)\n{\n\tint error;\n\tbool is_dir = d_is_dir(old_dentry);\n\tconst unsigned char *old_name;\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\n\t/*\n\t * Check source == target.\n\t * On overlayfs need to look at underlying inodes.\n\t */\n\tif (vfs_select_inode(old_dentry, 0) == vfs_select_inode(new_dentry, 0))\n\t\treturn 0;\n\n\terror = may_delete(old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(new_dir, new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(new_dir, new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename && !old_dir->i_op->rename2)\n\t\treturn -EPERM;\n\n\tif (flags && !old_dir->i_op->rename2)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(source, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(target, MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\told_name = fsnotify_oldname_init(old_dentry->d_name.name);\n\tdget(new_dentry);\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_lock(target);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (is_dir && !(flags & RENAME_EXCHANGE) && target)\n\t\tshrink_dcache_parent(new_dentry);\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (!old_dir->i_op->rename2) {\n\t\terror = old_dir->i_op->rename(old_dir, old_dentry,\n\t\t\t\t\t      new_dir, new_dentry);\n\t} else {\n\t\tWARN_ON(old_dir->i_op->rename != NULL);\n\t\terror = old_dir->i_op->rename2(old_dir, old_dentry,\n\t\t\t\t\t       new_dir, new_dentry, flags);\n\t}\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir)\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || (flags & RENAME_EXCHANGE))\n\t\tunlock_two_nondirectories(source, target);\n\telse if (target)\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, old_name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, old_dentry->d_name.name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\tfsnotify_oldname_free(old_name);\n\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_rename);\n\nSYSCALL_DEFINE5(renameat2, int, olddfd, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname, unsigned int, flags)\n{\n\tstruct dentry *old_dentry, *new_dentry;\n\tstruct dentry *trap;\n\tstruct path old_path, new_path;\n\tstruct qstr old_last, new_last;\n\tint old_type, new_type;\n\tstruct inode *delegated_inode = NULL;\n\tstruct filename *from;\n\tstruct filename *to;\n\tunsigned int lookup_flags = 0, target_flags = LOOKUP_RENAME_TARGET;\n\tbool should_retry = false;\n\tint error;\n\n\tif (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))\n\t\treturn -EINVAL;\n\n\tif ((flags & (RENAME_NOREPLACE | RENAME_WHITEOUT)) &&\n\t    (flags & RENAME_EXCHANGE))\n\t\treturn -EINVAL;\n\n\tif ((flags & RENAME_WHITEOUT) && !capable(CAP_MKNOD))\n\t\treturn -EPERM;\n\n\tif (flags & RENAME_EXCHANGE)\n\t\ttarget_flags = 0;\n\nretry:\n\tfrom = user_path_parent(olddfd, oldname,\n\t\t\t\t&old_path, &old_last, &old_type, lookup_flags);\n\tif (IS_ERR(from)) {\n\t\terror = PTR_ERR(from);\n\t\tgoto exit;\n\t}\n\n\tto = user_path_parent(newdfd, newname,\n\t\t\t\t&new_path, &new_last, &new_type, lookup_flags);\n\tif (IS_ERR(to)) {\n\t\terror = PTR_ERR(to);\n\t\tgoto exit1;\n\t}\n\n\terror = -EXDEV;\n\tif (old_path.mnt != new_path.mnt)\n\t\tgoto exit2;\n\n\terror = -EBUSY;\n\tif (old_type != LAST_NORM)\n\t\tgoto exit2;\n\n\tif (flags & RENAME_NOREPLACE)\n\t\terror = -EEXIST;\n\tif (new_type != LAST_NORM)\n\t\tgoto exit2;\n\n\terror = mnt_want_write(old_path.mnt);\n\tif (error)\n\t\tgoto exit2;\n\nretry_deleg:\n\ttrap = lock_rename(new_path.dentry, old_path.dentry);\n\n\told_dentry = __lookup_hash(&old_last, old_path.dentry, lookup_flags);\n\terror = PTR_ERR(old_dentry);\n\tif (IS_ERR(old_dentry))\n\t\tgoto exit3;\n\t/* source must exist */\n\terror = -ENOENT;\n\tif (d_is_negative(old_dentry))\n\t\tgoto exit4;\n\tnew_dentry = __lookup_hash(&new_last, new_path.dentry, lookup_flags | target_flags);\n\terror = PTR_ERR(new_dentry);\n\tif (IS_ERR(new_dentry))\n\t\tgoto exit4;\n\terror = -EEXIST;\n\tif ((flags & RENAME_NOREPLACE) && d_is_positive(new_dentry))\n\t\tgoto exit5;\n\tif (flags & RENAME_EXCHANGE) {\n\t\terror = -ENOENT;\n\t\tif (d_is_negative(new_dentry))\n\t\t\tgoto exit5;\n\n\t\tif (!d_is_dir(new_dentry)) {\n\t\t\terror = -ENOTDIR;\n\t\t\tif (new_last.name[new_last.len])\n\t\t\t\tgoto exit5;\n\t\t}\n\t}\n\t/* unless the source is a directory trailing slashes give -ENOTDIR */\n\tif (!d_is_dir(old_dentry)) {\n\t\terror = -ENOTDIR;\n\t\tif (old_last.name[old_last.len])\n\t\t\tgoto exit5;\n\t\tif (!(flags & RENAME_EXCHANGE) && new_last.name[new_last.len])\n\t\t\tgoto exit5;\n\t}\n\t/* source should not be ancestor of target */\n\terror = -EINVAL;\n\tif (old_dentry == trap)\n\t\tgoto exit5;\n\t/* target should not be an ancestor of source */\n\tif (!(flags & RENAME_EXCHANGE))\n\t\terror = -ENOTEMPTY;\n\tif (new_dentry == trap)\n\t\tgoto exit5;\n\n\terror = security_path_rename(&old_path, old_dentry,\n\t\t\t\t     &new_path, new_dentry, flags);\n\tif (error)\n\t\tgoto exit5;\n\terror = vfs_rename(old_path.dentry->d_inode, old_dentry,\n\t\t\t   new_path.dentry->d_inode, new_dentry,\n\t\t\t   &delegated_inode, flags);\nexit5:\n\tdput(new_dentry);\nexit4:\n\tdput(old_dentry);\nexit3:\n\tunlock_rename(new_path.dentry, old_path.dentry);\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\tmnt_drop_write(old_path.mnt);\nexit2:\n\tif (retry_estale(error, lookup_flags))\n\t\tshould_retry = true;\n\tpath_put(&new_path);\n\tputname(to);\nexit1:\n\tpath_put(&old_path);\n\tputname(from);\n\tif (should_retry) {\n\t\tshould_retry = false;\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nexit:\n\treturn error;\n}\n\nSYSCALL_DEFINE4(renameat, int, olddfd, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname)\n{\n\treturn sys_renameat2(olddfd, oldname, newdfd, newname, 0);\n}\n\nSYSCALL_DEFINE2(rename, const char __user *, oldname, const char __user *, newname)\n{\n\treturn sys_renameat2(AT_FDCWD, oldname, AT_FDCWD, newname, 0);\n}\n\nint vfs_whiteout(struct inode *dir, struct dentry *dentry)\n{\n\tint error = may_create(dir, dentry);\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->mknod)\n\t\treturn -EPERM;\n\n\treturn dir->i_op->mknod(dir, dentry,\n\t\t\t\tS_IFCHR | WHITEOUT_MODE, WHITEOUT_DEV);\n}\nEXPORT_SYMBOL(vfs_whiteout);\n\nint readlink_copy(char __user *buffer, int buflen, const char *link)\n{\n\tint len = PTR_ERR(link);\n\tif (IS_ERR(link))\n\t\tgoto out;\n\n\tlen = strlen(link);\n\tif (len > (unsigned) buflen)\n\t\tlen = buflen;\n\tif (copy_to_user(buffer, link, len))\n\t\tlen = -EFAULT;\nout:\n\treturn len;\n}\nEXPORT_SYMBOL(readlink_copy);\n\n/*\n * A helper for ->readlink().  This should be used *ONLY* for symlinks that\n * have ->get_link() not calling nd_jump_link().  Using (or not using) it\n * for any given inode is up to filesystem.\n */\nint generic_readlink(struct dentry *dentry, char __user *buffer, int buflen)\n{\n\tDEFINE_DELAYED_CALL(done);\n\tstruct inode *inode = d_inode(dentry);\n\tconst char *link = inode->i_link;\n\tint res;\n\n\tif (!link) {\n\t\tlink = inode->i_op->get_link(dentry, inode, &done);\n\t\tif (IS_ERR(link))\n\t\t\treturn PTR_ERR(link);\n\t}\n\tres = readlink_copy(buffer, buflen, link);\n\tdo_delayed_call(&done);\n\treturn res;\n}\nEXPORT_SYMBOL(generic_readlink);\n\n/* get the link contents into pagecache */\nconst char *page_get_link(struct dentry *dentry, struct inode *inode,\n\t\t\t  struct delayed_call *callback)\n{\n\tchar *kaddr;\n\tstruct page *page;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tif (!dentry) {\n\t\tpage = find_get_page(mapping, 0);\n\t\tif (!page)\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\tif (!PageUptodate(page)) {\n\t\t\tput_page(page);\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\t}\n\t} else {\n\t\tpage = read_mapping_page(mapping, 0, NULL);\n\t\tif (IS_ERR(page))\n\t\t\treturn (char*)page;\n\t}\n\tset_delayed_call(callback, page_put_link, page);\n\tBUG_ON(mapping_gfp_mask(mapping) & __GFP_HIGHMEM);\n\tkaddr = page_address(page);\n\tnd_terminate_link(kaddr, inode->i_size, PAGE_SIZE - 1);\n\treturn kaddr;\n}\n\nEXPORT_SYMBOL(page_get_link);\n\nvoid page_put_link(void *arg)\n{\n\tput_page(arg);\n}\nEXPORT_SYMBOL(page_put_link);\n\nint page_readlink(struct dentry *dentry, char __user *buffer, int buflen)\n{\n\tDEFINE_DELAYED_CALL(done);\n\tint res = readlink_copy(buffer, buflen,\n\t\t\t\tpage_get_link(dentry, d_inode(dentry),\n\t\t\t\t\t      &done));\n\tdo_delayed_call(&done);\n\treturn res;\n}\nEXPORT_SYMBOL(page_readlink);\n\n/*\n * The nofs argument instructs pagecache_write_begin to pass AOP_FLAG_NOFS\n */\nint __page_symlink(struct inode *inode, const char *symname, int len, int nofs)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\tvoid *fsdata;\n\tint err;\n\tunsigned int flags = AOP_FLAG_UNINTERRUPTIBLE;\n\tif (nofs)\n\t\tflags |= AOP_FLAG_NOFS;\n\nretry:\n\terr = pagecache_write_begin(NULL, mapping, 0, len-1,\n\t\t\t\tflags, &page, &fsdata);\n\tif (err)\n\t\tgoto fail;\n\n\tmemcpy(page_address(page), symname, len-1);\n\n\terr = pagecache_write_end(NULL, mapping, 0, len-1, len-1,\n\t\t\t\t\t\t\tpage, fsdata);\n\tif (err < 0)\n\t\tgoto fail;\n\tif (err < len-1)\n\t\tgoto retry;\n\n\tmark_inode_dirty(inode);\n\treturn 0;\nfail:\n\treturn err;\n}\nEXPORT_SYMBOL(__page_symlink);\n\nint page_symlink(struct inode *inode, const char *symname, int len)\n{\n\treturn __page_symlink(inode, symname, len,\n\t\t\t!mapping_gfp_constraint(inode->i_mapping, __GFP_FS));\n}\nEXPORT_SYMBOL(page_symlink);\n\nconst struct inode_operations page_symlink_inode_operations = {\n\t.readlink\t= generic_readlink,\n\t.get_link\t= page_get_link,\n};\nEXPORT_SYMBOL(page_symlink_inode_operations);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/namei.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n/*\n * Some corrections by tytso.\n */\n\n/* [Feb 1997 T. Schoebel-Theuer] Complete rewrite of the pathname\n * lookup logic.\n */\n/* [Feb-Apr 2000, AV] Rewrite to the new namespace architecture.\n */\n\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/wordpart.h>\n#include <linux/fs.h>\n#include <linux/filelock.h>\n#include <linux/namei.h>\n#include <linux/pagemap.h>\n#include <linux/sched/mm.h>\n#include <linux/fsnotify.h>\n#include <linux/personality.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/mount.h>\n#include <linux/audit.h>\n#include <linux/capability.h>\n#include <linux/file.h>\n#include <linux/fcntl.h>\n#include <linux/device_cgroup.h>\n#include <linux/fs_struct.h>\n#include <linux/posix_acl.h>\n#include <linux/hash.h>\n#include <linux/bitops.h>\n#include <linux/init_task.h>\n#include <linux/uaccess.h>\n\n#include \"internal.h\"\n#include \"mount.h\"\n\n/* [Feb-1997 T. Schoebel-Theuer]\n * Fundamental changes in the pathname lookup mechanisms (namei)\n * were necessary because of omirr.  The reason is that omirr needs\n * to know the _real_ pathname, not the user-supplied one, in case\n * of symlinks (and also when transname replacements occur).\n *\n * The new code replaces the old recursive symlink resolution with\n * an iterative one (in case of non-nested symlink chains).  It does\n * this with calls to <fs>_follow_link().\n * As a side effect, dir_namei(), _namei() and follow_link() are now \n * replaced with a single function lookup_dentry() that can handle all \n * the special cases of the former code.\n *\n * With the new dcache, the pathname is stored at each inode, at least as\n * long as the refcount of the inode is positive.  As a side effect, the\n * size of the dcache depends on the inode cache and thus is dynamic.\n *\n * [29-Apr-1998 C. Scott Ananian] Updated above description of symlink\n * resolution to correspond with current state of the code.\n *\n * Note that the symlink resolution is not *completely* iterative.\n * There is still a significant amount of tail- and mid- recursion in\n * the algorithm.  Also, note that <fs>_readlink() is not used in\n * lookup_dentry(): lookup_dentry() on the result of <fs>_readlink()\n * may return different results than <fs>_follow_link().  Many virtual\n * filesystems (including /proc) exhibit this behavior.\n */\n\n/* [24-Feb-97 T. Schoebel-Theuer] Side effects caused by new implementation:\n * New symlink semantics: when open() is called with flags O_CREAT | O_EXCL\n * and the name already exists in form of a symlink, try to create the new\n * name indicated by the symlink. The old code always complained that the\n * name already exists, due to not following the symlink even if its target\n * is nonexistent.  The new semantics affects also mknod() and link() when\n * the name is a symlink pointing to a non-existent name.\n *\n * I don't know which semantics is the right one, since I have no access\n * to standards. But I found by trial that HP-UX 9.0 has the full \"new\"\n * semantics implemented, while SunOS 4.1.1 and Solaris (SunOS 5.4) have the\n * \"old\" one. Personally, I think the new semantics is much more logical.\n * Note that \"ln old new\" where \"new\" is a symlink pointing to a non-existing\n * file does succeed in both HP-UX and SunOs, but not in Solaris\n * and in the old Linux semantics.\n */\n\n/* [16-Dec-97 Kevin Buhr] For security reasons, we change some symlink\n * semantics.  See the comments in \"open_namei\" and \"do_link\" below.\n *\n * [10-Sep-98 Alan Modra] Another symlink change.\n */\n\n/* [Feb-Apr 2000 AV] Complete rewrite. Rules for symlinks:\n *\tinside the path - always follow.\n *\tin the last component in creation/removal/renaming - never follow.\n *\tif LOOKUP_FOLLOW passed - follow.\n *\tif the pathname has trailing slashes - follow.\n *\totherwise - don't follow.\n * (applied in that order).\n *\n * [Jun 2000 AV] Inconsistent behaviour of open() in case if flags==O_CREAT\n * restored for 2.4. This is the last surviving part of old 4.2BSD bug.\n * During the 2.4 we need to fix the userland stuff depending on it -\n * hopefully we will be able to get rid of that wart in 2.5. So far only\n * XEmacs seems to be relying on it...\n */\n/*\n * [Sep 2001 AV] Single-semaphore locking scheme (kudos to David Holland)\n * implemented.  Let's see if raised priority of ->s_vfs_rename_mutex gives\n * any extra contention...\n */\n\n/* In order to reduce some races, while at the same time doing additional\n * checking and hopefully speeding things up, we copy filenames to the\n * kernel data space before using them..\n *\n * POSIX.1 2.4: an empty pathname is invalid (ENOENT).\n * PATH_MAX includes the nul terminator --RR.\n */\n\n#define EMBEDDED_NAME_MAX\t(PATH_MAX - offsetof(struct filename, iname))\n\nstatic inline void initname(struct filename *name, const char __user *uptr)\n{\n\tname->uptr = uptr;\n\tname->aname = NULL;\n\tatomic_set(&name->refcnt, 1);\n}\n\nstruct filename *\ngetname_flags(const char __user *filename, int flags)\n{\n\tstruct filename *result;\n\tchar *kname;\n\tint len;\n\n\tresult = audit_reusename(filename);\n\tif (result)\n\t\treturn result;\n\n\tresult = __getname();\n\tif (unlikely(!result))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t/*\n\t * First, try to embed the struct filename inside the names_cache\n\t * allocation\n\t */\n\tkname = (char *)result->iname;\n\tresult->name = kname;\n\n\tlen = strncpy_from_user(kname, filename, EMBEDDED_NAME_MAX);\n\t/*\n\t * Handle both empty path and copy failure in one go.\n\t */\n\tif (unlikely(len <= 0)) {\n\t\tif (unlikely(len < 0)) {\n\t\t\t__putname(result);\n\t\t\treturn ERR_PTR(len);\n\t\t}\n\n\t\t/* The empty path is special. */\n\t\tif (!(flags & LOOKUP_EMPTY)) {\n\t\t\t__putname(result);\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\t}\n\t}\n\n\t/*\n\t * Uh-oh. We have a name that's approaching PATH_MAX. Allocate a\n\t * separate struct filename so we can dedicate the entire\n\t * names_cache allocation for the pathname, and re-do the copy from\n\t * userland.\n\t */\n\tif (unlikely(len == EMBEDDED_NAME_MAX)) {\n\t\tconst size_t size = offsetof(struct filename, iname[1]);\n\t\tkname = (char *)result;\n\n\t\t/*\n\t\t * size is chosen that way we to guarantee that\n\t\t * result->iname[0] is within the same object and that\n\t\t * kname can't be equal to result->iname, no matter what.\n\t\t */\n\t\tresult = kzalloc(size, GFP_KERNEL);\n\t\tif (unlikely(!result)) {\n\t\t\t__putname(kname);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\tresult->name = kname;\n\t\tlen = strncpy_from_user(kname, filename, PATH_MAX);\n\t\tif (unlikely(len < 0)) {\n\t\t\t__putname(kname);\n\t\t\tkfree(result);\n\t\t\treturn ERR_PTR(len);\n\t\t}\n\t\t/* The empty path is special. */\n\t\tif (unlikely(!len) && !(flags & LOOKUP_EMPTY)) {\n\t\t\t__putname(kname);\n\t\t\tkfree(result);\n\t\t\treturn ERR_PTR(-ENOENT);\n\t\t}\n\t\tif (unlikely(len == PATH_MAX)) {\n\t\t\t__putname(kname);\n\t\t\tkfree(result);\n\t\t\treturn ERR_PTR(-ENAMETOOLONG);\n\t\t}\n\t}\n\tinitname(result, filename);\n\taudit_getname(result);\n\treturn result;\n}\n\nstruct filename *getname_uflags(const char __user *filename, int uflags)\n{\n\tint flags = (uflags & AT_EMPTY_PATH) ? LOOKUP_EMPTY : 0;\n\n\treturn getname_flags(filename, flags);\n}\n\nstruct filename *__getname_maybe_null(const char __user *pathname)\n{\n\tstruct filename *name;\n\tchar c;\n\n\t/* try to save on allocations; loss on um, though */\n\tif (get_user(c, pathname))\n\t\treturn ERR_PTR(-EFAULT);\n\tif (!c)\n\t\treturn NULL;\n\n\tname = getname_flags(pathname, LOOKUP_EMPTY);\n\tif (!IS_ERR(name) && !(name->name[0])) {\n\t\tputname(name);\n\t\tname = NULL;\n\t}\n\treturn name;\n}\n\nstruct filename *getname_kernel(const char * filename)\n{\n\tstruct filename *result;\n\tint len = strlen(filename) + 1;\n\n\tresult = __getname();\n\tif (unlikely(!result))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (len <= EMBEDDED_NAME_MAX) {\n\t\tresult->name = (char *)result->iname;\n\t} else if (len <= PATH_MAX) {\n\t\tconst size_t size = offsetof(struct filename, iname[1]);\n\t\tstruct filename *tmp;\n\n\t\ttmp = kmalloc(size, GFP_KERNEL);\n\t\tif (unlikely(!tmp)) {\n\t\t\t__putname(result);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\ttmp->name = (char *)result;\n\t\tresult = tmp;\n\t} else {\n\t\t__putname(result);\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\t}\n\tmemcpy((char *)result->name, filename, len);\n\tinitname(result, NULL);\n\taudit_getname(result);\n\treturn result;\n}\nEXPORT_SYMBOL(getname_kernel);\n\nvoid putname(struct filename *name)\n{\n\tint refcnt;\n\n\tif (IS_ERR_OR_NULL(name))\n\t\treturn;\n\n\trefcnt = atomic_read(&name->refcnt);\n\tif (refcnt != 1) {\n\t\tif (WARN_ON_ONCE(!refcnt))\n\t\t\treturn;\n\n\t\tif (!atomic_dec_and_test(&name->refcnt))\n\t\t\treturn;\n\t}\n\n\tif (name->name != name->iname) {\n\t\t__putname(name->name);\n\t\tkfree(name);\n\t} else\n\t\t__putname(name);\n}\nEXPORT_SYMBOL(putname);\n\n/**\n * check_acl - perform ACL permission checking\n * @idmap:\tidmap of the mount the inode was found from\n * @inode:\tinode to check permissions on\n * @mask:\tright to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)\n *\n * This function performs the ACL permission checking. Since this function\n * retrieve POSIX acls it needs to know whether it is called from a blocking or\n * non-blocking context and thus cares about the MAY_NOT_BLOCK bit.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nstatic int check_acl(struct mnt_idmap *idmap,\n\t\t     struct inode *inode, int mask)\n{\n#ifdef CONFIG_FS_POSIX_ACL\n\tstruct posix_acl *acl;\n\n\tif (mask & MAY_NOT_BLOCK) {\n\t\tacl = get_cached_acl_rcu(inode, ACL_TYPE_ACCESS);\n\t        if (!acl)\n\t                return -EAGAIN;\n\t\t/* no ->get_inode_acl() calls in RCU mode... */\n\t\tif (is_uncached_acl(acl))\n\t\t\treturn -ECHILD;\n\t        return posix_acl_permission(idmap, inode, acl, mask);\n\t}\n\n\tacl = get_inode_acl(inode, ACL_TYPE_ACCESS);\n\tif (IS_ERR(acl))\n\t\treturn PTR_ERR(acl);\n\tif (acl) {\n\t        int error = posix_acl_permission(idmap, inode, acl, mask);\n\t        posix_acl_release(acl);\n\t        return error;\n\t}\n#endif\n\n\treturn -EAGAIN;\n}\n\n/*\n * Very quick optimistic \"we know we have no ACL's\" check.\n *\n * Note that this is purely for ACL_TYPE_ACCESS, and purely\n * for the \"we have cached that there are no ACLs\" case.\n *\n * If this returns true, we know there are no ACLs. But if\n * it returns false, we might still not have ACLs (it could\n * be the is_uncached_acl() case).\n */\nstatic inline bool no_acl_inode(struct inode *inode)\n{\n#ifdef CONFIG_FS_POSIX_ACL\n\treturn likely(!READ_ONCE(inode->i_acl));\n#else\n\treturn true;\n#endif\n}\n\n/**\n * acl_permission_check - perform basic UNIX permission checking\n * @idmap:\tidmap of the mount the inode was found from\n * @inode:\tinode to check permissions on\n * @mask:\tright to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)\n *\n * This function performs the basic UNIX permission checking. Since this\n * function may retrieve POSIX acls it needs to know whether it is called from a\n * blocking or non-blocking context and thus cares about the MAY_NOT_BLOCK bit.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nstatic int acl_permission_check(struct mnt_idmap *idmap,\n\t\t\t\tstruct inode *inode, int mask)\n{\n\tunsigned int mode = inode->i_mode;\n\tvfsuid_t vfsuid;\n\n\t/*\n\t * Common cheap case: everybody has the requested\n\t * rights, and there are no ACLs to check. No need\n\t * to do any owner/group checks in that case.\n\t *\n\t *  - 'mask&7' is the requested permission bit set\n\t *  - multiplying by 0111 spreads them out to all of ugo\n\t *  - '& ~mode' looks for missing inode permission bits\n\t *  - the '!' is for \"no missing permissions\"\n\t *\n\t * After that, we just need to check that there are no\n\t * ACL's on the inode - do the 'IS_POSIXACL()' check last\n\t * because it will dereference the ->i_sb pointer and we\n\t * want to avoid that if at all possible.\n\t */\n\tif (!((mask & 7) * 0111 & ~mode)) {\n\t\tif (no_acl_inode(inode))\n\t\t\treturn 0;\n\t\tif (!IS_POSIXACL(inode))\n\t\t\treturn 0;\n\t}\n\n\t/* Are we the owner? If so, ACL's don't matter */\n\tvfsuid = i_uid_into_vfsuid(idmap, inode);\n\tif (likely(vfsuid_eq_kuid(vfsuid, current_fsuid()))) {\n\t\tmask &= 7;\n\t\tmode >>= 6;\n\t\treturn (mask & ~mode) ? -EACCES : 0;\n\t}\n\n\t/* Do we have ACL's? */\n\tif (IS_POSIXACL(inode) && (mode & S_IRWXG)) {\n\t\tint error = check_acl(idmap, inode, mask);\n\t\tif (error != -EAGAIN)\n\t\t\treturn error;\n\t}\n\n\t/* Only RWX matters for group/other mode bits */\n\tmask &= 7;\n\n\t/*\n\t * Are the group permissions different from\n\t * the other permissions in the bits we care\n\t * about? Need to check group ownership if so.\n\t */\n\tif (mask & (mode ^ (mode >> 3))) {\n\t\tvfsgid_t vfsgid = i_gid_into_vfsgid(idmap, inode);\n\t\tif (vfsgid_in_group_p(vfsgid))\n\t\t\tmode >>= 3;\n\t}\n\n\t/* Bits in 'mode' clear that we require? */\n\treturn (mask & ~mode) ? -EACCES : 0;\n}\n\n/**\n * generic_permission -  check for access rights on a Posix-like filesystem\n * @idmap:\tidmap of the mount the inode was found from\n * @inode:\tinode to check access rights for\n * @mask:\tright to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC,\n *\t\t%MAY_NOT_BLOCK ...)\n *\n * Used to check for read/write/execute permissions on a file.\n * We use \"fsuid\" for this, letting us set arbitrary permissions\n * for filesystem access without changing the \"normal\" uids which\n * are used for other things.\n *\n * generic_permission is rcu-walk aware. It returns -ECHILD in case an rcu-walk\n * request cannot be satisfied (eg. requires blocking or too much complexity).\n * It would then be called again in ref-walk mode.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint generic_permission(struct mnt_idmap *idmap, struct inode *inode,\n\t\t       int mask)\n{\n\tint ret;\n\n\t/*\n\t * Do the basic permission checks.\n\t */\n\tret = acl_permission_check(idmap, inode, mask);\n\tif (ret != -EACCES)\n\t\treturn ret;\n\n\tif (S_ISDIR(inode->i_mode)) {\n\t\t/* DACs are overridable for directories */\n\t\tif (!(mask & MAY_WRITE))\n\t\t\tif (capable_wrt_inode_uidgid(idmap, inode,\n\t\t\t\t\t\t     CAP_DAC_READ_SEARCH))\n\t\t\t\treturn 0;\n\t\tif (capable_wrt_inode_uidgid(idmap, inode,\n\t\t\t\t\t     CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\t\treturn -EACCES;\n\t}\n\n\t/*\n\t * Searching includes executable on directories, else just read.\n\t */\n\tmask &= MAY_READ | MAY_WRITE | MAY_EXEC;\n\tif (mask == MAY_READ)\n\t\tif (capable_wrt_inode_uidgid(idmap, inode,\n\t\t\t\t\t     CAP_DAC_READ_SEARCH))\n\t\t\treturn 0;\n\t/*\n\t * Read/write DACs are always overridable.\n\t * Executable DACs are overridable when there is\n\t * at least one exec bit set.\n\t */\n\tif (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))\n\t\tif (capable_wrt_inode_uidgid(idmap, inode,\n\t\t\t\t\t     CAP_DAC_OVERRIDE))\n\t\t\treturn 0;\n\n\treturn -EACCES;\n}\nEXPORT_SYMBOL(generic_permission);\n\n/**\n * do_inode_permission - UNIX permission checking\n * @idmap:\tidmap of the mount the inode was found from\n * @inode:\tinode to check permissions on\n * @mask:\tright to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC ...)\n *\n * We _really_ want to just do \"generic_permission()\" without\n * even looking at the inode->i_op values. So we keep a cache\n * flag in inode->i_opflags, that says \"this has not special\n * permission function, use the fast case\".\n */\nstatic inline int do_inode_permission(struct mnt_idmap *idmap,\n\t\t\t\t      struct inode *inode, int mask)\n{\n\tif (unlikely(!(inode->i_opflags & IOP_FASTPERM))) {\n\t\tif (likely(inode->i_op->permission))\n\t\t\treturn inode->i_op->permission(idmap, inode, mask);\n\n\t\t/* This gets set once for the inode lifetime */\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_opflags |= IOP_FASTPERM;\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\treturn generic_permission(idmap, inode, mask);\n}\n\n/**\n * sb_permission - Check superblock-level permissions\n * @sb: Superblock of inode to check permission on\n * @inode: Inode to check permission on\n * @mask: Right to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)\n *\n * Separate out file-system wide checks from inode-specific permission checks.\n */\nstatic int sb_permission(struct super_block *sb, struct inode *inode, int mask)\n{\n\tif (unlikely(mask & MAY_WRITE)) {\n\t\tumode_t mode = inode->i_mode;\n\n\t\t/* Nobody gets write access to a read-only fs. */\n\t\tif (sb_rdonly(sb) && (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode)))\n\t\t\treturn -EROFS;\n\t}\n\treturn 0;\n}\n\n/**\n * inode_permission - Check for access rights to a given inode\n * @idmap:\tidmap of the mount the inode was found from\n * @inode:\tInode to check permission on\n * @mask:\tRight to check for (%MAY_READ, %MAY_WRITE, %MAY_EXEC)\n *\n * Check for read/write/execute permissions on an inode.  We use fs[ug]id for\n * this, letting us set arbitrary permissions for filesystem access without\n * changing the \"normal\" UIDs which are used for other things.\n *\n * When checking for MAY_APPEND, MAY_WRITE must also be set in @mask.\n */\nint inode_permission(struct mnt_idmap *idmap,\n\t\t     struct inode *inode, int mask)\n{\n\tint retval;\n\n\tretval = sb_permission(inode->i_sb, inode, mask);\n\tif (unlikely(retval))\n\t\treturn retval;\n\n\tif (unlikely(mask & MAY_WRITE)) {\n\t\t/*\n\t\t * Nobody gets write access to an immutable file.\n\t\t */\n\t\tif (unlikely(IS_IMMUTABLE(inode)))\n\t\t\treturn -EPERM;\n\n\t\t/*\n\t\t * Updating mtime will likely cause i_uid and i_gid to be\n\t\t * written back improperly if their true value is unknown\n\t\t * to the vfs.\n\t\t */\n\t\tif (unlikely(HAS_UNMAPPED_ID(idmap, inode)))\n\t\t\treturn -EACCES;\n\t}\n\n\tretval = do_inode_permission(idmap, inode, mask);\n\tif (unlikely(retval))\n\t\treturn retval;\n\n\tretval = devcgroup_inode_permission(inode, mask);\n\tif (unlikely(retval))\n\t\treturn retval;\n\n\treturn security_inode_permission(inode, mask);\n}\nEXPORT_SYMBOL(inode_permission);\n\n/**\n * path_get - get a reference to a path\n * @path: path to get the reference to\n *\n * Given a path increment the reference count to the dentry and the vfsmount.\n */\nvoid path_get(const struct path *path)\n{\n\tmntget(path->mnt);\n\tdget(path->dentry);\n}\nEXPORT_SYMBOL(path_get);\n\n/**\n * path_put - put a reference to a path\n * @path: path to put the reference to\n *\n * Given a path decrement the reference count to the dentry and the vfsmount.\n */\nvoid path_put(const struct path *path)\n{\n\tdput(path->dentry);\n\tmntput(path->mnt);\n}\nEXPORT_SYMBOL(path_put);\n\n#define EMBEDDED_LEVELS 2\nstruct nameidata {\n\tstruct path\tpath;\n\tstruct qstr\tlast;\n\tstruct path\troot;\n\tstruct inode\t*inode; /* path.dentry.d_inode */\n\tunsigned int\tflags, state;\n\tunsigned\tseq, next_seq, m_seq, r_seq;\n\tint\t\tlast_type;\n\tunsigned\tdepth;\n\tint\t\ttotal_link_count;\n\tstruct saved {\n\t\tstruct path link;\n\t\tstruct delayed_call done;\n\t\tconst char *name;\n\t\tunsigned seq;\n\t} *stack, internal[EMBEDDED_LEVELS];\n\tstruct filename\t*name;\n\tconst char *pathname;\n\tstruct nameidata *saved;\n\tunsigned\troot_seq;\n\tint\t\tdfd;\n\tvfsuid_t\tdir_vfsuid;\n\tumode_t\t\tdir_mode;\n} __randomize_layout;\n\n#define ND_ROOT_PRESET 1\n#define ND_ROOT_GRABBED 2\n#define ND_JUMPED 4\n\nstatic void __set_nameidata(struct nameidata *p, int dfd, struct filename *name)\n{\n\tstruct nameidata *old = current->nameidata;\n\tp->stack = p->internal;\n\tp->depth = 0;\n\tp->dfd = dfd;\n\tp->name = name;\n\tp->pathname = likely(name) ? name->name : \"\";\n\tp->path.mnt = NULL;\n\tp->path.dentry = NULL;\n\tp->total_link_count = old ? old->total_link_count : 0;\n\tp->saved = old;\n\tcurrent->nameidata = p;\n}\n\nstatic inline void set_nameidata(struct nameidata *p, int dfd, struct filename *name,\n\t\t\t  const struct path *root)\n{\n\t__set_nameidata(p, dfd, name);\n\tp->state = 0;\n\tif (unlikely(root)) {\n\t\tp->state = ND_ROOT_PRESET;\n\t\tp->root = *root;\n\t}\n}\n\nstatic void restore_nameidata(void)\n{\n\tstruct nameidata *now = current->nameidata, *old = now->saved;\n\n\tcurrent->nameidata = old;\n\tif (old)\n\t\told->total_link_count = now->total_link_count;\n\tif (now->stack != now->internal)\n\t\tkfree(now->stack);\n}\n\nstatic bool nd_alloc_stack(struct nameidata *nd)\n{\n\tstruct saved *p;\n\n\tp= kmalloc_array(MAXSYMLINKS, sizeof(struct saved),\n\t\t\t nd->flags & LOOKUP_RCU ? GFP_ATOMIC : GFP_KERNEL);\n\tif (unlikely(!p))\n\t\treturn false;\n\tmemcpy(p, nd->internal, sizeof(nd->internal));\n\tnd->stack = p;\n\treturn true;\n}\n\n/**\n * path_connected - Verify that a dentry is below mnt.mnt_root\n * @mnt: The mountpoint to check.\n * @dentry: The dentry to check.\n *\n * Rename can sometimes move a file or directory outside of a bind\n * mount, path_connected allows those cases to be detected.\n */\nstatic bool path_connected(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct super_block *sb = mnt->mnt_sb;\n\n\t/* Bind mounts can have disconnected paths */\n\tif (mnt->mnt_root == sb->s_root)\n\t\treturn true;\n\n\treturn is_subdir(dentry, mnt->mnt_root);\n}\n\nstatic void drop_links(struct nameidata *nd)\n{\n\tint i = nd->depth;\n\twhile (i--) {\n\t\tstruct saved *last = nd->stack + i;\n\t\tdo_delayed_call(&last->done);\n\t\tclear_delayed_call(&last->done);\n\t}\n}\n\nstatic void leave_rcu(struct nameidata *nd)\n{\n\tnd->flags &= ~LOOKUP_RCU;\n\tnd->seq = nd->next_seq = 0;\n\trcu_read_unlock();\n}\n\nstatic void terminate_walk(struct nameidata *nd)\n{\n\tdrop_links(nd);\n\tif (!(nd->flags & LOOKUP_RCU)) {\n\t\tint i;\n\t\tpath_put(&nd->path);\n\t\tfor (i = 0; i < nd->depth; i++)\n\t\t\tpath_put(&nd->stack[i].link);\n\t\tif (nd->state & ND_ROOT_GRABBED) {\n\t\t\tpath_put(&nd->root);\n\t\t\tnd->state &= ~ND_ROOT_GRABBED;\n\t\t}\n\t} else {\n\t\tleave_rcu(nd);\n\t}\n\tnd->depth = 0;\n\tnd->path.mnt = NULL;\n\tnd->path.dentry = NULL;\n}\n\n/* path_put is needed afterwards regardless of success or failure */\nstatic bool __legitimize_path(struct path *path, unsigned seq, unsigned mseq)\n{\n\tint res = __legitimize_mnt(path->mnt, mseq);\n\tif (unlikely(res)) {\n\t\tif (res > 0)\n\t\t\tpath->mnt = NULL;\n\t\tpath->dentry = NULL;\n\t\treturn false;\n\t}\n\tif (unlikely(!lockref_get_not_dead(&path->dentry->d_lockref))) {\n\t\tpath->dentry = NULL;\n\t\treturn false;\n\t}\n\treturn !read_seqcount_retry(&path->dentry->d_seq, seq);\n}\n\nstatic inline bool legitimize_path(struct nameidata *nd,\n\t\t\t    struct path *path, unsigned seq)\n{\n\treturn __legitimize_path(path, seq, nd->m_seq);\n}\n\nstatic bool legitimize_links(struct nameidata *nd)\n{\n\tint i;\n\tif (unlikely(nd->flags & LOOKUP_CACHED)) {\n\t\tdrop_links(nd);\n\t\tnd->depth = 0;\n\t\treturn false;\n\t}\n\tfor (i = 0; i < nd->depth; i++) {\n\t\tstruct saved *last = nd->stack + i;\n\t\tif (unlikely(!legitimize_path(nd, &last->link, last->seq))) {\n\t\t\tdrop_links(nd);\n\t\t\tnd->depth = i + 1;\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\nstatic bool legitimize_root(struct nameidata *nd)\n{\n\t/* Nothing to do if nd->root is zero or is managed by the VFS user. */\n\tif (!nd->root.mnt || (nd->state & ND_ROOT_PRESET))\n\t\treturn true;\n\tnd->state |= ND_ROOT_GRABBED;\n\treturn legitimize_path(nd, &nd->root, nd->root_seq);\n}\n\n/*\n * Path walking has 2 modes, rcu-walk and ref-walk (see\n * Documentation/filesystems/path-lookup.txt).  In situations when we can't\n * continue in RCU mode, we attempt to drop out of rcu-walk mode and grab\n * normal reference counts on dentries and vfsmounts to transition to ref-walk\n * mode.  Refcounts are grabbed at the last known good point before rcu-walk\n * got stuck, so ref-walk may continue from there. If this is not successful\n * (eg. a seqcount has changed), then failure is returned and it's up to caller\n * to restart the path walk from the beginning in ref-walk mode.\n */\n\n/**\n * try_to_unlazy - try to switch to ref-walk mode.\n * @nd: nameidata pathwalk data\n * Returns: true on success, false on failure\n *\n * try_to_unlazy attempts to legitimize the current nd->path and nd->root\n * for ref-walk mode.\n * Must be called from rcu-walk context.\n * Nothing should touch nameidata between try_to_unlazy() failure and\n * terminate_walk().\n */\nstatic bool try_to_unlazy(struct nameidata *nd)\n{\n\tstruct dentry *parent = nd->path.dentry;\n\n\tBUG_ON(!(nd->flags & LOOKUP_RCU));\n\n\tif (unlikely(!legitimize_links(nd)))\n\t\tgoto out1;\n\tif (unlikely(!legitimize_path(nd, &nd->path, nd->seq)))\n\t\tgoto out;\n\tif (unlikely(!legitimize_root(nd)))\n\t\tgoto out;\n\tleave_rcu(nd);\n\tBUG_ON(nd->inode != parent->d_inode);\n\treturn true;\n\nout1:\n\tnd->path.mnt = NULL;\n\tnd->path.dentry = NULL;\nout:\n\tleave_rcu(nd);\n\treturn false;\n}\n\n/**\n * try_to_unlazy_next - try to switch to ref-walk mode.\n * @nd: nameidata pathwalk data\n * @dentry: next dentry to step into\n * Returns: true on success, false on failure\n *\n * Similar to try_to_unlazy(), but here we have the next dentry already\n * picked by rcu-walk and want to legitimize that in addition to the current\n * nd->path and nd->root for ref-walk mode.  Must be called from rcu-walk context.\n * Nothing should touch nameidata between try_to_unlazy_next() failure and\n * terminate_walk().\n */\nstatic bool try_to_unlazy_next(struct nameidata *nd, struct dentry *dentry)\n{\n\tint res;\n\tBUG_ON(!(nd->flags & LOOKUP_RCU));\n\n\tif (unlikely(!legitimize_links(nd)))\n\t\tgoto out2;\n\tres = __legitimize_mnt(nd->path.mnt, nd->m_seq);\n\tif (unlikely(res)) {\n\t\tif (res > 0)\n\t\t\tgoto out2;\n\t\tgoto out1;\n\t}\n\tif (unlikely(!lockref_get_not_dead(&nd->path.dentry->d_lockref)))\n\t\tgoto out1;\n\n\t/*\n\t * We need to move both the parent and the dentry from the RCU domain\n\t * to be properly refcounted. And the sequence number in the dentry\n\t * validates *both* dentry counters, since we checked the sequence\n\t * number of the parent after we got the child sequence number. So we\n\t * know the parent must still be valid if the child sequence number is\n\t */\n\tif (unlikely(!lockref_get_not_dead(&dentry->d_lockref)))\n\t\tgoto out;\n\tif (read_seqcount_retry(&dentry->d_seq, nd->next_seq))\n\t\tgoto out_dput;\n\t/*\n\t * Sequence counts matched. Now make sure that the root is\n\t * still valid and get it if required.\n\t */\n\tif (unlikely(!legitimize_root(nd)))\n\t\tgoto out_dput;\n\tleave_rcu(nd);\n\treturn true;\n\nout2:\n\tnd->path.mnt = NULL;\nout1:\n\tnd->path.dentry = NULL;\nout:\n\tleave_rcu(nd);\n\treturn false;\nout_dput:\n\tleave_rcu(nd);\n\tdput(dentry);\n\treturn false;\n}\n\nstatic inline int d_revalidate(struct inode *dir, const struct qstr *name,\n\t\t\t       struct dentry *dentry, unsigned int flags)\n{\n\tif (unlikely(dentry->d_flags & DCACHE_OP_REVALIDATE))\n\t\treturn dentry->d_op->d_revalidate(dir, name, dentry, flags);\n\telse\n\t\treturn 1;\n}\n\n/**\n * complete_walk - successful completion of path walk\n * @nd:  pointer nameidata\n *\n * If we had been in RCU mode, drop out of it and legitimize nd->path.\n * Revalidate the final result, unless we'd already done that during\n * the path walk or the filesystem doesn't ask for it.  Return 0 on\n * success, -error on failure.  In case of failure caller does not\n * need to drop nd->path.\n */\nstatic int complete_walk(struct nameidata *nd)\n{\n\tstruct dentry *dentry = nd->path.dentry;\n\tint status;\n\n\tif (nd->flags & LOOKUP_RCU) {\n\t\t/*\n\t\t * We don't want to zero nd->root for scoped-lookups or\n\t\t * externally-managed nd->root.\n\t\t */\n\t\tif (!(nd->state & ND_ROOT_PRESET))\n\t\t\tif (!(nd->flags & LOOKUP_IS_SCOPED))\n\t\t\t\tnd->root.mnt = NULL;\n\t\tnd->flags &= ~LOOKUP_CACHED;\n\t\tif (!try_to_unlazy(nd))\n\t\t\treturn -ECHILD;\n\t}\n\n\tif (unlikely(nd->flags & LOOKUP_IS_SCOPED)) {\n\t\t/*\n\t\t * While the guarantee of LOOKUP_IS_SCOPED is (roughly) \"don't\n\t\t * ever step outside the root during lookup\" and should already\n\t\t * be guaranteed by the rest of namei, we want to avoid a namei\n\t\t * BUG resulting in userspace being given a path that was not\n\t\t * scoped within the root at some point during the lookup.\n\t\t *\n\t\t * So, do a final sanity-check to make sure that in the\n\t\t * worst-case scenario (a complete bypass of LOOKUP_IS_SCOPED)\n\t\t * we won't silently return an fd completely outside of the\n\t\t * requested root to userspace.\n\t\t *\n\t\t * Userspace could move the path outside the root after this\n\t\t * check, but as discussed elsewhere this is not a concern (the\n\t\t * resolved file was inside the root at some point).\n\t\t */\n\t\tif (!path_is_under(&nd->path, &nd->root))\n\t\t\treturn -EXDEV;\n\t}\n\n\tif (likely(!(nd->state & ND_JUMPED)))\n\t\treturn 0;\n\n\tif (likely(!(dentry->d_flags & DCACHE_OP_WEAK_REVALIDATE)))\n\t\treturn 0;\n\n\tstatus = dentry->d_op->d_weak_revalidate(dentry, nd->flags);\n\tif (status > 0)\n\t\treturn 0;\n\n\tif (!status)\n\t\tstatus = -ESTALE;\n\n\treturn status;\n}\n\nstatic int set_root(struct nameidata *nd)\n{\n\tstruct fs_struct *fs = current->fs;\n\n\t/*\n\t * Jumping to the real root in a scoped-lookup is a BUG in namei, but we\n\t * still have to ensure it doesn't happen because it will cause a breakout\n\t * from the dirfd.\n\t */\n\tif (WARN_ON(nd->flags & LOOKUP_IS_SCOPED))\n\t\treturn -ENOTRECOVERABLE;\n\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tunsigned seq;\n\n\t\tdo {\n\t\t\tseq = read_seqbegin(&fs->seq);\n\t\t\tnd->root = fs->root;\n\t\t\tnd->root_seq = __read_seqcount_begin(&nd->root.dentry->d_seq);\n\t\t} while (read_seqretry(&fs->seq, seq));\n\t} else {\n\t\tget_fs_root(fs, &nd->root);\n\t\tnd->state |= ND_ROOT_GRABBED;\n\t}\n\treturn 0;\n}\n\nstatic int nd_jump_root(struct nameidata *nd)\n{\n\tif (unlikely(nd->flags & LOOKUP_BENEATH))\n\t\treturn -EXDEV;\n\tif (unlikely(nd->flags & LOOKUP_NO_XDEV)) {\n\t\t/* Absolute path arguments to path_init() are allowed. */\n\t\tif (nd->path.mnt != NULL && nd->path.mnt != nd->root.mnt)\n\t\t\treturn -EXDEV;\n\t}\n\tif (!nd->root.mnt) {\n\t\tint error = set_root(nd);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tstruct dentry *d;\n\t\tnd->path = nd->root;\n\t\td = nd->path.dentry;\n\t\tnd->inode = d->d_inode;\n\t\tnd->seq = nd->root_seq;\n\t\tif (read_seqcount_retry(&d->d_seq, nd->seq))\n\t\t\treturn -ECHILD;\n\t} else {\n\t\tpath_put(&nd->path);\n\t\tnd->path = nd->root;\n\t\tpath_get(&nd->path);\n\t\tnd->inode = nd->path.dentry->d_inode;\n\t}\n\tnd->state |= ND_JUMPED;\n\treturn 0;\n}\n\n/*\n * Helper to directly jump to a known parsed path from ->get_link,\n * caller must have taken a reference to path beforehand.\n */\nint nd_jump_link(const struct path *path)\n{\n\tint error = -ELOOP;\n\tstruct nameidata *nd = current->nameidata;\n\n\tif (unlikely(nd->flags & LOOKUP_NO_MAGICLINKS))\n\t\tgoto err;\n\n\terror = -EXDEV;\n\tif (unlikely(nd->flags & LOOKUP_NO_XDEV)) {\n\t\tif (nd->path.mnt != path->mnt)\n\t\t\tgoto err;\n\t}\n\t/* Not currently safe for scoped-lookups. */\n\tif (unlikely(nd->flags & LOOKUP_IS_SCOPED))\n\t\tgoto err;\n\n\tpath_put(&nd->path);\n\tnd->path = *path;\n\tnd->inode = nd->path.dentry->d_inode;\n\tnd->state |= ND_JUMPED;\n\treturn 0;\n\nerr:\n\tpath_put(path);\n\treturn error;\n}\n\nstatic inline void put_link(struct nameidata *nd)\n{\n\tstruct saved *last = nd->stack + --nd->depth;\n\tdo_delayed_call(&last->done);\n\tif (!(nd->flags & LOOKUP_RCU))\n\t\tpath_put(&last->link);\n}\n\nstatic int sysctl_protected_symlinks __read_mostly;\nstatic int sysctl_protected_hardlinks __read_mostly;\nstatic int sysctl_protected_fifos __read_mostly;\nstatic int sysctl_protected_regular __read_mostly;\n\n#ifdef CONFIG_SYSCTL\nstatic const struct ctl_table namei_sysctls[] = {\n\t{\n\t\t.procname\t= \"protected_symlinks\",\n\t\t.data\t\t= &sysctl_protected_symlinks,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{\n\t\t.procname\t= \"protected_hardlinks\",\n\t\t.data\t\t= &sysctl_protected_hardlinks,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{\n\t\t.procname\t= \"protected_fifos\",\n\t\t.data\t\t= &sysctl_protected_fifos,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_TWO,\n\t},\n\t{\n\t\t.procname\t= \"protected_regular\",\n\t\t.data\t\t= &sysctl_protected_regular,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_TWO,\n\t},\n};\n\nstatic int __init init_fs_namei_sysctls(void)\n{\n\tregister_sysctl_init(\"fs\", namei_sysctls);\n\treturn 0;\n}\nfs_initcall(init_fs_namei_sysctls);\n\n#endif /* CONFIG_SYSCTL */\n\n/**\n * may_follow_link - Check symlink following for unsafe situations\n * @nd: nameidata pathwalk data\n * @inode: Used for idmapping.\n *\n * In the case of the sysctl_protected_symlinks sysctl being enabled,\n * CAP_DAC_OVERRIDE needs to be specifically ignored if the symlink is\n * in a sticky world-writable directory. This is to protect privileged\n * processes from failing races against path names that may change out\n * from under them by way of other users creating malicious symlinks.\n * It will permit symlinks to be followed only when outside a sticky\n * world-writable directory, or when the uid of the symlink and follower\n * match, or when the directory owner matches the symlink's owner.\n *\n * Returns 0 if following the symlink is allowed, -ve on error.\n */\nstatic inline int may_follow_link(struct nameidata *nd, const struct inode *inode)\n{\n\tstruct mnt_idmap *idmap;\n\tvfsuid_t vfsuid;\n\n\tif (!sysctl_protected_symlinks)\n\t\treturn 0;\n\n\tidmap = mnt_idmap(nd->path.mnt);\n\tvfsuid = i_uid_into_vfsuid(idmap, inode);\n\t/* Allowed if owner and follower match. */\n\tif (vfsuid_eq_kuid(vfsuid, current_fsuid()))\n\t\treturn 0;\n\n\t/* Allowed if parent directory not sticky and world-writable. */\n\tif ((nd->dir_mode & (S_ISVTX|S_IWOTH)) != (S_ISVTX|S_IWOTH))\n\t\treturn 0;\n\n\t/* Allowed if parent directory and link owner match. */\n\tif (vfsuid_valid(nd->dir_vfsuid) && vfsuid_eq(nd->dir_vfsuid, vfsuid))\n\t\treturn 0;\n\n\tif (nd->flags & LOOKUP_RCU)\n\t\treturn -ECHILD;\n\n\taudit_inode(nd->name, nd->stack[0].link.dentry, 0);\n\taudit_log_path_denied(AUDIT_ANOM_LINK, \"follow_link\");\n\treturn -EACCES;\n}\n\n/**\n * safe_hardlink_source - Check for safe hardlink conditions\n * @idmap: idmap of the mount the inode was found from\n * @inode: the source inode to hardlink from\n *\n * Return false if at least one of the following conditions:\n *    - inode is not a regular file\n *    - inode is setuid\n *    - inode is setgid and group-exec\n *    - access failure for read and write\n *\n * Otherwise returns true.\n */\nstatic bool safe_hardlink_source(struct mnt_idmap *idmap,\n\t\t\t\t struct inode *inode)\n{\n\tumode_t mode = inode->i_mode;\n\n\t/* Special files should not get pinned to the filesystem. */\n\tif (!S_ISREG(mode))\n\t\treturn false;\n\n\t/* Setuid files should not get pinned to the filesystem. */\n\tif (mode & S_ISUID)\n\t\treturn false;\n\n\t/* Executable setgid files should not get pinned to the filesystem. */\n\tif ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP))\n\t\treturn false;\n\n\t/* Hardlinking to unreadable or unwritable sources is dangerous. */\n\tif (inode_permission(idmap, inode, MAY_READ | MAY_WRITE))\n\t\treturn false;\n\n\treturn true;\n}\n\n/**\n * may_linkat - Check permissions for creating a hardlink\n * @idmap: idmap of the mount the inode was found from\n * @link:  the source to hardlink from\n *\n * Block hardlink when all of:\n *  - sysctl_protected_hardlinks enabled\n *  - fsuid does not match inode\n *  - hardlink source is unsafe (see safe_hardlink_source() above)\n *  - not CAP_FOWNER in a namespace with the inode owner uid mapped\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n *\n * Returns 0 if successful, -ve on error.\n */\nint may_linkat(struct mnt_idmap *idmap, const struct path *link)\n{\n\tstruct inode *inode = link->dentry->d_inode;\n\n\t/* Inode writeback is not safe when the uid or gid are invalid. */\n\tif (!vfsuid_valid(i_uid_into_vfsuid(idmap, inode)) ||\n\t    !vfsgid_valid(i_gid_into_vfsgid(idmap, inode)))\n\t\treturn -EOVERFLOW;\n\n\tif (!sysctl_protected_hardlinks)\n\t\treturn 0;\n\n\t/* Source inode owner (or CAP_FOWNER) can hardlink all they like,\n\t * otherwise, it must be a safe source.\n\t */\n\tif (safe_hardlink_source(idmap, inode) ||\n\t    inode_owner_or_capable(idmap, inode))\n\t\treturn 0;\n\n\taudit_log_path_denied(AUDIT_ANOM_LINK, \"linkat\");\n\treturn -EPERM;\n}\n\n/**\n * may_create_in_sticky - Check whether an O_CREAT open in a sticky directory\n *\t\t\t  should be allowed, or not, on files that already\n *\t\t\t  exist.\n * @idmap: idmap of the mount the inode was found from\n * @nd: nameidata pathwalk data\n * @inode: the inode of the file to open\n *\n * Block an O_CREAT open of a FIFO (or a regular file) when:\n *   - sysctl_protected_fifos (or sysctl_protected_regular) is enabled\n *   - the file already exists\n *   - we are in a sticky directory\n *   - we don't own the file\n *   - the owner of the directory doesn't own the file\n *   - the directory is world writable\n * If the sysctl_protected_fifos (or sysctl_protected_regular) is set to 2\n * the directory doesn't have to be world writable: being group writable will\n * be enough.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n *\n * Returns 0 if the open is allowed, -ve on error.\n */\nstatic int may_create_in_sticky(struct mnt_idmap *idmap, struct nameidata *nd,\n\t\t\t\tstruct inode *const inode)\n{\n\tumode_t dir_mode = nd->dir_mode;\n\tvfsuid_t dir_vfsuid = nd->dir_vfsuid, i_vfsuid;\n\n\tif (likely(!(dir_mode & S_ISVTX)))\n\t\treturn 0;\n\n\tif (S_ISREG(inode->i_mode) && !sysctl_protected_regular)\n\t\treturn 0;\n\n\tif (S_ISFIFO(inode->i_mode) && !sysctl_protected_fifos)\n\t\treturn 0;\n\n\ti_vfsuid = i_uid_into_vfsuid(idmap, inode);\n\n\tif (vfsuid_eq(i_vfsuid, dir_vfsuid))\n\t\treturn 0;\n\n\tif (vfsuid_eq_kuid(i_vfsuid, current_fsuid()))\n\t\treturn 0;\n\n\tif (likely(dir_mode & 0002)) {\n\t\taudit_log_path_denied(AUDIT_ANOM_CREAT, \"sticky_create\");\n\t\treturn -EACCES;\n\t}\n\n\tif (dir_mode & 0020) {\n\t\tif (sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) {\n\t\t\taudit_log_path_denied(AUDIT_ANOM_CREAT,\n\t\t\t\t\t      \"sticky_create_fifo\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode)) {\n\t\t\taudit_log_path_denied(AUDIT_ANOM_CREAT,\n\t\t\t\t\t      \"sticky_create_regular\");\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n * follow_up - Find the mountpoint of path's vfsmount\n *\n * Given a path, find the mountpoint of its source file system.\n * Replace @path with the path of the mountpoint in the parent mount.\n * Up is towards /.\n *\n * Return 1 if we went up a level and 0 if we were already at the\n * root.\n */\nint follow_up(struct path *path)\n{\n\tstruct mount *mnt = real_mount(path->mnt);\n\tstruct mount *parent;\n\tstruct dentry *mountpoint;\n\n\tread_seqlock_excl(&mount_lock);\n\tparent = mnt->mnt_parent;\n\tif (parent == mnt) {\n\t\tread_sequnlock_excl(&mount_lock);\n\t\treturn 0;\n\t}\n\tmntget(&parent->mnt);\n\tmountpoint = dget(mnt->mnt_mountpoint);\n\tread_sequnlock_excl(&mount_lock);\n\tdput(path->dentry);\n\tpath->dentry = mountpoint;\n\tmntput(path->mnt);\n\tpath->mnt = &parent->mnt;\n\treturn 1;\n}\nEXPORT_SYMBOL(follow_up);\n\nstatic bool choose_mountpoint_rcu(struct mount *m, const struct path *root,\n\t\t\t\t  struct path *path, unsigned *seqp)\n{\n\twhile (mnt_has_parent(m)) {\n\t\tstruct dentry *mountpoint = m->mnt_mountpoint;\n\n\t\tm = m->mnt_parent;\n\t\tif (unlikely(root->dentry == mountpoint &&\n\t\t\t     root->mnt == &m->mnt))\n\t\t\tbreak;\n\t\tif (mountpoint != m->mnt.mnt_root) {\n\t\t\tpath->mnt = &m->mnt;\n\t\t\tpath->dentry = mountpoint;\n\t\t\t*seqp = read_seqcount_begin(&mountpoint->d_seq);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic bool choose_mountpoint(struct mount *m, const struct path *root,\n\t\t\t      struct path *path)\n{\n\tbool found;\n\n\trcu_read_lock();\n\twhile (1) {\n\t\tunsigned seq, mseq = read_seqbegin(&mount_lock);\n\n\t\tfound = choose_mountpoint_rcu(m, root, path, &seq);\n\t\tif (unlikely(!found)) {\n\t\t\tif (!read_seqretry(&mount_lock, mseq))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (likely(__legitimize_path(path, seq, mseq)))\n\t\t\t\tbreak;\n\t\t\trcu_read_unlock();\n\t\t\tpath_put(path);\n\t\t\trcu_read_lock();\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn found;\n}\n\n/*\n * Perform an automount\n * - return -EISDIR to tell follow_managed() to stop and return the path we\n *   were called with.\n */\nstatic int follow_automount(struct path *path, int *count, unsigned lookup_flags)\n{\n\tstruct dentry *dentry = path->dentry;\n\n\t/* We don't want to mount if someone's just doing a stat -\n\t * unless they're stat'ing a directory and appended a '/' to\n\t * the name.\n\t *\n\t * We do, however, want to mount if someone wants to open or\n\t * create a file of any type under the mountpoint, wants to\n\t * traverse through the mountpoint or wants to open the\n\t * mounted directory.  Also, autofs may mark negative dentries\n\t * as being automount points.  These will need the attentions\n\t * of the daemon to instantiate them before they can be used.\n\t */\n\tif (!(lookup_flags & (LOOKUP_PARENT | LOOKUP_DIRECTORY |\n\t\t\t   LOOKUP_OPEN | LOOKUP_CREATE | LOOKUP_AUTOMOUNT)) &&\n\t    dentry->d_inode)\n\t\treturn -EISDIR;\n\n\t/* No need to trigger automounts if mountpoint crossing is disabled. */\n\tif (lookup_flags & LOOKUP_NO_XDEV)\n\t\treturn -EXDEV;\n\n\tif (count && (*count)++ >= MAXSYMLINKS)\n\t\treturn -ELOOP;\n\n\treturn finish_automount(dentry->d_op->d_automount(path), path);\n}\n\n/*\n * mount traversal - out-of-line part.  One note on ->d_flags accesses -\n * dentries are pinned but not locked here, so negative dentry can go\n * positive right under us.  Use of smp_load_acquire() provides a barrier\n * sufficient for ->d_inode and ->d_flags consistency.\n */\nstatic int __traverse_mounts(struct path *path, unsigned flags, bool *jumped,\n\t\t\t     int *count, unsigned lookup_flags)\n{\n\tstruct vfsmount *mnt = path->mnt;\n\tbool need_mntput = false;\n\tint ret = 0;\n\n\twhile (flags & DCACHE_MANAGED_DENTRY) {\n\t\t/* Allow the filesystem to manage the transit without i_rwsem\n\t\t * being held. */\n\t\tif (flags & DCACHE_MANAGE_TRANSIT) {\n\t\t\tif (lookup_flags & LOOKUP_NO_XDEV) {\n\t\t\t\tret = -EXDEV;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret = path->dentry->d_op->d_manage(path, false);\n\t\t\tflags = smp_load_acquire(&path->dentry->d_flags);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (flags & DCACHE_MOUNTED) {\t// something's mounted on it..\n\t\t\tstruct vfsmount *mounted = lookup_mnt(path);\n\t\t\tif (mounted) {\t\t// ... in our namespace\n\t\t\t\tdput(path->dentry);\n\t\t\t\tif (need_mntput)\n\t\t\t\t\tmntput(path->mnt);\n\t\t\t\tpath->mnt = mounted;\n\t\t\t\tpath->dentry = dget(mounted->mnt_root);\n\t\t\t\t// here we know it's positive\n\t\t\t\tflags = path->dentry->d_flags;\n\t\t\t\tneed_mntput = true;\n\t\t\t\tif (unlikely(lookup_flags & LOOKUP_NO_XDEV)) {\n\t\t\t\t\tret = -EXDEV;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (!(flags & DCACHE_NEED_AUTOMOUNT))\n\t\t\tbreak;\n\n\t\t// uncovered automount point\n\t\tret = follow_automount(path, count, lookup_flags);\n\t\tflags = smp_load_acquire(&path->dentry->d_flags);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\n\tif (ret == -EISDIR)\n\t\tret = 0;\n\t// possible if you race with several mount --move\n\tif (need_mntput && path->mnt == mnt)\n\t\tmntput(path->mnt);\n\tif (!ret && unlikely(d_flags_negative(flags)))\n\t\tret = -ENOENT;\n\t*jumped = need_mntput;\n\treturn ret;\n}\n\nstatic inline int traverse_mounts(struct path *path, bool *jumped,\n\t\t\t\t  int *count, unsigned lookup_flags)\n{\n\tunsigned flags = smp_load_acquire(&path->dentry->d_flags);\n\n\t/* fastpath */\n\tif (likely(!(flags & DCACHE_MANAGED_DENTRY))) {\n\t\t*jumped = false;\n\t\tif (unlikely(d_flags_negative(flags)))\n\t\t\treturn -ENOENT;\n\t\treturn 0;\n\t}\n\treturn __traverse_mounts(path, flags, jumped, count, lookup_flags);\n}\n\nint follow_down_one(struct path *path)\n{\n\tstruct vfsmount *mounted;\n\n\tmounted = lookup_mnt(path);\n\tif (mounted) {\n\t\tdput(path->dentry);\n\t\tmntput(path->mnt);\n\t\tpath->mnt = mounted;\n\t\tpath->dentry = dget(mounted->mnt_root);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(follow_down_one);\n\n/*\n * Follow down to the covering mount currently visible to userspace.  At each\n * point, the filesystem owning that dentry may be queried as to whether the\n * caller is permitted to proceed or not.\n */\nint follow_down(struct path *path, unsigned int flags)\n{\n\tstruct vfsmount *mnt = path->mnt;\n\tbool jumped;\n\tint ret = traverse_mounts(path, &jumped, NULL, flags);\n\n\tif (path->mnt != mnt)\n\t\tmntput(mnt);\n\treturn ret;\n}\nEXPORT_SYMBOL(follow_down);\n\n/*\n * Try to skip to top of mountpoint pile in rcuwalk mode.  Fail if\n * we meet a managed dentry that would need blocking.\n */\nstatic bool __follow_mount_rcu(struct nameidata *nd, struct path *path)\n{\n\tstruct dentry *dentry = path->dentry;\n\tunsigned int flags = dentry->d_flags;\n\n\tif (likely(!(flags & DCACHE_MANAGED_DENTRY)))\n\t\treturn true;\n\n\tif (unlikely(nd->flags & LOOKUP_NO_XDEV))\n\t\treturn false;\n\n\tfor (;;) {\n\t\t/*\n\t\t * Don't forget we might have a non-mountpoint managed dentry\n\t\t * that wants to block transit.\n\t\t */\n\t\tif (unlikely(flags & DCACHE_MANAGE_TRANSIT)) {\n\t\t\tint res = dentry->d_op->d_manage(path, true);\n\t\t\tif (res)\n\t\t\t\treturn res == -EISDIR;\n\t\t\tflags = dentry->d_flags;\n\t\t}\n\n\t\tif (flags & DCACHE_MOUNTED) {\n\t\t\tstruct mount *mounted = __lookup_mnt(path->mnt, dentry);\n\t\t\tif (mounted) {\n\t\t\t\tpath->mnt = &mounted->mnt;\n\t\t\t\tdentry = path->dentry = mounted->mnt.mnt_root;\n\t\t\t\tnd->state |= ND_JUMPED;\n\t\t\t\tnd->next_seq = read_seqcount_begin(&dentry->d_seq);\n\t\t\t\tflags = dentry->d_flags;\n\t\t\t\t// makes sure that non-RCU pathwalk could reach\n\t\t\t\t// this state.\n\t\t\t\tif (read_seqretry(&mount_lock, nd->m_seq))\n\t\t\t\t\treturn false;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (read_seqretry(&mount_lock, nd->m_seq))\n\t\t\t\treturn false;\n\t\t}\n\t\treturn !(flags & DCACHE_NEED_AUTOMOUNT);\n\t}\n}\n\nstatic inline int handle_mounts(struct nameidata *nd, struct dentry *dentry,\n\t\t\t  struct path *path)\n{\n\tbool jumped;\n\tint ret;\n\n\tpath->mnt = nd->path.mnt;\n\tpath->dentry = dentry;\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tunsigned int seq = nd->next_seq;\n\t\tif (likely(__follow_mount_rcu(nd, path)))\n\t\t\treturn 0;\n\t\t// *path and nd->next_seq might've been clobbered\n\t\tpath->mnt = nd->path.mnt;\n\t\tpath->dentry = dentry;\n\t\tnd->next_seq = seq;\n\t\tif (!try_to_unlazy_next(nd, dentry))\n\t\t\treturn -ECHILD;\n\t}\n\tret = traverse_mounts(path, &jumped, &nd->total_link_count, nd->flags);\n\tif (jumped)\n\t\tnd->state |= ND_JUMPED;\n\tif (unlikely(ret)) {\n\t\tdput(path->dentry);\n\t\tif (path->mnt != nd->path.mnt)\n\t\t\tmntput(path->mnt);\n\t}\n\treturn ret;\n}\n\n/*\n * This looks up the name in dcache and possibly revalidates the found dentry.\n * NULL is returned if the dentry does not exist in the cache.\n */\nstatic struct dentry *lookup_dcache(const struct qstr *name,\n\t\t\t\t    struct dentry *dir,\n\t\t\t\t    unsigned int flags)\n{\n\tstruct dentry *dentry = d_lookup(dir, name);\n\tif (dentry) {\n\t\tint error = d_revalidate(dir->d_inode, name, dentry, flags);\n\t\tif (unlikely(error <= 0)) {\n\t\t\tif (!error)\n\t\t\t\td_invalidate(dentry);\n\t\t\tdput(dentry);\n\t\t\treturn ERR_PTR(error);\n\t\t}\n\t}\n\treturn dentry;\n}\n\n/*\n * Parent directory has inode locked exclusive.  This is one\n * and only case when ->lookup() gets called on non in-lookup\n * dentries - as the matter of fact, this only gets called\n * when directory is guaranteed to have no in-lookup children\n * at all.\n * Will return -ENOENT if name isn't found and LOOKUP_CREATE wasn't passed.\n * Will return -EEXIST if name is found and LOOKUP_EXCL was passed.\n */\nstruct dentry *lookup_one_qstr_excl(const struct qstr *name,\n\t\t\t\t    struct dentry *base, unsigned int flags)\n{\n\tstruct dentry *dentry;\n\tstruct dentry *old;\n\tstruct inode *dir;\n\n\tdentry = lookup_dcache(name, base, flags);\n\tif (dentry)\n\t\tgoto found;\n\n\t/* Don't create child dentry for a dead directory. */\n\tdir = base->d_inode;\n\tif (unlikely(IS_DEADDIR(dir)))\n\t\treturn ERR_PTR(-ENOENT);\n\n\tdentry = d_alloc(base, name);\n\tif (unlikely(!dentry))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\told = dir->i_op->lookup(dir, dentry, flags);\n\tif (unlikely(old)) {\n\t\tdput(dentry);\n\t\tdentry = old;\n\t}\nfound:\n\tif (IS_ERR(dentry))\n\t\treturn dentry;\n\tif (d_is_negative(dentry) && !(flags & LOOKUP_CREATE)) {\n\t\tdput(dentry);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\tif (d_is_positive(dentry) && (flags & LOOKUP_EXCL)) {\n\t\tdput(dentry);\n\t\treturn ERR_PTR(-EEXIST);\n\t}\n\treturn dentry;\n}\nEXPORT_SYMBOL(lookup_one_qstr_excl);\n\n/**\n * lookup_fast - do fast lockless (but racy) lookup of a dentry\n * @nd: current nameidata\n *\n * Do a fast, but racy lookup in the dcache for the given dentry, and\n * revalidate it. Returns a valid dentry pointer or NULL if one wasn't\n * found. On error, an ERR_PTR will be returned.\n *\n * If this function returns a valid dentry and the walk is no longer\n * lazy, the dentry will carry a reference that must later be put. If\n * RCU mode is still in force, then this is not the case and the dentry\n * must be legitimized before use. If this returns NULL, then the walk\n * will no longer be in RCU mode.\n */\nstatic struct dentry *lookup_fast(struct nameidata *nd)\n{\n\tstruct dentry *dentry, *parent = nd->path.dentry;\n\tint status = 1;\n\n\t/*\n\t * Rename seqlock is not required here because in the off chance\n\t * of a false negative due to a concurrent rename, the caller is\n\t * going to fall back to non-racy lookup.\n\t */\n\tif (nd->flags & LOOKUP_RCU) {\n\t\tdentry = __d_lookup_rcu(parent, &nd->last, &nd->next_seq);\n\t\tif (unlikely(!dentry)) {\n\t\t\tif (!try_to_unlazy(nd))\n\t\t\t\treturn ERR_PTR(-ECHILD);\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/*\n\t\t * This sequence count validates that the parent had no\n\t\t * changes while we did the lookup of the dentry above.\n\t\t */\n\t\tif (read_seqcount_retry(&parent->d_seq, nd->seq))\n\t\t\treturn ERR_PTR(-ECHILD);\n\n\t\tstatus = d_revalidate(nd->inode, &nd->last, dentry, nd->flags);\n\t\tif (likely(status > 0))\n\t\t\treturn dentry;\n\t\tif (!try_to_unlazy_next(nd, dentry))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\tif (status == -ECHILD)\n\t\t\t/* we'd been told to redo it in non-rcu mode */\n\t\t\tstatus = d_revalidate(nd->inode, &nd->last,\n\t\t\t\t\t      dentry, nd->flags);\n\t} else {\n\t\tdentry = __d_lookup(parent, &nd->last);\n\t\tif (unlikely(!dentry))\n\t\t\treturn NULL;\n\t\tstatus = d_revalidate(nd->inode, &nd->last, dentry, nd->flags);\n\t}\n\tif (unlikely(status <= 0)) {\n\t\tif (!status)\n\t\t\td_invalidate(dentry);\n\t\tdput(dentry);\n\t\treturn ERR_PTR(status);\n\t}\n\treturn dentry;\n}\n\n/* Fast lookup failed, do it the slow way */\nstatic struct dentry *__lookup_slow(const struct qstr *name,\n\t\t\t\t    struct dentry *dir,\n\t\t\t\t    unsigned int flags)\n{\n\tstruct dentry *dentry, *old;\n\tstruct inode *inode = dir->d_inode;\n\tDECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);\n\n\t/* Don't go there if it's already dead */\n\tif (unlikely(IS_DEADDIR(inode)))\n\t\treturn ERR_PTR(-ENOENT);\nagain:\n\tdentry = d_alloc_parallel(dir, name, &wq);\n\tif (IS_ERR(dentry))\n\t\treturn dentry;\n\tif (unlikely(!d_in_lookup(dentry))) {\n\t\tint error = d_revalidate(inode, name, dentry, flags);\n\t\tif (unlikely(error <= 0)) {\n\t\t\tif (!error) {\n\t\t\t\td_invalidate(dentry);\n\t\t\t\tdput(dentry);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tdput(dentry);\n\t\t\tdentry = ERR_PTR(error);\n\t\t}\n\t} else {\n\t\told = inode->i_op->lookup(inode, dentry, flags);\n\t\td_lookup_done(dentry);\n\t\tif (unlikely(old)) {\n\t\t\tdput(dentry);\n\t\t\tdentry = old;\n\t\t}\n\t}\n\treturn dentry;\n}\n\nstatic struct dentry *lookup_slow(const struct qstr *name,\n\t\t\t\t  struct dentry *dir,\n\t\t\t\t  unsigned int flags)\n{\n\tstruct inode *inode = dir->d_inode;\n\tstruct dentry *res;\n\tinode_lock_shared(inode);\n\tres = __lookup_slow(name, dir, flags);\n\tinode_unlock_shared(inode);\n\treturn res;\n}\n\nstatic struct dentry *lookup_slow_killable(const struct qstr *name,\n\t\t\t\t\t   struct dentry *dir,\n\t\t\t\t\t   unsigned int flags)\n{\n\tstruct inode *inode = dir->d_inode;\n\tstruct dentry *res;\n\n\tif (inode_lock_shared_killable(inode))\n\t\treturn ERR_PTR(-EINTR);\n\tres = __lookup_slow(name, dir, flags);\n\tinode_unlock_shared(inode);\n\treturn res;\n}\n\nstatic inline int may_lookup(struct mnt_idmap *idmap,\n\t\t\t     struct nameidata *restrict nd)\n{\n\tint err, mask;\n\n\tmask = nd->flags & LOOKUP_RCU ? MAY_NOT_BLOCK : 0;\n\terr = inode_permission(idmap, nd->inode, mask | MAY_EXEC);\n\tif (likely(!err))\n\t\treturn 0;\n\n\t// If we failed, and we weren't in LOOKUP_RCU, it's final\n\tif (!(nd->flags & LOOKUP_RCU))\n\t\treturn err;\n\n\t// Drop out of RCU mode to make sure it wasn't transient\n\tif (!try_to_unlazy(nd))\n\t\treturn -ECHILD;\t// redo it all non-lazy\n\n\tif (err != -ECHILD)\t// hard error\n\t\treturn err;\n\n\treturn inode_permission(idmap, nd->inode, MAY_EXEC);\n}\n\nstatic int reserve_stack(struct nameidata *nd, struct path *link)\n{\n\tif (unlikely(nd->total_link_count++ >= MAXSYMLINKS))\n\t\treturn -ELOOP;\n\n\tif (likely(nd->depth != EMBEDDED_LEVELS))\n\t\treturn 0;\n\tif (likely(nd->stack != nd->internal))\n\t\treturn 0;\n\tif (likely(nd_alloc_stack(nd)))\n\t\treturn 0;\n\n\tif (nd->flags & LOOKUP_RCU) {\n\t\t// we need to grab link before we do unlazy.  And we can't skip\n\t\t// unlazy even if we fail to grab the link - cleanup needs it\n\t\tbool grabbed_link = legitimize_path(nd, link, nd->next_seq);\n\n\t\tif (!try_to_unlazy(nd) || !grabbed_link)\n\t\t\treturn -ECHILD;\n\n\t\tif (nd_alloc_stack(nd))\n\t\t\treturn 0;\n\t}\n\treturn -ENOMEM;\n}\n\nenum {WALK_TRAILING = 1, WALK_MORE = 2, WALK_NOFOLLOW = 4};\n\nstatic const char *pick_link(struct nameidata *nd, struct path *link,\n\t\t     struct inode *inode, int flags)\n{\n\tstruct saved *last;\n\tconst char *res;\n\tint error = reserve_stack(nd, link);\n\n\tif (unlikely(error)) {\n\t\tif (!(nd->flags & LOOKUP_RCU))\n\t\t\tpath_put(link);\n\t\treturn ERR_PTR(error);\n\t}\n\tlast = nd->stack + nd->depth++;\n\tlast->link = *link;\n\tclear_delayed_call(&last->done);\n\tlast->seq = nd->next_seq;\n\n\tif (flags & WALK_TRAILING) {\n\t\terror = may_follow_link(nd, inode);\n\t\tif (unlikely(error))\n\t\t\treturn ERR_PTR(error);\n\t}\n\n\tif (unlikely(nd->flags & LOOKUP_NO_SYMLINKS) ||\n\t\t\tunlikely(link->mnt->mnt_flags & MNT_NOSYMFOLLOW))\n\t\treturn ERR_PTR(-ELOOP);\n\n\tif (unlikely(atime_needs_update(&last->link, inode))) {\n\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\tif (!try_to_unlazy(nd))\n\t\t\t\treturn ERR_PTR(-ECHILD);\n\t\t}\n\t\ttouch_atime(&last->link);\n\t\tcond_resched();\n\t}\n\n\terror = security_inode_follow_link(link->dentry, inode,\n\t\t\t\t\t   nd->flags & LOOKUP_RCU);\n\tif (unlikely(error))\n\t\treturn ERR_PTR(error);\n\n\tres = READ_ONCE(inode->i_link);\n\tif (!res) {\n\t\tconst char * (*get)(struct dentry *, struct inode *,\n\t\t\t\tstruct delayed_call *);\n\t\tget = inode->i_op->get_link;\n\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\tres = get(NULL, inode, &last->done);\n\t\t\tif (res == ERR_PTR(-ECHILD) && try_to_unlazy(nd))\n\t\t\t\tres = get(link->dentry, inode, &last->done);\n\t\t} else {\n\t\t\tres = get(link->dentry, inode, &last->done);\n\t\t}\n\t\tif (!res)\n\t\t\tgoto all_done;\n\t\tif (IS_ERR(res))\n\t\t\treturn res;\n\t}\n\tif (*res == '/') {\n\t\terror = nd_jump_root(nd);\n\t\tif (unlikely(error))\n\t\t\treturn ERR_PTR(error);\n\t\twhile (unlikely(*++res == '/'))\n\t\t\t;\n\t}\n\tif (*res)\n\t\treturn res;\nall_done: // pure jump\n\tput_link(nd);\n\treturn NULL;\n}\n\n/*\n * Do we need to follow links? We _really_ want to be able\n * to do this check without having to look at inode->i_op,\n * so we keep a cache of \"no, this doesn't need follow_link\"\n * for the common case.\n *\n * NOTE: dentry must be what nd->next_seq had been sampled from.\n */\nstatic const char *step_into(struct nameidata *nd, int flags,\n\t\t     struct dentry *dentry)\n{\n\tstruct path path;\n\tstruct inode *inode;\n\tint err = handle_mounts(nd, dentry, &path);\n\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\tinode = path.dentry->d_inode;\n\tif (likely(!d_is_symlink(path.dentry)) ||\n\t   ((flags & WALK_TRAILING) && !(nd->flags & LOOKUP_FOLLOW)) ||\n\t   (flags & WALK_NOFOLLOW)) {\n\t\t/* not a symlink or should not follow */\n\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\tif (read_seqcount_retry(&path.dentry->d_seq, nd->next_seq))\n\t\t\t\treturn ERR_PTR(-ECHILD);\n\t\t\tif (unlikely(!inode))\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t} else {\n\t\t\tdput(nd->path.dentry);\n\t\t\tif (nd->path.mnt != path.mnt)\n\t\t\t\tmntput(nd->path.mnt);\n\t\t}\n\t\tnd->path = path;\n\t\tnd->inode = inode;\n\t\tnd->seq = nd->next_seq;\n\t\treturn NULL;\n\t}\n\tif (nd->flags & LOOKUP_RCU) {\n\t\t/* make sure that d_is_symlink above matches inode */\n\t\tif (read_seqcount_retry(&path.dentry->d_seq, nd->next_seq))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t} else {\n\t\tif (path.mnt == nd->path.mnt)\n\t\t\tmntget(path.mnt);\n\t}\n\treturn pick_link(nd, &path, inode, flags);\n}\n\nstatic struct dentry *follow_dotdot_rcu(struct nameidata *nd)\n{\n\tstruct dentry *parent, *old;\n\n\tif (path_equal(&nd->path, &nd->root))\n\t\tgoto in_root;\n\tif (unlikely(nd->path.dentry == nd->path.mnt->mnt_root)) {\n\t\tstruct path path;\n\t\tunsigned seq;\n\t\tif (!choose_mountpoint_rcu(real_mount(nd->path.mnt),\n\t\t\t\t\t   &nd->root, &path, &seq))\n\t\t\tgoto in_root;\n\t\tif (unlikely(nd->flags & LOOKUP_NO_XDEV))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\tnd->path = path;\n\t\tnd->inode = path.dentry->d_inode;\n\t\tnd->seq = seq;\n\t\t// makes sure that non-RCU pathwalk could reach this state\n\t\tif (read_seqretry(&mount_lock, nd->m_seq))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\t/* we know that mountpoint was pinned */\n\t}\n\told = nd->path.dentry;\n\tparent = old->d_parent;\n\tnd->next_seq = read_seqcount_begin(&parent->d_seq);\n\t// makes sure that non-RCU pathwalk could reach this state\n\tif (read_seqcount_retry(&old->d_seq, nd->seq))\n\t\treturn ERR_PTR(-ECHILD);\n\tif (unlikely(!path_connected(nd->path.mnt, parent)))\n\t\treturn ERR_PTR(-ECHILD);\n\treturn parent;\nin_root:\n\tif (read_seqretry(&mount_lock, nd->m_seq))\n\t\treturn ERR_PTR(-ECHILD);\n\tif (unlikely(nd->flags & LOOKUP_BENEATH))\n\t\treturn ERR_PTR(-ECHILD);\n\tnd->next_seq = nd->seq;\n\treturn nd->path.dentry;\n}\n\nstatic struct dentry *follow_dotdot(struct nameidata *nd)\n{\n\tstruct dentry *parent;\n\n\tif (path_equal(&nd->path, &nd->root))\n\t\tgoto in_root;\n\tif (unlikely(nd->path.dentry == nd->path.mnt->mnt_root)) {\n\t\tstruct path path;\n\n\t\tif (!choose_mountpoint(real_mount(nd->path.mnt),\n\t\t\t\t       &nd->root, &path))\n\t\t\tgoto in_root;\n\t\tpath_put(&nd->path);\n\t\tnd->path = path;\n\t\tnd->inode = path.dentry->d_inode;\n\t\tif (unlikely(nd->flags & LOOKUP_NO_XDEV))\n\t\t\treturn ERR_PTR(-EXDEV);\n\t}\n\t/* rare case of legitimate dget_parent()... */\n\tparent = dget_parent(nd->path.dentry);\n\tif (unlikely(!path_connected(nd->path.mnt, parent))) {\n\t\tdput(parent);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\treturn parent;\n\nin_root:\n\tif (unlikely(nd->flags & LOOKUP_BENEATH))\n\t\treturn ERR_PTR(-EXDEV);\n\treturn dget(nd->path.dentry);\n}\n\nstatic const char *handle_dots(struct nameidata *nd, int type)\n{\n\tif (type == LAST_DOTDOT) {\n\t\tconst char *error = NULL;\n\t\tstruct dentry *parent;\n\n\t\tif (!nd->root.mnt) {\n\t\t\terror = ERR_PTR(set_root(nd));\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif (nd->flags & LOOKUP_RCU)\n\t\t\tparent = follow_dotdot_rcu(nd);\n\t\telse\n\t\t\tparent = follow_dotdot(nd);\n\t\tif (IS_ERR(parent))\n\t\t\treturn ERR_CAST(parent);\n\t\terror = step_into(nd, WALK_NOFOLLOW, parent);\n\t\tif (unlikely(error))\n\t\t\treturn error;\n\n\t\tif (unlikely(nd->flags & LOOKUP_IS_SCOPED)) {\n\t\t\t/*\n\t\t\t * If there was a racing rename or mount along our\n\t\t\t * path, then we can't be sure that \"..\" hasn't jumped\n\t\t\t * above nd->root (and so userspace should retry or use\n\t\t\t * some fallback).\n\t\t\t */\n\t\t\tsmp_rmb();\n\t\t\tif (__read_seqcount_retry(&mount_lock.seqcount, nd->m_seq))\n\t\t\t\treturn ERR_PTR(-EAGAIN);\n\t\t\tif (__read_seqcount_retry(&rename_lock.seqcount, nd->r_seq))\n\t\t\t\treturn ERR_PTR(-EAGAIN);\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic const char *walk_component(struct nameidata *nd, int flags)\n{\n\tstruct dentry *dentry;\n\t/*\n\t * \".\" and \"..\" are special - \"..\" especially so because it has\n\t * to be able to know about the current root directory and\n\t * parent relationships.\n\t */\n\tif (unlikely(nd->last_type != LAST_NORM)) {\n\t\tif (!(flags & WALK_MORE) && nd->depth)\n\t\t\tput_link(nd);\n\t\treturn handle_dots(nd, nd->last_type);\n\t}\n\tdentry = lookup_fast(nd);\n\tif (IS_ERR(dentry))\n\t\treturn ERR_CAST(dentry);\n\tif (unlikely(!dentry)) {\n\t\tdentry = lookup_slow(&nd->last, nd->path.dentry, nd->flags);\n\t\tif (IS_ERR(dentry))\n\t\t\treturn ERR_CAST(dentry);\n\t}\n\tif (!(flags & WALK_MORE) && nd->depth)\n\t\tput_link(nd);\n\treturn step_into(nd, flags, dentry);\n}\n\n/*\n * We can do the critical dentry name comparison and hashing\n * operations one word at a time, but we are limited to:\n *\n * - Architectures with fast unaligned word accesses. We could\n *   do a \"get_unaligned()\" if this helps and is sufficiently\n *   fast.\n *\n * - non-CONFIG_DEBUG_PAGEALLOC configurations (so that we\n *   do not trap on the (extremely unlikely) case of a page\n *   crossing operation.\n *\n * - Furthermore, we need an efficient 64-bit compile for the\n *   64-bit case in order to generate the \"number of bytes in\n *   the final mask\". Again, that could be replaced with a\n *   efficient population count instruction or similar.\n */\n#ifdef CONFIG_DCACHE_WORD_ACCESS\n\n#include <asm/word-at-a-time.h>\n\n#ifdef HASH_MIX\n\n/* Architecture provides HASH_MIX and fold_hash() in <asm/hash.h> */\n\n#elif defined(CONFIG_64BIT)\n/*\n * Register pressure in the mixing function is an issue, particularly\n * on 32-bit x86, but almost any function requires one state value and\n * one temporary.  Instead, use a function designed for two state values\n * and no temporaries.\n *\n * This function cannot create a collision in only two iterations, so\n * we have two iterations to achieve avalanche.  In those two iterations,\n * we have six layers of mixing, which is enough to spread one bit's\n * influence out to 2^6 = 64 state bits.\n *\n * Rotate constants are scored by considering either 64 one-bit input\n * deltas or 64*63/2 = 2016 two-bit input deltas, and finding the\n * probability of that delta causing a change to each of the 128 output\n * bits, using a sample of random initial states.\n *\n * The Shannon entropy of the computed probabilities is then summed\n * to produce a score.  Ideally, any input change has a 50% chance of\n * toggling any given output bit.\n *\n * Mixing scores (in bits) for (12,45):\n * Input delta: 1-bit      2-bit\n * 1 round:     713.3    42542.6\n * 2 rounds:   2753.7   140389.8\n * 3 rounds:   5954.1   233458.2\n * 4 rounds:   7862.6   256672.2\n * Perfect:    8192     258048\n *            (64*128) (64*63/2 * 128)\n */\n#define HASH_MIX(x, y, a)\t\\\n\t(\tx ^= (a),\t\\\n\ty ^= x,\tx = rol64(x,12),\\\n\tx += y,\ty = rol64(y,45),\\\n\ty *= 9\t\t\t)\n\n/*\n * Fold two longs into one 32-bit hash value.  This must be fast, but\n * latency isn't quite as critical, as there is a fair bit of additional\n * work done before the hash value is used.\n */\nstatic inline unsigned int fold_hash(unsigned long x, unsigned long y)\n{\n\ty ^= x * GOLDEN_RATIO_64;\n\ty *= GOLDEN_RATIO_64;\n\treturn y >> 32;\n}\n\n#else\t/* 32-bit case */\n\n/*\n * Mixing scores (in bits) for (7,20):\n * Input delta: 1-bit      2-bit\n * 1 round:     330.3     9201.6\n * 2 rounds:   1246.4    25475.4\n * 3 rounds:   1907.1    31295.1\n * 4 rounds:   2042.3    31718.6\n * Perfect:    2048      31744\n *            (32*64)   (32*31/2 * 64)\n */\n#define HASH_MIX(x, y, a)\t\\\n\t(\tx ^= (a),\t\\\n\ty ^= x,\tx = rol32(x, 7),\\\n\tx += y,\ty = rol32(y,20),\\\n\ty *= 9\t\t\t)\n\nstatic inline unsigned int fold_hash(unsigned long x, unsigned long y)\n{\n\t/* Use arch-optimized multiply if one exists */\n\treturn __hash_32(y ^ __hash_32(x));\n}\n\n#endif\n\n/*\n * Return the hash of a string of known length.  This is carfully\n * designed to match hash_name(), which is the more critical function.\n * In particular, we must end by hashing a final word containing 0..7\n * payload bytes, to match the way that hash_name() iterates until it\n * finds the delimiter after the name.\n */\nunsigned int full_name_hash(const void *salt, const char *name, unsigned int len)\n{\n\tunsigned long a, x = 0, y = (unsigned long)salt;\n\n\tfor (;;) {\n\t\tif (!len)\n\t\t\tgoto done;\n\t\ta = load_unaligned_zeropad(name);\n\t\tif (len < sizeof(unsigned long))\n\t\t\tbreak;\n\t\tHASH_MIX(x, y, a);\n\t\tname += sizeof(unsigned long);\n\t\tlen -= sizeof(unsigned long);\n\t}\n\tx ^= a & bytemask_from_count(len);\ndone:\n\treturn fold_hash(x, y);\n}\nEXPORT_SYMBOL(full_name_hash);\n\n/* Return the \"hash_len\" (hash and length) of a null-terminated string */\nu64 hashlen_string(const void *salt, const char *name)\n{\n\tunsigned long a = 0, x = 0, y = (unsigned long)salt;\n\tunsigned long adata, mask, len;\n\tconst struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;\n\n\tlen = 0;\n\tgoto inside;\n\n\tdo {\n\t\tHASH_MIX(x, y, a);\n\t\tlen += sizeof(unsigned long);\ninside:\n\t\ta = load_unaligned_zeropad(name+len);\n\t} while (!has_zero(a, &adata, &constants));\n\n\tadata = prep_zero_mask(a, adata, &constants);\n\tmask = create_zero_mask(adata);\n\tx ^= a & zero_bytemask(mask);\n\n\treturn hashlen_create(fold_hash(x, y), len + find_zero(mask));\n}\nEXPORT_SYMBOL(hashlen_string);\n\n/*\n * Calculate the length and hash of the path component, and\n * return the length as the result.\n */\nstatic inline const char *hash_name(struct nameidata *nd,\n\t\t\t\t    const char *name,\n\t\t\t\t    unsigned long *lastword)\n{\n\tunsigned long a, b, x, y = (unsigned long)nd->path.dentry;\n\tunsigned long adata, bdata, mask, len;\n\tconst struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;\n\n\t/*\n\t * The first iteration is special, because it can result in\n\t * '.' and '..' and has no mixing other than the final fold.\n\t */\n\ta = load_unaligned_zeropad(name);\n\tb = a ^ REPEAT_BYTE('/');\n\tif (has_zero(a, &adata, &constants) | has_zero(b, &bdata, &constants)) {\n\t\tadata = prep_zero_mask(a, adata, &constants);\n\t\tbdata = prep_zero_mask(b, bdata, &constants);\n\t\tmask = create_zero_mask(adata | bdata);\n\t\ta &= zero_bytemask(mask);\n\t\t*lastword = a;\n\t\tlen = find_zero(mask);\n\t\tnd->last.hash = fold_hash(a, y);\n\t\tnd->last.len = len;\n\t\treturn name + len;\n\t}\n\n\tlen = 0;\n\tx = 0;\n\tdo {\n\t\tHASH_MIX(x, y, a);\n\t\tlen += sizeof(unsigned long);\n\t\ta = load_unaligned_zeropad(name+len);\n\t\tb = a ^ REPEAT_BYTE('/');\n\t} while (!(has_zero(a, &adata, &constants) | has_zero(b, &bdata, &constants)));\n\n\tadata = prep_zero_mask(a, adata, &constants);\n\tbdata = prep_zero_mask(b, bdata, &constants);\n\tmask = create_zero_mask(adata | bdata);\n\ta &= zero_bytemask(mask);\n\tx ^= a;\n\tlen += find_zero(mask);\n\t*lastword = 0;\t\t// Multi-word components cannot be DOT or DOTDOT\n\n\tnd->last.hash = fold_hash(x, y);\n\tnd->last.len = len;\n\treturn name + len;\n}\n\n/*\n * Note that the 'last' word is always zero-masked, but\n * was loaded as a possibly big-endian word.\n */\n#ifdef __BIG_ENDIAN\n  #define LAST_WORD_IS_DOT\t(0x2eul << (BITS_PER_LONG-8))\n  #define LAST_WORD_IS_DOTDOT\t(0x2e2eul << (BITS_PER_LONG-16))\n#endif\n\n#else\t/* !CONFIG_DCACHE_WORD_ACCESS: Slow, byte-at-a-time version */\n\n/* Return the hash of a string of known length */\nunsigned int full_name_hash(const void *salt, const char *name, unsigned int len)\n{\n\tunsigned long hash = init_name_hash(salt);\n\twhile (len--)\n\t\thash = partial_name_hash((unsigned char)*name++, hash);\n\treturn end_name_hash(hash);\n}\nEXPORT_SYMBOL(full_name_hash);\n\n/* Return the \"hash_len\" (hash and length) of a null-terminated string */\nu64 hashlen_string(const void *salt, const char *name)\n{\n\tunsigned long hash = init_name_hash(salt);\n\tunsigned long len = 0, c;\n\n\tc = (unsigned char)*name;\n\twhile (c) {\n\t\tlen++;\n\t\thash = partial_name_hash(c, hash);\n\t\tc = (unsigned char)name[len];\n\t}\n\treturn hashlen_create(end_name_hash(hash), len);\n}\nEXPORT_SYMBOL(hashlen_string);\n\n/*\n * We know there's a real path component here of at least\n * one character.\n */\nstatic inline const char *hash_name(struct nameidata *nd, const char *name, unsigned long *lastword)\n{\n\tunsigned long hash = init_name_hash(nd->path.dentry);\n\tunsigned long len = 0, c, last = 0;\n\n\tc = (unsigned char)*name;\n\tdo {\n\t\tlast = (last << 8) + c;\n\t\tlen++;\n\t\thash = partial_name_hash(c, hash);\n\t\tc = (unsigned char)name[len];\n\t} while (c && c != '/');\n\n\t// This is reliable for DOT or DOTDOT, since the component\n\t// cannot contain NUL characters - top bits being zero means\n\t// we cannot have had any other pathnames.\n\t*lastword = last;\n\tnd->last.hash = end_name_hash(hash);\n\tnd->last.len = len;\n\treturn name + len;\n}\n\n#endif\n\n#ifndef LAST_WORD_IS_DOT\n  #define LAST_WORD_IS_DOT\t0x2e\n  #define LAST_WORD_IS_DOTDOT\t0x2e2e\n#endif\n\n/*\n * Name resolution.\n * This is the basic name resolution function, turning a pathname into\n * the final dentry. We expect 'base' to be positive and a directory.\n *\n * Returns 0 and nd will have valid dentry and mnt on success.\n * Returns error and drops reference to input namei data on failure.\n */\nstatic int link_path_walk(const char *name, struct nameidata *nd)\n{\n\tint depth = 0; // depth <= nd->depth\n\tint err;\n\n\tnd->last_type = LAST_ROOT;\n\tnd->flags |= LOOKUP_PARENT;\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\tif (*name == '/') {\n\t\tdo {\n\t\t\tname++;\n\t\t} while (unlikely(*name == '/'));\n\t}\n\tif (unlikely(!*name)) {\n\t\tnd->dir_mode = 0; // short-circuit the 'hardening' idiocy\n\t\treturn 0;\n\t}\n\n\t/* At this point we know we have a real path component. */\n\tfor(;;) {\n\t\tstruct mnt_idmap *idmap;\n\t\tconst char *link;\n\t\tunsigned long lastword;\n\n\t\tidmap = mnt_idmap(nd->path.mnt);\n\t\terr = may_lookup(idmap, nd);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\n\t\tnd->last.name = name;\n\t\tname = hash_name(nd, name, &lastword);\n\n\t\tswitch(lastword) {\n\t\tcase LAST_WORD_IS_DOTDOT:\n\t\t\tnd->last_type = LAST_DOTDOT;\n\t\t\tnd->state |= ND_JUMPED;\n\t\t\tbreak;\n\n\t\tcase LAST_WORD_IS_DOT:\n\t\t\tnd->last_type = LAST_DOT;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tnd->last_type = LAST_NORM;\n\t\t\tnd->state &= ~ND_JUMPED;\n\n\t\t\tstruct dentry *parent = nd->path.dentry;\n\t\t\tif (unlikely(parent->d_flags & DCACHE_OP_HASH)) {\n\t\t\t\terr = parent->d_op->d_hash(parent, &nd->last);\n\t\t\t\tif (err < 0)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tif (!*name)\n\t\t\tgoto OK;\n\t\t/*\n\t\t * If it wasn't NUL, we know it was '/'. Skip that\n\t\t * slash, and continue until no more slashes.\n\t\t */\n\t\tdo {\n\t\t\tname++;\n\t\t} while (unlikely(*name == '/'));\n\t\tif (unlikely(!*name)) {\nOK:\n\t\t\t/* pathname or trailing symlink, done */\n\t\t\tif (!depth) {\n\t\t\t\tnd->dir_vfsuid = i_uid_into_vfsuid(idmap, nd->inode);\n\t\t\t\tnd->dir_mode = nd->inode->i_mode;\n\t\t\t\tnd->flags &= ~LOOKUP_PARENT;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\t/* last component of nested symlink */\n\t\t\tname = nd->stack[--depth].name;\n\t\t\tlink = walk_component(nd, 0);\n\t\t} else {\n\t\t\t/* not the last component */\n\t\t\tlink = walk_component(nd, WALK_MORE);\n\t\t}\n\t\tif (unlikely(link)) {\n\t\t\tif (IS_ERR(link))\n\t\t\t\treturn PTR_ERR(link);\n\t\t\t/* a symlink to follow */\n\t\t\tnd->stack[depth++].name = name;\n\t\t\tname = link;\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(!d_can_lookup(nd->path.dentry))) {\n\t\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\t\tif (!try_to_unlazy(nd))\n\t\t\t\t\treturn -ECHILD;\n\t\t\t}\n\t\t\treturn -ENOTDIR;\n\t\t}\n\t}\n}\n\n/* must be paired with terminate_walk() */\nstatic const char *path_init(struct nameidata *nd, unsigned flags)\n{\n\tint error;\n\tconst char *s = nd->pathname;\n\n\t/* LOOKUP_CACHED requires RCU, ask caller to retry */\n\tif ((flags & (LOOKUP_RCU | LOOKUP_CACHED)) == LOOKUP_CACHED)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tif (!*s)\n\t\tflags &= ~LOOKUP_RCU;\n\tif (flags & LOOKUP_RCU)\n\t\trcu_read_lock();\n\telse\n\t\tnd->seq = nd->next_seq = 0;\n\n\tnd->flags = flags;\n\tnd->state |= ND_JUMPED;\n\n\tnd->m_seq = __read_seqcount_begin(&mount_lock.seqcount);\n\tnd->r_seq = __read_seqcount_begin(&rename_lock.seqcount);\n\tsmp_rmb();\n\n\tif (nd->state & ND_ROOT_PRESET) {\n\t\tstruct dentry *root = nd->root.dentry;\n\t\tstruct inode *inode = root->d_inode;\n\t\tif (*s && unlikely(!d_can_lookup(root)))\n\t\t\treturn ERR_PTR(-ENOTDIR);\n\t\tnd->path = nd->root;\n\t\tnd->inode = inode;\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\tnd->seq = read_seqcount_begin(&nd->path.dentry->d_seq);\n\t\t\tnd->root_seq = nd->seq;\n\t\t} else {\n\t\t\tpath_get(&nd->path);\n\t\t}\n\t\treturn s;\n\t}\n\n\tnd->root.mnt = NULL;\n\n\t/* Absolute pathname -- fetch the root (LOOKUP_IN_ROOT uses nd->dfd). */\n\tif (*s == '/' && !(flags & LOOKUP_IN_ROOT)) {\n\t\terror = nd_jump_root(nd);\n\t\tif (unlikely(error))\n\t\t\treturn ERR_PTR(error);\n\t\treturn s;\n\t}\n\n\t/* Relative pathname -- get the starting-point it is relative to. */\n\tif (nd->dfd == AT_FDCWD) {\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\tstruct fs_struct *fs = current->fs;\n\t\t\tunsigned seq;\n\n\t\t\tdo {\n\t\t\t\tseq = read_seqbegin(&fs->seq);\n\t\t\t\tnd->path = fs->pwd;\n\t\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t\t\tnd->seq = __read_seqcount_begin(&nd->path.dentry->d_seq);\n\t\t\t} while (read_seqretry(&fs->seq, seq));\n\t\t} else {\n\t\t\tget_fs_pwd(current->fs, &nd->path);\n\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t}\n\t} else {\n\t\t/* Caller must check execute permissions on the starting path component */\n\t\tCLASS(fd_raw, f)(nd->dfd);\n\t\tstruct dentry *dentry;\n\n\t\tif (fd_empty(f))\n\t\t\treturn ERR_PTR(-EBADF);\n\n\t\tif (flags & LOOKUP_LINKAT_EMPTY) {\n\t\t\tif (fd_file(f)->f_cred != current_cred() &&\n\t\t\t    !ns_capable(fd_file(f)->f_cred->user_ns, CAP_DAC_READ_SEARCH))\n\t\t\t\treturn ERR_PTR(-ENOENT);\n\t\t}\n\n\t\tdentry = fd_file(f)->f_path.dentry;\n\n\t\tif (*s && unlikely(!d_can_lookup(dentry)))\n\t\t\treturn ERR_PTR(-ENOTDIR);\n\n\t\tnd->path = fd_file(f)->f_path;\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t\tnd->seq = read_seqcount_begin(&nd->path.dentry->d_seq);\n\t\t} else {\n\t\t\tpath_get(&nd->path);\n\t\t\tnd->inode = nd->path.dentry->d_inode;\n\t\t}\n\t}\n\n\t/* For scoped-lookups we need to set the root to the dirfd as well. */\n\tif (flags & LOOKUP_IS_SCOPED) {\n\t\tnd->root = nd->path;\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\tnd->root_seq = nd->seq;\n\t\t} else {\n\t\t\tpath_get(&nd->root);\n\t\t\tnd->state |= ND_ROOT_GRABBED;\n\t\t}\n\t}\n\treturn s;\n}\n\nstatic inline const char *lookup_last(struct nameidata *nd)\n{\n\tif (nd->last_type == LAST_NORM && nd->last.name[nd->last.len])\n\t\tnd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\n\n\treturn walk_component(nd, WALK_TRAILING);\n}\n\nstatic int handle_lookup_down(struct nameidata *nd)\n{\n\tif (!(nd->flags & LOOKUP_RCU))\n\t\tdget(nd->path.dentry);\n\tnd->next_seq = nd->seq;\n\treturn PTR_ERR(step_into(nd, WALK_NOFOLLOW, nd->path.dentry));\n}\n\n/* Returns 0 and nd will be valid on success; Returns error, otherwise. */\nstatic int path_lookupat(struct nameidata *nd, unsigned flags, struct path *path)\n{\n\tconst char *s = path_init(nd, flags);\n\tint err;\n\n\tif (unlikely(flags & LOOKUP_DOWN) && !IS_ERR(s)) {\n\t\terr = handle_lookup_down(nd);\n\t\tif (unlikely(err < 0))\n\t\t\ts = ERR_PTR(err);\n\t}\n\n\twhile (!(err = link_path_walk(s, nd)) &&\n\t       (s = lookup_last(nd)) != NULL)\n\t\t;\n\tif (!err && unlikely(nd->flags & LOOKUP_MOUNTPOINT)) {\n\t\terr = handle_lookup_down(nd);\n\t\tnd->state &= ~ND_JUMPED; // no d_weak_revalidate(), please...\n\t}\n\tif (!err)\n\t\terr = complete_walk(nd);\n\n\tif (!err && nd->flags & LOOKUP_DIRECTORY)\n\t\tif (!d_can_lookup(nd->path.dentry))\n\t\t\terr = -ENOTDIR;\n\tif (!err) {\n\t\t*path = nd->path;\n\t\tnd->path.mnt = NULL;\n\t\tnd->path.dentry = NULL;\n\t}\n\tterminate_walk(nd);\n\treturn err;\n}\n\nint filename_lookup(int dfd, struct filename *name, unsigned flags,\n\t\t    struct path *path, struct path *root)\n{\n\tint retval;\n\tstruct nameidata nd;\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\tset_nameidata(&nd, dfd, name, root);\n\tretval = path_lookupat(&nd, flags | LOOKUP_RCU, path);\n\tif (unlikely(retval == -ECHILD))\n\t\tretval = path_lookupat(&nd, flags, path);\n\tif (unlikely(retval == -ESTALE))\n\t\tretval = path_lookupat(&nd, flags | LOOKUP_REVAL, path);\n\n\tif (likely(!retval))\n\t\taudit_inode(name, path->dentry,\n\t\t\t    flags & LOOKUP_MOUNTPOINT ? AUDIT_INODE_NOEVAL : 0);\n\trestore_nameidata();\n\treturn retval;\n}\n\n/* Returns 0 and nd will be valid on success; Returns error, otherwise. */\nstatic int path_parentat(struct nameidata *nd, unsigned flags,\n\t\t\t\tstruct path *parent)\n{\n\tconst char *s = path_init(nd, flags);\n\tint err = link_path_walk(s, nd);\n\tif (!err)\n\t\terr = complete_walk(nd);\n\tif (!err) {\n\t\t*parent = nd->path;\n\t\tnd->path.mnt = NULL;\n\t\tnd->path.dentry = NULL;\n\t}\n\tterminate_walk(nd);\n\treturn err;\n}\n\n/* Note: this does not consume \"name\" */\nstatic int __filename_parentat(int dfd, struct filename *name,\n\t\t\t       unsigned int flags, struct path *parent,\n\t\t\t       struct qstr *last, int *type,\n\t\t\t       const struct path *root)\n{\n\tint retval;\n\tstruct nameidata nd;\n\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\tset_nameidata(&nd, dfd, name, root);\n\tretval = path_parentat(&nd, flags | LOOKUP_RCU, parent);\n\tif (unlikely(retval == -ECHILD))\n\t\tretval = path_parentat(&nd, flags, parent);\n\tif (unlikely(retval == -ESTALE))\n\t\tretval = path_parentat(&nd, flags | LOOKUP_REVAL, parent);\n\tif (likely(!retval)) {\n\t\t*last = nd.last;\n\t\t*type = nd.last_type;\n\t\taudit_inode(name, parent->dentry, AUDIT_INODE_PARENT);\n\t}\n\trestore_nameidata();\n\treturn retval;\n}\n\nstatic int filename_parentat(int dfd, struct filename *name,\n\t\t\t     unsigned int flags, struct path *parent,\n\t\t\t     struct qstr *last, int *type)\n{\n\treturn __filename_parentat(dfd, name, flags, parent, last, type, NULL);\n}\n\n/* does lookup, returns the object with parent locked */\nstatic struct dentry *__start_removing_path(int dfd, struct filename *name,\n\t\t\t\t\t   struct path *path)\n{\n\tstruct path parent_path __free(path_put) = {};\n\tstruct dentry *d;\n\tstruct qstr last;\n\tint type, error;\n\n\terror = filename_parentat(dfd, name, 0, &parent_path, &last, &type);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\tif (unlikely(type != LAST_NORM))\n\t\treturn ERR_PTR(-EINVAL);\n\t/* don't fail immediately if it's r/o, at least try to report other errors */\n\terror = mnt_want_write(parent_path.mnt);\n\tinode_lock_nested(parent_path.dentry->d_inode, I_MUTEX_PARENT);\n\td = lookup_one_qstr_excl(&last, parent_path.dentry, 0);\n\tif (IS_ERR(d))\n\t\tgoto unlock;\n\tif (error)\n\t\tgoto fail;\n\tpath->dentry = no_free_ptr(parent_path.dentry);\n\tpath->mnt = no_free_ptr(parent_path.mnt);\n\treturn d;\n\nfail:\n\tdput(d);\n\td = ERR_PTR(error);\nunlock:\n\tinode_unlock(parent_path.dentry->d_inode);\n\tif (!error)\n\t\tmnt_drop_write(parent_path.mnt);\n\treturn d;\n}\n\n/**\n * kern_path_parent: lookup path returning parent and target\n * @name: path name\n * @path: path to store parent in\n *\n * The path @name should end with a normal component, not \".\" or \"..\" or \"/\".\n * A lookup is performed and if successful the parent information\n * is store in @parent and the dentry is returned.\n *\n * The dentry maybe negative, the parent will be positive.\n *\n * Returns:  dentry or error.\n */\nstruct dentry *kern_path_parent(const char *name, struct path *path)\n{\n\tstruct path parent_path __free(path_put) = {};\n\tstruct filename *filename __free(putname) = getname_kernel(name);\n\tstruct dentry *d;\n\tstruct qstr last;\n\tint type, error;\n\n\terror = filename_parentat(AT_FDCWD, filename, 0, &parent_path, &last, &type);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\tif (unlikely(type != LAST_NORM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\td = lookup_noperm_unlocked(&last, parent_path.dentry);\n\tif (IS_ERR(d))\n\t\treturn d;\n\tpath->dentry = no_free_ptr(parent_path.dentry);\n\tpath->mnt = no_free_ptr(parent_path.mnt);\n\treturn d;\n}\n\nstruct dentry *start_removing_path(const char *name, struct path *path)\n{\n\tstruct filename *filename = getname_kernel(name);\n\tstruct dentry *res = __start_removing_path(AT_FDCWD, filename, path);\n\n\tputname(filename);\n\treturn res;\n}\n\nstruct dentry *start_removing_user_path_at(int dfd,\n\t\t\t\t\t   const char __user *name,\n\t\t\t\t\t   struct path *path)\n{\n\tstruct filename *filename = getname(name);\n\tstruct dentry *res = __start_removing_path(dfd, filename, path);\n\n\tputname(filename);\n\treturn res;\n}\nEXPORT_SYMBOL(start_removing_user_path_at);\n\nint kern_path(const char *name, unsigned int flags, struct path *path)\n{\n\tstruct filename *filename = getname_kernel(name);\n\tint ret = filename_lookup(AT_FDCWD, filename, flags, path, NULL);\n\n\tputname(filename);\n\treturn ret;\n\n}\nEXPORT_SYMBOL(kern_path);\n\n/**\n * vfs_path_parent_lookup - lookup a parent path relative to a dentry-vfsmount pair\n * @filename: filename structure\n * @flags: lookup flags\n * @parent: pointer to struct path to fill\n * @last: last component\n * @type: type of the last component\n * @root: pointer to struct path of the base directory\n */\nint vfs_path_parent_lookup(struct filename *filename, unsigned int flags,\n\t\t\t   struct path *parent, struct qstr *last, int *type,\n\t\t\t   const struct path *root)\n{\n\treturn  __filename_parentat(AT_FDCWD, filename, flags, parent, last,\n\t\t\t\t    type, root);\n}\nEXPORT_SYMBOL(vfs_path_parent_lookup);\n\n/**\n * vfs_path_lookup - lookup a file path relative to a dentry-vfsmount pair\n * @dentry:  pointer to dentry of the base directory\n * @mnt: pointer to vfs mount of the base directory\n * @name: pointer to file name\n * @flags: lookup flags\n * @path: pointer to struct path to fill\n */\nint vfs_path_lookup(struct dentry *dentry, struct vfsmount *mnt,\n\t\t    const char *name, unsigned int flags,\n\t\t    struct path *path)\n{\n\tstruct filename *filename;\n\tstruct path root = {.mnt = mnt, .dentry = dentry};\n\tint ret;\n\n\tfilename = getname_kernel(name);\n\t/* the first argument of filename_lookup() is ignored with root */\n\tret = filename_lookup(AT_FDCWD, filename, flags, path, &root);\n\tputname(filename);\n\treturn ret;\n}\nEXPORT_SYMBOL(vfs_path_lookup);\n\nstatic int lookup_noperm_common(struct qstr *qname, struct dentry *base)\n{\n\tconst char *name = qname->name;\n\tu32 len = qname->len;\n\n\tqname->hash = full_name_hash(base, name, len);\n\tif (!len)\n\t\treturn -EACCES;\n\n\tif (is_dot_dotdot(name, len))\n\t\treturn -EACCES;\n\n\twhile (len--) {\n\t\tunsigned int c = *(const unsigned char *)name++;\n\t\tif (c == '/' || c == '\\0')\n\t\t\treturn -EACCES;\n\t}\n\t/*\n\t * See if the low-level filesystem might want\n\t * to use its own hash..\n\t */\n\tif (base->d_flags & DCACHE_OP_HASH) {\n\t\tint err = base->d_op->d_hash(base, qname);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic int lookup_one_common(struct mnt_idmap *idmap,\n\t\t\t     struct qstr *qname, struct dentry *base)\n{\n\tint err;\n\terr = lookup_noperm_common(qname, base);\n\tif (err < 0)\n\t\treturn err;\n\treturn inode_permission(idmap, base->d_inode, MAY_EXEC);\n}\n\n/**\n * try_lookup_noperm - filesystem helper to lookup single pathname component\n * @name:\tqstr storing pathname component to lookup\n * @base:\tbase directory to lookup from\n *\n * Look up a dentry by name in the dcache, returning NULL if it does not\n * currently exist.  The function does not try to create a dentry and if one\n * is found it doesn't try to revalidate it.\n *\n * Note that this routine is purely a helper for filesystem usage and should\n * not be called by generic code.  It does no permission checking.\n *\n * No locks need be held - only a counted reference to @base is needed.\n *\n */\nstruct dentry *try_lookup_noperm(struct qstr *name, struct dentry *base)\n{\n\tint err;\n\n\terr = lookup_noperm_common(name, base);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn d_lookup(base, name);\n}\nEXPORT_SYMBOL(try_lookup_noperm);\n\n/**\n * lookup_noperm - filesystem helper to lookup single pathname component\n * @name:\tqstr storing pathname component to lookup\n * @base:\tbase directory to lookup from\n *\n * Note that this routine is purely a helper for filesystem usage and should\n * not be called by generic code.  It does no permission checking.\n *\n * The caller must hold base->i_rwsem.\n */\nstruct dentry *lookup_noperm(struct qstr *name, struct dentry *base)\n{\n\tstruct dentry *dentry;\n\tint err;\n\n\tWARN_ON_ONCE(!inode_is_locked(base->d_inode));\n\n\terr = lookup_noperm_common(name, base);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tdentry = lookup_dcache(name, base, 0);\n\treturn dentry ? dentry : __lookup_slow(name, base, 0);\n}\nEXPORT_SYMBOL(lookup_noperm);\n\n/**\n * lookup_one - lookup single pathname component\n * @idmap:\tidmap of the mount the lookup is performed from\n * @name:\tqstr holding pathname component to lookup\n * @base:\tbase directory to lookup from\n *\n * This can be used for in-kernel filesystem clients such as file servers.\n *\n * The caller must hold base->i_rwsem.\n */\nstruct dentry *lookup_one(struct mnt_idmap *idmap, struct qstr *name,\n\t\t\t  struct dentry *base)\n{\n\tstruct dentry *dentry;\n\tint err;\n\n\tWARN_ON_ONCE(!inode_is_locked(base->d_inode));\n\n\terr = lookup_one_common(idmap, name, base);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tdentry = lookup_dcache(name, base, 0);\n\treturn dentry ? dentry : __lookup_slow(name, base, 0);\n}\nEXPORT_SYMBOL(lookup_one);\n\n/**\n * lookup_one_unlocked - lookup single pathname component\n * @idmap:\tidmap of the mount the lookup is performed from\n * @name:\tqstr olding pathname component to lookup\n * @base:\tbase directory to lookup from\n *\n * This can be used for in-kernel filesystem clients such as file servers.\n *\n * Unlike lookup_one, it should be called without the parent\n * i_rwsem held, and will take the i_rwsem itself if necessary.\n */\nstruct dentry *lookup_one_unlocked(struct mnt_idmap *idmap, struct qstr *name,\n\t\t\t\t   struct dentry *base)\n{\n\tint err;\n\tstruct dentry *ret;\n\n\terr = lookup_one_common(idmap, name, base);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tret = lookup_dcache(name, base, 0);\n\tif (!ret)\n\t\tret = lookup_slow(name, base, 0);\n\treturn ret;\n}\nEXPORT_SYMBOL(lookup_one_unlocked);\n\n/**\n * lookup_one_positive_killable - lookup single pathname component\n * @idmap:\tidmap of the mount the lookup is performed from\n * @name:\tqstr olding pathname component to lookup\n * @base:\tbase directory to lookup from\n *\n * This helper will yield ERR_PTR(-ENOENT) on negatives. The helper returns\n * known positive or ERR_PTR(). This is what most of the users want.\n *\n * Note that pinned negative with unlocked parent _can_ become positive at any\n * time, so callers of lookup_one_unlocked() need to be very careful; pinned\n * positives have >d_inode stable, so this one avoids such problems.\n *\n * This can be used for in-kernel filesystem clients such as file servers.\n *\n * It should be called without the parent i_rwsem held, and will take\n * the i_rwsem itself if necessary.  If a fatal signal is pending or\n * delivered, it will return %-EINTR if the lock is needed.\n */\nstruct dentry *lookup_one_positive_killable(struct mnt_idmap *idmap,\n\t\t\t\t\t    struct qstr *name,\n\t\t\t\t\t    struct dentry *base)\n{\n\tint err;\n\tstruct dentry *ret;\n\n\terr = lookup_one_common(idmap, name, base);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tret = lookup_dcache(name, base, 0);\n\tif (!ret)\n\t\tret = lookup_slow_killable(name, base, 0);\n\tif (!IS_ERR(ret) && d_flags_negative(smp_load_acquire(&ret->d_flags))) {\n\t\tdput(ret);\n\t\tret = ERR_PTR(-ENOENT);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(lookup_one_positive_killable);\n\n/**\n * lookup_one_positive_unlocked - lookup single pathname component\n * @idmap:\tidmap of the mount the lookup is performed from\n * @name:\tqstr holding pathname component to lookup\n * @base:\tbase directory to lookup from\n *\n * This helper will yield ERR_PTR(-ENOENT) on negatives. The helper returns\n * known positive or ERR_PTR(). This is what most of the users want.\n *\n * Note that pinned negative with unlocked parent _can_ become positive at any\n * time, so callers of lookup_one_unlocked() need to be very careful; pinned\n * positives have >d_inode stable, so this one avoids such problems.\n *\n * This can be used for in-kernel filesystem clients such as file servers.\n *\n * The helper should be called without i_rwsem held.\n */\nstruct dentry *lookup_one_positive_unlocked(struct mnt_idmap *idmap,\n\t\t\t\t\t    struct qstr *name,\n\t\t\t\t\t    struct dentry *base)\n{\n\tstruct dentry *ret = lookup_one_unlocked(idmap, name, base);\n\n\tif (!IS_ERR(ret) && d_flags_negative(smp_load_acquire(&ret->d_flags))) {\n\t\tdput(ret);\n\t\tret = ERR_PTR(-ENOENT);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(lookup_one_positive_unlocked);\n\n/**\n * lookup_noperm_unlocked - filesystem helper to lookup single pathname component\n * @name:\tpathname component to lookup\n * @base:\tbase directory to lookup from\n *\n * Note that this routine is purely a helper for filesystem usage and should\n * not be called by generic code. It does no permission checking.\n *\n * Unlike lookup_noperm(), it should be called without the parent\n * i_rwsem held, and will take the i_rwsem itself if necessary.\n *\n * Unlike try_lookup_noperm() it *does* revalidate the dentry if it already\n * existed.\n */\nstruct dentry *lookup_noperm_unlocked(struct qstr *name, struct dentry *base)\n{\n\tstruct dentry *ret;\n\tint err;\n\n\terr = lookup_noperm_common(name, base);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tret = lookup_dcache(name, base, 0);\n\tif (!ret)\n\t\tret = lookup_slow(name, base, 0);\n\treturn ret;\n}\nEXPORT_SYMBOL(lookup_noperm_unlocked);\n\n/*\n * Like lookup_noperm_unlocked(), except that it yields ERR_PTR(-ENOENT)\n * on negatives.  Returns known positive or ERR_PTR(); that's what\n * most of the users want.  Note that pinned negative with unlocked parent\n * _can_ become positive at any time, so callers of lookup_noperm_unlocked()\n * need to be very careful; pinned positives have ->d_inode stable, so\n * this one avoids such problems.\n */\nstruct dentry *lookup_noperm_positive_unlocked(struct qstr *name,\n\t\t\t\t\t       struct dentry *base)\n{\n\tstruct dentry *ret;\n\n\tret = lookup_noperm_unlocked(name, base);\n\tif (!IS_ERR(ret) && d_flags_negative(smp_load_acquire(&ret->d_flags))) {\n\t\tdput(ret);\n\t\tret = ERR_PTR(-ENOENT);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(lookup_noperm_positive_unlocked);\n\n#ifdef CONFIG_UNIX98_PTYS\nint path_pts(struct path *path)\n{\n\t/* Find something mounted on \"pts\" in the same directory as\n\t * the input path.\n\t */\n\tstruct dentry *parent = dget_parent(path->dentry);\n\tstruct dentry *child;\n\tstruct qstr this = QSTR_INIT(\"pts\", 3);\n\n\tif (unlikely(!path_connected(path->mnt, parent))) {\n\t\tdput(parent);\n\t\treturn -ENOENT;\n\t}\n\tdput(path->dentry);\n\tpath->dentry = parent;\n\tchild = d_hash_and_lookup(parent, &this);\n\tif (IS_ERR_OR_NULL(child))\n\t\treturn -ENOENT;\n\n\tpath->dentry = child;\n\tdput(parent);\n\tfollow_down(path, 0);\n\treturn 0;\n}\n#endif\n\nint user_path_at(int dfd, const char __user *name, unsigned flags,\n\t\t struct path *path)\n{\n\tstruct filename *filename = getname_flags(name, flags);\n\tint ret = filename_lookup(dfd, filename, flags, path, NULL);\n\n\tputname(filename);\n\treturn ret;\n}\nEXPORT_SYMBOL(user_path_at);\n\nint __check_sticky(struct mnt_idmap *idmap, struct inode *dir,\n\t\t   struct inode *inode)\n{\n\tkuid_t fsuid = current_fsuid();\n\n\tif (vfsuid_eq_kuid(i_uid_into_vfsuid(idmap, inode), fsuid))\n\t\treturn 0;\n\tif (vfsuid_eq_kuid(i_uid_into_vfsuid(idmap, dir), fsuid))\n\t\treturn 0;\n\treturn !capable_wrt_inode_uidgid(idmap, inode, CAP_FOWNER);\n}\nEXPORT_SYMBOL(__check_sticky);\n\n/*\n *\tCheck whether we can remove a link victim from directory dir, check\n *  whether the type of victim is right.\n *  1. We can't do it if dir is read-only (done in permission())\n *  2. We should have write and exec permissions on dir\n *  3. We can't remove anything from append-only dir\n *  4. We can't do anything with immutable dir (done in permission())\n *  5. If the sticky bit on dir is set we should either\n *\ta. be owner of dir, or\n *\tb. be owner of victim, or\n *\tc. have CAP_FOWNER capability\n *  6. If the victim is append-only or immutable we can't do antyhing with\n *     links pointing to it.\n *  7. If the victim has an unknown uid or gid we can't change the inode.\n *  8. If we were asked to remove a directory and victim isn't one - ENOTDIR.\n *  9. If we were asked to remove a non-directory and victim isn't one - EISDIR.\n * 10. We can't remove a root or mountpoint.\n * 11. We don't allow removal of NFS sillyrenamed files; it's handled by\n *     nfs_async_unlink().\n */\nstatic int may_delete(struct mnt_idmap *idmap, struct inode *dir,\n\t\t      struct dentry *victim, bool isdir)\n{\n\tstruct inode *inode = d_backing_inode(victim);\n\tint error;\n\n\tif (d_is_negative(victim))\n\t\treturn -ENOENT;\n\tBUG_ON(!inode);\n\n\tBUG_ON(victim->d_parent->d_inode != dir);\n\n\t/* Inode writeback is not safe when the uid or gid are invalid. */\n\tif (!vfsuid_valid(i_uid_into_vfsuid(idmap, inode)) ||\n\t    !vfsgid_valid(i_gid_into_vfsgid(idmap, inode)))\n\t\treturn -EOVERFLOW;\n\n\taudit_inode_child(dir, victim, AUDIT_TYPE_CHILD_DELETE);\n\n\terror = inode_permission(idmap, dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\tif (IS_APPEND(dir))\n\t\treturn -EPERM;\n\n\tif (check_sticky(idmap, dir, inode) || IS_APPEND(inode) ||\n\t    IS_IMMUTABLE(inode) || IS_SWAPFILE(inode) ||\n\t    HAS_UNMAPPED_ID(idmap, inode))\n\t\treturn -EPERM;\n\tif (isdir) {\n\t\tif (!d_is_dir(victim))\n\t\t\treturn -ENOTDIR;\n\t\tif (IS_ROOT(victim))\n\t\t\treturn -EBUSY;\n\t} else if (d_is_dir(victim))\n\t\treturn -EISDIR;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\tif (victim->d_flags & DCACHE_NFSFS_RENAMED)\n\t\treturn -EBUSY;\n\treturn 0;\n}\n\n/*\tCheck whether we can create an object with dentry child in directory\n *  dir.\n *  1. We can't do it if child already exists (open has special treatment for\n *     this case, but since we are inlined it's OK)\n *  2. We can't do it if dir is read-only (done in permission())\n *  3. We can't do it if the fs can't represent the fsuid or fsgid.\n *  4. We should have write and exec permissions on dir\n *  5. We can't do it if dir is immutable (done in permission())\n */\nstatic inline int may_create(struct mnt_idmap *idmap,\n\t\t\t     struct inode *dir, struct dentry *child)\n{\n\taudit_inode_child(dir, child, AUDIT_TYPE_CHILD_CREATE);\n\tif (child->d_inode)\n\t\treturn -EEXIST;\n\tif (IS_DEADDIR(dir))\n\t\treturn -ENOENT;\n\tif (!fsuidgid_has_mapping(dir->i_sb, idmap))\n\t\treturn -EOVERFLOW;\n\n\treturn inode_permission(idmap, dir, MAY_WRITE | MAY_EXEC);\n}\n\n// p1 != p2, both are on the same filesystem, ->s_vfs_rename_mutex is held\nstatic struct dentry *lock_two_directories(struct dentry *p1, struct dentry *p2)\n{\n\tstruct dentry *p = p1, *q = p2, *r;\n\n\twhile ((r = p->d_parent) != p2 && r != p)\n\t\tp = r;\n\tif (r == p2) {\n\t\t// p is a child of p2 and an ancestor of p1 or p1 itself\n\t\tinode_lock_nested(p2->d_inode, I_MUTEX_PARENT);\n\t\tinode_lock_nested(p1->d_inode, I_MUTEX_PARENT2);\n\t\treturn p;\n\t}\n\t// p is the root of connected component that contains p1\n\t// p2 does not occur on the path from p to p1\n\twhile ((r = q->d_parent) != p1 && r != p && r != q)\n\t\tq = r;\n\tif (r == p1) {\n\t\t// q is a child of p1 and an ancestor of p2 or p2 itself\n\t\tinode_lock_nested(p1->d_inode, I_MUTEX_PARENT);\n\t\tinode_lock_nested(p2->d_inode, I_MUTEX_PARENT2);\n\t\treturn q;\n\t} else if (likely(r == p)) {\n\t\t// both p2 and p1 are descendents of p\n\t\tinode_lock_nested(p1->d_inode, I_MUTEX_PARENT);\n\t\tinode_lock_nested(p2->d_inode, I_MUTEX_PARENT2);\n\t\treturn NULL;\n\t} else { // no common ancestor at the time we'd been called\n\t\tmutex_unlock(&p1->d_sb->s_vfs_rename_mutex);\n\t\treturn ERR_PTR(-EXDEV);\n\t}\n}\n\n/*\n * p1 and p2 should be directories on the same fs.\n */\nstruct dentry *lock_rename(struct dentry *p1, struct dentry *p2)\n{\n\tif (p1 == p2) {\n\t\tinode_lock_nested(p1->d_inode, I_MUTEX_PARENT);\n\t\treturn NULL;\n\t}\n\n\tmutex_lock(&p1->d_sb->s_vfs_rename_mutex);\n\treturn lock_two_directories(p1, p2);\n}\nEXPORT_SYMBOL(lock_rename);\n\n/*\n * c1 and p2 should be on the same fs.\n */\nstruct dentry *lock_rename_child(struct dentry *c1, struct dentry *p2)\n{\n\tif (READ_ONCE(c1->d_parent) == p2) {\n\t\t/*\n\t\t * hopefully won't need to touch ->s_vfs_rename_mutex at all.\n\t\t */\n\t\tinode_lock_nested(p2->d_inode, I_MUTEX_PARENT);\n\t\t/*\n\t\t * now that p2 is locked, nobody can move in or out of it,\n\t\t * so the test below is safe.\n\t\t */\n\t\tif (likely(c1->d_parent == p2))\n\t\t\treturn NULL;\n\n\t\t/*\n\t\t * c1 got moved out of p2 while we'd been taking locks;\n\t\t * unlock and fall back to slow case.\n\t\t */\n\t\tinode_unlock(p2->d_inode);\n\t}\n\n\tmutex_lock(&c1->d_sb->s_vfs_rename_mutex);\n\t/*\n\t * nobody can move out of any directories on this fs.\n\t */\n\tif (likely(c1->d_parent != p2))\n\t\treturn lock_two_directories(c1->d_parent, p2);\n\n\t/*\n\t * c1 got moved into p2 while we were taking locks;\n\t * we need p2 locked and ->s_vfs_rename_mutex unlocked,\n\t * for consistency with lock_rename().\n\t */\n\tinode_lock_nested(p2->d_inode, I_MUTEX_PARENT);\n\tmutex_unlock(&c1->d_sb->s_vfs_rename_mutex);\n\treturn NULL;\n}\nEXPORT_SYMBOL(lock_rename_child);\n\nvoid unlock_rename(struct dentry *p1, struct dentry *p2)\n{\n\tinode_unlock(p1->d_inode);\n\tif (p1 != p2) {\n\t\tinode_unlock(p2->d_inode);\n\t\tmutex_unlock(&p1->d_sb->s_vfs_rename_mutex);\n\t}\n}\nEXPORT_SYMBOL(unlock_rename);\n\n/**\n * vfs_prepare_mode - prepare the mode to be used for a new inode\n * @idmap:\tidmap of the mount the inode was found from\n * @dir:\tparent directory of the new inode\n * @mode:\tmode of the new inode\n * @mask_perms:\tallowed permission by the vfs\n * @type:\ttype of file to be created\n *\n * This helper consolidates and enforces vfs restrictions on the @mode of a new\n * object to be created.\n *\n * Umask stripping depends on whether the filesystem supports POSIX ACLs (see\n * the kernel documentation for mode_strip_umask()). Moving umask stripping\n * after setgid stripping allows the same ordering for both non-POSIX ACL and\n * POSIX ACL supporting filesystems.\n *\n * Note that it's currently valid for @type to be 0 if a directory is created.\n * Filesystems raise that flag individually and we need to check whether each\n * filesystem can deal with receiving S_IFDIR from the vfs before we enforce a\n * non-zero type.\n *\n * Returns: mode to be passed to the filesystem\n */\nstatic inline umode_t vfs_prepare_mode(struct mnt_idmap *idmap,\n\t\t\t\t       const struct inode *dir, umode_t mode,\n\t\t\t\t       umode_t mask_perms, umode_t type)\n{\n\tmode = mode_strip_sgid(idmap, dir, mode);\n\tmode = mode_strip_umask(dir, mode);\n\n\t/*\n\t * Apply the vfs mandated allowed permission mask and set the type of\n\t * file to be created before we call into the filesystem.\n\t */\n\tmode &= (mask_perms & ~S_IFMT);\n\tmode |= (type & S_IFMT);\n\n\treturn mode;\n}\n\n/**\n * vfs_create - create new file\n * @idmap:\tidmap of the mount the inode was found from\n * @dir:\tinode of the parent directory\n * @dentry:\tdentry of the child file\n * @mode:\tmode of the child file\n * @want_excl:\twhether the file must not yet exist\n *\n * Create a new file.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint vfs_create(struct mnt_idmap *idmap, struct inode *dir,\n\t       struct dentry *dentry, umode_t mode, bool want_excl)\n{\n\tint error;\n\n\terror = may_create(idmap, dir, dentry);\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->create)\n\t\treturn -EACCES;\t/* shouldn't it be ENOSYS? */\n\n\tmode = vfs_prepare_mode(idmap, dir, mode, S_IALLUGO, S_IFREG);\n\terror = security_inode_create(dir, dentry, mode);\n\tif (error)\n\t\treturn error;\n\terror = dir->i_op->create(idmap, dir, dentry, mode, want_excl);\n\tif (!error)\n\t\tfsnotify_create(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_create);\n\nint vfs_mkobj(struct dentry *dentry, umode_t mode,\n\t\tint (*f)(struct dentry *, umode_t, void *),\n\t\tvoid *arg)\n{\n\tstruct inode *dir = dentry->d_parent->d_inode;\n\tint error = may_create(&nop_mnt_idmap, dir, dentry);\n\tif (error)\n\t\treturn error;\n\n\tmode &= S_IALLUGO;\n\tmode |= S_IFREG;\n\terror = security_inode_create(dir, dentry, mode);\n\tif (error)\n\t\treturn error;\n\terror = f(dentry, mode, arg);\n\tif (!error)\n\t\tfsnotify_create(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_mkobj);\n\nbool may_open_dev(const struct path *path)\n{\n\treturn !(path->mnt->mnt_flags & MNT_NODEV) &&\n\t\t!(path->mnt->mnt_sb->s_iflags & SB_I_NODEV);\n}\n\nstatic int may_open(struct mnt_idmap *idmap, const struct path *path,\n\t\t    int acc_mode, int flag)\n{\n\tstruct dentry *dentry = path->dentry;\n\tstruct inode *inode = dentry->d_inode;\n\tint error;\n\n\tif (!inode)\n\t\treturn -ENOENT;\n\n\tswitch (inode->i_mode & S_IFMT) {\n\tcase S_IFLNK:\n\t\treturn -ELOOP;\n\tcase S_IFDIR:\n\t\tif (acc_mode & MAY_WRITE)\n\t\t\treturn -EISDIR;\n\t\tif (acc_mode & MAY_EXEC)\n\t\t\treturn -EACCES;\n\t\tbreak;\n\tcase S_IFBLK:\n\tcase S_IFCHR:\n\t\tif (!may_open_dev(path))\n\t\t\treturn -EACCES;\n\t\tfallthrough;\n\tcase S_IFIFO:\n\tcase S_IFSOCK:\n\t\tif (acc_mode & MAY_EXEC)\n\t\t\treturn -EACCES;\n\t\tflag &= ~O_TRUNC;\n\t\tbreak;\n\tcase S_IFREG:\n\t\tif ((acc_mode & MAY_EXEC) && path_noexec(path))\n\t\t\treturn -EACCES;\n\t\tbreak;\n\tdefault:\n\t\tVFS_BUG_ON_INODE(!IS_ANON_FILE(inode), inode);\n\t}\n\n\terror = inode_permission(idmap, inode, MAY_OPEN | acc_mode);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * An append-only file must be opened in append mode for writing.\n\t */\n\tif (IS_APPEND(inode)) {\n\t\tif  ((flag & O_ACCMODE) != O_RDONLY && !(flag & O_APPEND))\n\t\t\treturn -EPERM;\n\t\tif (flag & O_TRUNC)\n\t\t\treturn -EPERM;\n\t}\n\n\t/* O_NOATIME can only be set by the owner or superuser */\n\tif (flag & O_NOATIME && !inode_owner_or_capable(idmap, inode))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int handle_truncate(struct mnt_idmap *idmap, struct file *filp)\n{\n\tconst struct path *path = &filp->f_path;\n\tstruct inode *inode = path->dentry->d_inode;\n\tint error = get_write_access(inode);\n\tif (error)\n\t\treturn error;\n\n\terror = security_file_truncate(filp);\n\tif (!error) {\n\t\terror = do_truncate(idmap, path->dentry, 0,\n\t\t\t\t    ATTR_MTIME|ATTR_CTIME|ATTR_OPEN,\n\t\t\t\t    filp);\n\t}\n\tput_write_access(inode);\n\treturn error;\n}\n\nstatic inline int open_to_namei_flags(int flag)\n{\n\tif ((flag & O_ACCMODE) == 3)\n\t\tflag--;\n\treturn flag;\n}\n\nstatic int may_o_create(struct mnt_idmap *idmap,\n\t\t\tconst struct path *dir, struct dentry *dentry,\n\t\t\tumode_t mode)\n{\n\tint error = security_path_mknod(dir, dentry, mode, 0);\n\tif (error)\n\t\treturn error;\n\n\tif (!fsuidgid_has_mapping(dir->dentry->d_sb, idmap))\n\t\treturn -EOVERFLOW;\n\n\terror = inode_permission(idmap, dir->dentry->d_inode,\n\t\t\t\t MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\n\treturn security_inode_create(dir->dentry->d_inode, dentry, mode);\n}\n\n/*\n * Attempt to atomically look up, create and open a file from a negative\n * dentry.\n *\n * Returns 0 if successful.  The file will have been created and attached to\n * @file by the filesystem calling finish_open().\n *\n * If the file was looked up only or didn't need creating, FMODE_OPENED won't\n * be set.  The caller will need to perform the open themselves.  @path will\n * have been updated to point to the new dentry.  This may be negative.\n *\n * Returns an error code otherwise.\n */\nstatic struct dentry *atomic_open(struct nameidata *nd, struct dentry *dentry,\n\t\t\t\t  struct file *file,\n\t\t\t\t  int open_flag, umode_t mode)\n{\n\tstruct dentry *const DENTRY_NOT_SET = (void *) -1UL;\n\tstruct inode *dir =  nd->path.dentry->d_inode;\n\tint error;\n\n\tif (nd->flags & LOOKUP_DIRECTORY)\n\t\topen_flag |= O_DIRECTORY;\n\n\tfile->f_path.dentry = DENTRY_NOT_SET;\n\tfile->f_path.mnt = nd->path.mnt;\n\terror = dir->i_op->atomic_open(dir, dentry, file,\n\t\t\t\t       open_to_namei_flags(open_flag), mode);\n\td_lookup_done(dentry);\n\tif (!error) {\n\t\tif (file->f_mode & FMODE_OPENED) {\n\t\t\tif (unlikely(dentry != file->f_path.dentry)) {\n\t\t\t\tdput(dentry);\n\t\t\t\tdentry = dget(file->f_path.dentry);\n\t\t\t}\n\t\t} else if (WARN_ON(file->f_path.dentry == DENTRY_NOT_SET)) {\n\t\t\terror = -EIO;\n\t\t} else {\n\t\t\tif (file->f_path.dentry) {\n\t\t\t\tdput(dentry);\n\t\t\t\tdentry = file->f_path.dentry;\n\t\t\t}\n\t\t\tif (unlikely(d_is_negative(dentry)))\n\t\t\t\terror = -ENOENT;\n\t\t}\n\t}\n\tif (error) {\n\t\tdput(dentry);\n\t\tdentry = ERR_PTR(error);\n\t}\n\treturn dentry;\n}\n\n/*\n * Look up and maybe create and open the last component.\n *\n * Must be called with parent locked (exclusive in O_CREAT case).\n *\n * Returns 0 on success, that is, if\n *  the file was successfully atomically created (if necessary) and opened, or\n *  the file was not completely opened at this time, though lookups and\n *  creations were performed.\n * These case are distinguished by presence of FMODE_OPENED on file->f_mode.\n * In the latter case dentry returned in @path might be negative if O_CREAT\n * hadn't been specified.\n *\n * An error code is returned on failure.\n */\nstatic struct dentry *lookup_open(struct nameidata *nd, struct file *file,\n\t\t\t\t  const struct open_flags *op,\n\t\t\t\t  bool got_write)\n{\n\tstruct mnt_idmap *idmap;\n\tstruct dentry *dir = nd->path.dentry;\n\tstruct inode *dir_inode = dir->d_inode;\n\tint open_flag = op->open_flag;\n\tstruct dentry *dentry;\n\tint error, create_error = 0;\n\tumode_t mode = op->mode;\n\tDECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);\n\n\tif (unlikely(IS_DEADDIR(dir_inode)))\n\t\treturn ERR_PTR(-ENOENT);\n\n\tfile->f_mode &= ~FMODE_CREATED;\n\tdentry = d_lookup(dir, &nd->last);\n\tfor (;;) {\n\t\tif (!dentry) {\n\t\t\tdentry = d_alloc_parallel(dir, &nd->last, &wq);\n\t\t\tif (IS_ERR(dentry))\n\t\t\t\treturn dentry;\n\t\t}\n\t\tif (d_in_lookup(dentry))\n\t\t\tbreak;\n\n\t\terror = d_revalidate(dir_inode, &nd->last, dentry, nd->flags);\n\t\tif (likely(error > 0))\n\t\t\tbreak;\n\t\tif (error)\n\t\t\tgoto out_dput;\n\t\td_invalidate(dentry);\n\t\tdput(dentry);\n\t\tdentry = NULL;\n\t}\n\tif (dentry->d_inode) {\n\t\t/* Cached positive dentry: will open in f_op->open */\n\t\treturn dentry;\n\t}\n\n\tif (open_flag & O_CREAT)\n\t\taudit_inode(nd->name, dir, AUDIT_INODE_PARENT);\n\n\t/*\n\t * Checking write permission is tricky, bacuse we don't know if we are\n\t * going to actually need it: O_CREAT opens should work as long as the\n\t * file exists.  But checking existence breaks atomicity.  The trick is\n\t * to check access and if not granted clear O_CREAT from the flags.\n\t *\n\t * Another problem is returing the \"right\" error value (e.g. for an\n\t * O_EXCL open we want to return EEXIST not EROFS).\n\t */\n\tif (unlikely(!got_write))\n\t\topen_flag &= ~O_TRUNC;\n\tidmap = mnt_idmap(nd->path.mnt);\n\tif (open_flag & O_CREAT) {\n\t\tif (open_flag & O_EXCL)\n\t\t\topen_flag &= ~O_TRUNC;\n\t\tmode = vfs_prepare_mode(idmap, dir->d_inode, mode, mode, mode);\n\t\tif (likely(got_write))\n\t\t\tcreate_error = may_o_create(idmap, &nd->path,\n\t\t\t\t\t\t    dentry, mode);\n\t\telse\n\t\t\tcreate_error = -EROFS;\n\t}\n\tif (create_error)\n\t\topen_flag &= ~O_CREAT;\n\tif (dir_inode->i_op->atomic_open) {\n\t\tdentry = atomic_open(nd, dentry, file, open_flag, mode);\n\t\tif (unlikely(create_error) && dentry == ERR_PTR(-ENOENT))\n\t\t\tdentry = ERR_PTR(create_error);\n\t\treturn dentry;\n\t}\n\n\tif (d_in_lookup(dentry)) {\n\t\tstruct dentry *res = dir_inode->i_op->lookup(dir_inode, dentry,\n\t\t\t\t\t\t\t     nd->flags);\n\t\td_lookup_done(dentry);\n\t\tif (unlikely(res)) {\n\t\t\tif (IS_ERR(res)) {\n\t\t\t\terror = PTR_ERR(res);\n\t\t\t\tgoto out_dput;\n\t\t\t}\n\t\t\tdput(dentry);\n\t\t\tdentry = res;\n\t\t}\n\t}\n\n\t/* Negative dentry, just create the file */\n\tif (!dentry->d_inode && (open_flag & O_CREAT)) {\n\t\tfile->f_mode |= FMODE_CREATED;\n\t\taudit_inode_child(dir_inode, dentry, AUDIT_TYPE_CHILD_CREATE);\n\t\tif (!dir_inode->i_op->create) {\n\t\t\terror = -EACCES;\n\t\t\tgoto out_dput;\n\t\t}\n\n\t\terror = dir_inode->i_op->create(idmap, dir_inode, dentry,\n\t\t\t\t\t\tmode, open_flag & O_EXCL);\n\t\tif (error)\n\t\t\tgoto out_dput;\n\t}\n\tif (unlikely(create_error) && !dentry->d_inode) {\n\t\terror = create_error;\n\t\tgoto out_dput;\n\t}\n\treturn dentry;\n\nout_dput:\n\tdput(dentry);\n\treturn ERR_PTR(error);\n}\n\nstatic inline bool trailing_slashes(struct nameidata *nd)\n{\n\treturn (bool)nd->last.name[nd->last.len];\n}\n\nstatic struct dentry *lookup_fast_for_open(struct nameidata *nd, int open_flag)\n{\n\tstruct dentry *dentry;\n\n\tif (open_flag & O_CREAT) {\n\t\tif (trailing_slashes(nd))\n\t\t\treturn ERR_PTR(-EISDIR);\n\n\t\t/* Don't bother on an O_EXCL create */\n\t\tif (open_flag & O_EXCL)\n\t\t\treturn NULL;\n\t}\n\n\tif (trailing_slashes(nd))\n\t\tnd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;\n\n\tdentry = lookup_fast(nd);\n\tif (IS_ERR_OR_NULL(dentry))\n\t\treturn dentry;\n\n\tif (open_flag & O_CREAT) {\n\t\t/* Discard negative dentries. Need inode_lock to do the create */\n\t\tif (!dentry->d_inode) {\n\t\t\tif (!(nd->flags & LOOKUP_RCU))\n\t\t\t\tdput(dentry);\n\t\t\tdentry = NULL;\n\t\t}\n\t}\n\treturn dentry;\n}\n\nstatic const char *open_last_lookups(struct nameidata *nd,\n\t\t   struct file *file, const struct open_flags *op)\n{\n\tstruct dentry *dir = nd->path.dentry;\n\tint open_flag = op->open_flag;\n\tbool got_write = false;\n\tstruct dentry *dentry;\n\tconst char *res;\n\n\tnd->flags |= op->intent;\n\n\tif (nd->last_type != LAST_NORM) {\n\t\tif (nd->depth)\n\t\t\tput_link(nd);\n\t\treturn handle_dots(nd, nd->last_type);\n\t}\n\n\t/* We _can_ be in RCU mode here */\n\tdentry = lookup_fast_for_open(nd, open_flag);\n\tif (IS_ERR(dentry))\n\t\treturn ERR_CAST(dentry);\n\n\tif (likely(dentry))\n\t\tgoto finish_lookup;\n\n\tif (!(open_flag & O_CREAT)) {\n\t\tif (WARN_ON_ONCE(nd->flags & LOOKUP_RCU))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t} else {\n\t\tif (nd->flags & LOOKUP_RCU) {\n\t\t\tif (!try_to_unlazy(nd))\n\t\t\t\treturn ERR_PTR(-ECHILD);\n\t\t}\n\t}\n\n\tif (open_flag & (O_CREAT | O_TRUNC | O_WRONLY | O_RDWR)) {\n\t\tgot_write = !mnt_want_write(nd->path.mnt);\n\t\t/*\n\t\t * do _not_ fail yet - we might not need that or fail with\n\t\t * a different error; let lookup_open() decide; we'll be\n\t\t * dropping this one anyway.\n\t\t */\n\t}\n\tif (open_flag & O_CREAT)\n\t\tinode_lock(dir->d_inode);\n\telse\n\t\tinode_lock_shared(dir->d_inode);\n\tdentry = lookup_open(nd, file, op, got_write);\n\tif (!IS_ERR(dentry)) {\n\t\tif (file->f_mode & FMODE_CREATED)\n\t\t\tfsnotify_create(dir->d_inode, dentry);\n\t\tif (file->f_mode & FMODE_OPENED)\n\t\t\tfsnotify_open(file);\n\t}\n\tif (open_flag & O_CREAT)\n\t\tinode_unlock(dir->d_inode);\n\telse\n\t\tinode_unlock_shared(dir->d_inode);\n\n\tif (got_write)\n\t\tmnt_drop_write(nd->path.mnt);\n\n\tif (IS_ERR(dentry))\n\t\treturn ERR_CAST(dentry);\n\n\tif (file->f_mode & (FMODE_OPENED | FMODE_CREATED)) {\n\t\tdput(nd->path.dentry);\n\t\tnd->path.dentry = dentry;\n\t\treturn NULL;\n\t}\n\nfinish_lookup:\n\tif (nd->depth)\n\t\tput_link(nd);\n\tres = step_into(nd, WALK_TRAILING, dentry);\n\tif (unlikely(res))\n\t\tnd->flags &= ~(LOOKUP_OPEN|LOOKUP_CREATE|LOOKUP_EXCL);\n\treturn res;\n}\n\n/*\n * Handle the last step of open()\n */\nstatic int do_open(struct nameidata *nd,\n\t\t   struct file *file, const struct open_flags *op)\n{\n\tstruct mnt_idmap *idmap;\n\tint open_flag = op->open_flag;\n\tbool do_truncate;\n\tint acc_mode;\n\tint error;\n\n\tif (!(file->f_mode & (FMODE_OPENED | FMODE_CREATED))) {\n\t\terror = complete_walk(nd);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\tif (!(file->f_mode & FMODE_CREATED))\n\t\taudit_inode(nd->name, nd->path.dentry, 0);\n\tidmap = mnt_idmap(nd->path.mnt);\n\tif (open_flag & O_CREAT) {\n\t\tif ((open_flag & O_EXCL) && !(file->f_mode & FMODE_CREATED))\n\t\t\treturn -EEXIST;\n\t\tif (d_is_dir(nd->path.dentry))\n\t\t\treturn -EISDIR;\n\t\terror = may_create_in_sticky(idmap, nd,\n\t\t\t\t\t     d_backing_inode(nd->path.dentry));\n\t\tif (unlikely(error))\n\t\t\treturn error;\n\t}\n\tif ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup(nd->path.dentry))\n\t\treturn -ENOTDIR;\n\n\tdo_truncate = false;\n\tacc_mode = op->acc_mode;\n\tif (file->f_mode & FMODE_CREATED) {\n\t\t/* Don't check for write permission, don't truncate */\n\t\topen_flag &= ~O_TRUNC;\n\t\tacc_mode = 0;\n\t} else if (d_is_reg(nd->path.dentry) && open_flag & O_TRUNC) {\n\t\terror = mnt_want_write(nd->path.mnt);\n\t\tif (error)\n\t\t\treturn error;\n\t\tdo_truncate = true;\n\t}\n\terror = may_open(idmap, &nd->path, acc_mode, open_flag);\n\tif (!error && !(file->f_mode & FMODE_OPENED))\n\t\terror = vfs_open(&nd->path, file);\n\tif (!error)\n\t\terror = security_file_post_open(file, op->acc_mode);\n\tif (!error && do_truncate)\n\t\terror = handle_truncate(idmap, file);\n\tif (unlikely(error > 0)) {\n\t\tWARN_ON(1);\n\t\terror = -EINVAL;\n\t}\n\tif (do_truncate)\n\t\tmnt_drop_write(nd->path.mnt);\n\treturn error;\n}\n\n/**\n * vfs_tmpfile - create tmpfile\n * @idmap:\tidmap of the mount the inode was found from\n * @parentpath:\tpointer to the path of the base directory\n * @file:\tfile descriptor of the new tmpfile\n * @mode:\tmode of the new tmpfile\n *\n * Create a temporary file.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint vfs_tmpfile(struct mnt_idmap *idmap,\n\t\tconst struct path *parentpath,\n\t\tstruct file *file, umode_t mode)\n{\n\tstruct dentry *child;\n\tstruct inode *dir = d_inode(parentpath->dentry);\n\tstruct inode *inode;\n\tint error;\n\tint open_flag = file->f_flags;\n\n\t/* we want directory to be writable */\n\terror = inode_permission(idmap, dir, MAY_WRITE | MAY_EXEC);\n\tif (error)\n\t\treturn error;\n\tif (!dir->i_op->tmpfile)\n\t\treturn -EOPNOTSUPP;\n\tchild = d_alloc(parentpath->dentry, &slash_name);\n\tif (unlikely(!child))\n\t\treturn -ENOMEM;\n\tfile->f_path.mnt = parentpath->mnt;\n\tfile->f_path.dentry = child;\n\tmode = vfs_prepare_mode(idmap, dir, mode, mode, mode);\n\terror = dir->i_op->tmpfile(idmap, dir, file, mode);\n\tdput(child);\n\tif (file->f_mode & FMODE_OPENED)\n\t\tfsnotify_open(file);\n\tif (error)\n\t\treturn error;\n\t/* Don't check for other permissions, the inode was just created */\n\terror = may_open(idmap, &file->f_path, 0, file->f_flags);\n\tif (error)\n\t\treturn error;\n\tinode = file_inode(file);\n\tif (!(open_flag & O_EXCL)) {\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_state |= I_LINKABLE;\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\tsecurity_inode_post_create_tmpfile(idmap, inode);\n\treturn 0;\n}\n\n/**\n * kernel_tmpfile_open - open a tmpfile for kernel internal use\n * @idmap:\tidmap of the mount the inode was found from\n * @parentpath:\tpath of the base directory\n * @mode:\tmode of the new tmpfile\n * @open_flag:\tflags\n * @cred:\tcredentials for open\n *\n * Create and open a temporary file.  The file is not accounted in nr_files,\n * hence this is only for kernel internal use, and must not be installed into\n * file tables or such.\n */\nstruct file *kernel_tmpfile_open(struct mnt_idmap *idmap,\n\t\t\t\t const struct path *parentpath,\n\t\t\t\t umode_t mode, int open_flag,\n\t\t\t\t const struct cred *cred)\n{\n\tstruct file *file;\n\tint error;\n\n\tfile = alloc_empty_file_noaccount(open_flag, cred);\n\tif (IS_ERR(file))\n\t\treturn file;\n\n\terror = vfs_tmpfile(idmap, parentpath, file, mode);\n\tif (error) {\n\t\tfput(file);\n\t\tfile = ERR_PTR(error);\n\t}\n\treturn file;\n}\nEXPORT_SYMBOL(kernel_tmpfile_open);\n\nstatic int do_tmpfile(struct nameidata *nd, unsigned flags,\n\t\tconst struct open_flags *op,\n\t\tstruct file *file)\n{\n\tstruct path path;\n\tint error = path_lookupat(nd, flags | LOOKUP_DIRECTORY, &path);\n\n\tif (unlikely(error))\n\t\treturn error;\n\terror = mnt_want_write(path.mnt);\n\tif (unlikely(error))\n\t\tgoto out;\n\terror = vfs_tmpfile(mnt_idmap(path.mnt), &path, file, op->mode);\n\tif (error)\n\t\tgoto out2;\n\taudit_inode(nd->name, file->f_path.dentry, 0);\nout2:\n\tmnt_drop_write(path.mnt);\nout:\n\tpath_put(&path);\n\treturn error;\n}\n\nstatic int do_o_path(struct nameidata *nd, unsigned flags, struct file *file)\n{\n\tstruct path path;\n\tint error = path_lookupat(nd, flags, &path);\n\tif (!error) {\n\t\taudit_inode(nd->name, path.dentry, 0);\n\t\terror = vfs_open(&path, file);\n\t\tpath_put(&path);\n\t}\n\treturn error;\n}\n\nstatic struct file *path_openat(struct nameidata *nd,\n\t\t\tconst struct open_flags *op, unsigned flags)\n{\n\tstruct file *file;\n\tint error;\n\n\tfile = alloc_empty_file(op->open_flag, current_cred());\n\tif (IS_ERR(file))\n\t\treturn file;\n\n\tif (unlikely(file->f_flags & __O_TMPFILE)) {\n\t\terror = do_tmpfile(nd, flags, op, file);\n\t} else if (unlikely(file->f_flags & O_PATH)) {\n\t\terror = do_o_path(nd, flags, file);\n\t} else {\n\t\tconst char *s = path_init(nd, flags);\n\t\twhile (!(error = link_path_walk(s, nd)) &&\n\t\t       (s = open_last_lookups(nd, file, op)) != NULL)\n\t\t\t;\n\t\tif (!error)\n\t\t\terror = do_open(nd, file, op);\n\t\tterminate_walk(nd);\n\t}\n\tif (likely(!error)) {\n\t\tif (likely(file->f_mode & FMODE_OPENED))\n\t\t\treturn file;\n\t\tWARN_ON(1);\n\t\terror = -EINVAL;\n\t}\n\tfput_close(file);\n\tif (error == -EOPENSTALE) {\n\t\tif (flags & LOOKUP_RCU)\n\t\t\terror = -ECHILD;\n\t\telse\n\t\t\terror = -ESTALE;\n\t}\n\treturn ERR_PTR(error);\n}\n\nstruct file *do_filp_open(int dfd, struct filename *pathname,\n\t\tconst struct open_flags *op)\n{\n\tstruct nameidata nd;\n\tint flags = op->lookup_flags;\n\tstruct file *filp;\n\n\tset_nameidata(&nd, dfd, pathname, NULL);\n\tfilp = path_openat(&nd, op, flags | LOOKUP_RCU);\n\tif (unlikely(filp == ERR_PTR(-ECHILD)))\n\t\tfilp = path_openat(&nd, op, flags);\n\tif (unlikely(filp == ERR_PTR(-ESTALE)))\n\t\tfilp = path_openat(&nd, op, flags | LOOKUP_REVAL);\n\trestore_nameidata();\n\treturn filp;\n}\n\nstruct file *do_file_open_root(const struct path *root,\n\t\tconst char *name, const struct open_flags *op)\n{\n\tstruct nameidata nd;\n\tstruct file *file;\n\tstruct filename *filename;\n\tint flags = op->lookup_flags;\n\n\tif (d_is_symlink(root->dentry) && op->intent & LOOKUP_OPEN)\n\t\treturn ERR_PTR(-ELOOP);\n\n\tfilename = getname_kernel(name);\n\tif (IS_ERR(filename))\n\t\treturn ERR_CAST(filename);\n\n\tset_nameidata(&nd, -1, filename, root);\n\tfile = path_openat(&nd, op, flags | LOOKUP_RCU);\n\tif (unlikely(file == ERR_PTR(-ECHILD)))\n\t\tfile = path_openat(&nd, op, flags);\n\tif (unlikely(file == ERR_PTR(-ESTALE)))\n\t\tfile = path_openat(&nd, op, flags | LOOKUP_REVAL);\n\trestore_nameidata();\n\tputname(filename);\n\treturn file;\n}\n\nstatic struct dentry *filename_create(int dfd, struct filename *name,\n\t\t\t\t      struct path *path, unsigned int lookup_flags)\n{\n\tstruct dentry *dentry = ERR_PTR(-EEXIST);\n\tstruct qstr last;\n\tbool want_dir = lookup_flags & LOOKUP_DIRECTORY;\n\tunsigned int reval_flag = lookup_flags & LOOKUP_REVAL;\n\tunsigned int create_flags = LOOKUP_CREATE | LOOKUP_EXCL;\n\tint type;\n\tint error;\n\n\terror = filename_parentat(dfd, name, reval_flag, path, &last, &type);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\t/*\n\t * Yucky last component or no last component at all?\n\t * (foo/., foo/.., /////)\n\t */\n\tif (unlikely(type != LAST_NORM))\n\t\tgoto out;\n\n\t/* don't fail immediately if it's r/o, at least try to report other errors */\n\terror = mnt_want_write(path->mnt);\n\t/*\n\t * Do the final lookup.  Suppress 'create' if there is a trailing\n\t * '/', and a directory wasn't requested.\n\t */\n\tif (last.name[last.len] && !want_dir)\n\t\tcreate_flags &= ~LOOKUP_CREATE;\n\tinode_lock_nested(path->dentry->d_inode, I_MUTEX_PARENT);\n\tdentry = lookup_one_qstr_excl(&last, path->dentry,\n\t\t\t\t      reval_flag | create_flags);\n\tif (IS_ERR(dentry))\n\t\tgoto unlock;\n\n\tif (unlikely(error))\n\t\tgoto fail;\n\n\treturn dentry;\nfail:\n\tdput(dentry);\n\tdentry = ERR_PTR(error);\nunlock:\n\tinode_unlock(path->dentry->d_inode);\n\tif (!error)\n\t\tmnt_drop_write(path->mnt);\nout:\n\tpath_put(path);\n\treturn dentry;\n}\n\nstruct dentry *start_creating_path(int dfd, const char *pathname,\n\t\t\t\t   struct path *path, unsigned int lookup_flags)\n{\n\tstruct filename *filename = getname_kernel(pathname);\n\tstruct dentry *res = filename_create(dfd, filename, path, lookup_flags);\n\n\tputname(filename);\n\treturn res;\n}\nEXPORT_SYMBOL(start_creating_path);\n\nvoid end_creating_path(struct path *path, struct dentry *dentry)\n{\n\tif (!IS_ERR(dentry))\n\t\tdput(dentry);\n\tinode_unlock(path->dentry->d_inode);\n\tmnt_drop_write(path->mnt);\n\tpath_put(path);\n}\nEXPORT_SYMBOL(end_creating_path);\n\ninline struct dentry *start_creating_user_path(\n\tint dfd, const char __user *pathname,\n\tstruct path *path, unsigned int lookup_flags)\n{\n\tstruct filename *filename = getname(pathname);\n\tstruct dentry *res = filename_create(dfd, filename, path, lookup_flags);\n\n\tputname(filename);\n\treturn res;\n}\nEXPORT_SYMBOL(start_creating_user_path);\n\n/**\n * vfs_mknod - create device node or file\n * @idmap:\tidmap of the mount the inode was found from\n * @dir:\tinode of the parent directory\n * @dentry:\tdentry of the child device node\n * @mode:\tmode of the child device node\n * @dev:\tdevice number of device to create\n *\n * Create a device node or file.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint vfs_mknod(struct mnt_idmap *idmap, struct inode *dir,\n\t      struct dentry *dentry, umode_t mode, dev_t dev)\n{\n\tbool is_whiteout = S_ISCHR(mode) && dev == WHITEOUT_DEV;\n\tint error = may_create(idmap, dir, dentry);\n\n\tif (error)\n\t\treturn error;\n\n\tif ((S_ISCHR(mode) || S_ISBLK(mode)) && !is_whiteout &&\n\t    !capable(CAP_MKNOD))\n\t\treturn -EPERM;\n\n\tif (!dir->i_op->mknod)\n\t\treturn -EPERM;\n\n\tmode = vfs_prepare_mode(idmap, dir, mode, mode, mode);\n\terror = devcgroup_inode_mknod(mode, dev);\n\tif (error)\n\t\treturn error;\n\n\terror = security_inode_mknod(dir, dentry, mode, dev);\n\tif (error)\n\t\treturn error;\n\n\terror = dir->i_op->mknod(idmap, dir, dentry, mode, dev);\n\tif (!error)\n\t\tfsnotify_create(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_mknod);\n\nstatic int may_mknod(umode_t mode)\n{\n\tswitch (mode & S_IFMT) {\n\tcase S_IFREG:\n\tcase S_IFCHR:\n\tcase S_IFBLK:\n\tcase S_IFIFO:\n\tcase S_IFSOCK:\n\tcase 0: /* zero mode translates to S_IFREG */\n\t\treturn 0;\n\tcase S_IFDIR:\n\t\treturn -EPERM;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int do_mknodat(int dfd, struct filename *name, umode_t mode,\n\t\tunsigned int dev)\n{\n\tstruct mnt_idmap *idmap;\n\tstruct dentry *dentry;\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = 0;\n\n\terror = may_mknod(mode);\n\tif (error)\n\t\tgoto out1;\nretry:\n\tdentry = filename_create(dfd, name, &path, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out1;\n\n\terror = security_path_mknod(&path, dentry,\n\t\t\tmode_strip_umask(path.dentry->d_inode, mode), dev);\n\tif (error)\n\t\tgoto out2;\n\n\tidmap = mnt_idmap(path.mnt);\n\tswitch (mode & S_IFMT) {\n\t\tcase 0: case S_IFREG:\n\t\t\terror = vfs_create(idmap, path.dentry->d_inode,\n\t\t\t\t\t   dentry, mode, true);\n\t\t\tif (!error)\n\t\t\t\tsecurity_path_post_mknod(idmap, dentry);\n\t\t\tbreak;\n\t\tcase S_IFCHR: case S_IFBLK:\n\t\t\terror = vfs_mknod(idmap, path.dentry->d_inode,\n\t\t\t\t\t  dentry, mode, new_decode_dev(dev));\n\t\t\tbreak;\n\t\tcase S_IFIFO: case S_IFSOCK:\n\t\t\terror = vfs_mknod(idmap, path.dentry->d_inode,\n\t\t\t\t\t  dentry, mode, 0);\n\t\t\tbreak;\n\t}\nout2:\n\tend_creating_path(&path, dentry);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout1:\n\tputname(name);\n\treturn error;\n}\n\nSYSCALL_DEFINE4(mknodat, int, dfd, const char __user *, filename, umode_t, mode,\n\t\tunsigned int, dev)\n{\n\treturn do_mknodat(dfd, getname(filename), mode, dev);\n}\n\nSYSCALL_DEFINE3(mknod, const char __user *, filename, umode_t, mode, unsigned, dev)\n{\n\treturn do_mknodat(AT_FDCWD, getname(filename), mode, dev);\n}\n\n/**\n * vfs_mkdir - create directory returning correct dentry if possible\n * @idmap:\tidmap of the mount the inode was found from\n * @dir:\tinode of the parent directory\n * @dentry:\tdentry of the child directory\n * @mode:\tmode of the child directory\n *\n * Create a directory.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n *\n * In the event that the filesystem does not use the *@dentry but leaves it\n * negative or unhashes it and possibly splices a different one returning it,\n * the original dentry is dput() and the alternate is returned.\n *\n * In case of an error the dentry is dput() and an ERR_PTR() is returned.\n */\nstruct dentry *vfs_mkdir(struct mnt_idmap *idmap, struct inode *dir,\n\t\t\t struct dentry *dentry, umode_t mode)\n{\n\tint error;\n\tunsigned max_links = dir->i_sb->s_max_links;\n\tstruct dentry *de;\n\n\terror = may_create(idmap, dir, dentry);\n\tif (error)\n\t\tgoto err;\n\n\terror = -EPERM;\n\tif (!dir->i_op->mkdir)\n\t\tgoto err;\n\n\tmode = vfs_prepare_mode(idmap, dir, mode, S_IRWXUGO | S_ISVTX, 0);\n\terror = security_inode_mkdir(dir, dentry, mode);\n\tif (error)\n\t\tgoto err;\n\n\terror = -EMLINK;\n\tif (max_links && dir->i_nlink >= max_links)\n\t\tgoto err;\n\n\tde = dir->i_op->mkdir(idmap, dir, dentry, mode);\n\terror = PTR_ERR(de);\n\tif (IS_ERR(de))\n\t\tgoto err;\n\tif (de) {\n\t\tdput(dentry);\n\t\tdentry = de;\n\t}\n\tfsnotify_mkdir(dir, dentry);\n\treturn dentry;\n\nerr:\n\tdput(dentry);\n\treturn ERR_PTR(error);\n}\nEXPORT_SYMBOL(vfs_mkdir);\n\nint do_mkdirat(int dfd, struct filename *name, umode_t mode)\n{\n\tstruct dentry *dentry;\n\tstruct path path;\n\tint error;\n\tunsigned int lookup_flags = LOOKUP_DIRECTORY;\n\nretry:\n\tdentry = filename_create(dfd, name, &path, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out_putname;\n\n\terror = security_path_mkdir(&path, dentry,\n\t\t\tmode_strip_umask(path.dentry->d_inode, mode));\n\tif (!error) {\n\t\tdentry = vfs_mkdir(mnt_idmap(path.mnt), path.dentry->d_inode,\n\t\t\t\t  dentry, mode);\n\t\tif (IS_ERR(dentry))\n\t\t\terror = PTR_ERR(dentry);\n\t}\n\tend_creating_path(&path, dentry);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout_putname:\n\tputname(name);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(mkdirat, int, dfd, const char __user *, pathname, umode_t, mode)\n{\n\treturn do_mkdirat(dfd, getname(pathname), mode);\n}\n\nSYSCALL_DEFINE2(mkdir, const char __user *, pathname, umode_t, mode)\n{\n\treturn do_mkdirat(AT_FDCWD, getname(pathname), mode);\n}\n\n/**\n * vfs_rmdir - remove directory\n * @idmap:\tidmap of the mount the inode was found from\n * @dir:\tinode of the parent directory\n * @dentry:\tdentry of the child directory\n *\n * Remove a directory.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint vfs_rmdir(struct mnt_idmap *idmap, struct inode *dir,\n\t\t     struct dentry *dentry)\n{\n\tint error = may_delete(idmap, dir, dentry, 1);\n\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->rmdir)\n\t\treturn -EPERM;\n\n\tdget(dentry);\n\tinode_lock(dentry->d_inode);\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(dentry) ||\n\t    (dentry->d_inode->i_flags & S_KERNEL_FILE))\n\t\tgoto out;\n\n\terror = security_inode_rmdir(dir, dentry);\n\tif (error)\n\t\tgoto out;\n\n\terror = dir->i_op->rmdir(dir, dentry);\n\tif (error)\n\t\tgoto out;\n\n\tshrink_dcache_parent(dentry);\n\tdentry->d_inode->i_flags |= S_DEAD;\n\tdont_mount(dentry);\n\tdetach_mounts(dentry);\n\nout:\n\tinode_unlock(dentry->d_inode);\n\tdput(dentry);\n\tif (!error)\n\t\td_delete_notify(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_rmdir);\n\nint do_rmdir(int dfd, struct filename *name)\n{\n\tint error;\n\tstruct dentry *dentry;\n\tstruct path path;\n\tstruct qstr last;\n\tint type;\n\tunsigned int lookup_flags = 0;\nretry:\n\terror = filename_parentat(dfd, name, lookup_flags, &path, &last, &type);\n\tif (error)\n\t\tgoto exit1;\n\n\tswitch (type) {\n\tcase LAST_DOTDOT:\n\t\terror = -ENOTEMPTY;\n\t\tgoto exit2;\n\tcase LAST_DOT:\n\t\terror = -EINVAL;\n\t\tgoto exit2;\n\tcase LAST_ROOT:\n\t\terror = -EBUSY;\n\t\tgoto exit2;\n\t}\n\n\terror = mnt_want_write(path.mnt);\n\tif (error)\n\t\tgoto exit2;\n\n\tinode_lock_nested(path.dentry->d_inode, I_MUTEX_PARENT);\n\tdentry = lookup_one_qstr_excl(&last, path.dentry, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto exit3;\n\terror = security_path_rmdir(&path, dentry);\n\tif (error)\n\t\tgoto exit4;\n\terror = vfs_rmdir(mnt_idmap(path.mnt), path.dentry->d_inode, dentry);\nexit4:\n\tdput(dentry);\nexit3:\n\tinode_unlock(path.dentry->d_inode);\n\tmnt_drop_write(path.mnt);\nexit2:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nexit1:\n\tputname(name);\n\treturn error;\n}\n\nSYSCALL_DEFINE1(rmdir, const char __user *, pathname)\n{\n\treturn do_rmdir(AT_FDCWD, getname(pathname));\n}\n\n/**\n * vfs_unlink - unlink a filesystem object\n * @idmap:\tidmap of the mount the inode was found from\n * @dir:\tparent directory\n * @dentry:\tvictim\n * @delegated_inode: returns victim inode, if the inode is delegated.\n *\n * The caller must hold dir->i_rwsem exclusively.\n *\n * If vfs_unlink discovers a delegation, it will return -EWOULDBLOCK and\n * return a reference to the inode in delegated_inode.  The caller\n * should then break the delegation on that inode and retry.  Because\n * breaking a delegation may take a long time, the caller should drop\n * dir->i_rwsem before doing so.\n *\n * Alternatively, a caller may pass NULL for delegated_inode.  This may\n * be appropriate for callers that expect the underlying filesystem not\n * to be NFS exported.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint vfs_unlink(struct mnt_idmap *idmap, struct inode *dir,\n\t       struct dentry *dentry, struct inode **delegated_inode)\n{\n\tstruct inode *target = dentry->d_inode;\n\tint error = may_delete(idmap, dir, dentry, 0);\n\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->unlink)\n\t\treturn -EPERM;\n\n\tinode_lock(target);\n\tif (IS_SWAPFILE(target))\n\t\terror = -EPERM;\n\telse if (is_local_mountpoint(dentry))\n\t\terror = -EBUSY;\n\telse {\n\t\terror = security_inode_unlink(dir, dentry);\n\t\tif (!error) {\n\t\t\terror = try_break_deleg(target, delegated_inode);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t\terror = dir->i_op->unlink(dir, dentry);\n\t\t\tif (!error) {\n\t\t\t\tdont_mount(dentry);\n\t\t\t\tdetach_mounts(dentry);\n\t\t\t}\n\t\t}\n\t}\nout:\n\tinode_unlock(target);\n\n\t/* We don't d_delete() NFS sillyrenamed files--they still exist. */\n\tif (!error && dentry->d_flags & DCACHE_NFSFS_RENAMED) {\n\t\tfsnotify_unlink(dir, dentry);\n\t} else if (!error) {\n\t\tfsnotify_link_count(target);\n\t\td_delete_notify(dir, dentry);\n\t}\n\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_unlink);\n\n/*\n * Make sure that the actual truncation of the file will occur outside its\n * directory's i_rwsem.  Truncate can take a long time if there is a lot of\n * writeout happening, and we don't want to prevent access to the directory\n * while waiting on the I/O.\n */\nint do_unlinkat(int dfd, struct filename *name)\n{\n\tint error;\n\tstruct dentry *dentry;\n\tstruct path path;\n\tstruct qstr last;\n\tint type;\n\tstruct inode *inode = NULL;\n\tstruct inode *delegated_inode = NULL;\n\tunsigned int lookup_flags = 0;\nretry:\n\terror = filename_parentat(dfd, name, lookup_flags, &path, &last, &type);\n\tif (error)\n\t\tgoto exit1;\n\n\terror = -EISDIR;\n\tif (type != LAST_NORM)\n\t\tgoto exit2;\n\n\terror = mnt_want_write(path.mnt);\n\tif (error)\n\t\tgoto exit2;\nretry_deleg:\n\tinode_lock_nested(path.dentry->d_inode, I_MUTEX_PARENT);\n\tdentry = lookup_one_qstr_excl(&last, path.dentry, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (!IS_ERR(dentry)) {\n\n\t\t/* Why not before? Because we want correct error value */\n\t\tif (last.name[last.len])\n\t\t\tgoto slashes;\n\t\tinode = dentry->d_inode;\n\t\tihold(inode);\n\t\terror = security_path_unlink(&path, dentry);\n\t\tif (error)\n\t\t\tgoto exit3;\n\t\terror = vfs_unlink(mnt_idmap(path.mnt), path.dentry->d_inode,\n\t\t\t\t   dentry, &delegated_inode);\nexit3:\n\t\tdput(dentry);\n\t}\n\tinode_unlock(path.dentry->d_inode);\n\tif (inode)\n\t\tiput(inode);\t/* truncate the inode here */\n\tinode = NULL;\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\tmnt_drop_write(path.mnt);\nexit2:\n\tpath_put(&path);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tinode = NULL;\n\t\tgoto retry;\n\t}\nexit1:\n\tputname(name);\n\treturn error;\n\nslashes:\n\tif (d_is_dir(dentry))\n\t\terror = -EISDIR;\n\telse\n\t\terror = -ENOTDIR;\n\tgoto exit3;\n}\n\nSYSCALL_DEFINE3(unlinkat, int, dfd, const char __user *, pathname, int, flag)\n{\n\tif ((flag & ~AT_REMOVEDIR) != 0)\n\t\treturn -EINVAL;\n\n\tif (flag & AT_REMOVEDIR)\n\t\treturn do_rmdir(dfd, getname(pathname));\n\treturn do_unlinkat(dfd, getname(pathname));\n}\n\nSYSCALL_DEFINE1(unlink, const char __user *, pathname)\n{\n\treturn do_unlinkat(AT_FDCWD, getname(pathname));\n}\n\n/**\n * vfs_symlink - create symlink\n * @idmap:\tidmap of the mount the inode was found from\n * @dir:\tinode of the parent directory\n * @dentry:\tdentry of the child symlink file\n * @oldname:\tname of the file to link to\n *\n * Create a symlink.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint vfs_symlink(struct mnt_idmap *idmap, struct inode *dir,\n\t\tstruct dentry *dentry, const char *oldname)\n{\n\tint error;\n\n\terror = may_create(idmap, dir, dentry);\n\tif (error)\n\t\treturn error;\n\n\tif (!dir->i_op->symlink)\n\t\treturn -EPERM;\n\n\terror = security_inode_symlink(dir, dentry, oldname);\n\tif (error)\n\t\treturn error;\n\n\terror = dir->i_op->symlink(idmap, dir, dentry, oldname);\n\tif (!error)\n\t\tfsnotify_create(dir, dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_symlink);\n\nint do_symlinkat(struct filename *from, int newdfd, struct filename *to)\n{\n\tint error;\n\tstruct dentry *dentry;\n\tstruct path path;\n\tunsigned int lookup_flags = 0;\n\n\tif (IS_ERR(from)) {\n\t\terror = PTR_ERR(from);\n\t\tgoto out_putnames;\n\t}\nretry:\n\tdentry = filename_create(newdfd, to, &path, lookup_flags);\n\terror = PTR_ERR(dentry);\n\tif (IS_ERR(dentry))\n\t\tgoto out_putnames;\n\n\terror = security_path_symlink(&path, dentry, from->name);\n\tif (!error)\n\t\terror = vfs_symlink(mnt_idmap(path.mnt), path.dentry->d_inode,\n\t\t\t\t    dentry, from->name);\n\tend_creating_path(&path, dentry);\n\tif (retry_estale(error, lookup_flags)) {\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout_putnames:\n\tputname(to);\n\tputname(from);\n\treturn error;\n}\n\nSYSCALL_DEFINE3(symlinkat, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname)\n{\n\treturn do_symlinkat(getname(oldname), newdfd, getname(newname));\n}\n\nSYSCALL_DEFINE2(symlink, const char __user *, oldname, const char __user *, newname)\n{\n\treturn do_symlinkat(getname(oldname), AT_FDCWD, getname(newname));\n}\n\n/**\n * vfs_link - create a new link\n * @old_dentry:\tobject to be linked\n * @idmap:\tidmap of the mount\n * @dir:\tnew parent\n * @new_dentry:\twhere to create the new link\n * @delegated_inode: returns inode needing a delegation break\n *\n * The caller must hold dir->i_rwsem exclusively.\n *\n * If vfs_link discovers a delegation on the to-be-linked file in need\n * of breaking, it will return -EWOULDBLOCK and return a reference to the\n * inode in delegated_inode.  The caller should then break the delegation\n * and retry.  Because breaking a delegation may take a long time, the\n * caller should drop the i_rwsem before doing so.\n *\n * Alternatively, a caller may pass NULL for delegated_inode.  This may\n * be appropriate for callers that expect the underlying filesystem not\n * to be NFS exported.\n *\n * If the inode has been found through an idmapped mount the idmap of\n * the vfsmount must be passed through @idmap. This function will then take\n * care to map the inode according to @idmap before checking permissions.\n * On non-idmapped mounts or if permission checking is to be performed on the\n * raw inode simply pass @nop_mnt_idmap.\n */\nint vfs_link(struct dentry *old_dentry, struct mnt_idmap *idmap,\n\t     struct inode *dir, struct dentry *new_dentry,\n\t     struct inode **delegated_inode)\n{\n\tstruct inode *inode = old_dentry->d_inode;\n\tunsigned max_links = dir->i_sb->s_max_links;\n\tint error;\n\n\tif (!inode)\n\t\treturn -ENOENT;\n\n\terror = may_create(idmap, dir, new_dentry);\n\tif (error)\n\t\treturn error;\n\n\tif (dir->i_sb != inode->i_sb)\n\t\treturn -EXDEV;\n\n\t/*\n\t * A link to an append-only or immutable file cannot be created.\n\t */\n\tif (IS_APPEND(inode) || IS_IMMUTABLE(inode))\n\t\treturn -EPERM;\n\t/*\n\t * Updating the link count will likely cause i_uid and i_gid to\n\t * be written back improperly if their true value is unknown to\n\t * the vfs.\n\t */\n\tif (HAS_UNMAPPED_ID(idmap, inode))\n\t\treturn -EPERM;\n\tif (!dir->i_op->link)\n\t\treturn -EPERM;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn -EPERM;\n\n\terror = security_inode_link(old_dentry, dir, new_dentry);\n\tif (error)\n\t\treturn error;\n\n\tinode_lock(inode);\n\t/* Make sure we don't allow creating hardlink to an unlinked file */\n\tif (inode->i_nlink == 0 && !(inode->i_state & I_LINKABLE))\n\t\terror =  -ENOENT;\n\telse if (max_links && inode->i_nlink >= max_links)\n\t\terror = -EMLINK;\n\telse {\n\t\terror = try_break_deleg(inode, delegated_inode);\n\t\tif (!error)\n\t\t\terror = dir->i_op->link(old_dentry, dir, new_dentry);\n\t}\n\n\tif (!error && (inode->i_state & I_LINKABLE)) {\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_state &= ~I_LINKABLE;\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\tinode_unlock(inode);\n\tif (!error)\n\t\tfsnotify_link(dir, inode, new_dentry);\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_link);\n\n/*\n * Hardlinks are often used in delicate situations.  We avoid\n * security-related surprises by not following symlinks on the\n * newname.  --KAB\n *\n * We don't follow them on the oldname either to be compatible\n * with linux 2.0, and to avoid hard-linking to directories\n * and other special files.  --ADM\n */\nint do_linkat(int olddfd, struct filename *old, int newdfd,\n\t      struct filename *new, int flags)\n{\n\tstruct mnt_idmap *idmap;\n\tstruct dentry *new_dentry;\n\tstruct path old_path, new_path;\n\tstruct inode *delegated_inode = NULL;\n\tint how = 0;\n\tint error;\n\n\tif ((flags & ~(AT_SYMLINK_FOLLOW | AT_EMPTY_PATH)) != 0) {\n\t\terror = -EINVAL;\n\t\tgoto out_putnames;\n\t}\n\t/*\n\t * To use null names we require CAP_DAC_READ_SEARCH or\n\t * that the open-time creds of the dfd matches current.\n\t * This ensures that not everyone will be able to create\n\t * a hardlink using the passed file descriptor.\n\t */\n\tif (flags & AT_EMPTY_PATH)\n\t\thow |= LOOKUP_LINKAT_EMPTY;\n\n\tif (flags & AT_SYMLINK_FOLLOW)\n\t\thow |= LOOKUP_FOLLOW;\nretry:\n\terror = filename_lookup(olddfd, old, how, &old_path, NULL);\n\tif (error)\n\t\tgoto out_putnames;\n\n\tnew_dentry = filename_create(newdfd, new, &new_path,\n\t\t\t\t\t(how & LOOKUP_REVAL));\n\terror = PTR_ERR(new_dentry);\n\tif (IS_ERR(new_dentry))\n\t\tgoto out_putpath;\n\n\terror = -EXDEV;\n\tif (old_path.mnt != new_path.mnt)\n\t\tgoto out_dput;\n\tidmap = mnt_idmap(new_path.mnt);\n\terror = may_linkat(idmap, &old_path);\n\tif (unlikely(error))\n\t\tgoto out_dput;\n\terror = security_path_link(old_path.dentry, &new_path, new_dentry);\n\tif (error)\n\t\tgoto out_dput;\n\terror = vfs_link(old_path.dentry, idmap, new_path.dentry->d_inode,\n\t\t\t new_dentry, &delegated_inode);\nout_dput:\n\tend_creating_path(&new_path, new_dentry);\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error) {\n\t\t\tpath_put(&old_path);\n\t\t\tgoto retry;\n\t\t}\n\t}\n\tif (retry_estale(error, how)) {\n\t\tpath_put(&old_path);\n\t\thow |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nout_putpath:\n\tpath_put(&old_path);\nout_putnames:\n\tputname(old);\n\tputname(new);\n\n\treturn error;\n}\n\nSYSCALL_DEFINE5(linkat, int, olddfd, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname, int, flags)\n{\n\treturn do_linkat(olddfd, getname_uflags(oldname, flags),\n\t\tnewdfd, getname(newname), flags);\n}\n\nSYSCALL_DEFINE2(link, const char __user *, oldname, const char __user *, newname)\n{\n\treturn do_linkat(AT_FDCWD, getname(oldname), AT_FDCWD, getname(newname), 0);\n}\n\n/**\n * vfs_rename - rename a filesystem object\n * @rd:\t\tpointer to &struct renamedata info\n *\n * The caller must hold multiple mutexes--see lock_rename()).\n *\n * If vfs_rename discovers a delegation in need of breaking at either\n * the source or destination, it will return -EWOULDBLOCK and return a\n * reference to the inode in delegated_inode.  The caller should then\n * break the delegation and retry.  Because breaking a delegation may\n * take a long time, the caller should drop all locks before doing\n * so.\n *\n * Alternatively, a caller may pass NULL for delegated_inode.  This may\n * be appropriate for callers that expect the underlying filesystem not\n * to be NFS exported.\n *\n * The worst of all namespace operations - renaming directory. \"Perverted\"\n * doesn't even start to describe it. Somebody in UCB had a heck of a trip...\n * Problems:\n *\n *\ta) we can get into loop creation.\n *\tb) race potential - two innocent renames can create a loop together.\n *\t   That's where 4.4BSD screws up. Current fix: serialization on\n *\t   sb->s_vfs_rename_mutex. We might be more accurate, but that's another\n *\t   story.\n *\tc) we may have to lock up to _four_ objects - parents and victim (if it exists),\n *\t   and source (if it's a non-directory or a subdirectory that moves to\n *\t   different parent).\n *\t   And that - after we got ->i_rwsem on parents (until then we don't know\n *\t   whether the target exists).  Solution: try to be smart with locking\n *\t   order for inodes.  We rely on the fact that tree topology may change\n *\t   only under ->s_vfs_rename_mutex _and_ that parent of the object we\n *\t   move will be locked.  Thus we can rank directories by the tree\n *\t   (ancestors first) and rank all non-directories after them.\n *\t   That works since everybody except rename does \"lock parent, lookup,\n *\t   lock child\" and rename is under ->s_vfs_rename_mutex.\n *\t   HOWEVER, it relies on the assumption that any object with ->lookup()\n *\t   has no more than 1 dentry.  If \"hybrid\" objects will ever appear,\n *\t   we'd better make sure that there's no link(2) for them.\n *\td) conversion from fhandle to dentry may come in the wrong moment - when\n *\t   we are removing the target. Solution: we will have to grab ->i_rwsem\n *\t   in the fhandle_to_dentry code. [FIXME - current nfsfh.c relies on\n *\t   ->i_rwsem on parents, which works but leads to some truly excessive\n *\t   locking].\n */\nint vfs_rename(struct renamedata *rd)\n{\n\tint error;\n\tstruct inode *old_dir = d_inode(rd->old_parent);\n\tstruct inode *new_dir = d_inode(rd->new_parent);\n\tstruct dentry *old_dentry = rd->old_dentry;\n\tstruct dentry *new_dentry = rd->new_dentry;\n\tstruct inode **delegated_inode = rd->delegated_inode;\n\tunsigned int flags = rd->flags;\n\tbool is_dir = d_is_dir(old_dentry);\n\tstruct inode *source = old_dentry->d_inode;\n\tstruct inode *target = new_dentry->d_inode;\n\tbool new_is_dir = false;\n\tunsigned max_links = new_dir->i_sb->s_max_links;\n\tstruct name_snapshot old_name;\n\tbool lock_old_subdir, lock_new_subdir;\n\n\tif (source == target)\n\t\treturn 0;\n\n\terror = may_delete(rd->mnt_idmap, old_dir, old_dentry, is_dir);\n\tif (error)\n\t\treturn error;\n\n\tif (!target) {\n\t\terror = may_create(rd->mnt_idmap, new_dir, new_dentry);\n\t} else {\n\t\tnew_is_dir = d_is_dir(new_dentry);\n\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\terror = may_delete(rd->mnt_idmap, new_dir,\n\t\t\t\t\t   new_dentry, is_dir);\n\t\telse\n\t\t\terror = may_delete(rd->mnt_idmap, new_dir,\n\t\t\t\t\t   new_dentry, new_is_dir);\n\t}\n\tif (error)\n\t\treturn error;\n\n\tif (!old_dir->i_op->rename)\n\t\treturn -EPERM;\n\n\t/*\n\t * If we are going to change the parent - check write permissions,\n\t * we'll need to flip '..'.\n\t */\n\tif (new_dir != old_dir) {\n\t\tif (is_dir) {\n\t\t\terror = inode_permission(rd->mnt_idmap, source,\n\t\t\t\t\t\t MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t\tif ((flags & RENAME_EXCHANGE) && new_is_dir) {\n\t\t\terror = inode_permission(rd->mnt_idmap, target,\n\t\t\t\t\t\t MAY_WRITE);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\t}\n\n\terror = security_inode_rename(old_dir, old_dentry, new_dir, new_dentry,\n\t\t\t\t      flags);\n\tif (error)\n\t\treturn error;\n\n\ttake_dentry_name_snapshot(&old_name, old_dentry);\n\tdget(new_dentry);\n\t/*\n\t * Lock children.\n\t * The source subdirectory needs to be locked on cross-directory\n\t * rename or cross-directory exchange since its parent changes.\n\t * The target subdirectory needs to be locked on cross-directory\n\t * exchange due to parent change and on any rename due to becoming\n\t * a victim.\n\t * Non-directories need locking in all cases (for NFS reasons);\n\t * they get locked after any subdirectories (in inode address order).\n\t *\n\t * NOTE: WE ONLY LOCK UNRELATED DIRECTORIES IN CROSS-DIRECTORY CASE.\n\t * NEVER, EVER DO THAT WITHOUT ->s_vfs_rename_mutex.\n\t */\n\tlock_old_subdir = new_dir != old_dir;\n\tlock_new_subdir = new_dir != old_dir || !(flags & RENAME_EXCHANGE);\n\tif (is_dir) {\n\t\tif (lock_old_subdir)\n\t\t\tinode_lock_nested(source, I_MUTEX_CHILD);\n\t\tif (target && (!new_is_dir || lock_new_subdir))\n\t\t\tinode_lock(target);\n\t} else if (new_is_dir) {\n\t\tif (lock_new_subdir)\n\t\t\tinode_lock_nested(target, I_MUTEX_CHILD);\n\t\tinode_lock(source);\n\t} else {\n\t\tlock_two_nondirectories(source, target);\n\t}\n\n\terror = -EPERM;\n\tif (IS_SWAPFILE(source) || (target && IS_SWAPFILE(target)))\n\t\tgoto out;\n\n\terror = -EBUSY;\n\tif (is_local_mountpoint(old_dentry) || is_local_mountpoint(new_dentry))\n\t\tgoto out;\n\n\tif (max_links && new_dir != old_dir) {\n\t\terror = -EMLINK;\n\t\tif (is_dir && !new_is_dir && new_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t\tif ((flags & RENAME_EXCHANGE) && !is_dir && new_is_dir &&\n\t\t    old_dir->i_nlink >= max_links)\n\t\t\tgoto out;\n\t}\n\tif (!is_dir) {\n\t\terror = try_break_deleg(source, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\tif (target && !new_is_dir) {\n\t\terror = try_break_deleg(target, delegated_inode);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\terror = old_dir->i_op->rename(rd->mnt_idmap, old_dir, old_dentry,\n\t\t\t\t      new_dir, new_dentry, flags);\n\tif (error)\n\t\tgoto out;\n\n\tif (!(flags & RENAME_EXCHANGE) && target) {\n\t\tif (is_dir) {\n\t\t\tshrink_dcache_parent(new_dentry);\n\t\t\ttarget->i_flags |= S_DEAD;\n\t\t}\n\t\tdont_mount(new_dentry);\n\t\tdetach_mounts(new_dentry);\n\t}\n\tif (!(old_dir->i_sb->s_type->fs_flags & FS_RENAME_DOES_D_MOVE)) {\n\t\tif (!(flags & RENAME_EXCHANGE))\n\t\t\td_move(old_dentry, new_dentry);\n\t\telse\n\t\t\td_exchange(old_dentry, new_dentry);\n\t}\nout:\n\tif (!is_dir || lock_old_subdir)\n\t\tinode_unlock(source);\n\tif (target && (!new_is_dir || lock_new_subdir))\n\t\tinode_unlock(target);\n\tdput(new_dentry);\n\tif (!error) {\n\t\tfsnotify_move(old_dir, new_dir, &old_name.name, is_dir,\n\t\t\t      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);\n\t\tif (flags & RENAME_EXCHANGE) {\n\t\t\tfsnotify_move(new_dir, old_dir, &old_dentry->d_name,\n\t\t\t\t      new_is_dir, NULL, new_dentry);\n\t\t}\n\t}\n\trelease_dentry_name_snapshot(&old_name);\n\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_rename);\n\nint do_renameat2(int olddfd, struct filename *from, int newdfd,\n\t\t struct filename *to, unsigned int flags)\n{\n\tstruct renamedata rd;\n\tstruct dentry *old_dentry, *new_dentry;\n\tstruct dentry *trap;\n\tstruct path old_path, new_path;\n\tstruct qstr old_last, new_last;\n\tint old_type, new_type;\n\tstruct inode *delegated_inode = NULL;\n\tunsigned int lookup_flags = 0, target_flags =\n\t\tLOOKUP_RENAME_TARGET | LOOKUP_CREATE;\n\tbool should_retry = false;\n\tint error = -EINVAL;\n\n\tif (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))\n\t\tgoto put_names;\n\n\tif ((flags & (RENAME_NOREPLACE | RENAME_WHITEOUT)) &&\n\t    (flags & RENAME_EXCHANGE))\n\t\tgoto put_names;\n\n\tif (flags & RENAME_EXCHANGE)\n\t\ttarget_flags = 0;\n\tif (flags & RENAME_NOREPLACE)\n\t\ttarget_flags |= LOOKUP_EXCL;\n\nretry:\n\terror = filename_parentat(olddfd, from, lookup_flags, &old_path,\n\t\t\t\t  &old_last, &old_type);\n\tif (error)\n\t\tgoto put_names;\n\n\terror = filename_parentat(newdfd, to, lookup_flags, &new_path, &new_last,\n\t\t\t\t  &new_type);\n\tif (error)\n\t\tgoto exit1;\n\n\terror = -EXDEV;\n\tif (old_path.mnt != new_path.mnt)\n\t\tgoto exit2;\n\n\terror = -EBUSY;\n\tif (old_type != LAST_NORM)\n\t\tgoto exit2;\n\n\tif (flags & RENAME_NOREPLACE)\n\t\terror = -EEXIST;\n\tif (new_type != LAST_NORM)\n\t\tgoto exit2;\n\n\terror = mnt_want_write(old_path.mnt);\n\tif (error)\n\t\tgoto exit2;\n\nretry_deleg:\n\ttrap = lock_rename(new_path.dentry, old_path.dentry);\n\tif (IS_ERR(trap)) {\n\t\terror = PTR_ERR(trap);\n\t\tgoto exit_lock_rename;\n\t}\n\n\told_dentry = lookup_one_qstr_excl(&old_last, old_path.dentry,\n\t\t\t\t\t  lookup_flags);\n\terror = PTR_ERR(old_dentry);\n\tif (IS_ERR(old_dentry))\n\t\tgoto exit3;\n\tnew_dentry = lookup_one_qstr_excl(&new_last, new_path.dentry,\n\t\t\t\t\t  lookup_flags | target_flags);\n\terror = PTR_ERR(new_dentry);\n\tif (IS_ERR(new_dentry))\n\t\tgoto exit4;\n\tif (flags & RENAME_EXCHANGE) {\n\t\tif (!d_is_dir(new_dentry)) {\n\t\t\terror = -ENOTDIR;\n\t\t\tif (new_last.name[new_last.len])\n\t\t\t\tgoto exit5;\n\t\t}\n\t}\n\t/* unless the source is a directory trailing slashes give -ENOTDIR */\n\tif (!d_is_dir(old_dentry)) {\n\t\terror = -ENOTDIR;\n\t\tif (old_last.name[old_last.len])\n\t\t\tgoto exit5;\n\t\tif (!(flags & RENAME_EXCHANGE) && new_last.name[new_last.len])\n\t\t\tgoto exit5;\n\t}\n\t/* source should not be ancestor of target */\n\terror = -EINVAL;\n\tif (old_dentry == trap)\n\t\tgoto exit5;\n\t/* target should not be an ancestor of source */\n\tif (!(flags & RENAME_EXCHANGE))\n\t\terror = -ENOTEMPTY;\n\tif (new_dentry == trap)\n\t\tgoto exit5;\n\n\terror = security_path_rename(&old_path, old_dentry,\n\t\t\t\t     &new_path, new_dentry, flags);\n\tif (error)\n\t\tgoto exit5;\n\n\trd.old_parent\t   = old_path.dentry;\n\trd.old_dentry\t   = old_dentry;\n\trd.mnt_idmap\t   = mnt_idmap(old_path.mnt);\n\trd.new_parent\t   = new_path.dentry;\n\trd.new_dentry\t   = new_dentry;\n\trd.delegated_inode = &delegated_inode;\n\trd.flags\t   = flags;\n\terror = vfs_rename(&rd);\nexit5:\n\tdput(new_dentry);\nexit4:\n\tdput(old_dentry);\nexit3:\n\tunlock_rename(new_path.dentry, old_path.dentry);\nexit_lock_rename:\n\tif (delegated_inode) {\n\t\terror = break_deleg_wait(&delegated_inode);\n\t\tif (!error)\n\t\t\tgoto retry_deleg;\n\t}\n\tmnt_drop_write(old_path.mnt);\nexit2:\n\tif (retry_estale(error, lookup_flags))\n\t\tshould_retry = true;\n\tpath_put(&new_path);\nexit1:\n\tpath_put(&old_path);\n\tif (should_retry) {\n\t\tshould_retry = false;\n\t\tlookup_flags |= LOOKUP_REVAL;\n\t\tgoto retry;\n\t}\nput_names:\n\tputname(from);\n\tputname(to);\n\treturn error;\n}\n\nSYSCALL_DEFINE5(renameat2, int, olddfd, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname, unsigned int, flags)\n{\n\treturn do_renameat2(olddfd, getname(oldname), newdfd, getname(newname),\n\t\t\t\tflags);\n}\n\nSYSCALL_DEFINE4(renameat, int, olddfd, const char __user *, oldname,\n\t\tint, newdfd, const char __user *, newname)\n{\n\treturn do_renameat2(olddfd, getname(oldname), newdfd, getname(newname),\n\t\t\t\t0);\n}\n\nSYSCALL_DEFINE2(rename, const char __user *, oldname, const char __user *, newname)\n{\n\treturn do_renameat2(AT_FDCWD, getname(oldname), AT_FDCWD,\n\t\t\t\tgetname(newname), 0);\n}\n\nint readlink_copy(char __user *buffer, int buflen, const char *link, int linklen)\n{\n\tint copylen;\n\n\tcopylen = linklen;\n\tif (unlikely(copylen > (unsigned) buflen))\n\t\tcopylen = buflen;\n\tif (copy_to_user(buffer, link, copylen))\n\t\tcopylen = -EFAULT;\n\treturn copylen;\n}\n\n/**\n * vfs_readlink - copy symlink body into userspace buffer\n * @dentry: dentry on which to get symbolic link\n * @buffer: user memory pointer\n * @buflen: size of buffer\n *\n * Does not touch atime.  That's up to the caller if necessary\n *\n * Does not call security hook.\n */\nint vfs_readlink(struct dentry *dentry, char __user *buffer, int buflen)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tDEFINE_DELAYED_CALL(done);\n\tconst char *link;\n\tint res;\n\n\tif (inode->i_opflags & IOP_CACHED_LINK)\n\t\treturn readlink_copy(buffer, buflen, inode->i_link, inode->i_linklen);\n\n\tif (unlikely(!(inode->i_opflags & IOP_DEFAULT_READLINK))) {\n\t\tif (unlikely(inode->i_op->readlink))\n\t\t\treturn inode->i_op->readlink(dentry, buffer, buflen);\n\n\t\tif (!d_is_symlink(dentry))\n\t\t\treturn -EINVAL;\n\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_opflags |= IOP_DEFAULT_READLINK;\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\n\tlink = READ_ONCE(inode->i_link);\n\tif (!link) {\n\t\tlink = inode->i_op->get_link(dentry, inode, &done);\n\t\tif (IS_ERR(link))\n\t\t\treturn PTR_ERR(link);\n\t}\n\tres = readlink_copy(buffer, buflen, link, strlen(link));\n\tdo_delayed_call(&done);\n\treturn res;\n}\nEXPORT_SYMBOL(vfs_readlink);\n\n/**\n * vfs_get_link - get symlink body\n * @dentry: dentry on which to get symbolic link\n * @done: caller needs to free returned data with this\n *\n * Calls security hook and i_op->get_link() on the supplied inode.\n *\n * It does not touch atime.  That's up to the caller if necessary.\n *\n * Does not work on \"special\" symlinks like /proc/$$/fd/N\n */\nconst char *vfs_get_link(struct dentry *dentry, struct delayed_call *done)\n{\n\tconst char *res = ERR_PTR(-EINVAL);\n\tstruct inode *inode = d_inode(dentry);\n\n\tif (d_is_symlink(dentry)) {\n\t\tres = ERR_PTR(security_inode_readlink(dentry));\n\t\tif (!res)\n\t\t\tres = inode->i_op->get_link(dentry, inode, done);\n\t}\n\treturn res;\n}\nEXPORT_SYMBOL(vfs_get_link);\n\n/* get the link contents into pagecache */\nstatic char *__page_get_link(struct dentry *dentry, struct inode *inode,\n\t\t\t     struct delayed_call *callback)\n{\n\tstruct folio *folio;\n\tstruct address_space *mapping = inode->i_mapping;\n\n\tif (!dentry) {\n\t\tfolio = filemap_get_folio(mapping, 0);\n\t\tif (IS_ERR(folio))\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\tif (!folio_test_uptodate(folio)) {\n\t\t\tfolio_put(folio);\n\t\t\treturn ERR_PTR(-ECHILD);\n\t\t}\n\t} else {\n\t\tfolio = read_mapping_folio(mapping, 0, NULL);\n\t\tif (IS_ERR(folio))\n\t\t\treturn ERR_CAST(folio);\n\t}\n\tset_delayed_call(callback, page_put_link, folio);\n\tBUG_ON(mapping_gfp_mask(mapping) & __GFP_HIGHMEM);\n\treturn folio_address(folio);\n}\n\nconst char *page_get_link_raw(struct dentry *dentry, struct inode *inode,\n\t\t\t      struct delayed_call *callback)\n{\n\treturn __page_get_link(dentry, inode, callback);\n}\nEXPORT_SYMBOL_GPL(page_get_link_raw);\n\n/**\n * page_get_link() - An implementation of the get_link inode_operation.\n * @dentry: The directory entry which is the symlink.\n * @inode: The inode for the symlink.\n * @callback: Used to drop the reference to the symlink.\n *\n * Filesystems which store their symlinks in the page cache should use\n * this to implement the get_link() member of their inode_operations.\n *\n * Return: A pointer to the NUL-terminated symlink.\n */\nconst char *page_get_link(struct dentry *dentry, struct inode *inode,\n\t\t\t\t\tstruct delayed_call *callback)\n{\n\tchar *kaddr = __page_get_link(dentry, inode, callback);\n\n\tif (!IS_ERR(kaddr))\n\t\tnd_terminate_link(kaddr, inode->i_size, PAGE_SIZE - 1);\n\treturn kaddr;\n}\nEXPORT_SYMBOL(page_get_link);\n\n/**\n * page_put_link() - Drop the reference to the symlink.\n * @arg: The folio which contains the symlink.\n *\n * This is used internally by page_get_link().  It is exported for use\n * by filesystems which need to implement a variant of page_get_link()\n * themselves.  Despite the apparent symmetry, filesystems which use\n * page_get_link() do not need to call page_put_link().\n *\n * The argument, while it has a void pointer type, must be a pointer to\n * the folio which was retrieved from the page cache.  The delayed_call\n * infrastructure is used to drop the reference count once the caller\n * is done with the symlink.\n */\nvoid page_put_link(void *arg)\n{\n\tfolio_put(arg);\n}\nEXPORT_SYMBOL(page_put_link);\n\nint page_readlink(struct dentry *dentry, char __user *buffer, int buflen)\n{\n\tconst char *link;\n\tint res;\n\n\tDEFINE_DELAYED_CALL(done);\n\tlink = page_get_link(dentry, d_inode(dentry), &done);\n\tres = PTR_ERR(link);\n\tif (!IS_ERR(link))\n\t\tres = readlink_copy(buffer, buflen, link, strlen(link));\n\tdo_delayed_call(&done);\n\treturn res;\n}\nEXPORT_SYMBOL(page_readlink);\n\nint page_symlink(struct inode *inode, const char *symname, int len)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tconst struct address_space_operations *aops = mapping->a_ops;\n\tbool nofs = !mapping_gfp_constraint(mapping, __GFP_FS);\n\tstruct folio *folio;\n\tvoid *fsdata = NULL;\n\tint err;\n\tunsigned int flags;\n\nretry:\n\tif (nofs)\n\t\tflags = memalloc_nofs_save();\n\terr = aops->write_begin(NULL, mapping, 0, len-1, &folio, &fsdata);\n\tif (nofs)\n\t\tmemalloc_nofs_restore(flags);\n\tif (err)\n\t\tgoto fail;\n\n\tmemcpy(folio_address(folio), symname, len - 1);\n\n\terr = aops->write_end(NULL, mapping, 0, len - 1, len - 1,\n\t\t\t\t\t\tfolio, fsdata);\n\tif (err < 0)\n\t\tgoto fail;\n\tif (err < len-1)\n\t\tgoto retry;\n\n\tmark_inode_dirty(inode);\n\treturn 0;\nfail:\n\treturn err;\n}\nEXPORT_SYMBOL(page_symlink);\n\nconst struct inode_operations page_symlink_inode_operations = {\n\t.get_link\t= page_get_link,\n};\nEXPORT_SYMBOL(page_symlink_inode_operations);\n", "patch": "@@ -4213,7 +4213,11 @@ int vfs_rename(struct inode *old_dir, struct dentry *old_dentry,\n \tbool new_is_dir = false;\n \tunsigned max_links = new_dir->i_sb->s_max_links;\n \n-\tif (source == target)\n+\t/*\n+\t * Check source == target.\n+\t * On overlayfs need to look at underlying inodes.\n+\t */\n+\tif (vfs_select_inode(old_dentry, 0) == vfs_select_inode(new_dentry, 0))\n \t\treturn 0;\n \n \terror = may_delete(old_dir, old_dentry, is_dir);", "file_path": "files/2016_8\\99", "file_language": "c", "file_name": "fs/namei.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 38, "cve_id": "CVE-2016-5400", "cwe_id": ["CWE-119"], "cve_language": "C", "cve_description": "Memory leak in the airspy_probe function in drivers/media/usb/airspy/airspy.c in the airspy USB driver in the Linux kernel before 4.7 allows local users to cause a denial of service (memory consumption) via a crafted USB device that emulates many VFL_TYPE_SDR or VFL_TYPE_SUBDEV devices and performs many connect and disconnect operations.", "cvss": "4.6", "publish_date": "August 6, 2016", "AV": "PHYSICAL", "AC": "LOW", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "NONE", "A": "HIGH", "commit_id": "aa93d1fee85c890a34f2510a310e55ee76a27848", "commit_message": "media: fix airspy usb probe error path\n\nFix a memory leak on probe error of the airspy usb device driver.\n\nThe problem is triggered when more than 64 usb devices register with\nv4l2 of type VFL_TYPE_SDR or VFL_TYPE_SUBDEV.\n\nThe memory leak is caused by the probe function of the airspy driver\nmishandeling errors and not freeing the corresponding control structures\nwhen an error occours registering the device to v4l2 core.\n\nA badusb device can emulate 64 of these devices, and then through\ncontinual emulated connect/disconnect of the 65th device, cause the\nkernel to run out of RAM and crash the kernel, thus causing a local DOS\nvulnerability.\n\nFixes CVE-2016-5400\n\nSigned-off-by: James Patrick-Evans <james@jmp-e.com>\nReviewed-by: Kees Cook <keescook@chromium.org>\nCc: stable@vger.kernel.org # 3.17+\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "commit_date": "2016-07-15T21:15:40Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/aa93d1fee85c890a34f2510a310e55ee76a27848", "html_url": "https://github.com/torvalds/linux/commit/aa93d1fee85c890a34f2510a310e55ee76a27848", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "0ba169ac3600ae4032ee14b4192e6bf5d67723f5", "url_before": "https://api.github.com/repos/torvalds/linux/commits/0ba169ac3600ae4032ee14b4192e6bf5d67723f5", "html_url_before": "https://github.com/torvalds/linux/commit/0ba169ac3600ae4032ee14b4192e6bf5d67723f5"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/aa93d1fee85c890a34f2510a310e55ee76a27848/drivers/media/usb/airspy/airspy.c", "code": "/*\n * AirSpy SDR driver\n *\n * Copyright (C) 2014 Antti Palosaari <crope@iki.fi>\n *\n *    This program is free software; you can redistribute it and/or modify\n *    it under the terms of the GNU General Public License as published by\n *    the Free Software Foundation; either version 2 of the License, or\n *    (at your option) any later version.\n *\n *    This program is distributed in the hope that it will be useful,\n *    but WITHOUT ANY WARRANTY; without even the implied warranty of\n *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *    GNU General Public License for more details.\n */\n\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/usb.h>\n#include <media/v4l2-device.h>\n#include <media/v4l2-ioctl.h>\n#include <media/v4l2-ctrls.h>\n#include <media/v4l2-event.h>\n#include <media/videobuf2-v4l2.h>\n#include <media/videobuf2-vmalloc.h>\n\n/* AirSpy USB API commands (from AirSpy Library) */\nenum {\n\tCMD_INVALID                       = 0x00,\n\tCMD_RECEIVER_MODE                 = 0x01,\n\tCMD_SI5351C_WRITE                 = 0x02,\n\tCMD_SI5351C_READ                  = 0x03,\n\tCMD_R820T_WRITE                   = 0x04,\n\tCMD_R820T_READ                    = 0x05,\n\tCMD_SPIFLASH_ERASE                = 0x06,\n\tCMD_SPIFLASH_WRITE                = 0x07,\n\tCMD_SPIFLASH_READ                 = 0x08,\n\tCMD_BOARD_ID_READ                 = 0x09,\n\tCMD_VERSION_STRING_READ           = 0x0a,\n\tCMD_BOARD_PARTID_SERIALNO_READ    = 0x0b,\n\tCMD_SET_SAMPLE_RATE               = 0x0c,\n\tCMD_SET_FREQ                      = 0x0d,\n\tCMD_SET_LNA_GAIN                  = 0x0e,\n\tCMD_SET_MIXER_GAIN                = 0x0f,\n\tCMD_SET_VGA_GAIN                  = 0x10,\n\tCMD_SET_LNA_AGC                   = 0x11,\n\tCMD_SET_MIXER_AGC                 = 0x12,\n\tCMD_SET_PACKING                   = 0x13,\n};\n\n/*\n *       bEndpointAddress     0x81  EP 1 IN\n *         Transfer Type            Bulk\n *       wMaxPacketSize     0x0200  1x 512 bytes\n */\n#define MAX_BULK_BUFS            (6)\n#define BULK_BUFFER_SIZE         (128 * 512)\n\nstatic const struct v4l2_frequency_band bands[] = {\n\t{\n\t\t.tuner = 0,\n\t\t.type = V4L2_TUNER_ADC,\n\t\t.index = 0,\n\t\t.capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS,\n\t\t.rangelow   = 20000000,\n\t\t.rangehigh  = 20000000,\n\t},\n};\n\nstatic const struct v4l2_frequency_band bands_rf[] = {\n\t{\n\t\t.tuner = 1,\n\t\t.type = V4L2_TUNER_RF,\n\t\t.index = 0,\n\t\t.capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS,\n\t\t.rangelow   =   24000000,\n\t\t.rangehigh  = 1750000000,\n\t},\n};\n\n/* stream formats */\nstruct airspy_format {\n\tchar\t*name;\n\tu32\tpixelformat;\n\tu32\tbuffersize;\n};\n\n/* format descriptions for capture and preview */\nstatic struct airspy_format formats[] = {\n\t{\n\t\t.name\t\t= \"Real U12LE\",\n\t\t.pixelformat\t= V4L2_SDR_FMT_RU12LE,\n\t\t.buffersize\t= BULK_BUFFER_SIZE,\n\t},\n};\n\nstatic const unsigned int NUM_FORMATS = ARRAY_SIZE(formats);\n\n/* intermediate buffers with raw data from the USB device */\nstruct airspy_frame_buf {\n\t/* common v4l buffer stuff -- must be first */\n\tstruct vb2_v4l2_buffer vb;\n\tstruct list_head list;\n};\n\nstruct airspy {\n#define POWER_ON\t   1\n#define USB_STATE_URB_BUF  2\n\tunsigned long flags;\n\n\tstruct device *dev;\n\tstruct usb_device *udev;\n\tstruct video_device vdev;\n\tstruct v4l2_device v4l2_dev;\n\n\t/* videobuf2 queue and queued buffers list */\n\tstruct vb2_queue vb_queue;\n\tstruct list_head queued_bufs;\n\tspinlock_t queued_bufs_lock; /* Protects queued_bufs */\n\tunsigned sequence;\t     /* Buffer sequence counter */\n\tunsigned int vb_full;        /* vb is full and packets dropped */\n\n\t/* Note if taking both locks v4l2_lock must always be locked first! */\n\tstruct mutex v4l2_lock;      /* Protects everything else */\n\tstruct mutex vb_queue_lock;  /* Protects vb_queue and capt_file */\n\n\tstruct urb     *urb_list[MAX_BULK_BUFS];\n\tint            buf_num;\n\tunsigned long  buf_size;\n\tu8             *buf_list[MAX_BULK_BUFS];\n\tdma_addr_t     dma_addr[MAX_BULK_BUFS];\n\tint            urbs_initialized;\n\tint            urbs_submitted;\n\n\t/* USB control message buffer */\n\t#define BUF_SIZE 128\n\tu8 buf[BUF_SIZE];\n\n\t/* Current configuration */\n\tunsigned int f_adc;\n\tunsigned int f_rf;\n\tu32 pixelformat;\n\tu32 buffersize;\n\n\t/* Controls */\n\tstruct v4l2_ctrl_handler hdl;\n\tstruct v4l2_ctrl *lna_gain_auto;\n\tstruct v4l2_ctrl *lna_gain;\n\tstruct v4l2_ctrl *mixer_gain_auto;\n\tstruct v4l2_ctrl *mixer_gain;\n\tstruct v4l2_ctrl *if_gain;\n\n\t/* Sample rate calc */\n\tunsigned long jiffies_next;\n\tunsigned int sample;\n\tunsigned int sample_measured;\n};\n\n#define airspy_dbg_usb_control_msg(_dev, _r, _t, _v, _i, _b, _l) { \\\n\tchar *_direction; \\\n\tif (_t & USB_DIR_IN) \\\n\t\t_direction = \"<<<\"; \\\n\telse \\\n\t\t_direction = \">>>\"; \\\n\tdev_dbg(_dev, \"%02x %02x %02x %02x %02x %02x %02x %02x %s %*ph\\n\", \\\n\t\t\t_t, _r, _v & 0xff, _v >> 8, _i & 0xff, _i >> 8, \\\n\t\t\t_l & 0xff, _l >> 8, _direction, _l, _b); \\\n}\n\n/* execute firmware command */\nstatic int airspy_ctrl_msg(struct airspy *s, u8 request, u16 value, u16 index,\n\t\tu8 *data, u16 size)\n{\n\tint ret;\n\tunsigned int pipe;\n\tu8 requesttype;\n\n\tswitch (request) {\n\tcase CMD_RECEIVER_MODE:\n\tcase CMD_SET_FREQ:\n\t\tpipe = usb_sndctrlpipe(s->udev, 0);\n\t\trequesttype = (USB_TYPE_VENDOR | USB_DIR_OUT);\n\t\tbreak;\n\tcase CMD_BOARD_ID_READ:\n\tcase CMD_VERSION_STRING_READ:\n\tcase CMD_BOARD_PARTID_SERIALNO_READ:\n\tcase CMD_SET_LNA_GAIN:\n\tcase CMD_SET_MIXER_GAIN:\n\tcase CMD_SET_VGA_GAIN:\n\tcase CMD_SET_LNA_AGC:\n\tcase CMD_SET_MIXER_AGC:\n\t\tpipe = usb_rcvctrlpipe(s->udev, 0);\n\t\trequesttype = (USB_TYPE_VENDOR | USB_DIR_IN);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(s->dev, \"Unknown command %02x\\n\", request);\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/* write request */\n\tif (!(requesttype & USB_DIR_IN))\n\t\tmemcpy(s->buf, data, size);\n\n\tret = usb_control_msg(s->udev, pipe, request, requesttype, value,\n\t\t\tindex, s->buf, size, 1000);\n\tairspy_dbg_usb_control_msg(s->dev, request, requesttype, value,\n\t\t\tindex, s->buf, size);\n\tif (ret < 0) {\n\t\tdev_err(s->dev, \"usb_control_msg() failed %d request %02x\\n\",\n\t\t\t\tret, request);\n\t\tgoto err;\n\t}\n\n\t/* read request */\n\tif (requesttype & USB_DIR_IN)\n\t\tmemcpy(data, s->buf, size);\n\n\treturn 0;\nerr:\n\treturn ret;\n}\n\n/* Private functions */\nstatic struct airspy_frame_buf *airspy_get_next_fill_buf(struct airspy *s)\n{\n\tunsigned long flags;\n\tstruct airspy_frame_buf *buf = NULL;\n\n\tspin_lock_irqsave(&s->queued_bufs_lock, flags);\n\tif (list_empty(&s->queued_bufs))\n\t\tgoto leave;\n\n\tbuf = list_entry(s->queued_bufs.next,\n\t\t\tstruct airspy_frame_buf, list);\n\tlist_del(&buf->list);\nleave:\n\tspin_unlock_irqrestore(&s->queued_bufs_lock, flags);\n\treturn buf;\n}\n\nstatic unsigned int airspy_convert_stream(struct airspy *s,\n\t\tvoid *dst, void *src, unsigned int src_len)\n{\n\tunsigned int dst_len;\n\n\tif (s->pixelformat == V4L2_SDR_FMT_RU12LE) {\n\t\tmemcpy(dst, src, src_len);\n\t\tdst_len = src_len;\n\t} else {\n\t\tdst_len = 0;\n\t}\n\n\t/* calculate sample rate and output it in 10 seconds intervals */\n\tif (unlikely(time_is_before_jiffies(s->jiffies_next))) {\n\t\t#define MSECS 10000UL\n\t\tunsigned int msecs = jiffies_to_msecs(jiffies -\n\t\t\t\ts->jiffies_next + msecs_to_jiffies(MSECS));\n\t\tunsigned int samples = s->sample - s->sample_measured;\n\n\t\ts->jiffies_next = jiffies + msecs_to_jiffies(MSECS);\n\t\ts->sample_measured = s->sample;\n\t\tdev_dbg(s->dev, \"slen=%u samples=%u msecs=%u sample rate=%lu\\n\",\n\t\t\t\tsrc_len, samples, msecs,\n\t\t\t\tsamples * 1000UL / msecs);\n\t}\n\n\t/* total number of samples */\n\ts->sample += src_len / 2;\n\n\treturn dst_len;\n}\n\n/*\n * This gets called for the bulk stream pipe. This is done in interrupt\n * time, so it has to be fast, not crash, and not stall. Neat.\n */\nstatic void airspy_urb_complete(struct urb *urb)\n{\n\tstruct airspy *s = urb->context;\n\tstruct airspy_frame_buf *fbuf;\n\n\tdev_dbg_ratelimited(s->dev, \"status=%d length=%d/%d errors=%d\\n\",\n\t\t\turb->status, urb->actual_length,\n\t\t\turb->transfer_buffer_length, urb->error_count);\n\n\tswitch (urb->status) {\n\tcase 0:             /* success */\n\tcase -ETIMEDOUT:    /* NAK */\n\t\tbreak;\n\tcase -ECONNRESET:   /* kill */\n\tcase -ENOENT:\n\tcase -ESHUTDOWN:\n\t\treturn;\n\tdefault:            /* error */\n\t\tdev_err_ratelimited(s->dev, \"URB failed %d\\n\", urb->status);\n\t\tbreak;\n\t}\n\n\tif (likely(urb->actual_length > 0)) {\n\t\tvoid *ptr;\n\t\tunsigned int len;\n\t\t/* get free framebuffer */\n\t\tfbuf = airspy_get_next_fill_buf(s);\n\t\tif (unlikely(fbuf == NULL)) {\n\t\t\ts->vb_full++;\n\t\t\tdev_notice_ratelimited(s->dev,\n\t\t\t\t\t\"videobuf is full, %d packets dropped\\n\",\n\t\t\t\t\ts->vb_full);\n\t\t\tgoto skip;\n\t\t}\n\n\t\t/* fill framebuffer */\n\t\tptr = vb2_plane_vaddr(&fbuf->vb.vb2_buf, 0);\n\t\tlen = airspy_convert_stream(s, ptr, urb->transfer_buffer,\n\t\t\t\turb->actual_length);\n\t\tvb2_set_plane_payload(&fbuf->vb.vb2_buf, 0, len);\n\t\tfbuf->vb.vb2_buf.timestamp = ktime_get_ns();\n\t\tfbuf->vb.sequence = s->sequence++;\n\t\tvb2_buffer_done(&fbuf->vb.vb2_buf, VB2_BUF_STATE_DONE);\n\t}\nskip:\n\tusb_submit_urb(urb, GFP_ATOMIC);\n}\n\nstatic int airspy_kill_urbs(struct airspy *s)\n{\n\tint i;\n\n\tfor (i = s->urbs_submitted - 1; i >= 0; i--) {\n\t\tdev_dbg(s->dev, \"kill urb=%d\\n\", i);\n\t\t/* stop the URB */\n\t\tusb_kill_urb(s->urb_list[i]);\n\t}\n\ts->urbs_submitted = 0;\n\n\treturn 0;\n}\n\nstatic int airspy_submit_urbs(struct airspy *s)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < s->urbs_initialized; i++) {\n\t\tdev_dbg(s->dev, \"submit urb=%d\\n\", i);\n\t\tret = usb_submit_urb(s->urb_list[i], GFP_ATOMIC);\n\t\tif (ret) {\n\t\t\tdev_err(s->dev, \"Could not submit URB no. %d - get them all back\\n\",\n\t\t\t\t\ti);\n\t\t\tairspy_kill_urbs(s);\n\t\t\treturn ret;\n\t\t}\n\t\ts->urbs_submitted++;\n\t}\n\n\treturn 0;\n}\n\nstatic int airspy_free_stream_bufs(struct airspy *s)\n{\n\tif (test_bit(USB_STATE_URB_BUF, &s->flags)) {\n\t\twhile (s->buf_num) {\n\t\t\ts->buf_num--;\n\t\t\tdev_dbg(s->dev, \"free buf=%d\\n\", s->buf_num);\n\t\t\tusb_free_coherent(s->udev, s->buf_size,\n\t\t\t\t\t  s->buf_list[s->buf_num],\n\t\t\t\t\t  s->dma_addr[s->buf_num]);\n\t\t}\n\t}\n\tclear_bit(USB_STATE_URB_BUF, &s->flags);\n\n\treturn 0;\n}\n\nstatic int airspy_alloc_stream_bufs(struct airspy *s)\n{\n\ts->buf_num = 0;\n\ts->buf_size = BULK_BUFFER_SIZE;\n\n\tdev_dbg(s->dev, \"all in all I will use %u bytes for streaming\\n\",\n\t\t\tMAX_BULK_BUFS * BULK_BUFFER_SIZE);\n\n\tfor (s->buf_num = 0; s->buf_num < MAX_BULK_BUFS; s->buf_num++) {\n\t\ts->buf_list[s->buf_num] = usb_alloc_coherent(s->udev,\n\t\t\t\tBULK_BUFFER_SIZE, GFP_ATOMIC,\n\t\t\t\t&s->dma_addr[s->buf_num]);\n\t\tif (!s->buf_list[s->buf_num]) {\n\t\t\tdev_dbg(s->dev, \"alloc buf=%d failed\\n\", s->buf_num);\n\t\t\tairspy_free_stream_bufs(s);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdev_dbg(s->dev, \"alloc buf=%d %p (dma %llu)\\n\", s->buf_num,\n\t\t\t\ts->buf_list[s->buf_num],\n\t\t\t\t(long long)s->dma_addr[s->buf_num]);\n\t\tset_bit(USB_STATE_URB_BUF, &s->flags);\n\t}\n\n\treturn 0;\n}\n\nstatic int airspy_free_urbs(struct airspy *s)\n{\n\tint i;\n\n\tairspy_kill_urbs(s);\n\n\tfor (i = s->urbs_initialized - 1; i >= 0; i--) {\n\t\tif (s->urb_list[i]) {\n\t\t\tdev_dbg(s->dev, \"free urb=%d\\n\", i);\n\t\t\t/* free the URBs */\n\t\t\tusb_free_urb(s->urb_list[i]);\n\t\t}\n\t}\n\ts->urbs_initialized = 0;\n\n\treturn 0;\n}\n\nstatic int airspy_alloc_urbs(struct airspy *s)\n{\n\tint i, j;\n\n\t/* allocate the URBs */\n\tfor (i = 0; i < MAX_BULK_BUFS; i++) {\n\t\tdev_dbg(s->dev, \"alloc urb=%d\\n\", i);\n\t\ts->urb_list[i] = usb_alloc_urb(0, GFP_ATOMIC);\n\t\tif (!s->urb_list[i]) {\n\t\t\tdev_dbg(s->dev, \"failed\\n\");\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\tusb_free_urb(s->urb_list[j]);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tusb_fill_bulk_urb(s->urb_list[i],\n\t\t\t\ts->udev,\n\t\t\t\tusb_rcvbulkpipe(s->udev, 0x81),\n\t\t\t\ts->buf_list[i],\n\t\t\t\tBULK_BUFFER_SIZE,\n\t\t\t\tairspy_urb_complete, s);\n\n\t\ts->urb_list[i]->transfer_flags = URB_NO_TRANSFER_DMA_MAP;\n\t\ts->urb_list[i]->transfer_dma = s->dma_addr[i];\n\t\ts->urbs_initialized++;\n\t}\n\n\treturn 0;\n}\n\n/* Must be called with vb_queue_lock hold */\nstatic void airspy_cleanup_queued_bufs(struct airspy *s)\n{\n\tunsigned long flags;\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tspin_lock_irqsave(&s->queued_bufs_lock, flags);\n\twhile (!list_empty(&s->queued_bufs)) {\n\t\tstruct airspy_frame_buf *buf;\n\n\t\tbuf = list_entry(s->queued_bufs.next,\n\t\t\t\tstruct airspy_frame_buf, list);\n\t\tlist_del(&buf->list);\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t}\n\tspin_unlock_irqrestore(&s->queued_bufs_lock, flags);\n}\n\n/* The user yanked out the cable... */\nstatic void airspy_disconnect(struct usb_interface *intf)\n{\n\tstruct v4l2_device *v = usb_get_intfdata(intf);\n\tstruct airspy *s = container_of(v, struct airspy, v4l2_dev);\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tmutex_lock(&s->vb_queue_lock);\n\tmutex_lock(&s->v4l2_lock);\n\t/* No need to keep the urbs around after disconnection */\n\ts->udev = NULL;\n\tv4l2_device_disconnect(&s->v4l2_dev);\n\tvideo_unregister_device(&s->vdev);\n\tmutex_unlock(&s->v4l2_lock);\n\tmutex_unlock(&s->vb_queue_lock);\n\n\tv4l2_device_put(&s->v4l2_dev);\n}\n\n/* Videobuf2 operations */\nstatic int airspy_queue_setup(struct vb2_queue *vq,\n\t\tunsigned int *nbuffers,\n\t\tunsigned int *nplanes, unsigned int sizes[], void *alloc_ctxs[])\n{\n\tstruct airspy *s = vb2_get_drv_priv(vq);\n\n\tdev_dbg(s->dev, \"nbuffers=%d\\n\", *nbuffers);\n\n\t/* Need at least 8 buffers */\n\tif (vq->num_buffers + *nbuffers < 8)\n\t\t*nbuffers = 8 - vq->num_buffers;\n\t*nplanes = 1;\n\tsizes[0] = PAGE_ALIGN(s->buffersize);\n\n\tdev_dbg(s->dev, \"nbuffers=%d sizes[0]=%d\\n\", *nbuffers, sizes[0]);\n\treturn 0;\n}\n\nstatic void airspy_buf_queue(struct vb2_buffer *vb)\n{\n\tstruct vb2_v4l2_buffer *vbuf = to_vb2_v4l2_buffer(vb);\n\tstruct airspy *s = vb2_get_drv_priv(vb->vb2_queue);\n\tstruct airspy_frame_buf *buf =\n\t\t\tcontainer_of(vbuf, struct airspy_frame_buf, vb);\n\tunsigned long flags;\n\n\t/* Check the device has not disconnected between prep and queuing */\n\tif (unlikely(!s->udev)) {\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&s->queued_bufs_lock, flags);\n\tlist_add_tail(&buf->list, &s->queued_bufs);\n\tspin_unlock_irqrestore(&s->queued_bufs_lock, flags);\n}\n\nstatic int airspy_start_streaming(struct vb2_queue *vq, unsigned int count)\n{\n\tstruct airspy *s = vb2_get_drv_priv(vq);\n\tint ret;\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tif (!s->udev)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&s->v4l2_lock);\n\n\ts->sequence = 0;\n\n\tset_bit(POWER_ON, &s->flags);\n\n\tret = airspy_alloc_stream_bufs(s);\n\tif (ret)\n\t\tgoto err_clear_bit;\n\n\tret = airspy_alloc_urbs(s);\n\tif (ret)\n\t\tgoto err_free_stream_bufs;\n\n\tret = airspy_submit_urbs(s);\n\tif (ret)\n\t\tgoto err_free_urbs;\n\n\t/* start hardware streaming */\n\tret = airspy_ctrl_msg(s, CMD_RECEIVER_MODE, 1, 0, NULL, 0);\n\tif (ret)\n\t\tgoto err_kill_urbs;\n\n\tgoto exit_mutex_unlock;\n\nerr_kill_urbs:\n\tairspy_kill_urbs(s);\nerr_free_urbs:\n\tairspy_free_urbs(s);\nerr_free_stream_bufs:\n\tairspy_free_stream_bufs(s);\nerr_clear_bit:\n\tclear_bit(POWER_ON, &s->flags);\n\n\t/* return all queued buffers to vb2 */\n\t{\n\t\tstruct airspy_frame_buf *buf, *tmp;\n\n\t\tlist_for_each_entry_safe(buf, tmp, &s->queued_bufs, list) {\n\t\t\tlist_del(&buf->list);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf,\n\t\t\t\t\tVB2_BUF_STATE_QUEUED);\n\t\t}\n\t}\n\nexit_mutex_unlock:\n\tmutex_unlock(&s->v4l2_lock);\n\n\treturn ret;\n}\n\nstatic void airspy_stop_streaming(struct vb2_queue *vq)\n{\n\tstruct airspy *s = vb2_get_drv_priv(vq);\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tmutex_lock(&s->v4l2_lock);\n\n\t/* stop hardware streaming */\n\tairspy_ctrl_msg(s, CMD_RECEIVER_MODE, 0, 0, NULL, 0);\n\n\tairspy_kill_urbs(s);\n\tairspy_free_urbs(s);\n\tairspy_free_stream_bufs(s);\n\n\tairspy_cleanup_queued_bufs(s);\n\n\tclear_bit(POWER_ON, &s->flags);\n\n\tmutex_unlock(&s->v4l2_lock);\n}\n\nstatic struct vb2_ops airspy_vb2_ops = {\n\t.queue_setup            = airspy_queue_setup,\n\t.buf_queue              = airspy_buf_queue,\n\t.start_streaming        = airspy_start_streaming,\n\t.stop_streaming         = airspy_stop_streaming,\n\t.wait_prepare           = vb2_ops_wait_prepare,\n\t.wait_finish            = vb2_ops_wait_finish,\n};\n\nstatic int airspy_querycap(struct file *file, void *fh,\n\t\tstruct v4l2_capability *cap)\n{\n\tstruct airspy *s = video_drvdata(file);\n\n\tstrlcpy(cap->driver, KBUILD_MODNAME, sizeof(cap->driver));\n\tstrlcpy(cap->card, s->vdev.name, sizeof(cap->card));\n\tusb_make_path(s->udev, cap->bus_info, sizeof(cap->bus_info));\n\tcap->device_caps = V4L2_CAP_SDR_CAPTURE | V4L2_CAP_STREAMING |\n\t\t\tV4L2_CAP_READWRITE | V4L2_CAP_TUNER;\n\tcap->capabilities = cap->device_caps | V4L2_CAP_DEVICE_CAPS;\n\n\treturn 0;\n}\n\nstatic int airspy_enum_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_fmtdesc *f)\n{\n\tif (f->index >= NUM_FORMATS)\n\t\treturn -EINVAL;\n\n\tstrlcpy(f->description, formats[f->index].name, sizeof(f->description));\n\tf->pixelformat = formats[f->index].pixelformat;\n\n\treturn 0;\n}\n\nstatic int airspy_g_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_format *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\n\tf->fmt.sdr.pixelformat = s->pixelformat;\n\tf->fmt.sdr.buffersize = s->buffersize;\n\tmemset(f->fmt.sdr.reserved, 0, sizeof(f->fmt.sdr.reserved));\n\n\treturn 0;\n}\n\nstatic int airspy_s_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_format *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\tstruct vb2_queue *q = &s->vb_queue;\n\tint i;\n\n\tif (vb2_is_busy(q))\n\t\treturn -EBUSY;\n\n\tmemset(f->fmt.sdr.reserved, 0, sizeof(f->fmt.sdr.reserved));\n\tfor (i = 0; i < NUM_FORMATS; i++) {\n\t\tif (formats[i].pixelformat == f->fmt.sdr.pixelformat) {\n\t\t\ts->pixelformat = formats[i].pixelformat;\n\t\t\ts->buffersize = formats[i].buffersize;\n\t\t\tf->fmt.sdr.buffersize = formats[i].buffersize;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\ts->pixelformat = formats[0].pixelformat;\n\ts->buffersize = formats[0].buffersize;\n\tf->fmt.sdr.pixelformat = formats[0].pixelformat;\n\tf->fmt.sdr.buffersize = formats[0].buffersize;\n\n\treturn 0;\n}\n\nstatic int airspy_try_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_format *f)\n{\n\tint i;\n\n\tmemset(f->fmt.sdr.reserved, 0, sizeof(f->fmt.sdr.reserved));\n\tfor (i = 0; i < NUM_FORMATS; i++) {\n\t\tif (formats[i].pixelformat == f->fmt.sdr.pixelformat) {\n\t\t\tf->fmt.sdr.buffersize = formats[i].buffersize;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tf->fmt.sdr.pixelformat = formats[0].pixelformat;\n\tf->fmt.sdr.buffersize = formats[0].buffersize;\n\n\treturn 0;\n}\n\nstatic int airspy_s_tuner(struct file *file, void *priv,\n\t\tconst struct v4l2_tuner *v)\n{\n\tint ret;\n\n\tif (v->index == 0)\n\t\tret = 0;\n\telse if (v->index == 1)\n\t\tret = 0;\n\telse\n\t\tret = -EINVAL;\n\n\treturn ret;\n}\n\nstatic int airspy_g_tuner(struct file *file, void *priv, struct v4l2_tuner *v)\n{\n\tint ret;\n\n\tif (v->index == 0) {\n\t\tstrlcpy(v->name, \"AirSpy ADC\", sizeof(v->name));\n\t\tv->type = V4L2_TUNER_ADC;\n\t\tv->capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS;\n\t\tv->rangelow  = bands[0].rangelow;\n\t\tv->rangehigh = bands[0].rangehigh;\n\t\tret = 0;\n\t} else if (v->index == 1) {\n\t\tstrlcpy(v->name, \"AirSpy RF\", sizeof(v->name));\n\t\tv->type = V4L2_TUNER_RF;\n\t\tv->capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS;\n\t\tv->rangelow  = bands_rf[0].rangelow;\n\t\tv->rangehigh = bands_rf[0].rangehigh;\n\t\tret = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int airspy_g_frequency(struct file *file, void *priv,\n\t\tstruct v4l2_frequency *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\tint ret;\n\n\tif (f->tuner == 0) {\n\t\tf->type = V4L2_TUNER_ADC;\n\t\tf->frequency = s->f_adc;\n\t\tdev_dbg(s->dev, \"ADC frequency=%u Hz\\n\", s->f_adc);\n\t\tret = 0;\n\t} else if (f->tuner == 1) {\n\t\tf->type = V4L2_TUNER_RF;\n\t\tf->frequency = s->f_rf;\n\t\tdev_dbg(s->dev, \"RF frequency=%u Hz\\n\", s->f_rf);\n\t\tret = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int airspy_s_frequency(struct file *file, void *priv,\n\t\tconst struct v4l2_frequency *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\tint ret;\n\tu8 buf[4];\n\n\tif (f->tuner == 0) {\n\t\ts->f_adc = clamp_t(unsigned int, f->frequency,\n\t\t\t\tbands[0].rangelow,\n\t\t\t\tbands[0].rangehigh);\n\t\tdev_dbg(s->dev, \"ADC frequency=%u Hz\\n\", s->f_adc);\n\t\tret = 0;\n\t} else if (f->tuner == 1) {\n\t\ts->f_rf = clamp_t(unsigned int, f->frequency,\n\t\t\t\tbands_rf[0].rangelow,\n\t\t\t\tbands_rf[0].rangehigh);\n\t\tdev_dbg(s->dev, \"RF frequency=%u Hz\\n\", s->f_rf);\n\t\tbuf[0] = (s->f_rf >>  0) & 0xff;\n\t\tbuf[1] = (s->f_rf >>  8) & 0xff;\n\t\tbuf[2] = (s->f_rf >> 16) & 0xff;\n\t\tbuf[3] = (s->f_rf >> 24) & 0xff;\n\t\tret = airspy_ctrl_msg(s, CMD_SET_FREQ, 0, 0, buf, 4);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int airspy_enum_freq_bands(struct file *file, void *priv,\n\t\tstruct v4l2_frequency_band *band)\n{\n\tint ret;\n\n\tif (band->tuner == 0) {\n\t\tif (band->index >= ARRAY_SIZE(bands)) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\t*band = bands[band->index];\n\t\t\tret = 0;\n\t\t}\n\t} else if (band->tuner == 1) {\n\t\tif (band->index >= ARRAY_SIZE(bands_rf)) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\t*band = bands_rf[band->index];\n\t\t\tret = 0;\n\t\t}\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct v4l2_ioctl_ops airspy_ioctl_ops = {\n\t.vidioc_querycap          = airspy_querycap,\n\n\t.vidioc_enum_fmt_sdr_cap  = airspy_enum_fmt_sdr_cap,\n\t.vidioc_g_fmt_sdr_cap     = airspy_g_fmt_sdr_cap,\n\t.vidioc_s_fmt_sdr_cap     = airspy_s_fmt_sdr_cap,\n\t.vidioc_try_fmt_sdr_cap   = airspy_try_fmt_sdr_cap,\n\n\t.vidioc_reqbufs           = vb2_ioctl_reqbufs,\n\t.vidioc_create_bufs       = vb2_ioctl_create_bufs,\n\t.vidioc_prepare_buf       = vb2_ioctl_prepare_buf,\n\t.vidioc_querybuf          = vb2_ioctl_querybuf,\n\t.vidioc_qbuf              = vb2_ioctl_qbuf,\n\t.vidioc_dqbuf             = vb2_ioctl_dqbuf,\n\n\t.vidioc_streamon          = vb2_ioctl_streamon,\n\t.vidioc_streamoff         = vb2_ioctl_streamoff,\n\n\t.vidioc_g_tuner           = airspy_g_tuner,\n\t.vidioc_s_tuner           = airspy_s_tuner,\n\n\t.vidioc_g_frequency       = airspy_g_frequency,\n\t.vidioc_s_frequency       = airspy_s_frequency,\n\t.vidioc_enum_freq_bands   = airspy_enum_freq_bands,\n\n\t.vidioc_subscribe_event   = v4l2_ctrl_subscribe_event,\n\t.vidioc_unsubscribe_event = v4l2_event_unsubscribe,\n\t.vidioc_log_status        = v4l2_ctrl_log_status,\n};\n\nstatic const struct v4l2_file_operations airspy_fops = {\n\t.owner                    = THIS_MODULE,\n\t.open                     = v4l2_fh_open,\n\t.release                  = vb2_fop_release,\n\t.read                     = vb2_fop_read,\n\t.poll                     = vb2_fop_poll,\n\t.mmap                     = vb2_fop_mmap,\n\t.unlocked_ioctl           = video_ioctl2,\n};\n\nstatic struct video_device airspy_template = {\n\t.name                     = \"AirSpy SDR\",\n\t.release                  = video_device_release_empty,\n\t.fops                     = &airspy_fops,\n\t.ioctl_ops                = &airspy_ioctl_ops,\n};\n\nstatic void airspy_video_release(struct v4l2_device *v)\n{\n\tstruct airspy *s = container_of(v, struct airspy, v4l2_dev);\n\n\tv4l2_ctrl_handler_free(&s->hdl);\n\tv4l2_device_unregister(&s->v4l2_dev);\n\tkfree(s);\n}\n\nstatic int airspy_set_lna_gain(struct airspy *s)\n{\n\tint ret;\n\tu8 u8tmp;\n\n\tdev_dbg(s->dev, \"lna auto=%d->%d val=%d->%d\\n\",\n\t\t\ts->lna_gain_auto->cur.val, s->lna_gain_auto->val,\n\t\t\ts->lna_gain->cur.val, s->lna_gain->val);\n\n\tret = airspy_ctrl_msg(s, CMD_SET_LNA_AGC, 0, s->lna_gain_auto->val,\n\t\t\t&u8tmp, 1);\n\tif (ret)\n\t\tgoto err;\n\n\tif (s->lna_gain_auto->val == false) {\n\t\tret = airspy_ctrl_msg(s, CMD_SET_LNA_GAIN, 0, s->lna_gain->val,\n\t\t\t\t&u8tmp, 1);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\nerr:\n\tif (ret)\n\t\tdev_dbg(s->dev, \"failed=%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int airspy_set_mixer_gain(struct airspy *s)\n{\n\tint ret;\n\tu8 u8tmp;\n\n\tdev_dbg(s->dev, \"mixer auto=%d->%d val=%d->%d\\n\",\n\t\t\ts->mixer_gain_auto->cur.val, s->mixer_gain_auto->val,\n\t\t\ts->mixer_gain->cur.val, s->mixer_gain->val);\n\n\tret = airspy_ctrl_msg(s, CMD_SET_MIXER_AGC, 0, s->mixer_gain_auto->val,\n\t\t\t&u8tmp, 1);\n\tif (ret)\n\t\tgoto err;\n\n\tif (s->mixer_gain_auto->val == false) {\n\t\tret = airspy_ctrl_msg(s, CMD_SET_MIXER_GAIN, 0,\n\t\t\t\ts->mixer_gain->val, &u8tmp, 1);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\nerr:\n\tif (ret)\n\t\tdev_dbg(s->dev, \"failed=%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int airspy_set_if_gain(struct airspy *s)\n{\n\tint ret;\n\tu8 u8tmp;\n\n\tdev_dbg(s->dev, \"val=%d->%d\\n\", s->if_gain->cur.val, s->if_gain->val);\n\n\tret = airspy_ctrl_msg(s, CMD_SET_VGA_GAIN, 0, s->if_gain->val,\n\t\t\t&u8tmp, 1);\n\tif (ret)\n\t\tdev_dbg(s->dev, \"failed=%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int airspy_s_ctrl(struct v4l2_ctrl *ctrl)\n{\n\tstruct airspy *s = container_of(ctrl->handler, struct airspy, hdl);\n\tint ret;\n\n\tswitch (ctrl->id) {\n\tcase  V4L2_CID_RF_TUNER_LNA_GAIN_AUTO:\n\tcase  V4L2_CID_RF_TUNER_LNA_GAIN:\n\t\tret = airspy_set_lna_gain(s);\n\t\tbreak;\n\tcase  V4L2_CID_RF_TUNER_MIXER_GAIN_AUTO:\n\tcase  V4L2_CID_RF_TUNER_MIXER_GAIN:\n\t\tret = airspy_set_mixer_gain(s);\n\t\tbreak;\n\tcase  V4L2_CID_RF_TUNER_IF_GAIN:\n\t\tret = airspy_set_if_gain(s);\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(s->dev, \"unknown ctrl: id=%d name=%s\\n\",\n\t\t\t\tctrl->id, ctrl->name);\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct v4l2_ctrl_ops airspy_ctrl_ops = {\n\t.s_ctrl = airspy_s_ctrl,\n};\n\nstatic int airspy_probe(struct usb_interface *intf,\n\t\tconst struct usb_device_id *id)\n{\n\tstruct airspy *s;\n\tint ret;\n\tu8 u8tmp, buf[BUF_SIZE];\n\n\ts = kzalloc(sizeof(struct airspy), GFP_KERNEL);\n\tif (s == NULL) {\n\t\tdev_err(&intf->dev, \"Could not allocate memory for state\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&s->v4l2_lock);\n\tmutex_init(&s->vb_queue_lock);\n\tspin_lock_init(&s->queued_bufs_lock);\n\tINIT_LIST_HEAD(&s->queued_bufs);\n\ts->dev = &intf->dev;\n\ts->udev = interface_to_usbdev(intf);\n\ts->f_adc = bands[0].rangelow;\n\ts->f_rf = bands_rf[0].rangelow;\n\ts->pixelformat = formats[0].pixelformat;\n\ts->buffersize = formats[0].buffersize;\n\n\t/* Detect device */\n\tret = airspy_ctrl_msg(s, CMD_BOARD_ID_READ, 0, 0, &u8tmp, 1);\n\tif (ret == 0)\n\t\tret = airspy_ctrl_msg(s, CMD_VERSION_STRING_READ, 0, 0,\n\t\t\t\tbuf, BUF_SIZE);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Could not detect board\\n\");\n\t\tgoto err_free_mem;\n\t}\n\n\tbuf[BUF_SIZE - 1] = '\\0';\n\n\tdev_info(s->dev, \"Board ID: %02x\\n\", u8tmp);\n\tdev_info(s->dev, \"Firmware version: %s\\n\", buf);\n\n\t/* Init videobuf2 queue structure */\n\ts->vb_queue.type = V4L2_BUF_TYPE_SDR_CAPTURE;\n\ts->vb_queue.io_modes = VB2_MMAP | VB2_USERPTR | VB2_READ;\n\ts->vb_queue.drv_priv = s;\n\ts->vb_queue.buf_struct_size = sizeof(struct airspy_frame_buf);\n\ts->vb_queue.ops = &airspy_vb2_ops;\n\ts->vb_queue.mem_ops = &vb2_vmalloc_memops;\n\ts->vb_queue.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;\n\tret = vb2_queue_init(&s->vb_queue);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Could not initialize vb2 queue\\n\");\n\t\tgoto err_free_mem;\n\t}\n\n\t/* Init video_device structure */\n\ts->vdev = airspy_template;\n\ts->vdev.queue = &s->vb_queue;\n\ts->vdev.queue->lock = &s->vb_queue_lock;\n\tvideo_set_drvdata(&s->vdev, s);\n\n\t/* Register the v4l2_device structure */\n\ts->v4l2_dev.release = airspy_video_release;\n\tret = v4l2_device_register(&intf->dev, &s->v4l2_dev);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Failed to register v4l2-device (%d)\\n\", ret);\n\t\tgoto err_free_mem;\n\t}\n\n\t/* Register controls */\n\tv4l2_ctrl_handler_init(&s->hdl, 5);\n\ts->lna_gain_auto = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_LNA_GAIN_AUTO, 0, 1, 1, 0);\n\ts->lna_gain = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_LNA_GAIN, 0, 14, 1, 8);\n\tv4l2_ctrl_auto_cluster(2, &s->lna_gain_auto, 0, false);\n\ts->mixer_gain_auto = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_MIXER_GAIN_AUTO, 0, 1, 1, 0);\n\ts->mixer_gain = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_MIXER_GAIN, 0, 15, 1, 8);\n\tv4l2_ctrl_auto_cluster(2, &s->mixer_gain_auto, 0, false);\n\ts->if_gain = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_IF_GAIN, 0, 15, 1, 0);\n\tif (s->hdl.error) {\n\t\tret = s->hdl.error;\n\t\tdev_err(s->dev, \"Could not initialize controls\\n\");\n\t\tgoto err_free_controls;\n\t}\n\n\tv4l2_ctrl_handler_setup(&s->hdl);\n\n\ts->v4l2_dev.ctrl_handler = &s->hdl;\n\ts->vdev.v4l2_dev = &s->v4l2_dev;\n\ts->vdev.lock = &s->v4l2_lock;\n\n\tret = video_register_device(&s->vdev, VFL_TYPE_SDR, -1);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Failed to register as video device (%d)\\n\",\n\t\t\t\tret);\n\t\tgoto err_free_controls;\n\t}\n\tdev_info(s->dev, \"Registered as %s\\n\",\n\t\t\tvideo_device_node_name(&s->vdev));\n\tdev_notice(s->dev, \"SDR API is still slightly experimental and functionality changes may follow\\n\");\n\treturn 0;\n\nerr_free_controls:\n\tv4l2_ctrl_handler_free(&s->hdl);\n\tv4l2_device_unregister(&s->v4l2_dev);\nerr_free_mem:\n\tkfree(s);\n\treturn ret;\n}\n\n/* USB device ID list */\nstatic struct usb_device_id airspy_id_table[] = {\n\t{ USB_DEVICE(0x1d50, 0x60a1) }, /* AirSpy */\n\t{ }\n};\nMODULE_DEVICE_TABLE(usb, airspy_id_table);\n\n/* USB subsystem interface */\nstatic struct usb_driver airspy_driver = {\n\t.name                     = KBUILD_MODNAME,\n\t.probe                    = airspy_probe,\n\t.disconnect               = airspy_disconnect,\n\t.id_table                 = airspy_id_table,\n};\n\nmodule_usb_driver(airspy_driver);\n\nMODULE_AUTHOR(\"Antti Palosaari <crope@iki.fi>\");\nMODULE_DESCRIPTION(\"AirSpy SDR\");\nMODULE_LICENSE(\"GPL\");\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * AirSpy SDR driver\n *\n * Copyright (C) 2014 Antti Palosaari <crope@iki.fi>\n */\n\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/usb.h>\n#include <media/v4l2-device.h>\n#include <media/v4l2-ioctl.h>\n#include <media/v4l2-ctrls.h>\n#include <media/v4l2-event.h>\n#include <media/videobuf2-v4l2.h>\n#include <media/videobuf2-vmalloc.h>\n\n/* AirSpy USB API commands (from AirSpy Library) */\nenum {\n\tCMD_INVALID                       = 0x00,\n\tCMD_RECEIVER_MODE                 = 0x01,\n\tCMD_SI5351C_WRITE                 = 0x02,\n\tCMD_SI5351C_READ                  = 0x03,\n\tCMD_R820T_WRITE                   = 0x04,\n\tCMD_R820T_READ                    = 0x05,\n\tCMD_SPIFLASH_ERASE                = 0x06,\n\tCMD_SPIFLASH_WRITE                = 0x07,\n\tCMD_SPIFLASH_READ                 = 0x08,\n\tCMD_BOARD_ID_READ                 = 0x09,\n\tCMD_VERSION_STRING_READ           = 0x0a,\n\tCMD_BOARD_PARTID_SERIALNO_READ    = 0x0b,\n\tCMD_SET_SAMPLE_RATE               = 0x0c,\n\tCMD_SET_FREQ                      = 0x0d,\n\tCMD_SET_LNA_GAIN                  = 0x0e,\n\tCMD_SET_MIXER_GAIN                = 0x0f,\n\tCMD_SET_VGA_GAIN                  = 0x10,\n\tCMD_SET_LNA_AGC                   = 0x11,\n\tCMD_SET_MIXER_AGC                 = 0x12,\n\tCMD_SET_PACKING                   = 0x13,\n};\n\n/*\n *       bEndpointAddress     0x81  EP 1 IN\n *         Transfer Type            Bulk\n *       wMaxPacketSize     0x0200  1x 512 bytes\n */\n#define MAX_BULK_BUFS            (6)\n#define BULK_BUFFER_SIZE         (128 * 512)\n\nstatic const struct v4l2_frequency_band bands[] = {\n\t{\n\t\t.tuner = 0,\n\t\t.type = V4L2_TUNER_ADC,\n\t\t.index = 0,\n\t\t.capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS,\n\t\t.rangelow   = 20000000,\n\t\t.rangehigh  = 20000000,\n\t},\n};\n\nstatic const struct v4l2_frequency_band bands_rf[] = {\n\t{\n\t\t.tuner = 1,\n\t\t.type = V4L2_TUNER_RF,\n\t\t.index = 0,\n\t\t.capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS,\n\t\t.rangelow   =   24000000,\n\t\t.rangehigh  = 1750000000,\n\t},\n};\n\n/* stream formats */\nstruct airspy_format {\n\tu32\tpixelformat;\n\tu32\tbuffersize;\n};\n\n/* format descriptions for capture and preview */\nstatic struct airspy_format formats[] = {\n\t{\n\t\t.pixelformat\t= V4L2_SDR_FMT_RU12LE,\n\t\t.buffersize\t= BULK_BUFFER_SIZE,\n\t},\n};\n\nstatic const unsigned int NUM_FORMATS = ARRAY_SIZE(formats);\n\n/* intermediate buffers with raw data from the USB device */\nstruct airspy_frame_buf {\n\t/* common v4l buffer stuff -- must be first */\n\tstruct vb2_v4l2_buffer vb;\n\tstruct list_head list;\n};\n\nstruct airspy {\n#define POWER_ON\t   1\n#define USB_STATE_URB_BUF  2\n\tunsigned long flags;\n\n\tstruct device *dev;\n\tstruct usb_device *udev;\n\tstruct video_device vdev;\n\tstruct v4l2_device v4l2_dev;\n\n\t/* videobuf2 queue and queued buffers list */\n\tstruct vb2_queue vb_queue;\n\tstruct list_head queued_bufs;\n\tspinlock_t queued_bufs_lock; /* Protects queued_bufs */\n\tunsigned sequence;\t     /* Buffer sequence counter */\n\tunsigned int vb_full;        /* vb is full and packets dropped */\n\n\t/* Note if taking both locks v4l2_lock must always be locked first! */\n\tstruct mutex v4l2_lock;      /* Protects everything else */\n\tstruct mutex vb_queue_lock;  /* Protects vb_queue and capt_file */\n\n\tstruct urb     *urb_list[MAX_BULK_BUFS];\n\tint            buf_num;\n\tunsigned long  buf_size;\n\tu8             *buf_list[MAX_BULK_BUFS];\n\tdma_addr_t     dma_addr[MAX_BULK_BUFS];\n\tint            urbs_initialized;\n\tint            urbs_submitted;\n\n\t/* USB control message buffer */\n\t#define BUF_SIZE 128\n\tu8 *buf;\n\n\t/* Current configuration */\n\tunsigned int f_adc;\n\tunsigned int f_rf;\n\tu32 pixelformat;\n\tu32 buffersize;\n\n\t/* Controls */\n\tstruct v4l2_ctrl_handler hdl;\n\tstruct v4l2_ctrl *lna_gain_auto;\n\tstruct v4l2_ctrl *lna_gain;\n\tstruct v4l2_ctrl *mixer_gain_auto;\n\tstruct v4l2_ctrl *mixer_gain;\n\tstruct v4l2_ctrl *if_gain;\n\n\t/* Sample rate calc */\n\tunsigned long jiffies_next;\n\tunsigned int sample;\n\tunsigned int sample_measured;\n};\n\n#define airspy_dbg_usb_control_msg(_dev, _r, _t, _v, _i, _b, _l) { \\\n\tchar *_direction; \\\n\tif (_t & USB_DIR_IN) \\\n\t\t_direction = \"<<<\"; \\\n\telse \\\n\t\t_direction = \">>>\"; \\\n\tdev_dbg(_dev, \"%02x %02x %02x %02x %02x %02x %02x %02x %s %*ph\\n\", \\\n\t\t\t_t, _r, _v & 0xff, _v >> 8, _i & 0xff, _i >> 8, \\\n\t\t\t_l & 0xff, _l >> 8, _direction, _l, _b); \\\n}\n\n/* execute firmware command */\nstatic int airspy_ctrl_msg(struct airspy *s, u8 request, u16 value, u16 index,\n\t\tu8 *data, u16 size)\n{\n\tint ret;\n\tunsigned int pipe;\n\tu8 requesttype;\n\n\tswitch (request) {\n\tcase CMD_RECEIVER_MODE:\n\tcase CMD_SET_FREQ:\n\t\tpipe = usb_sndctrlpipe(s->udev, 0);\n\t\trequesttype = (USB_TYPE_VENDOR | USB_DIR_OUT);\n\t\tbreak;\n\tcase CMD_BOARD_ID_READ:\n\tcase CMD_VERSION_STRING_READ:\n\tcase CMD_BOARD_PARTID_SERIALNO_READ:\n\tcase CMD_SET_LNA_GAIN:\n\tcase CMD_SET_MIXER_GAIN:\n\tcase CMD_SET_VGA_GAIN:\n\tcase CMD_SET_LNA_AGC:\n\tcase CMD_SET_MIXER_AGC:\n\t\tpipe = usb_rcvctrlpipe(s->udev, 0);\n\t\trequesttype = (USB_TYPE_VENDOR | USB_DIR_IN);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(s->dev, \"Unknown command %02x\\n\", request);\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/* write request */\n\tif (!(requesttype & USB_DIR_IN))\n\t\tmemcpy(s->buf, data, size);\n\n\tret = usb_control_msg(s->udev, pipe, request, requesttype, value,\n\t\t\tindex, s->buf, size, 1000);\n\tairspy_dbg_usb_control_msg(s->dev, request, requesttype, value,\n\t\t\tindex, s->buf, size);\n\tif (ret < 0) {\n\t\tdev_err(s->dev, \"usb_control_msg() failed %d request %02x\\n\",\n\t\t\t\tret, request);\n\t\tgoto err;\n\t}\n\n\t/* read request */\n\tif (requesttype & USB_DIR_IN)\n\t\tmemcpy(data, s->buf, size);\n\n\treturn 0;\nerr:\n\treturn ret;\n}\n\n/* Private functions */\nstatic struct airspy_frame_buf *airspy_get_next_fill_buf(struct airspy *s)\n{\n\tunsigned long flags;\n\tstruct airspy_frame_buf *buf = NULL;\n\n\tspin_lock_irqsave(&s->queued_bufs_lock, flags);\n\tif (list_empty(&s->queued_bufs))\n\t\tgoto leave;\n\n\tbuf = list_entry(s->queued_bufs.next,\n\t\t\tstruct airspy_frame_buf, list);\n\tlist_del(&buf->list);\nleave:\n\tspin_unlock_irqrestore(&s->queued_bufs_lock, flags);\n\treturn buf;\n}\n\nstatic unsigned int airspy_convert_stream(struct airspy *s,\n\t\tvoid *dst, void *src, unsigned int src_len)\n{\n\tunsigned int dst_len;\n\n\tif (s->pixelformat == V4L2_SDR_FMT_RU12LE) {\n\t\tmemcpy(dst, src, src_len);\n\t\tdst_len = src_len;\n\t} else {\n\t\tdst_len = 0;\n\t}\n\n\t/* calculate sample rate and output it in 10 seconds intervals */\n\tif (unlikely(time_is_before_jiffies(s->jiffies_next))) {\n\t\t#define MSECS 10000UL\n\t\tunsigned int msecs = jiffies_to_msecs(jiffies -\n\t\t\t\ts->jiffies_next + msecs_to_jiffies(MSECS));\n\t\tunsigned int samples = s->sample - s->sample_measured;\n\n\t\ts->jiffies_next = jiffies + msecs_to_jiffies(MSECS);\n\t\ts->sample_measured = s->sample;\n\t\tdev_dbg(s->dev, \"slen=%u samples=%u msecs=%u sample rate=%lu\\n\",\n\t\t\t\tsrc_len, samples, msecs,\n\t\t\t\tsamples * 1000UL / msecs);\n\t}\n\n\t/* total number of samples */\n\ts->sample += src_len / 2;\n\n\treturn dst_len;\n}\n\n/*\n * This gets called for the bulk stream pipe. This is done in interrupt\n * time, so it has to be fast, not crash, and not stall. Neat.\n */\nstatic void airspy_urb_complete(struct urb *urb)\n{\n\tstruct airspy *s = urb->context;\n\tstruct airspy_frame_buf *fbuf;\n\n\tdev_dbg_ratelimited(s->dev, \"status=%d length=%d/%d errors=%d\\n\",\n\t\t\turb->status, urb->actual_length,\n\t\t\turb->transfer_buffer_length, urb->error_count);\n\n\tswitch (urb->status) {\n\tcase 0:             /* success */\n\tcase -ETIMEDOUT:    /* NAK */\n\t\tbreak;\n\tcase -ECONNRESET:   /* kill */\n\tcase -ENOENT:\n\tcase -ESHUTDOWN:\n\t\treturn;\n\tdefault:            /* error */\n\t\tdev_err_ratelimited(s->dev, \"URB failed %d\\n\", urb->status);\n\t\tbreak;\n\t}\n\n\tif (likely(urb->actual_length > 0)) {\n\t\tvoid *ptr;\n\t\tunsigned int len;\n\t\t/* get free framebuffer */\n\t\tfbuf = airspy_get_next_fill_buf(s);\n\t\tif (unlikely(fbuf == NULL)) {\n\t\t\ts->vb_full++;\n\t\t\tdev_notice_ratelimited(s->dev,\n\t\t\t\t\t\"video buffer is full, %d packets dropped\\n\",\n\t\t\t\t\ts->vb_full);\n\t\t\tgoto skip;\n\t\t}\n\n\t\t/* fill framebuffer */\n\t\tptr = vb2_plane_vaddr(&fbuf->vb.vb2_buf, 0);\n\t\tlen = airspy_convert_stream(s, ptr, urb->transfer_buffer,\n\t\t\t\turb->actual_length);\n\t\tvb2_set_plane_payload(&fbuf->vb.vb2_buf, 0, len);\n\t\tfbuf->vb.vb2_buf.timestamp = ktime_get_ns();\n\t\tfbuf->vb.sequence = s->sequence++;\n\t\tvb2_buffer_done(&fbuf->vb.vb2_buf, VB2_BUF_STATE_DONE);\n\t}\nskip:\n\tusb_submit_urb(urb, GFP_ATOMIC);\n}\n\nstatic int airspy_kill_urbs(struct airspy *s)\n{\n\tint i;\n\n\tfor (i = s->urbs_submitted - 1; i >= 0; i--) {\n\t\tdev_dbg(s->dev, \"kill urb=%d\\n\", i);\n\t\t/* stop the URB */\n\t\tusb_kill_urb(s->urb_list[i]);\n\t}\n\ts->urbs_submitted = 0;\n\n\treturn 0;\n}\n\nstatic int airspy_submit_urbs(struct airspy *s)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < s->urbs_initialized; i++) {\n\t\tdev_dbg(s->dev, \"submit urb=%d\\n\", i);\n\t\tret = usb_submit_urb(s->urb_list[i], GFP_ATOMIC);\n\t\tif (ret) {\n\t\t\tdev_err(s->dev, \"Could not submit URB no. %d - get them all back\\n\",\n\t\t\t\t\ti);\n\t\t\tairspy_kill_urbs(s);\n\t\t\treturn ret;\n\t\t}\n\t\ts->urbs_submitted++;\n\t}\n\n\treturn 0;\n}\n\nstatic int airspy_free_stream_bufs(struct airspy *s)\n{\n\tif (test_bit(USB_STATE_URB_BUF, &s->flags)) {\n\t\twhile (s->buf_num) {\n\t\t\ts->buf_num--;\n\t\t\tdev_dbg(s->dev, \"free buf=%d\\n\", s->buf_num);\n\t\t\tusb_free_coherent(s->udev, s->buf_size,\n\t\t\t\t\t  s->buf_list[s->buf_num],\n\t\t\t\t\t  s->dma_addr[s->buf_num]);\n\t\t}\n\t}\n\tclear_bit(USB_STATE_URB_BUF, &s->flags);\n\n\treturn 0;\n}\n\nstatic int airspy_alloc_stream_bufs(struct airspy *s)\n{\n\ts->buf_num = 0;\n\ts->buf_size = BULK_BUFFER_SIZE;\n\n\tdev_dbg(s->dev, \"all in all I will use %u bytes for streaming\\n\",\n\t\t\tMAX_BULK_BUFS * BULK_BUFFER_SIZE);\n\n\tfor (s->buf_num = 0; s->buf_num < MAX_BULK_BUFS; s->buf_num++) {\n\t\ts->buf_list[s->buf_num] = usb_alloc_coherent(s->udev,\n\t\t\t\tBULK_BUFFER_SIZE, GFP_ATOMIC,\n\t\t\t\t&s->dma_addr[s->buf_num]);\n\t\tif (!s->buf_list[s->buf_num]) {\n\t\t\tdev_dbg(s->dev, \"alloc buf=%d failed\\n\", s->buf_num);\n\t\t\tairspy_free_stream_bufs(s);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdev_dbg(s->dev, \"alloc buf=%d %p (dma %llu)\\n\", s->buf_num,\n\t\t\t\ts->buf_list[s->buf_num],\n\t\t\t\t(long long)s->dma_addr[s->buf_num]);\n\t\tset_bit(USB_STATE_URB_BUF, &s->flags);\n\t}\n\n\treturn 0;\n}\n\nstatic int airspy_free_urbs(struct airspy *s)\n{\n\tint i;\n\n\tairspy_kill_urbs(s);\n\n\tfor (i = s->urbs_initialized - 1; i >= 0; i--) {\n\t\tif (s->urb_list[i]) {\n\t\t\tdev_dbg(s->dev, \"free urb=%d\\n\", i);\n\t\t\t/* free the URBs */\n\t\t\tusb_free_urb(s->urb_list[i]);\n\t\t}\n\t}\n\ts->urbs_initialized = 0;\n\n\treturn 0;\n}\n\nstatic int airspy_alloc_urbs(struct airspy *s)\n{\n\tint i, j;\n\n\t/* allocate the URBs */\n\tfor (i = 0; i < MAX_BULK_BUFS; i++) {\n\t\tdev_dbg(s->dev, \"alloc urb=%d\\n\", i);\n\t\ts->urb_list[i] = usb_alloc_urb(0, GFP_ATOMIC);\n\t\tif (!s->urb_list[i]) {\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\tusb_free_urb(s->urb_list[j]);\n\t\t\t\ts->urb_list[j] = NULL;\n\t\t\t}\n\t\t\ts->urbs_initialized = 0;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tusb_fill_bulk_urb(s->urb_list[i],\n\t\t\t\ts->udev,\n\t\t\t\tusb_rcvbulkpipe(s->udev, 0x81),\n\t\t\t\ts->buf_list[i],\n\t\t\t\tBULK_BUFFER_SIZE,\n\t\t\t\tairspy_urb_complete, s);\n\n\t\ts->urb_list[i]->transfer_flags = URB_NO_TRANSFER_DMA_MAP;\n\t\ts->urb_list[i]->transfer_dma = s->dma_addr[i];\n\t\ts->urbs_initialized++;\n\t}\n\n\treturn 0;\n}\n\n/* Must be called with vb_queue_lock hold */\nstatic void airspy_cleanup_queued_bufs(struct airspy *s)\n{\n\tunsigned long flags;\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tspin_lock_irqsave(&s->queued_bufs_lock, flags);\n\twhile (!list_empty(&s->queued_bufs)) {\n\t\tstruct airspy_frame_buf *buf;\n\n\t\tbuf = list_entry(s->queued_bufs.next,\n\t\t\t\tstruct airspy_frame_buf, list);\n\t\tlist_del(&buf->list);\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t}\n\tspin_unlock_irqrestore(&s->queued_bufs_lock, flags);\n}\n\n/* The user yanked out the cable... */\nstatic void airspy_disconnect(struct usb_interface *intf)\n{\n\tstruct v4l2_device *v = usb_get_intfdata(intf);\n\tstruct airspy *s = container_of(v, struct airspy, v4l2_dev);\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tmutex_lock(&s->vb_queue_lock);\n\tmutex_lock(&s->v4l2_lock);\n\t/* No need to keep the urbs around after disconnection */\n\ts->udev = NULL;\n\tv4l2_device_disconnect(&s->v4l2_dev);\n\tvideo_unregister_device(&s->vdev);\n\tmutex_unlock(&s->v4l2_lock);\n\tmutex_unlock(&s->vb_queue_lock);\n\n\tv4l2_device_put(&s->v4l2_dev);\n}\n\n/* Videobuf2 operations */\nstatic int airspy_queue_setup(struct vb2_queue *vq,\n\t\tunsigned int *nbuffers,\n\t\tunsigned int *nplanes, unsigned int sizes[], struct device *alloc_devs[])\n{\n\tstruct airspy *s = vb2_get_drv_priv(vq);\n\tunsigned int q_num_bufs = vb2_get_num_buffers(vq);\n\n\tdev_dbg(s->dev, \"nbuffers=%d\\n\", *nbuffers);\n\n\t/* Need at least 8 buffers */\n\tif (q_num_bufs + *nbuffers < 8)\n\t\t*nbuffers = 8 - q_num_bufs;\n\t*nplanes = 1;\n\tsizes[0] = PAGE_ALIGN(s->buffersize);\n\n\tdev_dbg(s->dev, \"nbuffers=%d sizes[0]=%d\\n\", *nbuffers, sizes[0]);\n\treturn 0;\n}\n\nstatic void airspy_buf_queue(struct vb2_buffer *vb)\n{\n\tstruct vb2_v4l2_buffer *vbuf = to_vb2_v4l2_buffer(vb);\n\tstruct airspy *s = vb2_get_drv_priv(vb->vb2_queue);\n\tstruct airspy_frame_buf *buf =\n\t\t\tcontainer_of(vbuf, struct airspy_frame_buf, vb);\n\tunsigned long flags;\n\n\t/* Check the device has not disconnected between prep and queuing */\n\tif (unlikely(!s->udev)) {\n\t\tvb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&s->queued_bufs_lock, flags);\n\tlist_add_tail(&buf->list, &s->queued_bufs);\n\tspin_unlock_irqrestore(&s->queued_bufs_lock, flags);\n}\n\nstatic int airspy_start_streaming(struct vb2_queue *vq, unsigned int count)\n{\n\tstruct airspy *s = vb2_get_drv_priv(vq);\n\tint ret;\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tif (!s->udev)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&s->v4l2_lock);\n\n\ts->sequence = 0;\n\n\tset_bit(POWER_ON, &s->flags);\n\n\tret = airspy_alloc_stream_bufs(s);\n\tif (ret)\n\t\tgoto err_clear_bit;\n\n\tret = airspy_alloc_urbs(s);\n\tif (ret)\n\t\tgoto err_free_stream_bufs;\n\n\tret = airspy_submit_urbs(s);\n\tif (ret)\n\t\tgoto err_free_urbs;\n\n\t/* start hardware streaming */\n\tret = airspy_ctrl_msg(s, CMD_RECEIVER_MODE, 1, 0, NULL, 0);\n\tif (ret)\n\t\tgoto err_kill_urbs;\n\n\tgoto exit_mutex_unlock;\n\nerr_kill_urbs:\n\tairspy_kill_urbs(s);\nerr_free_urbs:\n\tairspy_free_urbs(s);\nerr_free_stream_bufs:\n\tairspy_free_stream_bufs(s);\nerr_clear_bit:\n\tclear_bit(POWER_ON, &s->flags);\n\n\t/* return all queued buffers to vb2 */\n\t{\n\t\tstruct airspy_frame_buf *buf, *tmp;\n\n\t\tlist_for_each_entry_safe(buf, tmp, &s->queued_bufs, list) {\n\t\t\tlist_del(&buf->list);\n\t\t\tvb2_buffer_done(&buf->vb.vb2_buf,\n\t\t\t\t\tVB2_BUF_STATE_QUEUED);\n\t\t}\n\t}\n\nexit_mutex_unlock:\n\tmutex_unlock(&s->v4l2_lock);\n\n\treturn ret;\n}\n\nstatic void airspy_stop_streaming(struct vb2_queue *vq)\n{\n\tstruct airspy *s = vb2_get_drv_priv(vq);\n\n\tdev_dbg(s->dev, \"\\n\");\n\n\tmutex_lock(&s->v4l2_lock);\n\n\t/* stop hardware streaming */\n\tairspy_ctrl_msg(s, CMD_RECEIVER_MODE, 0, 0, NULL, 0);\n\n\tairspy_kill_urbs(s);\n\tairspy_free_urbs(s);\n\tairspy_free_stream_bufs(s);\n\n\tairspy_cleanup_queued_bufs(s);\n\n\tclear_bit(POWER_ON, &s->flags);\n\n\tmutex_unlock(&s->v4l2_lock);\n}\n\nstatic const struct vb2_ops airspy_vb2_ops = {\n\t.queue_setup            = airspy_queue_setup,\n\t.buf_queue              = airspy_buf_queue,\n\t.start_streaming        = airspy_start_streaming,\n\t.stop_streaming         = airspy_stop_streaming,\n\t.wait_prepare           = vb2_ops_wait_prepare,\n\t.wait_finish            = vb2_ops_wait_finish,\n};\n\nstatic int airspy_querycap(struct file *file, void *fh,\n\t\tstruct v4l2_capability *cap)\n{\n\tstruct airspy *s = video_drvdata(file);\n\n\tstrscpy(cap->driver, KBUILD_MODNAME, sizeof(cap->driver));\n\tstrscpy(cap->card, s->vdev.name, sizeof(cap->card));\n\tusb_make_path(s->udev, cap->bus_info, sizeof(cap->bus_info));\n\treturn 0;\n}\n\nstatic int airspy_enum_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_fmtdesc *f)\n{\n\tif (f->index >= NUM_FORMATS)\n\t\treturn -EINVAL;\n\n\tf->pixelformat = formats[f->index].pixelformat;\n\n\treturn 0;\n}\n\nstatic int airspy_g_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_format *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\n\tf->fmt.sdr.pixelformat = s->pixelformat;\n\tf->fmt.sdr.buffersize = s->buffersize;\n\n\treturn 0;\n}\n\nstatic int airspy_s_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_format *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\tstruct vb2_queue *q = &s->vb_queue;\n\tint i;\n\n\tif (vb2_is_busy(q))\n\t\treturn -EBUSY;\n\n\tfor (i = 0; i < NUM_FORMATS; i++) {\n\t\tif (formats[i].pixelformat == f->fmt.sdr.pixelformat) {\n\t\t\ts->pixelformat = formats[i].pixelformat;\n\t\t\ts->buffersize = formats[i].buffersize;\n\t\t\tf->fmt.sdr.buffersize = formats[i].buffersize;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\ts->pixelformat = formats[0].pixelformat;\n\ts->buffersize = formats[0].buffersize;\n\tf->fmt.sdr.pixelformat = formats[0].pixelformat;\n\tf->fmt.sdr.buffersize = formats[0].buffersize;\n\n\treturn 0;\n}\n\nstatic int airspy_try_fmt_sdr_cap(struct file *file, void *priv,\n\t\tstruct v4l2_format *f)\n{\n\tint i;\n\n\tfor (i = 0; i < NUM_FORMATS; i++) {\n\t\tif (formats[i].pixelformat == f->fmt.sdr.pixelformat) {\n\t\t\tf->fmt.sdr.buffersize = formats[i].buffersize;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tf->fmt.sdr.pixelformat = formats[0].pixelformat;\n\tf->fmt.sdr.buffersize = formats[0].buffersize;\n\n\treturn 0;\n}\n\nstatic int airspy_s_tuner(struct file *file, void *priv,\n\t\tconst struct v4l2_tuner *v)\n{\n\tint ret;\n\n\tif (v->index == 0)\n\t\tret = 0;\n\telse if (v->index == 1)\n\t\tret = 0;\n\telse\n\t\tret = -EINVAL;\n\n\treturn ret;\n}\n\nstatic int airspy_g_tuner(struct file *file, void *priv, struct v4l2_tuner *v)\n{\n\tint ret;\n\n\tif (v->index == 0) {\n\t\tstrscpy(v->name, \"AirSpy ADC\", sizeof(v->name));\n\t\tv->type = V4L2_TUNER_ADC;\n\t\tv->capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS;\n\t\tv->rangelow  = bands[0].rangelow;\n\t\tv->rangehigh = bands[0].rangehigh;\n\t\tret = 0;\n\t} else if (v->index == 1) {\n\t\tstrscpy(v->name, \"AirSpy RF\", sizeof(v->name));\n\t\tv->type = V4L2_TUNER_RF;\n\t\tv->capability = V4L2_TUNER_CAP_1HZ | V4L2_TUNER_CAP_FREQ_BANDS;\n\t\tv->rangelow  = bands_rf[0].rangelow;\n\t\tv->rangehigh = bands_rf[0].rangehigh;\n\t\tret = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int airspy_g_frequency(struct file *file, void *priv,\n\t\tstruct v4l2_frequency *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\tint ret;\n\n\tif (f->tuner == 0) {\n\t\tf->type = V4L2_TUNER_ADC;\n\t\tf->frequency = s->f_adc;\n\t\tdev_dbg(s->dev, \"ADC frequency=%u Hz\\n\", s->f_adc);\n\t\tret = 0;\n\t} else if (f->tuner == 1) {\n\t\tf->type = V4L2_TUNER_RF;\n\t\tf->frequency = s->f_rf;\n\t\tdev_dbg(s->dev, \"RF frequency=%u Hz\\n\", s->f_rf);\n\t\tret = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int airspy_s_frequency(struct file *file, void *priv,\n\t\tconst struct v4l2_frequency *f)\n{\n\tstruct airspy *s = video_drvdata(file);\n\tint ret;\n\tu8 buf[4];\n\n\tif (f->tuner == 0) {\n\t\ts->f_adc = clamp_t(unsigned int, f->frequency,\n\t\t\t\tbands[0].rangelow,\n\t\t\t\tbands[0].rangehigh);\n\t\tdev_dbg(s->dev, \"ADC frequency=%u Hz\\n\", s->f_adc);\n\t\tret = 0;\n\t} else if (f->tuner == 1) {\n\t\ts->f_rf = clamp_t(unsigned int, f->frequency,\n\t\t\t\tbands_rf[0].rangelow,\n\t\t\t\tbands_rf[0].rangehigh);\n\t\tdev_dbg(s->dev, \"RF frequency=%u Hz\\n\", s->f_rf);\n\t\tbuf[0] = (s->f_rf >>  0) & 0xff;\n\t\tbuf[1] = (s->f_rf >>  8) & 0xff;\n\t\tbuf[2] = (s->f_rf >> 16) & 0xff;\n\t\tbuf[3] = (s->f_rf >> 24) & 0xff;\n\t\tret = airspy_ctrl_msg(s, CMD_SET_FREQ, 0, 0, buf, 4);\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic int airspy_enum_freq_bands(struct file *file, void *priv,\n\t\tstruct v4l2_frequency_band *band)\n{\n\tint ret;\n\n\tif (band->tuner == 0) {\n\t\tif (band->index >= ARRAY_SIZE(bands)) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\t*band = bands[band->index];\n\t\t\tret = 0;\n\t\t}\n\t} else if (band->tuner == 1) {\n\t\tif (band->index >= ARRAY_SIZE(bands_rf)) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\t*band = bands_rf[band->index];\n\t\t\tret = 0;\n\t\t}\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct v4l2_ioctl_ops airspy_ioctl_ops = {\n\t.vidioc_querycap          = airspy_querycap,\n\n\t.vidioc_enum_fmt_sdr_cap  = airspy_enum_fmt_sdr_cap,\n\t.vidioc_g_fmt_sdr_cap     = airspy_g_fmt_sdr_cap,\n\t.vidioc_s_fmt_sdr_cap     = airspy_s_fmt_sdr_cap,\n\t.vidioc_try_fmt_sdr_cap   = airspy_try_fmt_sdr_cap,\n\n\t.vidioc_reqbufs           = vb2_ioctl_reqbufs,\n\t.vidioc_create_bufs       = vb2_ioctl_create_bufs,\n\t.vidioc_prepare_buf       = vb2_ioctl_prepare_buf,\n\t.vidioc_querybuf          = vb2_ioctl_querybuf,\n\t.vidioc_qbuf              = vb2_ioctl_qbuf,\n\t.vidioc_dqbuf             = vb2_ioctl_dqbuf,\n\n\t.vidioc_streamon          = vb2_ioctl_streamon,\n\t.vidioc_streamoff         = vb2_ioctl_streamoff,\n\n\t.vidioc_g_tuner           = airspy_g_tuner,\n\t.vidioc_s_tuner           = airspy_s_tuner,\n\n\t.vidioc_g_frequency       = airspy_g_frequency,\n\t.vidioc_s_frequency       = airspy_s_frequency,\n\t.vidioc_enum_freq_bands   = airspy_enum_freq_bands,\n\n\t.vidioc_subscribe_event   = v4l2_ctrl_subscribe_event,\n\t.vidioc_unsubscribe_event = v4l2_event_unsubscribe,\n\t.vidioc_log_status        = v4l2_ctrl_log_status,\n};\n\nstatic const struct v4l2_file_operations airspy_fops = {\n\t.owner                    = THIS_MODULE,\n\t.open                     = v4l2_fh_open,\n\t.release                  = vb2_fop_release,\n\t.read                     = vb2_fop_read,\n\t.poll                     = vb2_fop_poll,\n\t.mmap                     = vb2_fop_mmap,\n\t.unlocked_ioctl           = video_ioctl2,\n};\n\nstatic const struct video_device airspy_template = {\n\t.name                     = \"AirSpy SDR\",\n\t.release                  = video_device_release_empty,\n\t.fops                     = &airspy_fops,\n\t.ioctl_ops                = &airspy_ioctl_ops,\n};\n\nstatic void airspy_video_release(struct v4l2_device *v)\n{\n\tstruct airspy *s = container_of(v, struct airspy, v4l2_dev);\n\n\tv4l2_ctrl_handler_free(&s->hdl);\n\tv4l2_device_unregister(&s->v4l2_dev);\n\tkfree(s->buf);\n\tkfree(s);\n}\n\nstatic int airspy_set_lna_gain(struct airspy *s)\n{\n\tint ret;\n\tu8 u8tmp;\n\n\tdev_dbg(s->dev, \"lna auto=%d->%d val=%d->%d\\n\",\n\t\t\ts->lna_gain_auto->cur.val, s->lna_gain_auto->val,\n\t\t\ts->lna_gain->cur.val, s->lna_gain->val);\n\n\tret = airspy_ctrl_msg(s, CMD_SET_LNA_AGC, 0, s->lna_gain_auto->val,\n\t\t\t&u8tmp, 1);\n\tif (ret)\n\t\tgoto err;\n\n\tif (s->lna_gain_auto->val == false) {\n\t\tret = airspy_ctrl_msg(s, CMD_SET_LNA_GAIN, 0, s->lna_gain->val,\n\t\t\t\t&u8tmp, 1);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\nerr:\n\tif (ret)\n\t\tdev_dbg(s->dev, \"failed=%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int airspy_set_mixer_gain(struct airspy *s)\n{\n\tint ret;\n\tu8 u8tmp;\n\n\tdev_dbg(s->dev, \"mixer auto=%d->%d val=%d->%d\\n\",\n\t\t\ts->mixer_gain_auto->cur.val, s->mixer_gain_auto->val,\n\t\t\ts->mixer_gain->cur.val, s->mixer_gain->val);\n\n\tret = airspy_ctrl_msg(s, CMD_SET_MIXER_AGC, 0, s->mixer_gain_auto->val,\n\t\t\t&u8tmp, 1);\n\tif (ret)\n\t\tgoto err;\n\n\tif (s->mixer_gain_auto->val == false) {\n\t\tret = airspy_ctrl_msg(s, CMD_SET_MIXER_GAIN, 0,\n\t\t\t\ts->mixer_gain->val, &u8tmp, 1);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\nerr:\n\tif (ret)\n\t\tdev_dbg(s->dev, \"failed=%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int airspy_set_if_gain(struct airspy *s)\n{\n\tint ret;\n\tu8 u8tmp;\n\n\tdev_dbg(s->dev, \"val=%d->%d\\n\", s->if_gain->cur.val, s->if_gain->val);\n\n\tret = airspy_ctrl_msg(s, CMD_SET_VGA_GAIN, 0, s->if_gain->val,\n\t\t\t&u8tmp, 1);\n\tif (ret)\n\t\tdev_dbg(s->dev, \"failed=%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int airspy_s_ctrl(struct v4l2_ctrl *ctrl)\n{\n\tstruct airspy *s = container_of(ctrl->handler, struct airspy, hdl);\n\tint ret;\n\n\tswitch (ctrl->id) {\n\tcase  V4L2_CID_RF_TUNER_LNA_GAIN_AUTO:\n\tcase  V4L2_CID_RF_TUNER_LNA_GAIN:\n\t\tret = airspy_set_lna_gain(s);\n\t\tbreak;\n\tcase  V4L2_CID_RF_TUNER_MIXER_GAIN_AUTO:\n\tcase  V4L2_CID_RF_TUNER_MIXER_GAIN:\n\t\tret = airspy_set_mixer_gain(s);\n\t\tbreak;\n\tcase  V4L2_CID_RF_TUNER_IF_GAIN:\n\t\tret = airspy_set_if_gain(s);\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(s->dev, \"unknown ctrl: id=%d name=%s\\n\",\n\t\t\t\tctrl->id, ctrl->name);\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct v4l2_ctrl_ops airspy_ctrl_ops = {\n\t.s_ctrl = airspy_s_ctrl,\n};\n\nstatic int airspy_probe(struct usb_interface *intf,\n\t\tconst struct usb_device_id *id)\n{\n\tstruct airspy *s;\n\tint ret;\n\tu8 u8tmp, *buf;\n\n\tbuf = NULL;\n\tret = -ENOMEM;\n\n\ts = kzalloc(sizeof(struct airspy), GFP_KERNEL);\n\tif (s == NULL) {\n\t\tdev_err(&intf->dev, \"Could not allocate memory for state\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\ts->buf = kzalloc(BUF_SIZE, GFP_KERNEL);\n\tif (!s->buf)\n\t\tgoto err_free_mem;\n\tbuf = kzalloc(BUF_SIZE, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto err_free_mem;\n\n\tmutex_init(&s->v4l2_lock);\n\tmutex_init(&s->vb_queue_lock);\n\tspin_lock_init(&s->queued_bufs_lock);\n\tINIT_LIST_HEAD(&s->queued_bufs);\n\ts->dev = &intf->dev;\n\ts->udev = interface_to_usbdev(intf);\n\ts->f_adc = bands[0].rangelow;\n\ts->f_rf = bands_rf[0].rangelow;\n\ts->pixelformat = formats[0].pixelformat;\n\ts->buffersize = formats[0].buffersize;\n\n\t/* Detect device */\n\tret = airspy_ctrl_msg(s, CMD_BOARD_ID_READ, 0, 0, &u8tmp, 1);\n\tif (ret == 0)\n\t\tret = airspy_ctrl_msg(s, CMD_VERSION_STRING_READ, 0, 0,\n\t\t\t\tbuf, BUF_SIZE);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Could not detect board\\n\");\n\t\tgoto err_free_mem;\n\t}\n\n\tbuf[BUF_SIZE - 1] = '\\0';\n\n\tdev_info(s->dev, \"Board ID: %02x\\n\", u8tmp);\n\tdev_info(s->dev, \"Firmware version: %s\\n\", buf);\n\n\t/* Init videobuf2 queue structure */\n\ts->vb_queue.type = V4L2_BUF_TYPE_SDR_CAPTURE;\n\ts->vb_queue.io_modes = VB2_MMAP | VB2_USERPTR | VB2_READ;\n\ts->vb_queue.drv_priv = s;\n\ts->vb_queue.buf_struct_size = sizeof(struct airspy_frame_buf);\n\ts->vb_queue.ops = &airspy_vb2_ops;\n\ts->vb_queue.mem_ops = &vb2_vmalloc_memops;\n\ts->vb_queue.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;\n\ts->vb_queue.lock = &s->vb_queue_lock;\n\tret = vb2_queue_init(&s->vb_queue);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Could not initialize vb2 queue\\n\");\n\t\tgoto err_free_mem;\n\t}\n\n\t/* Init video_device structure */\n\ts->vdev = airspy_template;\n\ts->vdev.queue = &s->vb_queue;\n\tvideo_set_drvdata(&s->vdev, s);\n\n\t/* Register the v4l2_device structure */\n\ts->v4l2_dev.release = airspy_video_release;\n\tret = v4l2_device_register(&intf->dev, &s->v4l2_dev);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Failed to register v4l2-device (%d)\\n\", ret);\n\t\tgoto err_free_mem;\n\t}\n\n\t/* Register controls */\n\tv4l2_ctrl_handler_init(&s->hdl, 5);\n\ts->lna_gain_auto = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_LNA_GAIN_AUTO, 0, 1, 1, 0);\n\ts->lna_gain = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_LNA_GAIN, 0, 14, 1, 8);\n\tv4l2_ctrl_auto_cluster(2, &s->lna_gain_auto, 0, false);\n\ts->mixer_gain_auto = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_MIXER_GAIN_AUTO, 0, 1, 1, 0);\n\ts->mixer_gain = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_MIXER_GAIN, 0, 15, 1, 8);\n\tv4l2_ctrl_auto_cluster(2, &s->mixer_gain_auto, 0, false);\n\ts->if_gain = v4l2_ctrl_new_std(&s->hdl, &airspy_ctrl_ops,\n\t\t\tV4L2_CID_RF_TUNER_IF_GAIN, 0, 15, 1, 0);\n\tif (s->hdl.error) {\n\t\tret = s->hdl.error;\n\t\tdev_err(s->dev, \"Could not initialize controls\\n\");\n\t\tgoto err_free_controls;\n\t}\n\n\tv4l2_ctrl_handler_setup(&s->hdl);\n\n\ts->v4l2_dev.ctrl_handler = &s->hdl;\n\ts->vdev.v4l2_dev = &s->v4l2_dev;\n\ts->vdev.lock = &s->v4l2_lock;\n\ts->vdev.device_caps = V4L2_CAP_SDR_CAPTURE | V4L2_CAP_STREAMING |\n\t\t\t      V4L2_CAP_READWRITE | V4L2_CAP_TUNER;\n\n\tret = video_register_device(&s->vdev, VFL_TYPE_SDR, -1);\n\tif (ret) {\n\t\tdev_err(s->dev, \"Failed to register as video device (%d)\\n\",\n\t\t\t\tret);\n\t\tgoto err_free_controls;\n\t}\n\n\t/* Free buf if success*/\n\tkfree(buf);\n\n\tdev_info(s->dev, \"Registered as %s\\n\",\n\t\t\tvideo_device_node_name(&s->vdev));\n\tdev_notice(s->dev, \"SDR API is still slightly experimental and functionality changes may follow\\n\");\n\treturn 0;\n\nerr_free_controls:\n\tv4l2_ctrl_handler_free(&s->hdl);\n\tv4l2_device_unregister(&s->v4l2_dev);\nerr_free_mem:\n\tkfree(buf);\n\tkfree(s->buf);\n\tkfree(s);\n\treturn ret;\n}\n\n/* USB device ID list */\nstatic const struct usb_device_id airspy_id_table[] = {\n\t{ USB_DEVICE(0x1d50, 0x60a1) }, /* AirSpy */\n\t{ }\n};\nMODULE_DEVICE_TABLE(usb, airspy_id_table);\n\n/* USB subsystem interface */\nstatic struct usb_driver airspy_driver = {\n\t.name                     = KBUILD_MODNAME,\n\t.probe                    = airspy_probe,\n\t.disconnect               = airspy_disconnect,\n\t.id_table                 = airspy_id_table,\n};\n\nmodule_usb_driver(airspy_driver);\n\nMODULE_AUTHOR(\"Antti Palosaari <crope@iki.fi>\");\nMODULE_DESCRIPTION(\"AirSpy SDR\");\nMODULE_LICENSE(\"GPL\");\n", "patch": "@@ -1072,7 +1072,7 @@ static int airspy_probe(struct usb_interface *intf,\n \tif (ret) {\n \t\tdev_err(s->dev, \"Failed to register as video device (%d)\\n\",\n \t\t\t\tret);\n-\t\tgoto err_unregister_v4l2_dev;\n+\t\tgoto err_free_controls;\n \t}\n \tdev_info(s->dev, \"Registered as %s\\n\",\n \t\t\tvideo_device_node_name(&s->vdev));\n@@ -1081,7 +1081,6 @@ static int airspy_probe(struct usb_interface *intf,\n \n err_free_controls:\n \tv4l2_ctrl_handler_free(&s->hdl);\n-err_unregister_v4l2_dev:\n \tv4l2_device_unregister(&s->v4l2_dev);\n err_free_mem:\n \tkfree(s);", "file_path": "files/2016_8\\100", "file_language": "c", "file_name": "drivers/media/usb/airspy/airspy.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 39, "cve_id": "CVE-2016-6197", "cwe_id": ["CWE-20"], "cve_language": "C", "cve_description": "fs/overlayfs/dir.c in the OverlayFS filesystem implementation in the Linux kernel before 4.6 does not properly verify the upper dentry before proceeding with unlink and rename system-call processing, which allows local users to cause a denial of service (system crash) via a rename system call that specifies a self-hardlink.", "cvss": "5.5", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "NONE", "A": "HIGH", "commit_id": "11f3710417d026ea2f4fcf362d866342c5274185", "commit_message": "ovl: verify upper dentry before unlink and rename\n\nUnlink and rename in overlayfs checked the upper dentry for staleness by\nverifying upper->d_parent against upperdir.  However the dentry can go\nstale also by being unhashed, for example.\n\nExpand the verification to actually look up the name again (under parent\nlock) and check if it matches the upper dentry.  This matches what the VFS\ndoes before passing the dentry to filesytem's unlink/rename methods, which\nexcludes any inconsistency caused by overlayfs.\n\nSigned-off-by: Miklos Szeredi <mszeredi@redhat.com>", "commit_date": "2016-03-21T16:31:44Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/11f3710417d026ea2f4fcf362d866342c5274185", "html_url": "https://github.com/torvalds/linux/commit/11f3710417d026ea2f4fcf362d866342c5274185", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "3c2de27d793bf55167804fc47954711e94f27be7", "url_before": "https://api.github.com/repos/torvalds/linux/commits/3c2de27d793bf55167804fc47954711e94f27be7", "html_url_before": "https://github.com/torvalds/linux/commit/3c2de27d793bf55167804fc47954711e94f27be7"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/11f3710417d026ea2f4fcf362d866342c5274185/fs/overlayfs/dir.c", "code": "/*\n *\n * Copyright (C) 2011 Novell Inc.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 as published by\n * the Free Software Foundation.\n */\n\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/xattr.h>\n#include <linux/security.h>\n#include <linux/cred.h>\n#include \"overlayfs.h\"\n\nvoid ovl_cleanup(struct inode *wdir, struct dentry *wdentry)\n{\n\tint err;\n\n\tdget(wdentry);\n\tif (d_is_dir(wdentry))\n\t\terr = ovl_do_rmdir(wdir, wdentry);\n\telse\n\t\terr = ovl_do_unlink(wdir, wdentry);\n\tdput(wdentry);\n\n\tif (err) {\n\t\tpr_err(\"overlayfs: cleanup of '%pd2' failed (%i)\\n\",\n\t\t       wdentry, err);\n\t}\n}\n\nstruct dentry *ovl_lookup_temp(struct dentry *workdir, struct dentry *dentry)\n{\n\tstruct dentry *temp;\n\tchar name[20];\n\n\tsnprintf(name, sizeof(name), \"#%lx\", (unsigned long) dentry);\n\n\ttemp = lookup_one_len(name, workdir, strlen(name));\n\tif (!IS_ERR(temp) && temp->d_inode) {\n\t\tpr_err(\"overlayfs: workdir/%s already exists\\n\", name);\n\t\tdput(temp);\n\t\ttemp = ERR_PTR(-EIO);\n\t}\n\n\treturn temp;\n}\n\n/* caller holds i_mutex on workdir */\nstatic struct dentry *ovl_whiteout(struct dentry *workdir,\n\t\t\t\t   struct dentry *dentry)\n{\n\tint err;\n\tstruct dentry *whiteout;\n\tstruct inode *wdir = workdir->d_inode;\n\n\twhiteout = ovl_lookup_temp(workdir, dentry);\n\tif (IS_ERR(whiteout))\n\t\treturn whiteout;\n\n\terr = ovl_do_whiteout(wdir, whiteout);\n\tif (err) {\n\t\tdput(whiteout);\n\t\twhiteout = ERR_PTR(err);\n\t}\n\n\treturn whiteout;\n}\n\nint ovl_create_real(struct inode *dir, struct dentry *newdentry,\n\t\t    struct kstat *stat, const char *link,\n\t\t    struct dentry *hardlink, bool debug)\n{\n\tint err;\n\n\tif (newdentry->d_inode)\n\t\treturn -ESTALE;\n\n\tif (hardlink) {\n\t\terr = ovl_do_link(hardlink, dir, newdentry, debug);\n\t} else {\n\t\tswitch (stat->mode & S_IFMT) {\n\t\tcase S_IFREG:\n\t\t\terr = ovl_do_create(dir, newdentry, stat->mode, debug);\n\t\t\tbreak;\n\n\t\tcase S_IFDIR:\n\t\t\terr = ovl_do_mkdir(dir, newdentry, stat->mode, debug);\n\t\t\tbreak;\n\n\t\tcase S_IFCHR:\n\t\tcase S_IFBLK:\n\t\tcase S_IFIFO:\n\t\tcase S_IFSOCK:\n\t\t\terr = ovl_do_mknod(dir, newdentry,\n\t\t\t\t\t   stat->mode, stat->rdev, debug);\n\t\t\tbreak;\n\n\t\tcase S_IFLNK:\n\t\t\terr = ovl_do_symlink(dir, newdentry, link, debug);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\terr = -EPERM;\n\t\t}\n\t}\n\tif (!err && WARN_ON(!newdentry->d_inode)) {\n\t\t/*\n\t\t * Not quite sure if non-instantiated dentry is legal or not.\n\t\t * VFS doesn't seem to care so check and warn here.\n\t\t */\n\t\terr = -ENOENT;\n\t}\n\treturn err;\n}\n\nstatic int ovl_set_opaque(struct dentry *upperdentry)\n{\n\treturn ovl_do_setxattr(upperdentry, OVL_XATTR_OPAQUE, \"y\", 1, 0);\n}\n\nstatic void ovl_remove_opaque(struct dentry *upperdentry)\n{\n\tint err;\n\n\terr = ovl_do_removexattr(upperdentry, OVL_XATTR_OPAQUE);\n\tif (err) {\n\t\tpr_warn(\"overlayfs: failed to remove opaque from '%s' (%i)\\n\",\n\t\t\tupperdentry->d_name.name, err);\n\t}\n}\n\nstatic int ovl_dir_getattr(struct vfsmount *mnt, struct dentry *dentry,\n\t\t\t struct kstat *stat)\n{\n\tint err;\n\tenum ovl_path_type type;\n\tstruct path realpath;\n\n\ttype = ovl_path_real(dentry, &realpath);\n\terr = vfs_getattr(&realpath, stat);\n\tif (err)\n\t\treturn err;\n\n\tstat->dev = dentry->d_sb->s_dev;\n\tstat->ino = dentry->d_inode->i_ino;\n\n\t/*\n\t * It's probably not worth it to count subdirs to get the\n\t * correct link count.  nlink=1 seems to pacify 'find' and\n\t * other utilities.\n\t */\n\tif (OVL_TYPE_MERGE(type))\n\t\tstat->nlink = 1;\n\n\treturn 0;\n}\n\nstatic int ovl_create_upper(struct dentry *dentry, struct inode *inode,\n\t\t\t    struct kstat *stat, const char *link,\n\t\t\t    struct dentry *hardlink)\n{\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *udir = upperdir->d_inode;\n\tstruct dentry *newdentry;\n\tint err;\n\n\tinode_lock_nested(udir, I_MUTEX_PARENT);\n\tnewdentry = lookup_one_len(dentry->d_name.name, upperdir,\n\t\t\t\t   dentry->d_name.len);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_unlock;\n\terr = ovl_create_real(udir, newdentry, stat, link, hardlink, false);\n\tif (err)\n\t\tgoto out_dput;\n\n\tovl_dentry_version_inc(dentry->d_parent);\n\tovl_dentry_update(dentry, newdentry);\n\tovl_copyattr(newdentry->d_inode, inode);\n\td_instantiate(dentry, inode);\n\tnewdentry = NULL;\nout_dput:\n\tdput(newdentry);\nout_unlock:\n\tinode_unlock(udir);\n\treturn err;\n}\n\nstatic int ovl_lock_rename_workdir(struct dentry *workdir,\n\t\t\t\t   struct dentry *upperdir)\n{\n\t/* Workdir should not be the same as upperdir */\n\tif (workdir == upperdir)\n\t\tgoto err;\n\n\t/* Workdir should not be subdir of upperdir and vice versa */\n\tif (lock_rename(workdir, upperdir) != NULL)\n\t\tgoto err_unlock;\n\n\treturn 0;\n\nerr_unlock:\n\tunlock_rename(workdir, upperdir);\nerr:\n\tpr_err(\"overlayfs: failed to lock workdir+upperdir\\n\");\n\treturn -EIO;\n}\n\nstatic struct dentry *ovl_clear_empty(struct dentry *dentry,\n\t\t\t\t      struct list_head *list)\n{\n\tstruct dentry *workdir = ovl_workdir(dentry);\n\tstruct inode *wdir = workdir->d_inode;\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *udir = upperdir->d_inode;\n\tstruct path upperpath;\n\tstruct dentry *upper;\n\tstruct dentry *opaquedir;\n\tstruct kstat stat;\n\tint err;\n\n\tif (WARN_ON(!workdir))\n\t\treturn ERR_PTR(-EROFS);\n\n\terr = ovl_lock_rename_workdir(workdir, upperdir);\n\tif (err)\n\t\tgoto out;\n\n\tovl_path_upper(dentry, &upperpath);\n\terr = vfs_getattr(&upperpath, &stat);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (!S_ISDIR(stat.mode))\n\t\tgoto out_unlock;\n\tupper = upperpath.dentry;\n\tif (upper->d_parent->d_inode != udir)\n\t\tgoto out_unlock;\n\n\topaquedir = ovl_lookup_temp(workdir, dentry);\n\terr = PTR_ERR(opaquedir);\n\tif (IS_ERR(opaquedir))\n\t\tgoto out_unlock;\n\n\terr = ovl_create_real(wdir, opaquedir, &stat, NULL, NULL, true);\n\tif (err)\n\t\tgoto out_dput;\n\n\terr = ovl_copy_xattr(upper, opaquedir);\n\tif (err)\n\t\tgoto out_cleanup;\n\n\terr = ovl_set_opaque(opaquedir);\n\tif (err)\n\t\tgoto out_cleanup;\n\n\tinode_lock(opaquedir->d_inode);\n\terr = ovl_set_attr(opaquedir, &stat);\n\tinode_unlock(opaquedir->d_inode);\n\tif (err)\n\t\tgoto out_cleanup;\n\n\terr = ovl_do_rename(wdir, opaquedir, udir, upper, RENAME_EXCHANGE);\n\tif (err)\n\t\tgoto out_cleanup;\n\n\tovl_cleanup_whiteouts(upper, list);\n\tovl_cleanup(wdir, upper);\n\tunlock_rename(workdir, upperdir);\n\n\t/* dentry's upper doesn't match now, get rid of it */\n\td_drop(dentry);\n\n\treturn opaquedir;\n\nout_cleanup:\n\tovl_cleanup(wdir, opaquedir);\nout_dput:\n\tdput(opaquedir);\nout_unlock:\n\tunlock_rename(workdir, upperdir);\nout:\n\treturn ERR_PTR(err);\n}\n\nstatic struct dentry *ovl_check_empty_and_clear(struct dentry *dentry)\n{\n\tint err;\n\tstruct dentry *ret = NULL;\n\tLIST_HEAD(list);\n\n\terr = ovl_check_empty_dir(dentry, &list);\n\tif (err)\n\t\tret = ERR_PTR(err);\n\telse {\n\t\t/*\n\t\t * If no upperdentry then skip clearing whiteouts.\n\t\t *\n\t\t * Can race with copy-up, since we don't hold the upperdir\n\t\t * mutex.  Doesn't matter, since copy-up can't create a\n\t\t * non-empty directory from an empty one.\n\t\t */\n\t\tif (ovl_dentry_upper(dentry))\n\t\t\tret = ovl_clear_empty(dentry, &list);\n\t}\n\n\tovl_cache_free(&list);\n\n\treturn ret;\n}\n\nstatic int ovl_create_over_whiteout(struct dentry *dentry, struct inode *inode,\n\t\t\t\t    struct kstat *stat, const char *link,\n\t\t\t\t    struct dentry *hardlink)\n{\n\tstruct dentry *workdir = ovl_workdir(dentry);\n\tstruct inode *wdir = workdir->d_inode;\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *udir = upperdir->d_inode;\n\tstruct dentry *upper;\n\tstruct dentry *newdentry;\n\tint err;\n\n\tif (WARN_ON(!workdir))\n\t\treturn -EROFS;\n\n\terr = ovl_lock_rename_workdir(workdir, upperdir);\n\tif (err)\n\t\tgoto out;\n\n\tnewdentry = ovl_lookup_temp(workdir, dentry);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_unlock;\n\n\tupper = lookup_one_len(dentry->d_name.name, upperdir,\n\t\t\t       dentry->d_name.len);\n\terr = PTR_ERR(upper);\n\tif (IS_ERR(upper))\n\t\tgoto out_dput;\n\n\terr = ovl_create_real(wdir, newdentry, stat, link, hardlink, true);\n\tif (err)\n\t\tgoto out_dput2;\n\n\tif (S_ISDIR(stat->mode)) {\n\t\terr = ovl_set_opaque(newdentry);\n\t\tif (err)\n\t\t\tgoto out_cleanup;\n\n\t\terr = ovl_do_rename(wdir, newdentry, udir, upper,\n\t\t\t\t    RENAME_EXCHANGE);\n\t\tif (err)\n\t\t\tgoto out_cleanup;\n\n\t\tovl_cleanup(wdir, upper);\n\t} else {\n\t\terr = ovl_do_rename(wdir, newdentry, udir, upper, 0);\n\t\tif (err)\n\t\t\tgoto out_cleanup;\n\t}\n\tovl_dentry_version_inc(dentry->d_parent);\n\tovl_dentry_update(dentry, newdentry);\n\tovl_copyattr(newdentry->d_inode, inode);\n\td_instantiate(dentry, inode);\n\tnewdentry = NULL;\nout_dput2:\n\tdput(upper);\nout_dput:\n\tdput(newdentry);\nout_unlock:\n\tunlock_rename(workdir, upperdir);\nout:\n\treturn err;\n\nout_cleanup:\n\tovl_cleanup(wdir, newdentry);\n\tgoto out_dput2;\n}\n\nstatic int ovl_create_or_link(struct dentry *dentry, int mode, dev_t rdev,\n\t\t\t      const char *link, struct dentry *hardlink)\n{\n\tint err;\n\tstruct inode *inode;\n\tstruct kstat stat = {\n\t\t.mode = mode,\n\t\t.rdev = rdev,\n\t};\n\n\terr = -ENOMEM;\n\tinode = ovl_new_inode(dentry->d_sb, mode, dentry->d_fsdata);\n\tif (!inode)\n\t\tgoto out;\n\n\terr = ovl_copy_up(dentry->d_parent);\n\tif (err)\n\t\tgoto out_iput;\n\n\tif (!ovl_dentry_is_opaque(dentry)) {\n\t\terr = ovl_create_upper(dentry, inode, &stat, link, hardlink);\n\t} else {\n\t\tconst struct cred *old_cred;\n\t\tstruct cred *override_cred;\n\n\t\terr = -ENOMEM;\n\t\toverride_cred = prepare_creds();\n\t\tif (!override_cred)\n\t\t\tgoto out_iput;\n\n\t\t/*\n\t\t * CAP_SYS_ADMIN for setting opaque xattr\n\t\t * CAP_DAC_OVERRIDE for create in workdir, rename\n\t\t * CAP_FOWNER for removing whiteout from sticky dir\n\t\t */\n\t\tcap_raise(override_cred->cap_effective, CAP_SYS_ADMIN);\n\t\tcap_raise(override_cred->cap_effective, CAP_DAC_OVERRIDE);\n\t\tcap_raise(override_cred->cap_effective, CAP_FOWNER);\n\t\told_cred = override_creds(override_cred);\n\n\t\terr = ovl_create_over_whiteout(dentry, inode, &stat, link,\n\t\t\t\t\t       hardlink);\n\n\t\trevert_creds(old_cred);\n\t\tput_cred(override_cred);\n\t}\n\n\tif (!err)\n\t\tinode = NULL;\nout_iput:\n\tiput(inode);\nout:\n\treturn err;\n}\n\nstatic int ovl_create_object(struct dentry *dentry, int mode, dev_t rdev,\n\t\t\t     const char *link)\n{\n\tint err;\n\n\terr = ovl_want_write(dentry);\n\tif (!err) {\n\t\terr = ovl_create_or_link(dentry, mode, rdev, link, NULL);\n\t\tovl_drop_write(dentry);\n\t}\n\n\treturn err;\n}\n\nstatic int ovl_create(struct inode *dir, struct dentry *dentry, umode_t mode,\n\t\t      bool excl)\n{\n\treturn ovl_create_object(dentry, (mode & 07777) | S_IFREG, 0, NULL);\n}\n\nstatic int ovl_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)\n{\n\treturn ovl_create_object(dentry, (mode & 07777) | S_IFDIR, 0, NULL);\n}\n\nstatic int ovl_mknod(struct inode *dir, struct dentry *dentry, umode_t mode,\n\t\t     dev_t rdev)\n{\n\t/* Don't allow creation of \"whiteout\" on overlay */\n\tif (S_ISCHR(mode) && rdev == WHITEOUT_DEV)\n\t\treturn -EPERM;\n\n\treturn ovl_create_object(dentry, mode, rdev, NULL);\n}\n\nstatic int ovl_symlink(struct inode *dir, struct dentry *dentry,\n\t\t       const char *link)\n{\n\treturn ovl_create_object(dentry, S_IFLNK, 0, link);\n}\n\nstatic int ovl_link(struct dentry *old, struct inode *newdir,\n\t\t    struct dentry *new)\n{\n\tint err;\n\tstruct dentry *upper;\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\tupper = ovl_dentry_upper(old);\n\terr = ovl_create_or_link(new, upper->d_inode->i_mode, 0, NULL, upper);\n\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\treturn err;\n}\n\nstatic int ovl_remove_and_whiteout(struct dentry *dentry, bool is_dir)\n{\n\tstruct dentry *workdir = ovl_workdir(dentry);\n\tstruct inode *wdir = workdir->d_inode;\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *udir = upperdir->d_inode;\n\tstruct dentry *whiteout;\n\tstruct dentry *upper;\n\tstruct dentry *opaquedir = NULL;\n\tint err;\n\n\tif (WARN_ON(!workdir))\n\t\treturn -EROFS;\n\n\tif (is_dir) {\n\t\tif (OVL_TYPE_MERGE_OR_LOWER(ovl_path_type(dentry))) {\n\t\t\topaquedir = ovl_check_empty_and_clear(dentry);\n\t\t\terr = PTR_ERR(opaquedir);\n\t\t\tif (IS_ERR(opaquedir))\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\tLIST_HEAD(list);\n\n\t\t\t/*\n\t\t\t * When removing an empty opaque directory, then it\n\t\t\t * makes no sense to replace it with an exact replica of\n\t\t\t * itself.  But emptiness still needs to be checked.\n\t\t\t */\n\t\t\terr = ovl_check_empty_dir(dentry, &list);\n\t\t\tovl_cache_free(&list);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\terr = ovl_lock_rename_workdir(workdir, upperdir);\n\tif (err)\n\t\tgoto out_dput;\n\n\twhiteout = ovl_whiteout(workdir, dentry);\n\terr = PTR_ERR(whiteout);\n\tif (IS_ERR(whiteout))\n\t\tgoto out_unlock;\n\n\tupper = ovl_dentry_upper(dentry);\n\tif (!upper) {\n\t\tupper = lookup_one_len(dentry->d_name.name, upperdir,\n\t\t\t\t       dentry->d_name.len);\n\t\terr = PTR_ERR(upper);\n\t\tif (IS_ERR(upper))\n\t\t\tgoto kill_whiteout;\n\n\t\terr = ovl_do_rename(wdir, whiteout, udir, upper, 0);\n\t\tdput(upper);\n\t\tif (err)\n\t\t\tgoto kill_whiteout;\n\t} else {\n\t\tint flags = 0;\n\n\t\tif (opaquedir)\n\t\t\tupper = opaquedir;\n\t\terr = -ESTALE;\n\t\tif (upper->d_parent != upperdir)\n\t\t\tgoto kill_whiteout;\n\n\t\tif (is_dir)\n\t\t\tflags |= RENAME_EXCHANGE;\n\n\t\terr = ovl_do_rename(wdir, whiteout, udir, upper, flags);\n\t\tif (err)\n\t\t\tgoto kill_whiteout;\n\n\t\tif (is_dir)\n\t\t\tovl_cleanup(wdir, upper);\n\t}\n\tovl_dentry_version_inc(dentry->d_parent);\nout_d_drop:\n\td_drop(dentry);\n\tdput(whiteout);\nout_unlock:\n\tunlock_rename(workdir, upperdir);\nout_dput:\n\tdput(opaquedir);\nout:\n\treturn err;\n\nkill_whiteout:\n\tovl_cleanup(wdir, whiteout);\n\tgoto out_d_drop;\n}\n\nstatic int ovl_remove_upper(struct dentry *dentry, bool is_dir)\n{\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *dir = upperdir->d_inode;\n\tstruct dentry *upper;\n\tint err;\n\n\tinode_lock_nested(dir, I_MUTEX_PARENT);\n\tupper = lookup_one_len(dentry->d_name.name, upperdir,\n\t\t\t       dentry->d_name.len);\n\terr = PTR_ERR(upper);\n\tif (IS_ERR(upper))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (upper == ovl_dentry_upper(dentry)) {\n\t\tif (is_dir)\n\t\t\terr = vfs_rmdir(dir, upper);\n\t\telse\n\t\t\terr = vfs_unlink(dir, upper, NULL);\n\t\tovl_dentry_version_inc(dentry->d_parent);\n\t}\n\tdput(upper);\n\n\t/*\n\t * Keeping this dentry hashed would mean having to release\n\t * upperpath/lowerpath, which could only be done if we are the\n\t * sole user of this dentry.  Too tricky...  Just unhash for\n\t * now.\n\t */\n\tif (!err)\n\t\td_drop(dentry);\nout_unlock:\n\tinode_unlock(dir);\n\n\treturn err;\n}\n\nstatic inline int ovl_check_sticky(struct dentry *dentry)\n{\n\tstruct inode *dir = ovl_dentry_real(dentry->d_parent)->d_inode;\n\tstruct inode *inode = ovl_dentry_real(dentry)->d_inode;\n\n\tif (check_sticky(dir, inode))\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int ovl_do_remove(struct dentry *dentry, bool is_dir)\n{\n\tenum ovl_path_type type;\n\tint err;\n\n\terr = ovl_check_sticky(dentry);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_want_write(dentry);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(dentry->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\ttype = ovl_path_type(dentry);\n\tif (OVL_TYPE_PURE_UPPER(type)) {\n\t\terr = ovl_remove_upper(dentry, is_dir);\n\t} else {\n\t\tconst struct cred *old_cred;\n\t\tstruct cred *override_cred;\n\n\t\terr = -ENOMEM;\n\t\toverride_cred = prepare_creds();\n\t\tif (!override_cred)\n\t\t\tgoto out_drop_write;\n\n\t\t/*\n\t\t * CAP_SYS_ADMIN for setting xattr on whiteout, opaque dir\n\t\t * CAP_DAC_OVERRIDE for create in workdir, rename\n\t\t * CAP_FOWNER for removing whiteout from sticky dir\n\t\t * CAP_FSETID for chmod of opaque dir\n\t\t * CAP_CHOWN for chown of opaque dir\n\t\t */\n\t\tcap_raise(override_cred->cap_effective, CAP_SYS_ADMIN);\n\t\tcap_raise(override_cred->cap_effective, CAP_DAC_OVERRIDE);\n\t\tcap_raise(override_cred->cap_effective, CAP_FOWNER);\n\t\tcap_raise(override_cred->cap_effective, CAP_FSETID);\n\t\tcap_raise(override_cred->cap_effective, CAP_CHOWN);\n\t\told_cred = override_creds(override_cred);\n\n\t\terr = ovl_remove_and_whiteout(dentry, is_dir);\n\n\t\trevert_creds(old_cred);\n\t\tput_cred(override_cred);\n\t}\nout_drop_write:\n\tovl_drop_write(dentry);\nout:\n\treturn err;\n}\n\nstatic int ovl_unlink(struct inode *dir, struct dentry *dentry)\n{\n\treturn ovl_do_remove(dentry, false);\n}\n\nstatic int ovl_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\treturn ovl_do_remove(dentry, true);\n}\n\nstatic int ovl_rename2(struct inode *olddir, struct dentry *old,\n\t\t       struct inode *newdir, struct dentry *new,\n\t\t       unsigned int flags)\n{\n\tint err;\n\tenum ovl_path_type old_type;\n\tenum ovl_path_type new_type;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry;\n\tstruct dentry *newdentry;\n\tstruct dentry *trap;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool new_create = false;\n\tbool cleanup_whiteout = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = false;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tstruct cred *override_cred = NULL;\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\terr = ovl_check_sticky(old);\n\tif (err)\n\t\tgoto out;\n\n\t/* Don't copy up directory trees */\n\told_type = ovl_path_type(old);\n\terr = -EXDEV;\n\tif (OVL_TYPE_MERGE_OR_LOWER(old_type) && is_dir)\n\t\tgoto out;\n\n\tif (new->d_inode) {\n\t\terr = ovl_check_sticky(new);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (d_is_dir(new))\n\t\t\tnew_is_dir = true;\n\n\t\tnew_type = ovl_path_type(new);\n\t\terr = -EXDEV;\n\t\tif (!overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir)\n\t\t\tgoto out;\n\n\t\terr = 0;\n\t\tif (!OVL_TYPE_UPPER(new_type) && !OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_lower(old)->d_inode ==\n\t\t\t    ovl_dentry_lower(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (OVL_TYPE_UPPER(new_type) && OVL_TYPE_UPPER(old_type)) {\n\t\t\tif (ovl_dentry_upper(old)->d_inode ==\n\t\t\t    ovl_dentry_upper(new)->d_inode)\n\t\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tif (ovl_dentry_is_opaque(new))\n\t\t\tnew_type = __OVL_PATH_UPPER;\n\t\telse\n\t\t\tnew_type = __OVL_PATH_UPPER | __OVL_PATH_PURE;\n\t}\n\n\terr = ovl_want_write(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out_drop_write;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out_drop_write;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out_drop_write;\n\t}\n\n\told_opaque = !OVL_TYPE_PURE_UPPER(old_type);\n\tnew_opaque = !OVL_TYPE_PURE_UPPER(new_type);\n\n\tif (old_opaque || new_opaque) {\n\t\terr = -ENOMEM;\n\t\toverride_cred = prepare_creds();\n\t\tif (!override_cred)\n\t\t\tgoto out_drop_write;\n\n\t\t/*\n\t\t * CAP_SYS_ADMIN for setting xattr on whiteout, opaque dir\n\t\t * CAP_DAC_OVERRIDE for create in workdir\n\t\t * CAP_FOWNER for removing whiteout from sticky dir\n\t\t * CAP_FSETID for chmod of opaque dir\n\t\t * CAP_CHOWN for chown of opaque dir\n\t\t */\n\t\tcap_raise(override_cred->cap_effective, CAP_SYS_ADMIN);\n\t\tcap_raise(override_cred->cap_effective, CAP_DAC_OVERRIDE);\n\t\tcap_raise(override_cred->cap_effective, CAP_FOWNER);\n\t\tcap_raise(override_cred->cap_effective, CAP_FSETID);\n\t\tcap_raise(override_cred->cap_effective, CAP_CHOWN);\n\t\told_cred = override_creds(override_cred);\n\t}\n\n\tif (overwrite && OVL_TYPE_MERGE_OR_LOWER(new_type) && new_is_dir) {\n\t\topaquedir = ovl_check_empty_and_clear(new);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\tif (overwrite) {\n\t\tif (old_opaque) {\n\t\t\tif (new->d_inode || !new_opaque) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && !new->d_inode && new_opaque) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\n\n\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n\t\t\t\t   old->d_name.len);\n\terr = PTR_ERR(olddentry);\n\tif (IS_ERR(olddentry))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif (olddentry != ovl_dentry_upper(old))\n\t\tgoto out_dput_old;\n\n\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n\t\t\t\t   new->d_name.len);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_dput_old;\n\n\terr = -ESTALE;\n\tif (ovl_dentry_upper(new)) {\n\t\tif (opaquedir) {\n\t\t\tif (newdentry != opaquedir)\n\t\t\t\tgoto out_dput;\n\t\t} else {\n\t\t\tif (newdentry != ovl_dentry_upper(new))\n\t\t\t\tgoto out_dput;\n\t\t}\n\t} else {\n\t\tnew_create = true;\n\t\tif (!d_is_negative(newdentry) &&\n\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n\t\t\tgoto out_dput;\n\t}\n\n\tif (olddentry == trap)\n\t\tgoto out_dput;\n\tif (newdentry == trap)\n\t\tgoto out_dput;\n\n\tif (is_dir && !old_opaque && new_opaque) {\n\t\terr = ovl_set_opaque(olddentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\tif (!overwrite && new_is_dir && old_opaque && !new_opaque) {\n\t\terr = ovl_set_opaque(newdentry);\n\t\tif (err)\n\t\t\tgoto out_dput;\n\t}\n\n\tif (old_opaque || new_opaque) {\n\t\terr = ovl_do_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t    new_upperdir->d_inode, newdentry,\n\t\t\t\t    flags);\n\t} else {\n\t\t/* No debug for the plain case */\n\t\tBUG_ON(flags & ~RENAME_EXCHANGE);\n\t\terr = vfs_rename(old_upperdir->d_inode, olddentry,\n\t\t\t\t new_upperdir->d_inode, newdentry,\n\t\t\t\t NULL, flags);\n\t}\n\n\tif (err) {\n\t\tif (is_dir && !old_opaque && new_opaque)\n\t\t\tovl_remove_opaque(olddentry);\n\t\tif (!overwrite && new_is_dir && old_opaque && !new_opaque)\n\t\t\tovl_remove_opaque(newdentry);\n\t\tgoto out_dput;\n\t}\n\n\tif (is_dir && old_opaque && !new_opaque)\n\t\tovl_remove_opaque(olddentry);\n\tif (!overwrite && new_is_dir && !old_opaque && new_opaque)\n\t\tovl_remove_opaque(newdentry);\n\n\t/*\n\t * Old dentry now lives in different location. Dentries in\n\t * lowerstack are stale. We cannot drop them here because\n\t * access to them is lockless. This could be only pure upper\n\t * or opaque directory - numlower is zero. Or upper non-dir\n\t * entry - its pureness is tracked by flag opaque.\n\t */\n\tif (old_opaque != new_opaque) {\n\t\tovl_dentry_set_opaque(old, new_opaque);\n\t\tif (!overwrite)\n\t\t\tovl_dentry_set_opaque(new, old_opaque);\n\t}\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(old_upperdir->d_inode, newdentry);\n\n\tovl_dentry_version_inc(old->d_parent);\n\tovl_dentry_version_inc(new->d_parent);\n\nout_dput:\n\tdput(newdentry);\nout_dput_old:\n\tdput(olddentry);\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\nout_revert_creds:\n\tif (old_opaque || new_opaque) {\n\t\trevert_creds(old_cred);\n\t\tput_cred(override_cred);\n\t}\nout_drop_write:\n\tovl_drop_write(old);\nout:\n\tdput(opaquedir);\n\treturn err;\n}\n\nconst struct inode_operations ovl_dir_inode_operations = {\n\t.lookup\t\t= ovl_lookup,\n\t.mkdir\t\t= ovl_mkdir,\n\t.symlink\t= ovl_symlink,\n\t.unlink\t\t= ovl_unlink,\n\t.rmdir\t\t= ovl_rmdir,\n\t.rename2\t= ovl_rename2,\n\t.link\t\t= ovl_link,\n\t.setattr\t= ovl_setattr,\n\t.create\t\t= ovl_create,\n\t.mknod\t\t= ovl_mknod,\n\t.permission\t= ovl_permission,\n\t.getattr\t= ovl_dir_getattr,\n\t.setxattr\t= ovl_setxattr,\n\t.getxattr\t= ovl_getxattr,\n\t.listxattr\t= ovl_listxattr,\n\t.removexattr\t= ovl_removexattr,\n};\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *\n * Copyright (C) 2011 Novell Inc.\n */\n\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/xattr.h>\n#include <linux/security.h>\n#include <linux/cred.h>\n#include <linux/module.h>\n#include <linux/posix_acl.h>\n#include <linux/posix_acl_xattr.h>\n#include <linux/atomic.h>\n#include <linux/ratelimit.h>\n#include <linux/backing-file.h>\n#include \"overlayfs.h\"\n\nstatic unsigned short ovl_redirect_max = 256;\nmodule_param_named(redirect_max, ovl_redirect_max, ushort, 0644);\nMODULE_PARM_DESC(redirect_max,\n\t\t \"Maximum length of absolute redirect xattr value\");\n\nstatic int ovl_set_redirect(struct dentry *dentry, bool samedir);\n\nstatic int ovl_cleanup_locked(struct ovl_fs *ofs, struct inode *wdir,\n\t\t\t      struct dentry *wdentry)\n{\n\tint err;\n\n\tdget(wdentry);\n\tif (d_is_dir(wdentry))\n\t\terr = ovl_do_rmdir(ofs, wdir, wdentry);\n\telse\n\t\terr = ovl_do_unlink(ofs, wdir, wdentry);\n\tdput(wdentry);\n\n\tif (err) {\n\t\tpr_err(\"cleanup of '%pd2' failed (%i)\\n\",\n\t\t       wdentry, err);\n\t}\n\n\treturn err;\n}\n\nint ovl_cleanup(struct ovl_fs *ofs, struct dentry *workdir,\n\t\tstruct dentry *wdentry)\n{\n\tint err;\n\n\terr = ovl_parent_lock(workdir, wdentry);\n\tif (err)\n\t\treturn err;\n\n\tovl_cleanup_locked(ofs, workdir->d_inode, wdentry);\n\tovl_parent_unlock(workdir);\n\n\treturn 0;\n}\n\nstruct dentry *ovl_lookup_temp(struct ovl_fs *ofs, struct dentry *workdir)\n{\n\tstruct dentry *temp;\n\tchar name[20];\n\tstatic atomic_t temp_id = ATOMIC_INIT(0);\n\n\t/* counter is allowed to wrap, since temp dentries are ephemeral */\n\tsnprintf(name, sizeof(name), \"#%x\", atomic_inc_return(&temp_id));\n\n\ttemp = ovl_lookup_upper(ofs, name, workdir, strlen(name));\n\tif (!IS_ERR(temp) && temp->d_inode) {\n\t\tpr_err(\"workdir/%s already exists\\n\", name);\n\t\tdput(temp);\n\t\ttemp = ERR_PTR(-EIO);\n\t}\n\n\treturn temp;\n}\n\nstatic struct dentry *ovl_whiteout(struct ovl_fs *ofs)\n{\n\tint err;\n\tstruct dentry *whiteout;\n\tstruct dentry *workdir = ofs->workdir;\n\tstruct inode *wdir = workdir->d_inode;\n\n\tguard(mutex)(&ofs->whiteout_lock);\n\n\tif (!ofs->whiteout) {\n\t\tinode_lock_nested(wdir, I_MUTEX_PARENT);\n\t\twhiteout = ovl_lookup_temp(ofs, workdir);\n\t\tif (!IS_ERR(whiteout)) {\n\t\t\terr = ovl_do_whiteout(ofs, wdir, whiteout);\n\t\t\tif (err) {\n\t\t\t\tdput(whiteout);\n\t\t\t\twhiteout = ERR_PTR(err);\n\t\t\t}\n\t\t}\n\t\tinode_unlock(wdir);\n\t\tif (IS_ERR(whiteout))\n\t\t\treturn whiteout;\n\t\tofs->whiteout = whiteout;\n\t}\n\n\tif (!ofs->no_shared_whiteout) {\n\t\tinode_lock_nested(wdir, I_MUTEX_PARENT);\n\t\twhiteout = ovl_lookup_temp(ofs, workdir);\n\t\tif (!IS_ERR(whiteout)) {\n\t\t\terr = ovl_do_link(ofs, ofs->whiteout, wdir, whiteout);\n\t\t\tif (err) {\n\t\t\t\tdput(whiteout);\n\t\t\t\twhiteout = ERR_PTR(err);\n\t\t\t}\n\t\t}\n\t\tinode_unlock(wdir);\n\t\tif (!IS_ERR(whiteout))\n\t\t\treturn whiteout;\n\t\tif (PTR_ERR(whiteout) != -EMLINK) {\n\t\t\tpr_warn(\"Failed to link whiteout - disabling whiteout inode sharing(nlink=%u, err=%lu)\\n\",\n\t\t\t\tofs->whiteout->d_inode->i_nlink,\n\t\t\t\tPTR_ERR(whiteout));\n\t\t\tofs->no_shared_whiteout = true;\n\t\t}\n\t}\n\twhiteout = ofs->whiteout;\n\tofs->whiteout = NULL;\n\treturn whiteout;\n}\n\nint ovl_cleanup_and_whiteout(struct ovl_fs *ofs, struct dentry *dir,\n\t\t\t     struct dentry *dentry)\n{\n\tstruct dentry *whiteout;\n\tint err;\n\tint flags = 0;\n\n\twhiteout = ovl_whiteout(ofs);\n\terr = PTR_ERR(whiteout);\n\tif (IS_ERR(whiteout))\n\t\treturn err;\n\n\tif (d_is_dir(dentry))\n\t\tflags = RENAME_EXCHANGE;\n\n\terr = ovl_lock_rename_workdir(ofs->workdir, whiteout, dir, dentry);\n\tif (!err) {\n\t\terr = ovl_do_rename(ofs, ofs->workdir, whiteout, dir, dentry, flags);\n\t\tunlock_rename(ofs->workdir, dir);\n\t}\n\tif (err)\n\t\tgoto kill_whiteout;\n\tif (flags)\n\t\tovl_cleanup(ofs, ofs->workdir, dentry);\n\nout:\n\tdput(whiteout);\n\treturn err;\n\nkill_whiteout:\n\tovl_cleanup(ofs, ofs->workdir, whiteout);\n\tgoto out;\n}\n\nstruct dentry *ovl_create_real(struct ovl_fs *ofs, struct dentry *parent,\n\t\t\t       struct dentry *newdentry, struct ovl_cattr *attr)\n{\n\tstruct inode *dir = parent->d_inode;\n\tint err;\n\n\tif (IS_ERR(newdentry))\n\t\treturn newdentry;\n\n\terr = -ESTALE;\n\tif (newdentry->d_inode)\n\t\tgoto out;\n\n\tif (attr->hardlink) {\n\t\terr = ovl_do_link(ofs, attr->hardlink, dir, newdentry);\n\t} else {\n\t\tswitch (attr->mode & S_IFMT) {\n\t\tcase S_IFREG:\n\t\t\terr = ovl_do_create(ofs, dir, newdentry, attr->mode);\n\t\t\tbreak;\n\n\t\tcase S_IFDIR:\n\t\t\t/* mkdir is special... */\n\t\t\tnewdentry =  ovl_do_mkdir(ofs, dir, newdentry, attr->mode);\n\t\t\terr = PTR_ERR_OR_ZERO(newdentry);\n\t\t\t/* expect to inherit casefolding from workdir/upperdir */\n\t\t\tif (!err && ofs->casefold != ovl_dentry_casefolded(newdentry)) {\n\t\t\t\tpr_warn_ratelimited(\"wrong inherited casefold (%pd2)\\n\",\n\t\t\t\t\t\t    newdentry);\n\t\t\t\tdput(newdentry);\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase S_IFCHR:\n\t\tcase S_IFBLK:\n\t\tcase S_IFIFO:\n\t\tcase S_IFSOCK:\n\t\t\terr = ovl_do_mknod(ofs, dir, newdentry, attr->mode,\n\t\t\t\t\t   attr->rdev);\n\t\t\tbreak;\n\n\t\tcase S_IFLNK:\n\t\t\terr = ovl_do_symlink(ofs, dir, newdentry, attr->link);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\terr = -EPERM;\n\t\t}\n\t}\n\tif (!err && WARN_ON(!newdentry->d_inode)) {\n\t\t/*\n\t\t * Not quite sure if non-instantiated dentry is legal or not.\n\t\t * VFS doesn't seem to care so check and warn here.\n\t\t */\n\t\terr = -EIO;\n\t}\nout:\n\tif (err) {\n\t\tif (!IS_ERR(newdentry))\n\t\t\tdput(newdentry);\n\t\treturn ERR_PTR(err);\n\t}\n\treturn newdentry;\n}\n\nstruct dentry *ovl_create_temp(struct ovl_fs *ofs, struct dentry *workdir,\n\t\t\t       struct ovl_cattr *attr)\n{\n\tstruct dentry *ret;\n\tinode_lock_nested(workdir->d_inode, I_MUTEX_PARENT);\n\tret = ovl_create_real(ofs, workdir,\n\t\t\t      ovl_lookup_temp(ofs, workdir), attr);\n\tinode_unlock(workdir->d_inode);\n\treturn ret;\n}\n\nstatic int ovl_set_opaque_xerr(struct dentry *dentry, struct dentry *upper,\n\t\t\t       int xerr)\n{\n\tstruct ovl_fs *ofs = OVL_FS(dentry->d_sb);\n\tint err;\n\n\terr = ovl_check_setxattr(ofs, upper, OVL_XATTR_OPAQUE, \"y\", 1, xerr);\n\tif (!err)\n\t\tovl_dentry_set_opaque(dentry);\n\n\treturn err;\n}\n\nstatic int ovl_set_opaque(struct dentry *dentry, struct dentry *upperdentry)\n{\n\t/*\n\t * Fail with -EIO when trying to create opaque dir and upper doesn't\n\t * support xattrs. ovl_rename() calls ovl_set_opaque_xerr(-EXDEV) to\n\t * return a specific error for noxattr case.\n\t */\n\treturn ovl_set_opaque_xerr(dentry, upperdentry, -EIO);\n}\n\n/*\n * Common operations required to be done after creation of file on upper.\n * If @hardlink is false, then @inode is a pre-allocated inode, we may or\n * may not use to instantiate the new dentry.\n */\nstatic int ovl_instantiate(struct dentry *dentry, struct inode *inode,\n\t\t\t   struct dentry *newdentry, bool hardlink, struct file *tmpfile)\n{\n\tstruct ovl_inode_params oip = {\n\t\t.upperdentry = newdentry,\n\t\t.newinode = inode,\n\t};\n\n\tovl_dentry_set_upper_alias(dentry);\n\tovl_dentry_init_reval(dentry, newdentry, NULL);\n\n\tif (!hardlink) {\n\t\t/*\n\t\t * ovl_obtain_alias() can be called after ovl_create_real()\n\t\t * and before we get here, so we may get an inode from cache\n\t\t * with the same real upperdentry that is not the inode we\n\t\t * pre-allocated.  In this case we will use the cached inode\n\t\t * to instantiate the new dentry.\n\t\t *\n\t\t * XXX: if we ever use ovl_obtain_alias() to decode directory\n\t\t * file handles, need to use ovl_get_inode_locked() and\n\t\t * d_instantiate_new() here to prevent from creating two\n\t\t * hashed directory inode aliases.  We then need to return\n\t\t * the obtained alias to ovl_mkdir().\n\t\t */\n\t\tinode = ovl_get_inode(dentry->d_sb, &oip);\n\t\tif (IS_ERR(inode))\n\t\t\treturn PTR_ERR(inode);\n\t\tif (inode == oip.newinode)\n\t\t\tovl_set_flag(OVL_UPPERDATA, inode);\n\t} else {\n\t\tWARN_ON(ovl_inode_real(inode) != d_inode(newdentry));\n\t\tdput(newdentry);\n\t\tinc_nlink(inode);\n\t}\n\n\tif (tmpfile)\n\t\td_mark_tmpfile(tmpfile, inode);\n\n\td_instantiate(dentry, inode);\n\tif (inode != oip.newinode) {\n\t\tpr_warn_ratelimited(\"newly created inode found in cache (%pd2)\\n\",\n\t\t\t\t    dentry);\n\t}\n\n\t/* Force lookup of new upper hardlink to find its lower */\n\tif (hardlink)\n\t\td_drop(dentry);\n\n\treturn 0;\n}\n\nstatic bool ovl_type_merge(struct dentry *dentry)\n{\n\treturn OVL_TYPE_MERGE(ovl_path_type(dentry));\n}\n\nstatic bool ovl_type_origin(struct dentry *dentry)\n{\n\treturn OVL_TYPE_ORIGIN(ovl_path_type(dentry));\n}\n\nstatic int ovl_create_upper(struct dentry *dentry, struct inode *inode,\n\t\t\t    struct ovl_cattr *attr)\n{\n\tstruct ovl_fs *ofs = OVL_FS(dentry->d_sb);\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *udir = upperdir->d_inode;\n\tstruct dentry *newdentry;\n\tint err;\n\n\tinode_lock_nested(udir, I_MUTEX_PARENT);\n\tnewdentry = ovl_create_real(ofs, upperdir,\n\t\t\t\t    ovl_lookup_upper(ofs, dentry->d_name.name,\n\t\t\t\t\t\t     upperdir, dentry->d_name.len),\n\t\t\t\t    attr);\n\tinode_unlock(udir);\n\tif (IS_ERR(newdentry))\n\t\treturn PTR_ERR(newdentry);\n\n\tif (ovl_type_merge(dentry->d_parent) && d_is_dir(newdentry) &&\n\t    !ovl_allow_offline_changes(ofs)) {\n\t\t/* Setting opaque here is just an optimization, allow to fail */\n\t\tovl_set_opaque(dentry, newdentry);\n\t}\n\n\tovl_dir_modified(dentry->d_parent, false);\n\terr = ovl_instantiate(dentry, inode, newdentry, !!attr->hardlink, NULL);\n\tif (err)\n\t\tgoto out_cleanup;\n\treturn 0;\n\nout_cleanup:\n\tovl_cleanup(ofs, upperdir, newdentry);\n\tdput(newdentry);\n\treturn err;\n}\n\nstatic struct dentry *ovl_clear_empty(struct dentry *dentry,\n\t\t\t\t      struct list_head *list)\n{\n\tstruct ovl_fs *ofs = OVL_FS(dentry->d_sb);\n\tstruct dentry *workdir = ovl_workdir(dentry);\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct path upperpath;\n\tstruct dentry *upper;\n\tstruct dentry *opaquedir;\n\tstruct kstat stat;\n\tint err;\n\n\tif (WARN_ON(!workdir))\n\t\treturn ERR_PTR(-EROFS);\n\n\tovl_path_upper(dentry, &upperpath);\n\terr = vfs_getattr(&upperpath, &stat,\n\t\t\t  STATX_BASIC_STATS, AT_STATX_SYNC_AS_STAT);\n\tif (err)\n\t\tgoto out;\n\n\terr = -ESTALE;\n\tif (!S_ISDIR(stat.mode))\n\t\tgoto out;\n\tupper = upperpath.dentry;\n\n\topaquedir = ovl_create_temp(ofs, workdir, OVL_CATTR(stat.mode));\n\terr = PTR_ERR(opaquedir);\n\tif (IS_ERR(opaquedir))\n\t\tgoto out;\n\n\terr = ovl_lock_rename_workdir(workdir, opaquedir, upperdir, upper);\n\tif (err)\n\t\tgoto out_cleanup_unlocked;\n\n\terr = ovl_copy_xattr(dentry->d_sb, &upperpath, opaquedir);\n\tif (err)\n\t\tgoto out_cleanup;\n\n\terr = ovl_set_opaque(dentry, opaquedir);\n\tif (err)\n\t\tgoto out_cleanup;\n\n\tinode_lock(opaquedir->d_inode);\n\terr = ovl_set_attr(ofs, opaquedir, &stat);\n\tinode_unlock(opaquedir->d_inode);\n\tif (err)\n\t\tgoto out_cleanup;\n\n\terr = ovl_do_rename(ofs, workdir, opaquedir, upperdir, upper, RENAME_EXCHANGE);\n\tunlock_rename(workdir, upperdir);\n\tif (err)\n\t\tgoto out_cleanup_unlocked;\n\n\tovl_cleanup_whiteouts(ofs, upper, list);\n\tovl_cleanup(ofs, workdir, upper);\n\n\t/* dentry's upper doesn't match now, get rid of it */\n\td_drop(dentry);\n\n\treturn opaquedir;\n\nout_cleanup:\n\tunlock_rename(workdir, upperdir);\nout_cleanup_unlocked:\n\tovl_cleanup(ofs, workdir, opaquedir);\n\tdput(opaquedir);\nout:\n\treturn ERR_PTR(err);\n}\n\nstatic int ovl_set_upper_acl(struct ovl_fs *ofs, struct dentry *upperdentry,\n\t\t\t     const char *acl_name, struct posix_acl *acl)\n{\n\tif (!IS_ENABLED(CONFIG_FS_POSIX_ACL) || !acl)\n\t\treturn 0;\n\n\treturn ovl_do_set_acl(ofs, upperdentry, acl_name, acl);\n}\n\nstatic int ovl_create_over_whiteout(struct dentry *dentry, struct inode *inode,\n\t\t\t\t    struct ovl_cattr *cattr)\n{\n\tstruct ovl_fs *ofs = OVL_FS(dentry->d_sb);\n\tstruct dentry *workdir = ovl_workdir(dentry);\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct dentry *upper;\n\tstruct dentry *newdentry;\n\tint err;\n\tstruct posix_acl *acl, *default_acl;\n\tbool hardlink = !!cattr->hardlink;\n\n\tif (WARN_ON(!workdir))\n\t\treturn -EROFS;\n\n\tif (!hardlink) {\n\t\terr = posix_acl_create(dentry->d_parent->d_inode,\n\t\t\t\t       &cattr->mode, &default_acl, &acl);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tupper = ovl_lookup_upper_unlocked(ofs, dentry->d_name.name, upperdir,\n\t\t\t\t\t  dentry->d_name.len);\n\terr = PTR_ERR(upper);\n\tif (IS_ERR(upper))\n\t\tgoto out;\n\n\terr = -ESTALE;\n\tif (d_is_negative(upper) || !ovl_upper_is_whiteout(ofs, upper))\n\t\tgoto out_dput;\n\n\tnewdentry = ovl_create_temp(ofs, workdir, cattr);\n\terr = PTR_ERR(newdentry);\n\tif (IS_ERR(newdentry))\n\t\tgoto out_dput;\n\n\terr = ovl_lock_rename_workdir(workdir, newdentry, upperdir, upper);\n\tif (err)\n\t\tgoto out_cleanup_unlocked;\n\n\t/*\n\t * mode could have been mutilated due to umask (e.g. sgid directory)\n\t */\n\tif (!hardlink &&\n\t    !S_ISLNK(cattr->mode) &&\n\t    newdentry->d_inode->i_mode != cattr->mode) {\n\t\tstruct iattr attr = {\n\t\t\t.ia_valid = ATTR_MODE,\n\t\t\t.ia_mode = cattr->mode,\n\t\t};\n\t\tinode_lock(newdentry->d_inode);\n\t\terr = ovl_do_notify_change(ofs, newdentry, &attr);\n\t\tinode_unlock(newdentry->d_inode);\n\t\tif (err)\n\t\t\tgoto out_cleanup;\n\t}\n\tif (!hardlink) {\n\t\terr = ovl_set_upper_acl(ofs, newdentry,\n\t\t\t\t\tXATTR_NAME_POSIX_ACL_ACCESS, acl);\n\t\tif (err)\n\t\t\tgoto out_cleanup;\n\n\t\terr = ovl_set_upper_acl(ofs, newdentry,\n\t\t\t\t\tXATTR_NAME_POSIX_ACL_DEFAULT, default_acl);\n\t\tif (err)\n\t\t\tgoto out_cleanup;\n\t}\n\n\tif (!hardlink && S_ISDIR(cattr->mode)) {\n\t\terr = ovl_set_opaque(dentry, newdentry);\n\t\tif (err)\n\t\t\tgoto out_cleanup;\n\n\t\terr = ovl_do_rename(ofs, workdir, newdentry, upperdir, upper,\n\t\t\t\t    RENAME_EXCHANGE);\n\t\tunlock_rename(workdir, upperdir);\n\t\tif (err)\n\t\t\tgoto out_cleanup_unlocked;\n\n\t\tovl_cleanup(ofs, workdir, upper);\n\t} else {\n\t\terr = ovl_do_rename(ofs, workdir, newdentry, upperdir, upper, 0);\n\t\tunlock_rename(workdir, upperdir);\n\t\tif (err)\n\t\t\tgoto out_cleanup_unlocked;\n\t}\n\tovl_dir_modified(dentry->d_parent, false);\n\terr = ovl_instantiate(dentry, inode, newdentry, hardlink, NULL);\n\tif (err) {\n\t\tovl_cleanup(ofs, upperdir, newdentry);\n\t\tdput(newdentry);\n\t}\nout_dput:\n\tdput(upper);\nout:\n\tif (!hardlink) {\n\t\tposix_acl_release(acl);\n\t\tposix_acl_release(default_acl);\n\t}\n\treturn err;\n\nout_cleanup:\n\tunlock_rename(workdir, upperdir);\nout_cleanup_unlocked:\n\tovl_cleanup(ofs, workdir, newdentry);\n\tdput(newdentry);\n\tgoto out_dput;\n}\n\nstatic const struct cred *ovl_setup_cred_for_create(struct dentry *dentry,\n\t\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t\t    umode_t mode,\n\t\t\t\t\t\t    const struct cred *old_cred)\n{\n\tint err;\n\tstruct cred *override_cred;\n\n\toverride_cred = prepare_creds();\n\tif (!override_cred)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\toverride_cred->fsuid = inode->i_uid;\n\toverride_cred->fsgid = inode->i_gid;\n\terr = security_dentry_create_files_as(dentry, mode, &dentry->d_name,\n\t\t\t\t\t      old_cred, override_cred);\n\tif (err) {\n\t\tput_cred(override_cred);\n\t\treturn ERR_PTR(err);\n\t}\n\n\t/*\n\t * Caller is going to match this with revert_creds() and drop\n\t * referenec on the returned creds.\n\t * We must be called with creator creds already, otherwise we risk\n\t * leaking creds.\n\t */\n\told_cred = override_creds(override_cred);\n\tWARN_ON_ONCE(old_cred != ovl_creds(dentry->d_sb));\n\n\treturn override_cred;\n}\n\nstatic int ovl_create_or_link(struct dentry *dentry, struct inode *inode,\n\t\t\t      struct ovl_cattr *attr, bool origin)\n{\n\tint err;\n\tconst struct cred *old_cred, *new_cred = NULL;\n\tstruct dentry *parent = dentry->d_parent;\n\n\told_cred = ovl_override_creds(dentry->d_sb);\n\n\t/*\n\t * When linking a file with copy up origin into a new parent, mark the\n\t * new parent dir \"impure\".\n\t */\n\tif (origin) {\n\t\terr = ovl_set_impure(parent, ovl_dentry_upper(parent));\n\t\tif (err)\n\t\t\tgoto out_revert_creds;\n\t}\n\n\tif (!attr->hardlink) {\n\t\t/*\n\t\t * In the creation cases(create, mkdir, mknod, symlink),\n\t\t * ovl should transfer current's fs{u,g}id to underlying\n\t\t * fs. Because underlying fs want to initialize its new\n\t\t * inode owner using current's fs{u,g}id. And in this\n\t\t * case, the @inode is a new inode that is initialized\n\t\t * in inode_init_owner() to current's fs{u,g}id. So use\n\t\t * the inode's i_{u,g}id to override the cred's fs{u,g}id.\n\t\t *\n\t\t * But in the other hardlink case, ovl_link() does not\n\t\t * create a new inode, so just use the ovl mounter's\n\t\t * fs{u,g}id.\n\t\t */\n\t\tnew_cred = ovl_setup_cred_for_create(dentry, inode, attr->mode,\n\t\t\t\t\t\t     old_cred);\n\t\terr = PTR_ERR(new_cred);\n\t\tif (IS_ERR(new_cred)) {\n\t\t\tnew_cred = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\tif (!ovl_dentry_is_whiteout(dentry))\n\t\terr = ovl_create_upper(dentry, inode, attr);\n\telse\n\t\terr = ovl_create_over_whiteout(dentry, inode, attr);\n\nout_revert_creds:\n\tovl_revert_creds(old_cred);\n\tput_cred(new_cred);\n\treturn err;\n}\n\nstatic int ovl_create_object(struct dentry *dentry, int mode, dev_t rdev,\n\t\t\t     const char *link)\n{\n\tint err;\n\tstruct inode *inode;\n\tstruct ovl_cattr attr = {\n\t\t.rdev = rdev,\n\t\t.link = link,\n\t};\n\n\terr = ovl_copy_up(dentry->d_parent);\n\tif (err)\n\t\treturn err;\n\n\terr = ovl_want_write(dentry);\n\tif (err)\n\t\tgoto out;\n\n\t/* Preallocate inode to be used by ovl_get_inode() */\n\terr = -ENOMEM;\n\tinode = ovl_new_inode(dentry->d_sb, mode, rdev);\n\tif (!inode)\n\t\tgoto out_drop_write;\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_state |= I_CREATING;\n\tspin_unlock(&inode->i_lock);\n\n\tinode_init_owner(&nop_mnt_idmap, inode, dentry->d_parent->d_inode, mode);\n\tattr.mode = inode->i_mode;\n\n\terr = ovl_create_or_link(dentry, inode, &attr, false);\n\t/* Did we end up using the preallocated inode? */\n\tif (inode != d_inode(dentry))\n\t\tiput(inode);\n\nout_drop_write:\n\tovl_drop_write(dentry);\nout:\n\treturn err;\n}\n\nstatic int ovl_create(struct mnt_idmap *idmap, struct inode *dir,\n\t\t      struct dentry *dentry, umode_t mode, bool excl)\n{\n\treturn ovl_create_object(dentry, (mode & 07777) | S_IFREG, 0, NULL);\n}\n\nstatic struct dentry *ovl_mkdir(struct mnt_idmap *idmap, struct inode *dir,\n\t\t\t\tstruct dentry *dentry, umode_t mode)\n{\n\treturn ERR_PTR(ovl_create_object(dentry, (mode & 07777) | S_IFDIR, 0, NULL));\n}\n\nstatic int ovl_mknod(struct mnt_idmap *idmap, struct inode *dir,\n\t\t     struct dentry *dentry, umode_t mode, dev_t rdev)\n{\n\t/* Don't allow creation of \"whiteout\" on overlay */\n\tif (S_ISCHR(mode) && rdev == WHITEOUT_DEV)\n\t\treturn -EPERM;\n\n\treturn ovl_create_object(dentry, mode, rdev, NULL);\n}\n\nstatic int ovl_symlink(struct mnt_idmap *idmap, struct inode *dir,\n\t\t       struct dentry *dentry, const char *link)\n{\n\treturn ovl_create_object(dentry, S_IFLNK, 0, link);\n}\n\nstatic int ovl_set_link_redirect(struct dentry *dentry)\n{\n\tconst struct cred *old_cred;\n\tint err;\n\n\told_cred = ovl_override_creds(dentry->d_sb);\n\terr = ovl_set_redirect(dentry, false);\n\tovl_revert_creds(old_cred);\n\n\treturn err;\n}\n\nstatic int ovl_link(struct dentry *old, struct inode *newdir,\n\t\t    struct dentry *new)\n{\n\tint err;\n\tstruct inode *inode;\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_nlink_start(old);\n\tif (err)\n\t\tgoto out;\n\n\tif (ovl_is_metacopy_dentry(old)) {\n\t\terr = ovl_set_link_redirect(old);\n\t\tif (err)\n\t\t\tgoto out_nlink_end;\n\t}\n\n\tinode = d_inode(old);\n\tihold(inode);\n\n\terr = ovl_create_or_link(new, inode,\n\t\t\t&(struct ovl_cattr) {.hardlink = ovl_dentry_upper(old)},\n\t\t\tovl_type_origin(old));\n\tif (err)\n\t\tiput(inode);\n\nout_nlink_end:\n\tovl_nlink_end(old);\nout:\n\treturn err;\n}\n\nstatic bool ovl_matches_upper(struct dentry *dentry, struct dentry *upper)\n{\n\treturn d_inode(ovl_dentry_upper(dentry)) == d_inode(upper);\n}\n\nstatic int ovl_remove_and_whiteout(struct dentry *dentry,\n\t\t\t\t   struct list_head *list)\n{\n\tstruct ovl_fs *ofs = OVL_FS(dentry->d_sb);\n\tstruct dentry *workdir = ovl_workdir(dentry);\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct dentry *upper;\n\tstruct dentry *opaquedir = NULL;\n\tint err;\n\n\tif (WARN_ON(!workdir))\n\t\treturn -EROFS;\n\n\tif (!list_empty(list)) {\n\t\topaquedir = ovl_clear_empty(dentry, list);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir))\n\t\t\tgoto out;\n\t}\n\n\tupper = ovl_lookup_upper_unlocked(ofs, dentry->d_name.name, upperdir,\n\t\t\t\t\t  dentry->d_name.len);\n\terr = PTR_ERR(upper);\n\tif (IS_ERR(upper))\n\t\tgoto out_dput;\n\n\terr = -ESTALE;\n\tif ((opaquedir && upper != opaquedir) ||\n\t    (!opaquedir && ovl_dentry_upper(dentry) &&\n\t     !ovl_matches_upper(dentry, upper))) {\n\t\tgoto out_dput_upper;\n\t}\n\n\terr = ovl_cleanup_and_whiteout(ofs, upperdir, upper);\n\tif (!err)\n\t\tovl_dir_modified(dentry->d_parent, true);\n\n\td_drop(dentry);\nout_dput_upper:\n\tdput(upper);\nout_dput:\n\tdput(opaquedir);\nout:\n\treturn err;\n}\n\nstatic int ovl_remove_upper(struct dentry *dentry, bool is_dir,\n\t\t\t    struct list_head *list)\n{\n\tstruct ovl_fs *ofs = OVL_FS(dentry->d_sb);\n\tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n\tstruct inode *dir = upperdir->d_inode;\n\tstruct dentry *upper;\n\tstruct dentry *opaquedir = NULL;\n\tint err;\n\n\tif (!list_empty(list)) {\n\t\topaquedir = ovl_clear_empty(dentry, list);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir))\n\t\t\tgoto out;\n\t}\n\n\tinode_lock_nested(dir, I_MUTEX_PARENT);\n\tupper = ovl_lookup_upper(ofs, dentry->d_name.name, upperdir,\n\t\t\t\t dentry->d_name.len);\n\terr = PTR_ERR(upper);\n\tif (IS_ERR(upper))\n\t\tgoto out_unlock;\n\n\terr = -ESTALE;\n\tif ((opaquedir && upper != opaquedir) ||\n\t    (!opaquedir && !ovl_matches_upper(dentry, upper)))\n\t\tgoto out_dput_upper;\n\n\tif (is_dir)\n\t\terr = ovl_do_rmdir(ofs, dir, upper);\n\telse\n\t\terr = ovl_do_unlink(ofs, dir, upper);\n\tovl_dir_modified(dentry->d_parent, ovl_type_origin(dentry));\n\n\t/*\n\t * Keeping this dentry hashed would mean having to release\n\t * upperpath/lowerpath, which could only be done if we are the\n\t * sole user of this dentry.  Too tricky...  Just unhash for\n\t * now.\n\t */\n\tif (!err)\n\t\td_drop(dentry);\nout_dput_upper:\n\tdput(upper);\nout_unlock:\n\tinode_unlock(dir);\n\tdput(opaquedir);\nout:\n\treturn err;\n}\n\nstatic bool ovl_pure_upper(struct dentry *dentry)\n{\n\treturn !ovl_dentry_lower(dentry) &&\n\t       !ovl_test_flag(OVL_WHITEOUTS, d_inode(dentry));\n}\n\nstatic void ovl_drop_nlink(struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tstruct dentry *alias;\n\n\t/* Try to find another, hashed alias */\n\tspin_lock(&inode->i_lock);\n\thlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {\n\t\tif (alias != dentry && !d_unhashed(alias))\n\t\t\tbreak;\n\t}\n\tspin_unlock(&inode->i_lock);\n\n\t/*\n\t * Changes to underlying layers may cause i_nlink to lose sync with\n\t * reality.  In this case prevent the link count from going to zero\n\t * prematurely.\n\t */\n\tif (inode->i_nlink > !!alias)\n\t\tdrop_nlink(inode);\n}\n\nstatic int ovl_do_remove(struct dentry *dentry, bool is_dir)\n{\n\tint err;\n\tconst struct cred *old_cred;\n\tbool lower_positive = ovl_lower_positive(dentry);\n\tLIST_HEAD(list);\n\n\t/* No need to clean pure upper removed by vfs_rmdir() */\n\tif (is_dir && (lower_positive || !ovl_pure_upper(dentry))) {\n\t\terr = ovl_check_empty_dir(dentry, &list);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = ovl_copy_up(dentry->d_parent);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_nlink_start(dentry);\n\tif (err)\n\t\tgoto out;\n\n\told_cred = ovl_override_creds(dentry->d_sb);\n\tif (!lower_positive)\n\t\terr = ovl_remove_upper(dentry, is_dir, &list);\n\telse\n\t\terr = ovl_remove_and_whiteout(dentry, &list);\n\tovl_revert_creds(old_cred);\n\tif (!err) {\n\t\tif (is_dir)\n\t\t\tclear_nlink(dentry->d_inode);\n\t\telse\n\t\t\tovl_drop_nlink(dentry);\n\t}\n\tovl_nlink_end(dentry);\n\n\t/*\n\t * Copy ctime\n\t *\n\t * Note: we fail to update ctime if there was no copy-up, only a\n\t * whiteout\n\t */\n\tif (ovl_dentry_upper(dentry))\n\t\tovl_copyattr(d_inode(dentry));\n\nout:\n\tovl_cache_free(&list);\n\treturn err;\n}\n\nstatic int ovl_unlink(struct inode *dir, struct dentry *dentry)\n{\n\treturn ovl_do_remove(dentry, false);\n}\n\nstatic int ovl_rmdir(struct inode *dir, struct dentry *dentry)\n{\n\treturn ovl_do_remove(dentry, true);\n}\n\nstatic bool ovl_type_merge_or_lower(struct dentry *dentry)\n{\n\tenum ovl_path_type type = ovl_path_type(dentry);\n\n\treturn OVL_TYPE_MERGE(type) || !OVL_TYPE_UPPER(type);\n}\n\nstatic bool ovl_can_move(struct dentry *dentry)\n{\n\treturn ovl_redirect_dir(OVL_FS(dentry->d_sb)) ||\n\t\t!d_is_dir(dentry) || !ovl_type_merge_or_lower(dentry);\n}\n\nstatic char *ovl_get_redirect(struct dentry *dentry, bool abs_redirect)\n{\n\tchar *buf, *ret;\n\tstruct dentry *d, *tmp;\n\tint buflen = ovl_redirect_max + 1;\n\n\tif (!abs_redirect) {\n\t\tret = kstrndup(dentry->d_name.name, dentry->d_name.len,\n\t\t\t       GFP_KERNEL);\n\t\tgoto out;\n\t}\n\n\tbuf = ret = kmalloc(buflen, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tbuflen--;\n\tbuf[buflen] = '\\0';\n\tfor (d = dget(dentry); !IS_ROOT(d);) {\n\t\tconst char *name;\n\t\tint thislen;\n\n\t\tspin_lock(&d->d_lock);\n\t\tname = ovl_dentry_get_redirect(d);\n\t\tif (name) {\n\t\t\tthislen = strlen(name);\n\t\t} else {\n\t\t\tname = d->d_name.name;\n\t\t\tthislen = d->d_name.len;\n\t\t}\n\n\t\t/* If path is too long, fall back to userspace move */\n\t\tif (thislen + (name[0] != '/') > buflen) {\n\t\t\tret = ERR_PTR(-EXDEV);\n\t\t\tspin_unlock(&d->d_lock);\n\t\t\tgoto out_put;\n\t\t}\n\n\t\tbuflen -= thislen;\n\t\tmemcpy(&buf[buflen], name, thislen);\n\t\tspin_unlock(&d->d_lock);\n\t\ttmp = dget_parent(d);\n\n\t\tdput(d);\n\t\td = tmp;\n\n\t\t/* Absolute redirect: finished */\n\t\tif (buf[buflen] == '/')\n\t\t\tbreak;\n\t\tbuflen--;\n\t\tbuf[buflen] = '/';\n\t}\n\tret = kstrdup(&buf[buflen], GFP_KERNEL);\nout_put:\n\tdput(d);\n\tkfree(buf);\nout:\n\treturn ret ? ret : ERR_PTR(-ENOMEM);\n}\n\nstatic bool ovl_need_absolute_redirect(struct dentry *dentry, bool samedir)\n{\n\tstruct dentry *lowerdentry;\n\n\tif (!samedir)\n\t\treturn true;\n\n\tif (d_is_dir(dentry))\n\t\treturn false;\n\n\t/*\n\t * For non-dir hardlinked files, we need absolute redirects\n\t * in general as two upper hardlinks could be in different\n\t * dirs. We could put a relative redirect now and convert\n\t * it to absolute redirect later. But when nlink > 1 and\n\t * indexing is on, that means relative redirect needs to be\n\t * converted to absolute during copy up of another lower\n\t * hardllink as well.\n\t *\n\t * So without optimizing too much, just check if lower is\n\t * a hard link or not. If lower is hard link, put absolute\n\t * redirect.\n\t */\n\tlowerdentry = ovl_dentry_lower(dentry);\n\treturn (d_inode(lowerdentry)->i_nlink > 1);\n}\n\nstatic int ovl_set_redirect(struct dentry *dentry, bool samedir)\n{\n\tint err;\n\tstruct ovl_fs *ofs = OVL_FS(dentry->d_sb);\n\tconst char *redirect = ovl_dentry_get_redirect(dentry);\n\tbool absolute_redirect = ovl_need_absolute_redirect(dentry, samedir);\n\n\tif (redirect && (!absolute_redirect || redirect[0] == '/'))\n\t\treturn 0;\n\n\tredirect = ovl_get_redirect(dentry, absolute_redirect);\n\tif (IS_ERR(redirect))\n\t\treturn PTR_ERR(redirect);\n\n\terr = ovl_check_setxattr(ofs, ovl_dentry_upper(dentry),\n\t\t\t\t OVL_XATTR_REDIRECT,\n\t\t\t\t redirect, strlen(redirect), -EXDEV);\n\tif (!err) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tovl_dentry_set_redirect(dentry, redirect);\n\t\tspin_unlock(&dentry->d_lock);\n\t} else {\n\t\tkfree(redirect);\n\t\tpr_warn_ratelimited(\"failed to set redirect (%i)\\n\",\n\t\t\t\t    err);\n\t\t/* Fall back to userspace copy-up */\n\t\terr = -EXDEV;\n\t}\n\treturn err;\n}\n\nstatic int ovl_rename(struct mnt_idmap *idmap, struct inode *olddir,\n\t\t      struct dentry *old, struct inode *newdir,\n\t\t      struct dentry *new, unsigned int flags)\n{\n\tint err;\n\tstruct dentry *old_upperdir;\n\tstruct dentry *new_upperdir;\n\tstruct dentry *olddentry = NULL;\n\tstruct dentry *newdentry = NULL;\n\tstruct dentry *trap, *de;\n\tbool old_opaque;\n\tbool new_opaque;\n\tbool cleanup_whiteout = false;\n\tbool update_nlink = false;\n\tbool overwrite = !(flags & RENAME_EXCHANGE);\n\tbool is_dir = d_is_dir(old);\n\tbool new_is_dir = d_is_dir(new);\n\tbool samedir = olddir == newdir;\n\tstruct dentry *opaquedir = NULL;\n\tconst struct cred *old_cred = NULL;\n\tstruct ovl_fs *ofs = OVL_FS(old->d_sb);\n\tLIST_HEAD(list);\n\n\terr = -EINVAL;\n\tif (flags & ~(RENAME_EXCHANGE | RENAME_NOREPLACE))\n\t\tgoto out;\n\n\tflags &= ~RENAME_NOREPLACE;\n\n\t/* Don't copy up directory trees */\n\terr = -EXDEV;\n\tif (!ovl_can_move(old))\n\t\tgoto out;\n\tif (!overwrite && !ovl_can_move(new))\n\t\tgoto out;\n\n\tif (overwrite && new_is_dir && !ovl_pure_upper(new)) {\n\t\terr = ovl_check_empty_dir(new, &list);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (overwrite) {\n\t\tif (ovl_lower_positive(old)) {\n\t\t\tif (!ovl_dentry_is_whiteout(new)) {\n\t\t\t\t/* Whiteout source */\n\t\t\t\tflags |= RENAME_WHITEOUT;\n\t\t\t} else {\n\t\t\t\t/* Switch whiteouts */\n\t\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\t}\n\t\t} else if (is_dir && ovl_dentry_is_whiteout(new)) {\n\t\t\tflags |= RENAME_EXCHANGE;\n\t\t\tcleanup_whiteout = true;\n\t\t}\n\t}\n\n\terr = ovl_copy_up(old);\n\tif (err)\n\t\tgoto out;\n\n\terr = ovl_copy_up(new->d_parent);\n\tif (err)\n\t\tgoto out;\n\tif (!overwrite) {\n\t\terr = ovl_copy_up(new);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else if (d_inode(new)) {\n\t\terr = ovl_nlink_start(new);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tupdate_nlink = true;\n\t}\n\n\tif (!update_nlink) {\n\t\t/* ovl_nlink_start() took ovl_want_write() */\n\t\terr = ovl_want_write(old);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\told_cred = ovl_override_creds(old->d_sb);\n\n\tif (!list_empty(&list)) {\n\t\topaquedir = ovl_clear_empty(new, &list);\n\t\terr = PTR_ERR(opaquedir);\n\t\tif (IS_ERR(opaquedir)) {\n\t\t\topaquedir = NULL;\n\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\told_upperdir = ovl_dentry_upper(old->d_parent);\n\tnew_upperdir = ovl_dentry_upper(new->d_parent);\n\n\tif (!samedir) {\n\t\t/*\n\t\t * When moving a merge dir or non-dir with copy up origin into\n\t\t * a new parent, we are marking the new parent dir \"impure\".\n\t\t * When ovl_iterate() iterates an \"impure\" upper dir, it will\n\t\t * lookup the origin inodes of the entries to fill d_ino.\n\t\t */\n\t\tif (ovl_type_origin(old)) {\n\t\t\terr = ovl_set_impure(new->d_parent, new_upperdir);\n\t\t\tif (err)\n\t\t\t\tgoto out_revert_creds;\n\t\t}\n\t\tif (!overwrite && ovl_type_origin(new)) {\n\t\t\terr = ovl_set_impure(old->d_parent, old_upperdir);\n\t\t\tif (err)\n\t\t\t\tgoto out_revert_creds;\n\t\t}\n\t}\n\n\ttrap = lock_rename(new_upperdir, old_upperdir);\n\tif (IS_ERR(trap)) {\n\t\terr = PTR_ERR(trap);\n\t\tgoto out_revert_creds;\n\t}\n\n\tde = ovl_lookup_upper(ofs, old->d_name.name, old_upperdir,\n\t\t\t      old->d_name.len);\n\terr = PTR_ERR(de);\n\tif (IS_ERR(de))\n\t\tgoto out_unlock;\n\tolddentry = de;\n\n\terr = -ESTALE;\n\tif (!ovl_matches_upper(old, olddentry))\n\t\tgoto out_unlock;\n\n\tde = ovl_lookup_upper(ofs, new->d_name.name, new_upperdir,\n\t\t\t      new->d_name.len);\n\terr = PTR_ERR(de);\n\tif (IS_ERR(de))\n\t\tgoto out_unlock;\n\tnewdentry = de;\n\n\told_opaque = ovl_dentry_is_opaque(old);\n\tnew_opaque = ovl_dentry_is_opaque(new);\n\n\terr = -ESTALE;\n\tif (d_inode(new) && ovl_dentry_upper(new)) {\n\t\tif (opaquedir) {\n\t\t\tif (newdentry != opaquedir)\n\t\t\t\tgoto out_unlock;\n\t\t} else {\n\t\t\tif (!ovl_matches_upper(new, newdentry))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\tif (!d_is_negative(newdentry)) {\n\t\t\tif (!new_opaque || !ovl_upper_is_whiteout(ofs, newdentry))\n\t\t\t\tgoto out_unlock;\n\t\t} else {\n\t\t\tif (flags & RENAME_EXCHANGE)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tif (olddentry == trap)\n\t\tgoto out_unlock;\n\tif (newdentry == trap)\n\t\tgoto out_unlock;\n\n\tif (olddentry->d_inode == newdentry->d_inode)\n\t\tgoto out_unlock;\n\n\terr = 0;\n\tif (ovl_type_merge_or_lower(old))\n\t\terr = ovl_set_redirect(old, samedir);\n\telse if (is_dir && !old_opaque && ovl_type_merge(new->d_parent))\n\t\terr = ovl_set_opaque_xerr(old, olddentry, -EXDEV);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tif (!overwrite && ovl_type_merge_or_lower(new))\n\t\terr = ovl_set_redirect(new, samedir);\n\telse if (!overwrite && new_is_dir && !new_opaque &&\n\t\t ovl_type_merge(old->d_parent))\n\t\terr = ovl_set_opaque_xerr(new, newdentry, -EXDEV);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = ovl_do_rename(ofs, old_upperdir, olddentry,\n\t\t\t    new_upperdir, newdentry, flags);\n\tunlock_rename(new_upperdir, old_upperdir);\n\tif (err)\n\t\tgoto out_revert_creds;\n\n\tif (cleanup_whiteout)\n\t\tovl_cleanup(ofs, old_upperdir, newdentry);\n\n\tif (overwrite && d_inode(new)) {\n\t\tif (new_is_dir)\n\t\t\tclear_nlink(d_inode(new));\n\t\telse\n\t\t\tovl_drop_nlink(new);\n\t}\n\n\tovl_dir_modified(old->d_parent, ovl_type_origin(old) ||\n\t\t\t (!overwrite && ovl_type_origin(new)));\n\tovl_dir_modified(new->d_parent, ovl_type_origin(old) ||\n\t\t\t (d_inode(new) && ovl_type_origin(new)));\n\n\t/* copy ctime: */\n\tovl_copyattr(d_inode(old));\n\tif (d_inode(new) && ovl_dentry_upper(new))\n\t\tovl_copyattr(d_inode(new));\n\nout_revert_creds:\n\tovl_revert_creds(old_cred);\n\tif (update_nlink)\n\t\tovl_nlink_end(new);\n\telse\n\t\tovl_drop_write(old);\nout:\n\tdput(newdentry);\n\tdput(olddentry);\n\tdput(opaquedir);\n\tovl_cache_free(&list);\n\treturn err;\n\nout_unlock:\n\tunlock_rename(new_upperdir, old_upperdir);\n\tgoto out_revert_creds;\n}\n\nstatic int ovl_create_tmpfile(struct file *file, struct dentry *dentry,\n\t\t\t      struct inode *inode, umode_t mode)\n{\n\tconst struct cred *old_cred, *new_cred = NULL;\n\tstruct path realparentpath;\n\tstruct file *realfile;\n\tstruct ovl_file *of;\n\tstruct dentry *newdentry;\n\t/* It's okay to set O_NOATIME, since the owner will be current fsuid */\n\tint flags = file->f_flags | OVL_OPEN_FLAGS;\n\tint err;\n\n\told_cred = ovl_override_creds(dentry->d_sb);\n\tnew_cred = ovl_setup_cred_for_create(dentry, inode, mode, old_cred);\n\terr = PTR_ERR(new_cred);\n\tif (IS_ERR(new_cred)) {\n\t\tnew_cred = NULL;\n\t\tgoto out_revert_creds;\n\t}\n\n\tovl_path_upper(dentry->d_parent, &realparentpath);\n\trealfile = backing_tmpfile_open(&file->f_path, flags, &realparentpath,\n\t\t\t\t\tmode, current_cred());\n\terr = PTR_ERR_OR_ZERO(realfile);\n\tpr_debug(\"tmpfile/open(%pd2, 0%o) = %i\\n\", realparentpath.dentry, mode, err);\n\tif (err)\n\t\tgoto out_revert_creds;\n\n\tof = ovl_file_alloc(realfile);\n\tif (!of) {\n\t\tfput(realfile);\n\t\terr = -ENOMEM;\n\t\tgoto out_revert_creds;\n\t}\n\n\t/* ovl_instantiate() consumes the newdentry reference on success */\n\tnewdentry = dget(realfile->f_path.dentry);\n\terr = ovl_instantiate(dentry, inode, newdentry, false, file);\n\tif (!err) {\n\t\tfile->private_data = of;\n\t} else {\n\t\tdput(newdentry);\n\t\tovl_file_free(of);\n\t}\nout_revert_creds:\n\tovl_revert_creds(old_cred);\n\tput_cred(new_cred);\n\treturn err;\n}\n\nstatic int ovl_dummy_open(struct inode *inode, struct file *file)\n{\n\treturn 0;\n}\n\nstatic int ovl_tmpfile(struct mnt_idmap *idmap, struct inode *dir,\n\t\t       struct file *file, umode_t mode)\n{\n\tint err;\n\tstruct dentry *dentry = file->f_path.dentry;\n\tstruct inode *inode;\n\n\tif (!OVL_FS(dentry->d_sb)->tmpfile)\n\t\treturn -EOPNOTSUPP;\n\n\terr = ovl_copy_up(dentry->d_parent);\n\tif (err)\n\t\treturn err;\n\n\terr = ovl_want_write(dentry);\n\tif (err)\n\t\treturn err;\n\n\terr = -ENOMEM;\n\tinode = ovl_new_inode(dentry->d_sb, mode, 0);\n\tif (!inode)\n\t\tgoto drop_write;\n\n\tinode_init_owner(&nop_mnt_idmap, inode, dir, mode);\n\terr = ovl_create_tmpfile(file, dentry, inode, inode->i_mode);\n\tif (err)\n\t\tgoto put_inode;\n\n\t/*\n\t * Check if the preallocated inode was actually used.  Having something\n\t * else assigned to the dentry shouldn't happen as that would indicate\n\t * that the backing tmpfile \"leaked\" out of overlayfs.\n\t */\n\terr = -EIO;\n\tif (WARN_ON(inode != d_inode(dentry)))\n\t\tgoto put_realfile;\n\n\t/* inode reference was transferred to dentry */\n\tinode = NULL;\n\terr = finish_open(file, dentry, ovl_dummy_open);\nput_realfile:\n\t/* Without FMODE_OPENED ->release() won't be called on @file */\n\tif (!(file->f_mode & FMODE_OPENED))\n\t\tovl_file_free(file->private_data);\nput_inode:\n\tiput(inode);\ndrop_write:\n\tovl_drop_write(dentry);\n\treturn err;\n}\n\nconst struct inode_operations ovl_dir_inode_operations = {\n\t.lookup\t\t= ovl_lookup,\n\t.mkdir\t\t= ovl_mkdir,\n\t.symlink\t= ovl_symlink,\n\t.unlink\t\t= ovl_unlink,\n\t.rmdir\t\t= ovl_rmdir,\n\t.rename\t\t= ovl_rename,\n\t.link\t\t= ovl_link,\n\t.setattr\t= ovl_setattr,\n\t.create\t\t= ovl_create,\n\t.mknod\t\t= ovl_mknod,\n\t.permission\t= ovl_permission,\n\t.getattr\t= ovl_getattr,\n\t.listxattr\t= ovl_listxattr,\n\t.get_inode_acl\t= ovl_get_inode_acl,\n\t.get_acl\t= ovl_get_acl,\n\t.set_acl\t= ovl_set_acl,\n\t.update_time\t= ovl_update_time,\n\t.fileattr_get\t= ovl_fileattr_get,\n\t.fileattr_set\t= ovl_fileattr_set,\n\t.tmpfile\t= ovl_tmpfile,\n};\n", "patch": "@@ -596,21 +596,25 @@ static int ovl_remove_upper(struct dentry *dentry, bool is_dir)\n {\n \tstruct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);\n \tstruct inode *dir = upperdir->d_inode;\n-\tstruct dentry *upper = ovl_dentry_upper(dentry);\n+\tstruct dentry *upper;\n \tint err;\n \n \tinode_lock_nested(dir, I_MUTEX_PARENT);\n+\tupper = lookup_one_len(dentry->d_name.name, upperdir,\n+\t\t\t       dentry->d_name.len);\n+\terr = PTR_ERR(upper);\n+\tif (IS_ERR(upper))\n+\t\tgoto out_unlock;\n+\n \terr = -ESTALE;\n-\tif (upper->d_parent == upperdir) {\n-\t\t/* Don't let d_delete() think it can reset d_inode */\n-\t\tdget(upper);\n+\tif (upper == ovl_dentry_upper(dentry)) {\n \t\tif (is_dir)\n \t\t\terr = vfs_rmdir(dir, upper);\n \t\telse\n \t\t\terr = vfs_unlink(dir, upper, NULL);\n-\t\tdput(upper);\n \t\tovl_dentry_version_inc(dentry->d_parent);\n \t}\n+\tdput(upper);\n \n \t/*\n \t * Keeping this dentry hashed would mean having to release\n@@ -620,6 +624,7 @@ static int ovl_remove_upper(struct dentry *dentry, bool is_dir)\n \t */\n \tif (!err)\n \t\td_drop(dentry);\n+out_unlock:\n \tinode_unlock(dir);\n \n \treturn err;\n@@ -840,29 +845,39 @@ static int ovl_rename2(struct inode *olddir, struct dentry *old,\n \n \ttrap = lock_rename(new_upperdir, old_upperdir);\n \n-\tolddentry = ovl_dentry_upper(old);\n-\tnewdentry = ovl_dentry_upper(new);\n-\tif (newdentry) {\n+\n+\tolddentry = lookup_one_len(old->d_name.name, old_upperdir,\n+\t\t\t\t   old->d_name.len);\n+\terr = PTR_ERR(olddentry);\n+\tif (IS_ERR(olddentry))\n+\t\tgoto out_unlock;\n+\n+\terr = -ESTALE;\n+\tif (olddentry != ovl_dentry_upper(old))\n+\t\tgoto out_dput_old;\n+\n+\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n+\t\t\t\t   new->d_name.len);\n+\terr = PTR_ERR(newdentry);\n+\tif (IS_ERR(newdentry))\n+\t\tgoto out_dput_old;\n+\n+\terr = -ESTALE;\n+\tif (ovl_dentry_upper(new)) {\n \t\tif (opaquedir) {\n-\t\t\tnewdentry = opaquedir;\n-\t\t\topaquedir = NULL;\n+\t\t\tif (newdentry != opaquedir)\n+\t\t\t\tgoto out_dput;\n \t\t} else {\n-\t\t\tdget(newdentry);\n+\t\t\tif (newdentry != ovl_dentry_upper(new))\n+\t\t\t\tgoto out_dput;\n \t\t}\n \t} else {\n \t\tnew_create = true;\n-\t\tnewdentry = lookup_one_len(new->d_name.name, new_upperdir,\n-\t\t\t\t\t   new->d_name.len);\n-\t\terr = PTR_ERR(newdentry);\n-\t\tif (IS_ERR(newdentry))\n-\t\t\tgoto out_unlock;\n+\t\tif (!d_is_negative(newdentry) &&\n+\t\t    (!new_opaque || !ovl_is_whiteout(newdentry)))\n+\t\t\tgoto out_dput;\n \t}\n \n-\terr = -ESTALE;\n-\tif (olddentry->d_parent != old_upperdir)\n-\t\tgoto out_dput;\n-\tif (newdentry->d_parent != new_upperdir)\n-\t\tgoto out_dput;\n \tif (olddentry == trap)\n \t\tgoto out_dput;\n \tif (newdentry == trap)\n@@ -925,6 +940,8 @@ static int ovl_rename2(struct inode *olddir, struct dentry *old,\n \n out_dput:\n \tdput(newdentry);\n+out_dput_old:\n+\tdput(olddentry);\n out_unlock:\n \tunlock_rename(new_upperdir, old_upperdir);\n out_revert_creds:", "file_path": "files/2016_8\\101", "file_language": "c", "file_name": "fs/overlayfs/dir.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 40, "cve_id": "CVE-2016-5696", "cwe_id": ["CWE-200"], "cve_language": "C", "cve_description": "net/ipv4/tcp_input.c in the Linux kernel before 4.7 does not properly determine the rate of challenge ACK segments, which makes it easier for remote attackers to hijack TCP sessions via a blind in-window attack.", "cvss": "4.8", "publish_date": "August 6, 2016", "AV": "NETWORK", "AC": "HIGH", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "LOW", "A": "LOW", "commit_id": "75ff39ccc1bd5d3c455b6822ab09e533c551f758", "commit_message": "tcp: make challenge acks less predictable\n\nYue Cao claims that current host rate limiting of challenge ACKS\n(RFC 5961) could leak enough information to allow a patient attacker\nto hijack TCP sessions. He will soon provide details in an academic\npaper.\n\nThis patch increases the default limit from 100 to 1000, and adds\nsome randomization so that the attacker can no longer hijack\nsessions without spending a considerable amount of probes.\n\nBased on initial analysis and patch from Linus.\n\nNote that we also have per socket rate limiting, so it is tempting\nto remove the host limit in the future.\n\nv2: randomize the count of challenge acks per second, not the period.\n\nFixes: 282f23c6ee34 (\"tcp: implement RFC 5961 3.2\")\nReported-by: Yue Cao <ycao009@ucr.edu>\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nSuggested-by: Linus Torvalds <torvalds@linux-foundation.org>\nCc: Yuchung Cheng <ycheng@google.com>\nCc: Neal Cardwell <ncardwell@google.com>\nAcked-by: Neal Cardwell <ncardwell@google.com>\nAcked-by: Yuchung Cheng <ycheng@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "commit_date": "2016-07-11T20:33:35Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/75ff39ccc1bd5d3c455b6822ab09e533c551f758", "html_url": "https://github.com/torvalds/linux/commit/75ff39ccc1bd5d3c455b6822ab09e533c551f758", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "a612769774a30e4fc143c4cb6395c12573415660", "url_before": "https://api.github.com/repos/torvalds/linux/commits/a612769774a30e4fc143c4cb6395c12573415660", "html_url_before": "https://github.com/torvalds/linux/commit/a612769774a30e4fc143c4cb6395c12573415660"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/75ff39ccc1bd5d3c455b6822ab09e533c551f758/net/ipv4/tcp_input.c", "code": "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n */\n\n/*\n * Changes:\n *\t\tPedro Roque\t:\tFast Retransmit/Recovery.\n *\t\t\t\t\tTwo receive queues.\n *\t\t\t\t\tRetransmit queue handled by TCP.\n *\t\t\t\t\tBetter retransmit timer handling.\n *\t\t\t\t\tNew congestion avoidance.\n *\t\t\t\t\tHeader prediction.\n *\t\t\t\t\tVariable renaming.\n *\n *\t\tEric\t\t:\tFast Retransmit.\n *\t\tRandy Scott\t:\tMSS option defines.\n *\t\tEric Schenk\t:\tFixes to slow start algorithm.\n *\t\tEric Schenk\t:\tYet another double ACK bug.\n *\t\tEric Schenk\t:\tDelayed ACK bug fixes.\n *\t\tEric Schenk\t:\tFloyd style fast retrans war avoidance.\n *\t\tDavid S. Miller\t:\tDon't allow zero congestion window.\n *\t\tEric Schenk\t:\tFix retransmitter so that it sends\n *\t\t\t\t\tnext packet on ack of previous packet.\n *\t\tAndi Kleen\t:\tMoved open_request checking here\n *\t\t\t\t\tand process RSTs for open_requests.\n *\t\tAndi Kleen\t:\tBetter prune_queue, and other fixes.\n *\t\tAndrey Savochkin:\tFix RTT measurements in the presence of\n *\t\t\t\t\ttimestamps.\n *\t\tAndrey Savochkin:\tCheck sequence numbers correctly when\n *\t\t\t\t\tremoving SACKs due to in sequence incoming\n *\t\t\t\t\tdata segments.\n *\t\tAndi Kleen:\t\tMake sure we never ack data there is not\n *\t\t\t\t\tenough room for. Also make this condition\n *\t\t\t\t\ta fatal error if it might still happen.\n *\t\tAndi Kleen:\t\tAdd tcp_measure_rcv_mss to make\n *\t\t\t\t\tconnections with MSS<min(MTU,ann. MSS)\n *\t\t\t\t\twork without delayed acks.\n *\t\tAndi Kleen:\t\tProcess packets with PSH set in the\n *\t\t\t\t\tfast path.\n *\t\tJ Hadi Salim:\t\tECN support\n *\t \tAndrei Gurtov,\n *\t\tPasi Sarolahti,\n *\t\tPanu Kuhlberg:\t\tExperimental audit of TCP (re)transmission\n *\t\t\t\t\tengine. Lots of bugs are found.\n *\t\tPasi Sarolahti:\t\tF-RTO for dealing with spurious RTOs\n */\n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/sysctl.h>\n#include <linux/kernel.h>\n#include <linux/prefetch.h>\n#include <net/dst.h>\n#include <net/tcp.h>\n#include <net/inet_common.h>\n#include <linux/ipsec.h>\n#include <asm/unaligned.h>\n#include <linux/errqueue.h>\n\nint sysctl_tcp_timestamps __read_mostly = 1;\nint sysctl_tcp_window_scaling __read_mostly = 1;\nint sysctl_tcp_sack __read_mostly = 1;\nint sysctl_tcp_fack __read_mostly = 1;\nint sysctl_tcp_max_reordering __read_mostly = 300;\nint sysctl_tcp_dsack __read_mostly = 1;\nint sysctl_tcp_app_win __read_mostly = 31;\nint sysctl_tcp_adv_win_scale __read_mostly = 1;\nEXPORT_SYMBOL(sysctl_tcp_adv_win_scale);\n\n/* rfc5961 challenge ack rate limiting */\nint sysctl_tcp_challenge_ack_limit = 1000;\n\nint sysctl_tcp_stdurg __read_mostly;\nint sysctl_tcp_rfc1337 __read_mostly;\nint sysctl_tcp_max_orphans __read_mostly = NR_FILE;\nint sysctl_tcp_frto __read_mostly = 2;\nint sysctl_tcp_min_rtt_wlen __read_mostly = 300;\n\nint sysctl_tcp_thin_dupack __read_mostly;\n\nint sysctl_tcp_moderate_rcvbuf __read_mostly = 1;\nint sysctl_tcp_early_retrans __read_mostly = 3;\nint sysctl_tcp_invalid_ratelimit __read_mostly = HZ/2;\n\n#define FLAG_DATA\t\t0x01 /* Incoming frame contained data.\t\t*/\n#define FLAG_WIN_UPDATE\t\t0x02 /* Incoming ACK was a window update.\t*/\n#define FLAG_DATA_ACKED\t\t0x04 /* This ACK acknowledged new data.\t\t*/\n#define FLAG_RETRANS_DATA_ACKED\t0x08 /* \"\" \"\" some of which was retransmitted.\t*/\n#define FLAG_SYN_ACKED\t\t0x10 /* This ACK acknowledged SYN.\t\t*/\n#define FLAG_DATA_SACKED\t0x20 /* New SACK.\t\t\t\t*/\n#define FLAG_ECE\t\t0x40 /* ECE in this ACK\t\t\t\t*/\n#define FLAG_LOST_RETRANS\t0x80 /* This ACK marks some retransmission lost */\n#define FLAG_SLOWPATH\t\t0x100 /* Do not skip RFC checks for window update.*/\n#define FLAG_ORIG_SACK_ACKED\t0x200 /* Never retransmitted data are (s)acked\t*/\n#define FLAG_SND_UNA_ADVANCED\t0x400 /* Snd_una was changed (!= FLAG_DATA_ACKED) */\n#define FLAG_DSACKING_ACK\t0x800 /* SACK blocks contained D-SACK info */\n#define FLAG_SACK_RENEGING\t0x2000 /* snd_una advanced to a sacked seq */\n#define FLAG_UPDATE_TS_RECENT\t0x4000 /* tcp_replace_ts_recent() */\n\n#define FLAG_ACKED\t\t(FLAG_DATA_ACKED|FLAG_SYN_ACKED)\n#define FLAG_NOT_DUP\t\t(FLAG_DATA|FLAG_WIN_UPDATE|FLAG_ACKED)\n#define FLAG_CA_ALERT\t\t(FLAG_DATA_SACKED|FLAG_ECE)\n#define FLAG_FORWARD_PROGRESS\t(FLAG_ACKED|FLAG_DATA_SACKED)\n\n#define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)\n#define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))\n\n#define REXMIT_NONE\t0 /* no loss recovery to do */\n#define REXMIT_LOST\t1 /* retransmit packets marked lost */\n#define REXMIT_NEW\t2 /* FRTO-style transmit of unsent/new packets */\n\n/* Adapt the MSS value used to make delayed ack decision to the\n * real world.\n */\nstatic void tcp_measure_rcv_mss(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst unsigned int lss = icsk->icsk_ack.last_seg_size;\n\tunsigned int len;\n\n\ticsk->icsk_ack.last_seg_size = 0;\n\n\t/* skb->len may jitter because of SACKs, even if peer\n\t * sends good full-sized frames.\n\t */\n\tlen = skb_shinfo(skb)->gso_size ? : skb->len;\n\tif (len >= icsk->icsk_ack.rcv_mss) {\n\t\ticsk->icsk_ack.rcv_mss = len;\n\t} else {\n\t\t/* Otherwise, we make more careful check taking into account,\n\t\t * that SACKs block is variable.\n\t\t *\n\t\t * \"len\" is invariant segment length, including TCP header.\n\t\t */\n\t\tlen += skb->data - skb_transport_header(skb);\n\t\tif (len >= TCP_MSS_DEFAULT + sizeof(struct tcphdr) ||\n\t\t    /* If PSH is not set, packet should be\n\t\t     * full sized, provided peer TCP is not badly broken.\n\t\t     * This observation (if it is correct 8)) allows\n\t\t     * to handle super-low mtu links fairly.\n\t\t     */\n\t\t    (len >= TCP_MIN_MSS + sizeof(struct tcphdr) &&\n\t\t     !(tcp_flag_word(tcp_hdr(skb)) & TCP_REMNANT))) {\n\t\t\t/* Subtract also invariant (if peer is RFC compliant),\n\t\t\t * tcp header plus fixed timestamp option length.\n\t\t\t * Resulting \"len\" is MSS free of SACK jitter.\n\t\t\t */\n\t\t\tlen -= tcp_sk(sk)->tcp_header_len;\n\t\t\ticsk->icsk_ack.last_seg_size = len;\n\t\t\tif (len == lss) {\n\t\t\t\ticsk->icsk_ack.rcv_mss = len;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tif (icsk->icsk_ack.pending & ICSK_ACK_PUSHED)\n\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED2;\n\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t}\n}\n\nstatic void tcp_incr_quickack(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tunsigned int quickacks = tcp_sk(sk)->rcv_wnd / (2 * icsk->icsk_ack.rcv_mss);\n\n\tif (quickacks == 0)\n\t\tquickacks = 2;\n\tif (quickacks > icsk->icsk_ack.quick)\n\t\ticsk->icsk_ack.quick = min(quickacks, TCP_MAX_QUICKACKS);\n}\n\nstatic void tcp_enter_quickack_mode(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\ttcp_incr_quickack(sk);\n\ticsk->icsk_ack.pingpong = 0;\n\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n}\n\n/* Send ACKs quickly, if \"quick\" count is not exhausted\n * and the session is not interactive.\n */\n\nstatic bool tcp_in_quickack_mode(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\n\treturn (dst && dst_metric(dst, RTAX_QUICKACK)) ||\n\t\t(icsk->icsk_ack.quick && !icsk->icsk_ack.pingpong);\n}\n\nstatic void tcp_ecn_queue_cwr(struct tcp_sock *tp)\n{\n\tif (tp->ecn_flags & TCP_ECN_OK)\n\t\ttp->ecn_flags |= TCP_ECN_QUEUE_CWR;\n}\n\nstatic void tcp_ecn_accept_cwr(struct tcp_sock *tp, const struct sk_buff *skb)\n{\n\tif (tcp_hdr(skb)->cwr)\n\t\ttp->ecn_flags &= ~TCP_ECN_DEMAND_CWR;\n}\n\nstatic void tcp_ecn_withdraw_cwr(struct tcp_sock *tp)\n{\n\ttp->ecn_flags &= ~TCP_ECN_DEMAND_CWR;\n}\n\nstatic void __tcp_ecn_check_ce(struct tcp_sock *tp, const struct sk_buff *skb)\n{\n\tswitch (TCP_SKB_CB(skb)->ip_dsfield & INET_ECN_MASK) {\n\tcase INET_ECN_NOT_ECT:\n\t\t/* Funny extension: if ECT is not set on a segment,\n\t\t * and we already seen ECT on a previous segment,\n\t\t * it is probably a retransmit.\n\t\t */\n\t\tif (tp->ecn_flags & TCP_ECN_SEEN)\n\t\t\ttcp_enter_quickack_mode((struct sock *)tp);\n\t\tbreak;\n\tcase INET_ECN_CE:\n\t\tif (tcp_ca_needs_ecn((struct sock *)tp))\n\t\t\ttcp_ca_event((struct sock *)tp, CA_EVENT_ECN_IS_CE);\n\n\t\tif (!(tp->ecn_flags & TCP_ECN_DEMAND_CWR)) {\n\t\t\t/* Better not delay acks, sender can have a very low cwnd */\n\t\t\ttcp_enter_quickack_mode((struct sock *)tp);\n\t\t\ttp->ecn_flags |= TCP_ECN_DEMAND_CWR;\n\t\t}\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\tdefault:\n\t\tif (tcp_ca_needs_ecn((struct sock *)tp))\n\t\t\ttcp_ca_event((struct sock *)tp, CA_EVENT_ECN_NO_CE);\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\t}\n}\n\nstatic void tcp_ecn_check_ce(struct tcp_sock *tp, const struct sk_buff *skb)\n{\n\tif (tp->ecn_flags & TCP_ECN_OK)\n\t\t__tcp_ecn_check_ce(tp, skb);\n}\n\nstatic void tcp_ecn_rcv_synack(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif ((tp->ecn_flags & TCP_ECN_OK) && (!th->ece || th->cwr))\n\t\ttp->ecn_flags &= ~TCP_ECN_OK;\n}\n\nstatic void tcp_ecn_rcv_syn(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif ((tp->ecn_flags & TCP_ECN_OK) && (!th->ece || !th->cwr))\n\t\ttp->ecn_flags &= ~TCP_ECN_OK;\n}\n\nstatic bool tcp_ecn_rcv_ecn_echo(const struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tif (th->ece && !th->syn && (tp->ecn_flags & TCP_ECN_OK))\n\t\treturn true;\n\treturn false;\n}\n\n/* Buffer size and advertised window tuning.\n *\n * 1. Tuning sk->sk_sndbuf, when connection enters established state.\n */\n\nstatic void tcp_sndbuf_expand(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint sndmem, per_mss;\n\tu32 nr_segs;\n\n\t/* Worst case is non GSO/TSO : each frame consumes one skb\n\t * and skb->head is kmalloced using power of two area of memory\n\t */\n\tper_mss = max_t(u32, tp->rx_opt.mss_clamp, tp->mss_cache) +\n\t\t  MAX_TCP_HEADER +\n\t\t  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tper_mss = roundup_pow_of_two(per_mss) +\n\t\t  SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\tnr_segs = max_t(u32, TCP_INIT_CWND, tp->snd_cwnd);\n\tnr_segs = max_t(u32, nr_segs, tp->reordering + 1);\n\n\t/* Fast Recovery (RFC 5681 3.2) :\n\t * Cubic needs 1.7 factor, rounded to 2 to include\n\t * extra cushion (application might react slowly to POLLOUT)\n\t */\n\tsndmem = 2 * nr_segs * per_mss;\n\n\tif (sk->sk_sndbuf < sndmem)\n\t\tsk->sk_sndbuf = min(sndmem, sysctl_tcp_wmem[2]);\n}\n\n/* 2. Tuning advertised window (window_clamp, rcv_ssthresh)\n *\n * All tcp_full_space() is split to two parts: \"network\" buffer, allocated\n * forward and advertised in receiver window (tp->rcv_wnd) and\n * \"application buffer\", required to isolate scheduling/application\n * latencies from network.\n * window_clamp is maximal advertised window. It can be less than\n * tcp_full_space(), in this case tcp_full_space() - window_clamp\n * is reserved for \"application\" buffer. The less window_clamp is\n * the smoother our behaviour from viewpoint of network, but the lower\n * throughput and the higher sensitivity of the connection to losses. 8)\n *\n * rcv_ssthresh is more strict window_clamp used at \"slow start\"\n * phase to predict further behaviour of this connection.\n * It is used for two goals:\n * - to enforce header prediction at sender, even when application\n *   requires some significant \"application buffer\". It is check #1.\n * - to prevent pruning of receive queue because of misprediction\n *   of receiver window. Check #2.\n *\n * The scheme does not work when sender sends good segments opening\n * window and then starts to feed us spaghetti. But it should work\n * in common situations. Otherwise, we have to rely on queue collapsing.\n */\n\n/* Slow part of check#2. */\nstatic int __tcp_grow_window(const struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t/* Optimize this! */\n\tint truesize = tcp_win_from_space(skb->truesize) >> 1;\n\tint window = tcp_win_from_space(sysctl_tcp_rmem[2]) >> 1;\n\n\twhile (tp->rcv_ssthresh <= window) {\n\t\tif (truesize <= skb->len)\n\t\t\treturn 2 * inet_csk(sk)->icsk_ack.rcv_mss;\n\n\t\ttruesize >>= 1;\n\t\twindow >>= 1;\n\t}\n\treturn 0;\n}\n\nstatic void tcp_grow_window(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Check #1 */\n\tif (tp->rcv_ssthresh < tp->window_clamp &&\n\t    (int)tp->rcv_ssthresh < tcp_space(sk) &&\n\t    !tcp_under_memory_pressure(sk)) {\n\t\tint incr;\n\n\t\t/* Check #2. Increase window, if skb with such overhead\n\t\t * will fit to rcvbuf in future.\n\t\t */\n\t\tif (tcp_win_from_space(skb->truesize) <= skb->len)\n\t\t\tincr = 2 * tp->advmss;\n\t\telse\n\t\t\tincr = __tcp_grow_window(sk, skb);\n\n\t\tif (incr) {\n\t\t\tincr = max_t(int, incr, 2 * skb->len);\n\t\t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh + incr,\n\t\t\t\t\t       tp->window_clamp);\n\t\t\tinet_csk(sk)->icsk_ack.quick |= 1;\n\t\t}\n\t}\n}\n\n/* 3. Tuning rcvbuf, when connection enters established state. */\nstatic void tcp_fixup_rcvbuf(struct sock *sk)\n{\n\tu32 mss = tcp_sk(sk)->advmss;\n\tint rcvmem;\n\n\trcvmem = 2 * SKB_TRUESIZE(mss + MAX_TCP_HEADER) *\n\t\t tcp_default_init_rwnd(mss);\n\n\t/* Dynamic Right Sizing (DRS) has 2 to 3 RTT latency\n\t * Allow enough cushion so that sender is not limited by our window\n\t */\n\tif (sysctl_tcp_moderate_rcvbuf)\n\t\trcvmem <<= 2;\n\n\tif (sk->sk_rcvbuf < rcvmem)\n\t\tsk->sk_rcvbuf = min(rcvmem, sysctl_tcp_rmem[2]);\n}\n\n/* 4. Try to fixup all. It is made immediately after connection enters\n *    established state.\n */\nvoid tcp_init_buffer_space(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint maxwin;\n\n\tif (!(sk->sk_userlocks & SOCK_RCVBUF_LOCK))\n\t\ttcp_fixup_rcvbuf(sk);\n\tif (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK))\n\t\ttcp_sndbuf_expand(sk);\n\n\ttp->rcvq_space.space = tp->rcv_wnd;\n\ttp->rcvq_space.time = tcp_time_stamp;\n\ttp->rcvq_space.seq = tp->copied_seq;\n\n\tmaxwin = tcp_full_space(sk);\n\n\tif (tp->window_clamp >= maxwin) {\n\t\ttp->window_clamp = maxwin;\n\n\t\tif (sysctl_tcp_app_win && maxwin > 4 * tp->advmss)\n\t\t\ttp->window_clamp = max(maxwin -\n\t\t\t\t\t       (maxwin >> sysctl_tcp_app_win),\n\t\t\t\t\t       4 * tp->advmss);\n\t}\n\n\t/* Force reservation of one segment. */\n\tif (sysctl_tcp_app_win &&\n\t    tp->window_clamp > 2 * tp->advmss &&\n\t    tp->window_clamp + tp->advmss > maxwin)\n\t\ttp->window_clamp = max(2 * tp->advmss, maxwin - tp->advmss);\n\n\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, tp->window_clamp);\n\ttp->snd_cwnd_stamp = tcp_time_stamp;\n}\n\n/* 5. Recalculate window clamp after socket hit its memory bounds. */\nstatic void tcp_clamp_window(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_ack.quick = 0;\n\n\tif (sk->sk_rcvbuf < sysctl_tcp_rmem[2] &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK) &&\n\t    !tcp_under_memory_pressure(sk) &&\n\t    sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)) {\n\t\tsk->sk_rcvbuf = min(atomic_read(&sk->sk_rmem_alloc),\n\t\t\t\t    sysctl_tcp_rmem[2]);\n\t}\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf)\n\t\ttp->rcv_ssthresh = min(tp->window_clamp, 2U * tp->advmss);\n}\n\n/* Initialize RCV_MSS value.\n * RCV_MSS is an our guess about MSS used by the peer.\n * We haven't any direct information about the MSS.\n * It's better to underestimate the RCV_MSS rather than overestimate.\n * Overestimations make us ACKing less frequently than needed.\n * Underestimations are more easy to detect and fix by tcp_measure_rcv_mss().\n */\nvoid tcp_initialize_rcv_mss(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int hint = min_t(unsigned int, tp->advmss, tp->mss_cache);\n\n\thint = min(hint, tp->rcv_wnd / 2);\n\thint = min(hint, TCP_MSS_DEFAULT);\n\thint = max(hint, TCP_MIN_MSS);\n\n\tinet_csk(sk)->icsk_ack.rcv_mss = hint;\n}\nEXPORT_SYMBOL(tcp_initialize_rcv_mss);\n\n/* Receiver \"autotuning\" code.\n *\n * The algorithm for RTT estimation w/o timestamps is based on\n * Dynamic Right-Sizing (DRS) by Wu Feng and Mike Fisk of LANL.\n * <http://public.lanl.gov/radiant/pubs.html#DRS>\n *\n * More detail on this code can be found at\n * <http://staff.psc.edu/jheffner/>,\n * though this reference is out of date.  A new paper\n * is pending.\n */\nstatic void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)\n{\n\tu32 new_sample = tp->rcv_rtt_est.rtt;\n\tlong m = sample;\n\n\tif (m == 0)\n\t\tm = 1;\n\n\tif (new_sample != 0) {\n\t\t/* If we sample in larger samples in the non-timestamp\n\t\t * case, we could grossly overestimate the RTT especially\n\t\t * with chatty applications or bulk transfer apps which\n\t\t * are stalled on filesystem I/O.\n\t\t *\n\t\t * Also, since we are only going for a minimum in the\n\t\t * non-timestamp case, we do not smooth things out\n\t\t * else with timestamps disabled convergence takes too\n\t\t * long.\n\t\t */\n\t\tif (!win_dep) {\n\t\t\tm -= (new_sample >> 3);\n\t\t\tnew_sample += m;\n\t\t} else {\n\t\t\tm <<= 3;\n\t\t\tif (m < new_sample)\n\t\t\t\tnew_sample = m;\n\t\t}\n\t} else {\n\t\t/* No previous measure. */\n\t\tnew_sample = m << 3;\n\t}\n\n\tif (tp->rcv_rtt_est.rtt != new_sample)\n\t\ttp->rcv_rtt_est.rtt = new_sample;\n}\n\nstatic inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)\n{\n\tif (tp->rcv_rtt_est.time == 0)\n\t\tgoto new_measure;\n\tif (before(tp->rcv_nxt, tp->rcv_rtt_est.seq))\n\t\treturn;\n\ttcp_rcv_rtt_update(tp, tcp_time_stamp - tp->rcv_rtt_est.time, 1);\n\nnew_measure:\n\ttp->rcv_rtt_est.seq = tp->rcv_nxt + tp->rcv_wnd;\n\ttp->rcv_rtt_est.time = tcp_time_stamp;\n}\n\nstatic inline void tcp_rcv_rtt_measure_ts(struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tif (tp->rx_opt.rcv_tsecr &&\n\t    (TCP_SKB_CB(skb)->end_seq -\n\t     TCP_SKB_CB(skb)->seq >= inet_csk(sk)->icsk_ack.rcv_mss))\n\t\ttcp_rcv_rtt_update(tp, tcp_time_stamp - tp->rx_opt.rcv_tsecr, 0);\n}\n\n/*\n * This function should be called every time data is copied to user space.\n * It calculates the appropriate TCP receive buffer space.\n */\nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint time;\n\tint copied;\n\n\ttime = tcp_time_stamp - tp->rcvq_space.time;\n\tif (time < (tp->rcv_rtt_est.rtt >> 3) || tp->rcv_rtt_est.rtt == 0)\n\t\treturn;\n\n\t/* Number of bytes copied to user in last RTT */\n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\t/* A bit of theory :\n\t * copied = bytes received in previous RTT, our base window\n\t * To cope with packet losses, we need a 2x factor\n\t * To cope with slow start, and sender growing its cwin by 100 %\n\t * every RTT, we need a 4x factor, because the ACK we are sending\n\t * now is for the next RTT, not the current one :\n\t * <prev RTT . ><current RTT .. ><next RTT .... >\n\t */\n\n\tif (sysctl_tcp_moderate_rcvbuf &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) {\n\t\tint rcvwin, rcvmem, rcvbuf;\n\n\t\t/* minimal window to cope with packet losses, assuming\n\t\t * steady state. Add some cushion because of small variations.\n\t\t */\n\t\trcvwin = (copied << 1) + 16 * tp->advmss;\n\n\t\t/* If rate increased by 25%,\n\t\t *\tassume slow start, rcvwin = 3 * copied\n\t\t * If rate increased by 50%,\n\t\t *\tassume sender can use 2x growth, rcvwin = 4 * copied\n\t\t */\n\t\tif (copied >=\n\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 2)) {\n\t\t\tif (copied >=\n\t\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 1))\n\t\t\t\trcvwin <<= 1;\n\t\t\telse\n\t\t\t\trcvwin += (rcvwin >> 1);\n\t\t}\n\n\t\trcvmem = SKB_TRUESIZE(tp->advmss + MAX_TCP_HEADER);\n\t\twhile (tcp_win_from_space(rcvmem) < tp->advmss)\n\t\t\trcvmem += 128;\n\n\t\trcvbuf = min(rcvwin / tp->advmss * rcvmem, sysctl_tcp_rmem[2]);\n\t\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\t\tsk->sk_rcvbuf = rcvbuf;\n\n\t\t\t/* Make the window clamp follow along.  */\n\t\t\ttp->window_clamp = rcvwin;\n\t\t}\n\t}\n\ttp->rcvq_space.space = copied;\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tcp_time_stamp;\n}\n\n/* There is something which you must keep in mind when you analyze the\n * behavior of the tp->ato delayed ack timeout interval.  When a\n * connection starts up, we want to ack as quickly as possible.  The\n * problem is that \"good\" TCP's do slow start at the beginning of data\n * transmission.  The means that until we send the first few ACK's the\n * sender will sit on his end and only queue most of his data, because\n * he can only send snd_cwnd unacked packets at any given time.  For\n * each ACK we send, he increments snd_cwnd and transmits more of his\n * queue.  -DaveM\n */\nstatic void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now;\n\n\tinet_csk_schedule_ack(sk);\n\n\ttcp_measure_rcv_mss(sk, skb);\n\n\ttcp_rcv_rtt_measure(tp);\n\n\tnow = tcp_time_stamp;\n\n\tif (!icsk->icsk_ack.ato) {\n\t\t/* The _first_ data packet received, initialize\n\t\t * delayed ACK engine.\n\t\t */\n\t\ttcp_incr_quickack(sk);\n\t\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n\t} else {\n\t\tint m = now - icsk->icsk_ack.lrcvtime;\n\n\t\tif (m <= TCP_ATO_MIN / 2) {\n\t\t\t/* The fastest case is the first. */\n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + TCP_ATO_MIN / 2;\n\t\t} else if (m < icsk->icsk_ack.ato) {\n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + m;\n\t\t\tif (icsk->icsk_ack.ato > icsk->icsk_rto)\n\t\t\t\ticsk->icsk_ack.ato = icsk->icsk_rto;\n\t\t} else if (m > icsk->icsk_rto) {\n\t\t\t/* Too long gap. Apparently sender failed to\n\t\t\t * restart window, so that we send ACKs quickly.\n\t\t\t */\n\t\t\ttcp_incr_quickack(sk);\n\t\t\tsk_mem_reclaim(sk);\n\t\t}\n\t}\n\ticsk->icsk_ack.lrcvtime = now;\n\n\ttcp_ecn_check_ce(tp, skb);\n\n\tif (skb->len >= 128)\n\t\ttcp_grow_window(sk, skb);\n}\n\n/* Called to compute a smoothed rtt estimate. The data fed to this\n * routine either comes from timestamps, or from segments that were\n * known _not_ to have been retransmitted [see Karn/Partridge\n * Proceedings SIGCOMM 87]. The algorithm is from the SIGCOMM 88\n * piece by Van Jacobson.\n * NOTE: the next three routines used to be one big routine.\n * To save cycles in the RFC 1323 implementation it was better to break\n * it up into three procedures. -- erics\n */\nstatic void tcp_rtt_estimator(struct sock *sk, long mrtt_us)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tlong m = mrtt_us; /* RTT */\n\tu32 srtt = tp->srtt_us;\n\n\t/*\tThe following amusing code comes from Jacobson's\n\t *\tarticle in SIGCOMM '88.  Note that rtt and mdev\n\t *\tare scaled versions of rtt and mean deviation.\n\t *\tThis is designed to be as fast as possible\n\t *\tm stands for \"measurement\".\n\t *\n\t *\tOn a 1990 paper the rto value is changed to:\n\t *\tRTO = rtt + 4 * mdev\n\t *\n\t * Funny. This algorithm seems to be very broken.\n\t * These formulae increase RTO, when it should be decreased, increase\n\t * too slowly, when it should be increased quickly, decrease too quickly\n\t * etc. I guess in BSD RTO takes ONE value, so that it is absolutely\n\t * does not matter how to _calculate_ it. Seems, it was trap\n\t * that VJ failed to avoid. 8)\n\t */\n\tif (srtt != 0) {\n\t\tm -= (srtt >> 3);\t/* m is now error in rtt est */\n\t\tsrtt += m;\t\t/* rtt = 7/8 rtt + 1/8 new */\n\t\tif (m < 0) {\n\t\t\tm = -m;\t\t/* m is now abs(error) */\n\t\t\tm -= (tp->mdev_us >> 2);   /* similar update on mdev */\n\t\t\t/* This is similar to one of Eifel findings.\n\t\t\t * Eifel blocks mdev updates when rtt decreases.\n\t\t\t * This solution is a bit different: we use finer gain\n\t\t\t * for mdev in this case (alpha*beta).\n\t\t\t * Like Eifel it also prevents growth of rto,\n\t\t\t * but also it limits too fast rto decreases,\n\t\t\t * happening in pure Eifel.\n\t\t\t */\n\t\t\tif (m > 0)\n\t\t\t\tm >>= 3;\n\t\t} else {\n\t\t\tm -= (tp->mdev_us >> 2);   /* similar update on mdev */\n\t\t}\n\t\ttp->mdev_us += m;\t\t/* mdev = 3/4 mdev + 1/4 new */\n\t\tif (tp->mdev_us > tp->mdev_max_us) {\n\t\t\ttp->mdev_max_us = tp->mdev_us;\n\t\t\tif (tp->mdev_max_us > tp->rttvar_us)\n\t\t\t\ttp->rttvar_us = tp->mdev_max_us;\n\t\t}\n\t\tif (after(tp->snd_una, tp->rtt_seq)) {\n\t\t\tif (tp->mdev_max_us < tp->rttvar_us)\n\t\t\t\ttp->rttvar_us -= (tp->rttvar_us - tp->mdev_max_us) >> 2;\n\t\t\ttp->rtt_seq = tp->snd_nxt;\n\t\t\ttp->mdev_max_us = tcp_rto_min_us(sk);\n\t\t}\n\t} else {\n\t\t/* no previous measure. */\n\t\tsrtt = m << 3;\t\t/* take the measured time to be rtt */\n\t\ttp->mdev_us = m << 1;\t/* make sure rto = 3*rtt */\n\t\ttp->rttvar_us = max(tp->mdev_us, tcp_rto_min_us(sk));\n\t\ttp->mdev_max_us = tp->rttvar_us;\n\t\ttp->rtt_seq = tp->snd_nxt;\n\t}\n\ttp->srtt_us = max(1U, srtt);\n}\n\n/* Set the sk_pacing_rate to allow proper sizing of TSO packets.\n * Note: TCP stack does not yet implement pacing.\n * FQ packet scheduler can be used to implement cheap but effective\n * TCP pacing, to smooth the burst on large writes when packets\n * in flight is significantly lower than cwnd (or rwin)\n */\nint sysctl_tcp_pacing_ss_ratio __read_mostly = 200;\nint sysctl_tcp_pacing_ca_ratio __read_mostly = 120;\n\nstatic void tcp_update_pacing_rate(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu64 rate;\n\n\t/* set sk_pacing_rate to 200 % of current rate (mss * cwnd / srtt) */\n\trate = (u64)tp->mss_cache * ((USEC_PER_SEC / 100) << 3);\n\n\t/* current rate is (cwnd * mss) / srtt\n\t * In Slow Start [1], set sk_pacing_rate to 200 % the current rate.\n\t * In Congestion Avoidance phase, set it to 120 % the current rate.\n\t *\n\t * [1] : Normal Slow Start condition is (tp->snd_cwnd < tp->snd_ssthresh)\n\t *\t If snd_cwnd >= (tp->snd_ssthresh / 2), we are approaching\n\t *\t end of slow start and should slow down.\n\t */\n\tif (tp->snd_cwnd < tp->snd_ssthresh / 2)\n\t\trate *= sysctl_tcp_pacing_ss_ratio;\n\telse\n\t\trate *= sysctl_tcp_pacing_ca_ratio;\n\n\trate *= max(tp->snd_cwnd, tp->packets_out);\n\n\tif (likely(tp->srtt_us))\n\t\tdo_div(rate, tp->srtt_us);\n\n\t/* ACCESS_ONCE() is needed because sch_fq fetches sk_pacing_rate\n\t * without any lock. We want to make sure compiler wont store\n\t * intermediate values in this location.\n\t */\n\tACCESS_ONCE(sk->sk_pacing_rate) = min_t(u64, rate,\n\t\t\t\t\t\tsk->sk_max_pacing_rate);\n}\n\n/* Calculate rto without backoff.  This is the second half of Van Jacobson's\n * routine referred to above.\n */\nstatic void tcp_set_rto(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t/* Old crap is replaced with new one. 8)\n\t *\n\t * More seriously:\n\t * 1. If rtt variance happened to be less 50msec, it is hallucination.\n\t *    It cannot be less due to utterly erratic ACK generation made\n\t *    at least by solaris and freebsd. \"Erratic ACKs\" has _nothing_\n\t *    to do with delayed acks, because at cwnd>2 true delack timeout\n\t *    is invisible. Actually, Linux-2.4 also generates erratic\n\t *    ACKs in some circumstances.\n\t */\n\tinet_csk(sk)->icsk_rto = __tcp_set_rto(tp);\n\n\t/* 2. Fixups made earlier cannot be right.\n\t *    If we do not estimate RTO correctly without them,\n\t *    all the algo is pure shit and should be replaced\n\t *    with correct one. It is exactly, which we pretend to do.\n\t */\n\n\t/* NOTE: clamping at TCP_RTO_MIN is not required, current algo\n\t * guarantees that rto is higher.\n\t */\n\ttcp_bound_rto(sk);\n}\n\n__u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst)\n{\n\t__u32 cwnd = (dst ? dst_metric(dst, RTAX_INITCWND) : 0);\n\n\tif (!cwnd)\n\t\tcwnd = TCP_INIT_CWND;\n\treturn min_t(__u32, cwnd, tp->snd_cwnd_clamp);\n}\n\n/*\n * Packet counting of FACK is based on in-order assumptions, therefore TCP\n * disables it when reordering is detected\n */\nvoid tcp_disable_fack(struct tcp_sock *tp)\n{\n\t/* RFC3517 uses different metric in lost marker => reset on change */\n\tif (tcp_is_fack(tp))\n\t\ttp->lost_skb_hint = NULL;\n\ttp->rx_opt.sack_ok &= ~TCP_FACK_ENABLED;\n}\n\n/* Take a notice that peer is sending D-SACKs */\nstatic void tcp_dsack_seen(struct tcp_sock *tp)\n{\n\ttp->rx_opt.sack_ok |= TCP_DSACK_SEEN;\n}\n\nstatic void tcp_update_reordering(struct sock *sk, const int metric,\n\t\t\t\t  const int ts)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tif (metric > tp->reordering) {\n\t\tint mib_idx;\n\n\t\ttp->reordering = min(sysctl_tcp_max_reordering, metric);\n\n\t\t/* This exciting event is worth to be remembered. 8) */\n\t\tif (ts)\n\t\t\tmib_idx = LINUX_MIB_TCPTSREORDER;\n\t\telse if (tcp_is_reno(tp))\n\t\t\tmib_idx = LINUX_MIB_TCPRENOREORDER;\n\t\telse if (tcp_is_fack(tp))\n\t\t\tmib_idx = LINUX_MIB_TCPFACKREORDER;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPSACKREORDER;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n#if FASTRETRANS_DEBUG > 1\n\t\tpr_debug(\"Disorder%d %d %u f%u s%u rr%d\\n\",\n\t\t\t tp->rx_opt.sack_ok, inet_csk(sk)->icsk_ca_state,\n\t\t\t tp->reordering,\n\t\t\t tp->fackets_out,\n\t\t\t tp->sacked_out,\n\t\t\t tp->undo_marker ? tp->undo_retrans : 0);\n#endif\n\t\ttcp_disable_fack(tp);\n\t}\n\n\tif (metric > 0)\n\t\ttcp_disable_early_retrans(tp);\n\ttp->rack.reord = 1;\n}\n\n/* This must be called before lost_out is incremented */\nstatic void tcp_verify_retransmit_hint(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tif (!tp->retransmit_skb_hint ||\n\t    before(TCP_SKB_CB(skb)->seq,\n\t\t   TCP_SKB_CB(tp->retransmit_skb_hint)->seq))\n\t\ttp->retransmit_skb_hint = skb;\n\n\tif (!tp->lost_out ||\n\t    after(TCP_SKB_CB(skb)->end_seq, tp->retransmit_high))\n\t\ttp->retransmit_high = TCP_SKB_CB(skb)->end_seq;\n}\n\nstatic void tcp_skb_mark_lost(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tif (!(TCP_SKB_CB(skb)->sacked & (TCPCB_LOST|TCPCB_SACKED_ACKED))) {\n\t\ttcp_verify_retransmit_hint(tp, skb);\n\n\t\ttp->lost_out += tcp_skb_pcount(skb);\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_LOST;\n\t}\n}\n\nvoid tcp_skb_mark_lost_uncond_verify(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\ttcp_verify_retransmit_hint(tp, skb);\n\n\tif (!(TCP_SKB_CB(skb)->sacked & (TCPCB_LOST|TCPCB_SACKED_ACKED))) {\n\t\ttp->lost_out += tcp_skb_pcount(skb);\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_LOST;\n\t}\n}\n\n/* This procedure tags the retransmission queue when SACKs arrive.\n *\n * We have three tag bits: SACKED(S), RETRANS(R) and LOST(L).\n * Packets in queue with these bits set are counted in variables\n * sacked_out, retrans_out and lost_out, correspondingly.\n *\n * Valid combinations are:\n * Tag  InFlight\tDescription\n * 0\t1\t\t- orig segment is in flight.\n * S\t0\t\t- nothing flies, orig reached receiver.\n * L\t0\t\t- nothing flies, orig lost by net.\n * R\t2\t\t- both orig and retransmit are in flight.\n * L|R\t1\t\t- orig is lost, retransmit is in flight.\n * S|R  1\t\t- orig reached receiver, retrans is still in flight.\n * (L|S|R is logically valid, it could occur when L|R is sacked,\n *  but it is equivalent to plain S and code short-curcuits it to S.\n *  L|S is logically invalid, it would mean -1 packet in flight 8))\n *\n * These 6 states form finite state machine, controlled by the following events:\n * 1. New ACK (+SACK) arrives. (tcp_sacktag_write_queue())\n * 2. Retransmission. (tcp_retransmit_skb(), tcp_xmit_retransmit_queue())\n * 3. Loss detection event of two flavors:\n *\tA. Scoreboard estimator decided the packet is lost.\n *\t   A'. Reno \"three dupacks\" marks head of queue lost.\n *\t   A''. Its FACK modification, head until snd.fack is lost.\n *\tB. SACK arrives sacking SND.NXT at the moment, when the\n *\t   segment was retransmitted.\n * 4. D-SACK added new rule: D-SACK changes any tag to S.\n *\n * It is pleasant to note, that state diagram turns out to be commutative,\n * so that we are allowed not to be bothered by order of our actions,\n * when multiple events arrive simultaneously. (see the function below).\n *\n * Reordering detection.\n * --------------------\n * Reordering metric is maximal distance, which a packet can be displaced\n * in packet stream. With SACKs we can estimate it:\n *\n * 1. SACK fills old hole and the corresponding segment was not\n *    ever retransmitted -> reordering. Alas, we cannot use it\n *    when segment was retransmitted.\n * 2. The last flaw is solved with D-SACK. D-SACK arrives\n *    for retransmitted and already SACKed segment -> reordering..\n * Both of these heuristics are not used in Loss state, when we cannot\n * account for retransmits accurately.\n *\n * SACK block validation.\n * ----------------------\n *\n * SACK block range validation checks that the received SACK block fits to\n * the expected sequence limits, i.e., it is between SND.UNA and SND.NXT.\n * Note that SND.UNA is not included to the range though being valid because\n * it means that the receiver is rather inconsistent with itself reporting\n * SACK reneging when it should advance SND.UNA. Such SACK block this is\n * perfectly valid, however, in light of RFC2018 which explicitly states\n * that \"SACK block MUST reflect the newest segment.  Even if the newest\n * segment is going to be discarded ...\", not that it looks very clever\n * in case of head skb. Due to potentional receiver driven attacks, we\n * choose to avoid immediate execution of a walk in write queue due to\n * reneging and defer head skb's loss recovery to standard loss recovery\n * procedure that will eventually trigger (nothing forbids us doing this).\n *\n * Implements also blockage to start_seq wrap-around. Problem lies in the\n * fact that though start_seq (s) is before end_seq (i.e., not reversed),\n * there's no guarantee that it will be before snd_nxt (n). The problem\n * happens when start_seq resides between end_seq wrap (e_w) and snd_nxt\n * wrap (s_w):\n *\n *         <- outs wnd ->                          <- wrapzone ->\n *         u     e      n                         u_w   e_w  s n_w\n *         |     |      |                          |     |   |  |\n * |<------------+------+----- TCP seqno space --------------+---------->|\n * ...-- <2^31 ->|                                           |<--------...\n * ...---- >2^31 ------>|                                    |<--------...\n *\n * Current code wouldn't be vulnerable but it's better still to discard such\n * crazy SACK blocks. Doing this check for start_seq alone closes somewhat\n * similar case (end_seq after snd_nxt wrap) as earlier reversed check in\n * snd_nxt wrap -> snd_una region will then become \"well defined\", i.e.,\n * equal to the ideal case (infinite seqno space without wrap caused issues).\n *\n * With D-SACK the lower bound is extended to cover sequence space below\n * SND.UNA down to undo_marker, which is the last point of interest. Yet\n * again, D-SACK block must not to go across snd_una (for the same reason as\n * for the normal SACK blocks, explained above). But there all simplicity\n * ends, TCP might receive valid D-SACKs below that. As long as they reside\n * fully below undo_marker they do not affect behavior in anyway and can\n * therefore be safely ignored. In rare cases (which are more or less\n * theoretical ones), the D-SACK will nicely cross that boundary due to skb\n * fragmentation and packet reordering past skb's retransmission. To consider\n * them correctly, the acceptable range must be extended even more though\n * the exact amount is rather hard to quantify. However, tp->max_window can\n * be used as an exaggerated estimate.\n */\nstatic bool tcp_is_sackblock_valid(struct tcp_sock *tp, bool is_dsack,\n\t\t\t\t   u32 start_seq, u32 end_seq)\n{\n\t/* Too far in future, or reversed (interpretation is ambiguous) */\n\tif (after(end_seq, tp->snd_nxt) || !before(start_seq, end_seq))\n\t\treturn false;\n\n\t/* Nasty start_seq wrap-around check (see comments above) */\n\tif (!before(start_seq, tp->snd_nxt))\n\t\treturn false;\n\n\t/* In outstanding window? ...This is valid exit for D-SACKs too.\n\t * start_seq == snd_una is non-sensical (see comments above)\n\t */\n\tif (after(start_seq, tp->snd_una))\n\t\treturn true;\n\n\tif (!is_dsack || !tp->undo_marker)\n\t\treturn false;\n\n\t/* ...Then it's D-SACK, and must reside below snd_una completely */\n\tif (after(end_seq, tp->snd_una))\n\t\treturn false;\n\n\tif (!before(start_seq, tp->undo_marker))\n\t\treturn true;\n\n\t/* Too old */\n\tif (!after(end_seq, tp->undo_marker))\n\t\treturn false;\n\n\t/* Undo_marker boundary crossing (overestimates a lot). Known already:\n\t *   start_seq < undo_marker and end_seq >= undo_marker.\n\t */\n\treturn !before(start_seq, end_seq - tp->max_window);\n}\n\nstatic bool tcp_check_dsack(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\t    struct tcp_sack_block_wire *sp, int num_sacks,\n\t\t\t    u32 prior_snd_una)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 start_seq_0 = get_unaligned_be32(&sp[0].start_seq);\n\tu32 end_seq_0 = get_unaligned_be32(&sp[0].end_seq);\n\tbool dup_sack = false;\n\n\tif (before(start_seq_0, TCP_SKB_CB(ack_skb)->ack_seq)) {\n\t\tdup_sack = true;\n\t\ttcp_dsack_seen(tp);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECV);\n\t} else if (num_sacks > 1) {\n\t\tu32 end_seq_1 = get_unaligned_be32(&sp[1].end_seq);\n\t\tu32 start_seq_1 = get_unaligned_be32(&sp[1].start_seq);\n\n\t\tif (!after(end_seq_0, end_seq_1) &&\n\t\t    !before(start_seq_0, start_seq_1)) {\n\t\t\tdup_sack = true;\n\t\t\ttcp_dsack_seen(tp);\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPDSACKOFORECV);\n\t\t}\n\t}\n\n\t/* D-SACK for already forgotten data... Do dumb counting. */\n\tif (dup_sack && tp->undo_marker && tp->undo_retrans > 0 &&\n\t    !after(end_seq_0, prior_snd_una) &&\n\t    after(end_seq_0, tp->undo_marker))\n\t\ttp->undo_retrans--;\n\n\treturn dup_sack;\n}\n\nstruct tcp_sacktag_state {\n\tint\treord;\n\tint\tfack_count;\n\t/* Timestamps for earliest and latest never-retransmitted segment\n\t * that was SACKed. RTO needs the earliest RTT to stay conservative,\n\t * but congestion control should still get an accurate delay signal.\n\t */\n\tstruct skb_mstamp first_sackt;\n\tstruct skb_mstamp last_sackt;\n\tint\tflag;\n};\n\n/* Check if skb is fully within the SACK block. In presence of GSO skbs,\n * the incoming SACK may not exactly match but we can find smaller MSS\n * aligned portion of it that matches. Therefore we might need to fragment\n * which may fail and creates some hassle (caller must handle error case\n * returns).\n *\n * FIXME: this could be merged to shift decision code\n */\nstatic int tcp_match_skb_to_sack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  u32 start_seq, u32 end_seq)\n{\n\tint err;\n\tbool in_sack;\n\tunsigned int pkt_len;\n\tunsigned int mss;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (tcp_skb_pcount(skb) > 1 && !in_sack &&\n\t    after(TCP_SKB_CB(skb)->end_seq, start_seq)) {\n\t\tmss = tcp_skb_mss(skb);\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\n\t\tif (!in_sack) {\n\t\t\tpkt_len = start_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\tpkt_len = mss;\n\t\t} else {\n\t\t\tpkt_len = end_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* Round if necessary so that SACKs cover only full MSSes\n\t\t * and/or the remaining small portion (if present)\n\t\t */\n\t\tif (pkt_len > mss) {\n\t\t\tunsigned int new_len = (pkt_len / mss) * mss;\n\t\t\tif (!in_sack && new_len < pkt_len) {\n\t\t\t\tnew_len += mss;\n\t\t\t\tif (new_len >= skb->len)\n\t\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tpkt_len = new_len;\n\t\t}\n\t\terr = tcp_fragment(sk, skb, pkt_len, mss, GFP_ATOMIC);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn in_sack;\n}\n\n/* Mark the given newly-SACKed range as such, adjusting counters and hints. */\nstatic u8 tcp_sacktag_one(struct sock *sk,\n\t\t\t  struct tcp_sacktag_state *state, u8 sacked,\n\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t  int dup_sack, int pcount,\n\t\t\t  const struct skb_mstamp *xmit_time)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint fack_count = state->fack_count;\n\n\t/* Account D-SACK for retransmitted packet. */\n\tif (dup_sack && (sacked & TCPCB_RETRANS)) {\n\t\tif (tp->undo_marker && tp->undo_retrans > 0 &&\n\t\t    after(end_seq, tp->undo_marker))\n\t\t\ttp->undo_retrans--;\n\t\tif (sacked & TCPCB_SACKED_ACKED)\n\t\t\tstate->reord = min(fack_count, state->reord);\n\t}\n\n\t/* Nothing to do; acked frame is about to be dropped (was ACKed). */\n\tif (!after(end_seq, tp->snd_una))\n\t\treturn sacked;\n\n\tif (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\ttcp_rack_advance(tp, xmit_time, sacked);\n\n\t\tif (sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t/* If the segment is not tagged as lost,\n\t\t\t * we do not clear RETRANS, believing\n\t\t\t * that retransmission is still in flight.\n\t\t\t */\n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~(TCPCB_LOST|TCPCB_SACKED_RETRANS);\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t\ttp->retrans_out -= pcount;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(sacked & TCPCB_RETRANS)) {\n\t\t\t\t/* New sack for not retransmitted frame,\n\t\t\t\t * which was in hole. It is reordering.\n\t\t\t\t */\n\t\t\t\tif (before(start_seq,\n\t\t\t\t\t   tcp_highest_sack_seq(tp)))\n\t\t\t\t\tstate->reord = min(fack_count,\n\t\t\t\t\t\t\t   state->reord);\n\t\t\t\tif (!after(end_seq, tp->high_seq))\n\t\t\t\t\tstate->flag |= FLAG_ORIG_SACK_ACKED;\n\t\t\t\tif (state->first_sackt.v64 == 0)\n\t\t\t\t\tstate->first_sackt = *xmit_time;\n\t\t\t\tstate->last_sackt = *xmit_time;\n\t\t\t}\n\n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~TCPCB_LOST;\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t}\n\t\t}\n\n\t\tsacked |= TCPCB_SACKED_ACKED;\n\t\tstate->flag |= FLAG_DATA_SACKED;\n\t\ttp->sacked_out += pcount;\n\t\ttp->delivered += pcount;  /* Out-of-order packets delivered */\n\n\t\tfack_count += pcount;\n\n\t\t/* Lost marker hint past SACKed? Tweak RFC3517 cnt */\n\t\tif (!tcp_is_fack(tp) && tp->lost_skb_hint &&\n\t\t    before(start_seq, TCP_SKB_CB(tp->lost_skb_hint)->seq))\n\t\t\ttp->lost_cnt_hint += pcount;\n\n\t\tif (fack_count > tp->fackets_out)\n\t\t\ttp->fackets_out = fack_count;\n\t}\n\n\t/* D-SACK. We can detect redundant retransmission in S|R and plain R\n\t * frames and clear it. undo_retrans is decreased above, L|R frames\n\t * are accounted above as well.\n\t */\n\tif (dup_sack && (sacked & TCPCB_SACKED_RETRANS)) {\n\t\tsacked &= ~TCPCB_SACKED_RETRANS;\n\t\ttp->retrans_out -= pcount;\n\t}\n\n\treturn sacked;\n}\n\n/* Shift newly-SACKed bytes from this skb to the immediately previous\n * already-SACKed sk_buff. Mark the newly-SACKed bytes as such.\n */\nstatic bool tcp_shifted_skb(struct sock *sk, struct sk_buff *skb,\n\t\t\t    struct tcp_sacktag_state *state,\n\t\t\t    unsigned int pcount, int shifted, int mss,\n\t\t\t    bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *prev = tcp_write_queue_prev(sk, skb);\n\tu32 start_seq = TCP_SKB_CB(skb)->seq;\t/* start of newly-SACKed */\n\tu32 end_seq = start_seq + shifted;\t/* end of newly-SACKed */\n\n\tBUG_ON(!pcount);\n\n\t/* Adjust counters and hints for the newly sacked sequence\n\t * range but discard the return value since prev is already\n\t * marked. We must tag the range first because the seq\n\t * advancement below implicitly advances\n\t * tcp_highest_sack_seq() when skb is highest_sack.\n\t */\n\ttcp_sacktag_one(sk, state, TCP_SKB_CB(skb)->sacked,\n\t\t\tstart_seq, end_seq, dup_sack, pcount,\n\t\t\t&skb->skb_mstamp);\n\n\tif (skb == tp->lost_skb_hint)\n\t\ttp->lost_cnt_hint += pcount;\n\n\tTCP_SKB_CB(prev)->end_seq += shifted;\n\tTCP_SKB_CB(skb)->seq += shifted;\n\n\ttcp_skb_pcount_add(prev, pcount);\n\tBUG_ON(tcp_skb_pcount(skb) < pcount);\n\ttcp_skb_pcount_add(skb, -pcount);\n\n\t/* When we're adding to gso_segs == 1, gso_size will be zero,\n\t * in theory this shouldn't be necessary but as long as DSACK\n\t * code can come after this skb later on it's better to keep\n\t * setting gso_size to something.\n\t */\n\tif (!TCP_SKB_CB(prev)->tcp_gso_size)\n\t\tTCP_SKB_CB(prev)->tcp_gso_size = mss;\n\n\t/* CHECKME: To clear or not to clear? Mimics normal skb currently */\n\tif (tcp_skb_pcount(skb) <= 1)\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = 0;\n\n\t/* Difference in this won't matter, both ACKed by the same cumul. ACK */\n\tTCP_SKB_CB(prev)->sacked |= (TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS);\n\n\tif (skb->len > 0) {\n\t\tBUG_ON(!tcp_skb_pcount(skb));\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTED);\n\t\treturn false;\n\t}\n\n\t/* Whole SKB was eaten :-) */\n\n\tif (skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = prev;\n\tif (skb == tp->lost_skb_hint) {\n\t\ttp->lost_skb_hint = prev;\n\t\ttp->lost_cnt_hint -= tcp_skb_pcount(prev);\n\t}\n\n\tTCP_SKB_CB(prev)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(prev)->eor = TCP_SKB_CB(skb)->eor;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\tTCP_SKB_CB(prev)->end_seq++;\n\n\tif (skb == tcp_highest_sack(sk))\n\t\ttcp_advance_highest_sack(sk, skb);\n\n\ttcp_skb_collapse_tstamp(prev, skb);\n\ttcp_unlink_write_queue(skb, sk);\n\tsk_wmem_free_skb(sk, skb);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKMERGED);\n\n\treturn true;\n}\n\n/* I wish gso_size would have a bit more sane initialization than\n * something-or-zero which complicates things\n */\nstatic int tcp_skb_seglen(const struct sk_buff *skb)\n{\n\treturn tcp_skb_pcount(skb) == 1 ? skb->len : tcp_skb_mss(skb);\n}\n\n/* Shifting pages past head area doesn't work */\nstatic int skb_can_shift(const struct sk_buff *skb)\n{\n\treturn !skb_headlen(skb) && skb_is_nonlinear(skb);\n}\n\n/* Try collapsing SACK blocks spanning across multiple skbs to a single\n * skb.\n */\nstatic struct sk_buff *tcp_shift_skb_data(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct tcp_sacktag_state *state,\n\t\t\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t\t\t  bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *prev;\n\tint mss;\n\tint pcount = 0;\n\tint len;\n\tint in_sack;\n\n\tif (!sk_can_gso(sk))\n\t\tgoto fallback;\n\n\t/* Normally R but no L won't result in plain S */\n\tif (!dup_sack &&\n\t    (TCP_SKB_CB(skb)->sacked & (TCPCB_LOST|TCPCB_SACKED_RETRANS)) == TCPCB_SACKED_RETRANS)\n\t\tgoto fallback;\n\tif (!skb_can_shift(skb))\n\t\tgoto fallback;\n\t/* This frame is about to be dropped (was ACKed). */\n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una))\n\t\tgoto fallback;\n\n\t/* Can only happen with delayed DSACK + discard craziness */\n\tif (unlikely(skb == tcp_write_queue_head(sk)))\n\t\tgoto fallback;\n\tprev = tcp_write_queue_prev(sk, skb);\n\n\tif ((TCP_SKB_CB(prev)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED)\n\t\tgoto fallback;\n\n\tif (!tcp_skb_can_collapse_to(prev))\n\t\tgoto fallback;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (in_sack) {\n\t\tlen = skb->len;\n\t\tpcount = tcp_skb_pcount(skb);\n\t\tmss = tcp_skb_seglen(skb);\n\n\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\n\t\t * drop this restriction as unnecessary\n\t\t */\n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\t} else {\n\t\tif (!after(TCP_SKB_CB(skb)->end_seq, start_seq))\n\t\t\tgoto noop;\n\t\t/* CHECKME: This is non-MSS split case only?, this will\n\t\t * cause skipped skbs due to advancing loop btw, original\n\t\t * has that feature too\n\t\t */\n\t\tif (tcp_skb_pcount(skb) <= 1)\n\t\t\tgoto noop;\n\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\t\tif (!in_sack) {\n\t\t\t/* TODO: head merge to next could be attempted here\n\t\t\t * if (!after(TCP_SKB_CB(skb)->end_seq, end_seq)),\n\t\t\t * though it might not be worth of the additional hassle\n\t\t\t *\n\t\t\t * ...we can probably just fallback to what was done\n\t\t\t * previously. We could try merging non-SACKed ones\n\t\t\t * as well but it probably isn't going to buy off\n\t\t\t * because later SACKs might again split them, and\n\t\t\t * it would make skb timestamp tracking considerably\n\t\t\t * harder problem.\n\t\t\t */\n\t\t\tgoto fallback;\n\t\t}\n\n\t\tlen = end_seq - TCP_SKB_CB(skb)->seq;\n\t\tBUG_ON(len < 0);\n\t\tBUG_ON(len > skb->len);\n\n\t\t/* MSS boundaries should be honoured or else pcount will\n\t\t * severely break even though it makes things bit trickier.\n\t\t * Optimize common case to avoid most of the divides\n\t\t */\n\t\tmss = tcp_skb_mss(skb);\n\n\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\n\t\t * drop this restriction as unnecessary\n\t\t */\n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\n\t\tif (len == mss) {\n\t\t\tpcount = 1;\n\t\t} else if (len < mss) {\n\t\t\tgoto noop;\n\t\t} else {\n\t\t\tpcount = len / mss;\n\t\t\tlen = pcount * mss;\n\t\t}\n\t}\n\n\t/* tcp_sacktag_one() won't SACK-tag ranges below snd_una */\n\tif (!after(TCP_SKB_CB(skb)->seq + len, tp->snd_una))\n\t\tgoto fallback;\n\n\tif (!skb_shift(prev, skb, len))\n\t\tgoto fallback;\n\tif (!tcp_shifted_skb(sk, skb, state, pcount, len, mss, dup_sack))\n\t\tgoto out;\n\n\t/* Hole filled allows collapsing with the next as well, this is very\n\t * useful when hole on every nth skb pattern happens\n\t */\n\tif (prev == tcp_write_queue_tail(sk))\n\t\tgoto out;\n\tskb = tcp_write_queue_next(sk, prev);\n\n\tif (!skb_can_shift(skb) ||\n\t    (skb == tcp_send_head(sk)) ||\n\t    ((TCP_SKB_CB(skb)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED) ||\n\t    (mss != tcp_skb_seglen(skb)))\n\t\tgoto out;\n\n\tlen = skb->len;\n\tif (skb_shift(prev, skb, len)) {\n\t\tpcount += tcp_skb_pcount(skb);\n\t\ttcp_shifted_skb(sk, skb, state, tcp_skb_pcount(skb), len, mss, 0);\n\t}\n\nout:\n\tstate->fack_count += pcount;\n\treturn prev;\n\nnoop:\n\treturn skb;\n\nfallback:\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTFALLBACK);\n\treturn NULL;\n}\n\nstatic struct sk_buff *tcp_sacktag_walk(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\tu32 start_seq, u32 end_seq,\n\t\t\t\t\tbool dup_sack_in)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *tmp;\n\n\ttcp_for_write_queue_from(skb, sk) {\n\t\tint in_sack = 0;\n\t\tbool dup_sack = dup_sack_in;\n\n\t\tif (skb == tcp_send_head(sk))\n\t\t\tbreak;\n\n\t\t/* queue is in-order => we can short-circuit the walk early */\n\t\tif (!before(TCP_SKB_CB(skb)->seq, end_seq))\n\t\t\tbreak;\n\n\t\tif (next_dup  &&\n\t\t    before(TCP_SKB_CB(skb)->seq, next_dup->end_seq)) {\n\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\tnext_dup->start_seq,\n\t\t\t\t\t\t\tnext_dup->end_seq);\n\t\t\tif (in_sack > 0)\n\t\t\t\tdup_sack = true;\n\t\t}\n\n\t\t/* skb reference here is a bit tricky to get right, since\n\t\t * shifting can eat and free both this skb and the next,\n\t\t * so not even _safe variant of the loop is enough.\n\t\t */\n\t\tif (in_sack <= 0) {\n\t\t\ttmp = tcp_shift_skb_data(sk, skb, state,\n\t\t\t\t\t\t start_seq, end_seq, dup_sack);\n\t\t\tif (tmp) {\n\t\t\t\tif (tmp != skb) {\n\t\t\t\t\tskb = tmp;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tin_sack = 0;\n\t\t\t} else {\n\t\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\t\tstart_seq,\n\t\t\t\t\t\t\t\tend_seq);\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(in_sack < 0))\n\t\t\tbreak;\n\n\t\tif (in_sack) {\n\t\t\tTCP_SKB_CB(skb)->sacked =\n\t\t\t\ttcp_sacktag_one(sk,\n\t\t\t\t\t\tstate,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->sacked,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->seq,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->end_seq,\n\t\t\t\t\t\tdup_sack,\n\t\t\t\t\t\ttcp_skb_pcount(skb),\n\t\t\t\t\t\t&skb->skb_mstamp);\n\n\t\t\tif (!before(TCP_SKB_CB(skb)->seq,\n\t\t\t\t    tcp_highest_sack_seq(tp)))\n\t\t\t\ttcp_advance_highest_sack(sk, skb);\n\t\t}\n\n\t\tstate->fack_count += tcp_skb_pcount(skb);\n\t}\n\treturn skb;\n}\n\n/* Avoid all extra work that is being done by sacktag while walking in\n * a normal way\n */\nstatic struct sk_buff *tcp_sacktag_skip(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\tu32 skip_to_seq)\n{\n\ttcp_for_write_queue_from(skb, sk) {\n\t\tif (skb == tcp_send_head(sk))\n\t\t\tbreak;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, skip_to_seq))\n\t\t\tbreak;\n\n\t\tstate->fack_count += tcp_skb_pcount(skb);\n\t}\n\treturn skb;\n}\n\nstatic struct sk_buff *tcp_maybe_skipping_dsack(struct sk_buff *skb,\n\t\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\t\tu32 skip_to_seq)\n{\n\tif (!next_dup)\n\t\treturn skb;\n\n\tif (before(next_dup->start_seq, skip_to_seq)) {\n\t\tskb = tcp_sacktag_skip(skb, sk, state, next_dup->start_seq);\n\t\tskb = tcp_sacktag_walk(skb, sk, NULL, state,\n\t\t\t\t       next_dup->start_seq, next_dup->end_seq,\n\t\t\t\t       1);\n\t}\n\n\treturn skb;\n}\n\nstatic int tcp_sack_cache_ok(const struct tcp_sock *tp, const struct tcp_sack_block *cache)\n{\n\treturn cache < tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n}\n\nstatic int\ntcp_sacktag_write_queue(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\tu32 prior_snd_una, struct tcp_sacktag_state *state)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst unsigned char *ptr = (skb_transport_header(ack_skb) +\n\t\t\t\t    TCP_SKB_CB(ack_skb)->sacked);\n\tstruct tcp_sack_block_wire *sp_wire = (struct tcp_sack_block_wire *)(ptr+2);\n\tstruct tcp_sack_block sp[TCP_NUM_SACKS];\n\tstruct tcp_sack_block *cache;\n\tstruct sk_buff *skb;\n\tint num_sacks = min(TCP_NUM_SACKS, (ptr[1] - TCPOLEN_SACK_BASE) >> 3);\n\tint used_sacks;\n\tbool found_dup_sack = false;\n\tint i, j;\n\tint first_sack_index;\n\n\tstate->flag = 0;\n\tstate->reord = tp->packets_out;\n\n\tif (!tp->sacked_out) {\n\t\tif (WARN_ON(tp->fackets_out))\n\t\t\ttp->fackets_out = 0;\n\t\ttcp_highest_sack_reset(sk);\n\t}\n\n\tfound_dup_sack = tcp_check_dsack(sk, ack_skb, sp_wire,\n\t\t\t\t\t num_sacks, prior_snd_una);\n\tif (found_dup_sack)\n\t\tstate->flag |= FLAG_DSACKING_ACK;\n\n\t/* Eliminate too old ACKs, but take into\n\t * account more or less fresh ones, they can\n\t * contain valid SACK info.\n\t */\n\tif (before(TCP_SKB_CB(ack_skb)->ack_seq, prior_snd_una - tp->max_window))\n\t\treturn 0;\n\n\tif (!tp->packets_out)\n\t\tgoto out;\n\n\tused_sacks = 0;\n\tfirst_sack_index = 0;\n\tfor (i = 0; i < num_sacks; i++) {\n\t\tbool dup_sack = !i && found_dup_sack;\n\n\t\tsp[used_sacks].start_seq = get_unaligned_be32(&sp_wire[i].start_seq);\n\t\tsp[used_sacks].end_seq = get_unaligned_be32(&sp_wire[i].end_seq);\n\n\t\tif (!tcp_is_sackblock_valid(tp, dup_sack,\n\t\t\t\t\t    sp[used_sacks].start_seq,\n\t\t\t\t\t    sp[used_sacks].end_seq)) {\n\t\t\tint mib_idx;\n\n\t\t\tif (dup_sack) {\n\t\t\t\tif (!tp->undo_marker)\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDNOUNDO;\n\t\t\t\telse\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDOLD;\n\t\t\t} else {\n\t\t\t\t/* Don't count olds caused by ACK reordering */\n\t\t\t\tif ((TCP_SKB_CB(ack_skb)->ack_seq != tp->snd_una) &&\n\t\t\t\t    !after(sp[used_sacks].end_seq, tp->snd_una))\n\t\t\t\t\tcontinue;\n\t\t\t\tmib_idx = LINUX_MIB_TCPSACKDISCARD;\n\t\t\t}\n\n\t\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t\t\tif (i == 0)\n\t\t\t\tfirst_sack_index = -1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Ignore very old stuff early */\n\t\tif (!after(sp[used_sacks].end_seq, prior_snd_una))\n\t\t\tcontinue;\n\n\t\tused_sacks++;\n\t}\n\n\t/* order SACK blocks to allow in order walk of the retrans queue */\n\tfor (i = used_sacks - 1; i > 0; i--) {\n\t\tfor (j = 0; j < i; j++) {\n\t\t\tif (after(sp[j].start_seq, sp[j + 1].start_seq)) {\n\t\t\t\tswap(sp[j], sp[j + 1]);\n\n\t\t\t\t/* Track where the first SACK block goes to */\n\t\t\t\tif (j == first_sack_index)\n\t\t\t\t\tfirst_sack_index = j + 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tskb = tcp_write_queue_head(sk);\n\tstate->fack_count = 0;\n\ti = 0;\n\n\tif (!tp->sacked_out) {\n\t\t/* It's already past, so skip checking against it */\n\t\tcache = tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n\t} else {\n\t\tcache = tp->recv_sack_cache;\n\t\t/* Skip empty blocks in at head of the cache */\n\t\twhile (tcp_sack_cache_ok(tp, cache) && !cache->start_seq &&\n\t\t       !cache->end_seq)\n\t\t\tcache++;\n\t}\n\n\twhile (i < used_sacks) {\n\t\tu32 start_seq = sp[i].start_seq;\n\t\tu32 end_seq = sp[i].end_seq;\n\t\tbool dup_sack = (found_dup_sack && (i == first_sack_index));\n\t\tstruct tcp_sack_block *next_dup = NULL;\n\n\t\tif (found_dup_sack && ((i + 1) == first_sack_index))\n\t\t\tnext_dup = &sp[i + 1];\n\n\t\t/* Skip too early cached blocks */\n\t\twhile (tcp_sack_cache_ok(tp, cache) &&\n\t\t       !before(start_seq, cache->end_seq))\n\t\t\tcache++;\n\n\t\t/* Can skip some work by looking recv_sack_cache? */\n\t\tif (tcp_sack_cache_ok(tp, cache) && !dup_sack &&\n\t\t    after(end_seq, cache->start_seq)) {\n\n\t\t\t/* Head todo? */\n\t\t\tif (before(start_seq, cache->start_seq)) {\n\t\t\t\tskb = tcp_sacktag_skip(skb, sk, state,\n\t\t\t\t\t\t       start_seq);\n\t\t\t\tskb = tcp_sacktag_walk(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       start_seq,\n\t\t\t\t\t\t       cache->start_seq,\n\t\t\t\t\t\t       dup_sack);\n\t\t\t}\n\n\t\t\t/* Rest of the block already fully processed? */\n\t\t\tif (!after(end_seq, cache->end_seq))\n\t\t\t\tgoto advance_sp;\n\n\t\t\tskb = tcp_maybe_skipping_dsack(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       cache->end_seq);\n\n\t\t\t/* ...tail remains todo... */\n\t\t\tif (tcp_highest_sack_seq(tp) == cache->end_seq) {\n\t\t\t\t/* ...but better entrypoint exists! */\n\t\t\t\tskb = tcp_highest_sack(sk);\n\t\t\t\tif (!skb)\n\t\t\t\t\tbreak;\n\t\t\t\tstate->fack_count = tp->fackets_out;\n\t\t\t\tcache++;\n\t\t\t\tgoto walk;\n\t\t\t}\n\n\t\t\tskb = tcp_sacktag_skip(skb, sk, state, cache->end_seq);\n\t\t\t/* Check overlap against next cached too (past this one already) */\n\t\t\tcache++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!before(start_seq, tcp_highest_sack_seq(tp))) {\n\t\t\tskb = tcp_highest_sack(sk);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\tstate->fack_count = tp->fackets_out;\n\t\t}\n\t\tskb = tcp_sacktag_skip(skb, sk, state, start_seq);\n\nwalk:\n\t\tskb = tcp_sacktag_walk(skb, sk, next_dup, state,\n\t\t\t\t       start_seq, end_seq, dup_sack);\n\nadvance_sp:\n\t\ti++;\n\t}\n\n\t/* Clear the head of the cache sack blocks so we can skip it next time */\n\tfor (i = 0; i < ARRAY_SIZE(tp->recv_sack_cache) - used_sacks; i++) {\n\t\ttp->recv_sack_cache[i].start_seq = 0;\n\t\ttp->recv_sack_cache[i].end_seq = 0;\n\t}\n\tfor (j = 0; j < used_sacks; j++)\n\t\ttp->recv_sack_cache[i++] = sp[j];\n\n\tif ((state->reord < tp->fackets_out) &&\n\t    ((inet_csk(sk)->icsk_ca_state != TCP_CA_Loss) || tp->undo_marker))\n\t\ttcp_update_reordering(sk, tp->fackets_out - state->reord, 0);\n\n\ttcp_verify_left_out(tp);\nout:\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tWARN_ON((int)tcp_packets_in_flight(tp) < 0);\n#endif\n\treturn state->flag;\n}\n\n/* Limits sacked_out so that sum with lost_out isn't ever larger than\n * packets_out. Returns false if sacked_out adjustement wasn't necessary.\n */\nstatic bool tcp_limit_reno_sacked(struct tcp_sock *tp)\n{\n\tu32 holes;\n\n\tholes = max(tp->lost_out, 1U);\n\tholes = min(holes, tp->packets_out);\n\n\tif ((tp->sacked_out + holes) > tp->packets_out) {\n\t\ttp->sacked_out = tp->packets_out - holes;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* If we receive more dupacks than we expected counting segments\n * in assumption of absent reordering, interpret this as reordering.\n * The only another reason could be bug in receiver TCP.\n */\nstatic void tcp_check_reno_reordering(struct sock *sk, const int addend)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tif (tcp_limit_reno_sacked(tp))\n\t\ttcp_update_reordering(sk, tp->packets_out + addend, 0);\n}\n\n/* Emulate SACKs for SACKless connection: account for a new dupack. */\n\nstatic void tcp_add_reno_sack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 prior_sacked = tp->sacked_out;\n\n\ttp->sacked_out++;\n\ttcp_check_reno_reordering(sk, 0);\n\tif (tp->sacked_out > prior_sacked)\n\t\ttp->delivered++; /* Some out-of-order packet is delivered */\n\ttcp_verify_left_out(tp);\n}\n\n/* Account for ACK, ACKing some data in Reno Recovery phase. */\n\nstatic void tcp_remove_reno_sacks(struct sock *sk, int acked)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (acked > 0) {\n\t\t/* One ACK acked hole. The rest eat duplicate ACKs. */\n\t\ttp->delivered += max_t(int, acked - tp->sacked_out, 1);\n\t\tif (acked - 1 >= tp->sacked_out)\n\t\t\ttp->sacked_out = 0;\n\t\telse\n\t\t\ttp->sacked_out -= acked - 1;\n\t}\n\ttcp_check_reno_reordering(sk, acked);\n\ttcp_verify_left_out(tp);\n}\n\nstatic inline void tcp_reset_reno_sack(struct tcp_sock *tp)\n{\n\ttp->sacked_out = 0;\n}\n\nvoid tcp_clear_retrans(struct tcp_sock *tp)\n{\n\ttp->retrans_out = 0;\n\ttp->lost_out = 0;\n\ttp->undo_marker = 0;\n\ttp->undo_retrans = -1;\n\ttp->fackets_out = 0;\n\ttp->sacked_out = 0;\n}\n\nstatic inline void tcp_init_undo(struct tcp_sock *tp)\n{\n\ttp->undo_marker = tp->snd_una;\n\t/* Retransmission still in flight may cause DSACKs later. */\n\ttp->undo_retrans = tp->retrans_out ? : -1;\n}\n\n/* Enter Loss state. If we detect SACK reneging, forget all SACK information\n * and reset tags completely, otherwise preserve SACKs. If receiver\n * dropped its ofo queue, we will know this due to reneging detection.\n */\nvoid tcp_enter_loss(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct sk_buff *skb;\n\tbool new_recovery = icsk->icsk_ca_state < TCP_CA_Recovery;\n\tbool is_reneg;\t\t\t/* is receiver reneging on SACKs? */\n\n\t/* Reduce ssthresh if it has not yet been made inside this window. */\n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder ||\n\t    !after(tp->high_seq, tp->snd_una) ||\n\t    (icsk->icsk_ca_state == TCP_CA_Loss && !icsk->icsk_retransmits)) {\n\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->snd_ssthresh = icsk->icsk_ca_ops->ssthresh(sk);\n\t\ttcp_ca_event(sk, CA_EVENT_LOSS);\n\t\ttcp_init_undo(tp);\n\t}\n\ttp->snd_cwnd\t   = 1;\n\ttp->snd_cwnd_cnt   = 0;\n\ttp->snd_cwnd_stamp = tcp_time_stamp;\n\n\ttp->retrans_out = 0;\n\ttp->lost_out = 0;\n\n\tif (tcp_is_reno(tp))\n\t\ttcp_reset_reno_sack(tp);\n\n\tskb = tcp_write_queue_head(sk);\n\tis_reneg = skb && (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED);\n\tif (is_reneg) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSACKRENEGING);\n\t\ttp->sacked_out = 0;\n\t\ttp->fackets_out = 0;\n\t}\n\ttcp_clear_all_retrans_hints(tp);\n\n\ttcp_for_write_queue(skb, sk) {\n\t\tif (skb == tcp_send_head(sk))\n\t\t\tbreak;\n\n\t\tTCP_SKB_CB(skb)->sacked &= (~TCPCB_TAGBITS)|TCPCB_SACKED_ACKED;\n\t\tif (!(TCP_SKB_CB(skb)->sacked&TCPCB_SACKED_ACKED) || is_reneg) {\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_ACKED;\n\t\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_LOST;\n\t\t\ttp->lost_out += tcp_skb_pcount(skb);\n\t\t\ttp->retransmit_high = TCP_SKB_CB(skb)->end_seq;\n\t\t}\n\t}\n\ttcp_verify_left_out(tp);\n\n\t/* Timeout in disordered state after receiving substantial DUPACKs\n\t * suggests that the degree of reordering is over-estimated.\n\t */\n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder &&\n\t    tp->sacked_out >= net->ipv4.sysctl_tcp_reordering)\n\t\ttp->reordering = min_t(unsigned int, tp->reordering,\n\t\t\t\t       net->ipv4.sysctl_tcp_reordering);\n\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\ttp->high_seq = tp->snd_nxt;\n\ttcp_ecn_queue_cwr(tp);\n\n\t/* F-RTO RFC5682 sec 3.1 step 1: retransmit SND.UNA if no previous\n\t * loss recovery is underway except recurring timeout(s) on\n\t * the same SND.UNA (sec 3.2). Disable F-RTO on path MTU probing\n\t */\n\ttp->frto = sysctl_tcp_frto &&\n\t\t   (new_recovery || icsk->icsk_retransmits) &&\n\t\t   !inet_csk(sk)->icsk_mtup.probe_size;\n}\n\n/* If ACK arrived pointing to a remembered SACK, it means that our\n * remembered SACKs do not reflect real state of receiver i.e.\n * receiver _host_ is heavily congested (or buggy).\n *\n * To avoid big spurious retransmission bursts due to transient SACK\n * scoreboard oddities that look like reneging, we give the receiver a\n * little time (max(RTT/2, 10ms)) to send us some more ACKs that will\n * restore sanity to the SACK scoreboard. If the apparent reneging\n * persists until this RTO then we'll clear the SACK scoreboard.\n */\nstatic bool tcp_check_sack_reneging(struct sock *sk, int flag)\n{\n\tif (flag & FLAG_SACK_RENEGING) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tunsigned long delay = max(usecs_to_jiffies(tp->srtt_us >> 4),\n\t\t\t\t\t  msecs_to_jiffies(10));\n\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t  delay, TCP_RTO_MAX);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline int tcp_fackets_out(const struct tcp_sock *tp)\n{\n\treturn tcp_is_reno(tp) ? tp->sacked_out + 1 : tp->fackets_out;\n}\n\n/* Heurestics to calculate number of duplicate ACKs. There's no dupACKs\n * counter when SACK is enabled (without SACK, sacked_out is used for\n * that purpose).\n *\n * Instead, with FACK TCP uses fackets_out that includes both SACKed\n * segments up to the highest received SACK block so far and holes in\n * between them.\n *\n * With reordering, holes may still be in flight, so RFC3517 recovery\n * uses pure sacked_out (total number of SACKed segments) even though\n * it violates the RFC that uses duplicate ACKs, often these are equal\n * but when e.g. out-of-window ACKs or packet duplication occurs,\n * they differ. Since neither occurs due to loss, TCP should really\n * ignore them.\n */\nstatic inline int tcp_dupack_heuristics(const struct tcp_sock *tp)\n{\n\treturn tcp_is_fack(tp) ? tp->fackets_out : tp->sacked_out + 1;\n}\n\nstatic bool tcp_pause_early_retransmit(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long delay;\n\n\t/* Delay early retransmit and entering fast recovery for\n\t * max(RTT/4, 2msec) unless ack has ECE mark, no RTT samples\n\t * available, or RTO is scheduled to fire first.\n\t */\n\tif (sysctl_tcp_early_retrans < 2 || sysctl_tcp_early_retrans > 3 ||\n\t    (flag & FLAG_ECE) || !tp->srtt_us)\n\t\treturn false;\n\n\tdelay = max(usecs_to_jiffies(tp->srtt_us >> 5),\n\t\t    msecs_to_jiffies(2));\n\n\tif (!time_after(inet_csk(sk)->icsk_timeout, (jiffies + delay)))\n\t\treturn false;\n\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_EARLY_RETRANS, delay,\n\t\t\t\t  TCP_RTO_MAX);\n\treturn true;\n}\n\n/* Linux NewReno/SACK/FACK/ECN state machine.\n * --------------------------------------\n *\n * \"Open\"\tNormal state, no dubious events, fast path.\n * \"Disorder\"   In all the respects it is \"Open\",\n *\t\tbut requires a bit more attention. It is entered when\n *\t\twe see some SACKs or dupacks. It is split of \"Open\"\n *\t\tmainly to move some processing from fast path to slow one.\n * \"CWR\"\tCWND was reduced due to some Congestion Notification event.\n *\t\tIt can be ECN, ICMP source quench, local device congestion.\n * \"Recovery\"\tCWND was reduced, we are fast-retransmitting.\n * \"Loss\"\tCWND was reduced due to RTO timeout or SACK reneging.\n *\n * tcp_fastretrans_alert() is entered:\n * - each incoming ACK, if state is not \"Open\"\n * - when arrived ACK is unusual, namely:\n *\t* SACK\n *\t* Duplicate ACK.\n *\t* ECN ECE.\n *\n * Counting packets in flight is pretty simple.\n *\n *\tin_flight = packets_out - left_out + retrans_out\n *\n *\tpackets_out is SND.NXT-SND.UNA counted in packets.\n *\n *\tretrans_out is number of retransmitted segments.\n *\n *\tleft_out is number of segments left network, but not ACKed yet.\n *\n *\t\tleft_out = sacked_out + lost_out\n *\n *     sacked_out: Packets, which arrived to receiver out of order\n *\t\t   and hence not ACKed. With SACKs this number is simply\n *\t\t   amount of SACKed data. Even without SACKs\n *\t\t   it is easy to give pretty reliable estimate of this number,\n *\t\t   counting duplicate ACKs.\n *\n *       lost_out: Packets lost by network. TCP has no explicit\n *\t\t   \"loss notification\" feedback from network (for now).\n *\t\t   It means that this number can be only _guessed_.\n *\t\t   Actually, it is the heuristics to predict lossage that\n *\t\t   distinguishes different algorithms.\n *\n *\tF.e. after RTO, when all the queue is considered as lost,\n *\tlost_out = packets_out and in_flight = retrans_out.\n *\n *\t\tEssentially, we have now two algorithms counting\n *\t\tlost packets.\n *\n *\t\tFACK: It is the simplest heuristics. As soon as we decided\n *\t\tthat something is lost, we decide that _all_ not SACKed\n *\t\tpackets until the most forward SACK are lost. I.e.\n *\t\tlost_out = fackets_out - sacked_out and left_out = fackets_out.\n *\t\tIt is absolutely correct estimate, if network does not reorder\n *\t\tpackets. And it loses any connection to reality when reordering\n *\t\ttakes place. We use FACK by default until reordering\n *\t\tis suspected on the path to this destination.\n *\n *\t\tNewReno: when Recovery is entered, we assume that one segment\n *\t\tis lost (classic Reno). While we are in Recovery and\n *\t\ta partial ACK arrives, we assume that one more packet\n *\t\tis lost (NewReno). This heuristics are the same in NewReno\n *\t\tand SACK.\n *\n *  Imagine, that's all! Forget about all this shamanism about CWND inflation\n *  deflation etc. CWND is real congestion window, never inflated, changes\n *  only according to classic VJ rules.\n *\n * Really tricky (and requiring careful tuning) part of algorithm\n * is hidden in functions tcp_time_to_recover() and tcp_xmit_retransmit_queue().\n * The first determines the moment _when_ we should reduce CWND and,\n * hence, slow down forward transmission. In fact, it determines the moment\n * when we decide that hole is caused by loss, rather than by a reorder.\n *\n * tcp_xmit_retransmit_queue() decides, _what_ we should retransmit to fill\n * holes, caused by lost packets.\n *\n * And the most logically complicated part of algorithm is undo\n * heuristics. We detect false retransmits due to both too early\n * fast retransmit (reordering) and underestimated RTO, analyzing\n * timestamps and D-SACKs. When we detect that some segments were\n * retransmitted by mistake and CWND reduction was wrong, we undo\n * window reduction and abort recovery phase. This logic is hidden\n * inside several functions named tcp_try_undo_<something>.\n */\n\n/* This function decides, when we should leave Disordered state\n * and enter Recovery phase, reducing congestion window.\n *\n * Main question: may we further continue forward transmission\n * with the same cwnd?\n */\nstatic bool tcp_time_to_recover(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u32 packets_out;\n\tint tcp_reordering = sock_net(sk)->ipv4.sysctl_tcp_reordering;\n\n\t/* Trick#1: The loss is proven. */\n\tif (tp->lost_out)\n\t\treturn true;\n\n\t/* Not-A-Trick#2 : Classic rule... */\n\tif (tcp_dupack_heuristics(tp) > tp->reordering)\n\t\treturn true;\n\n\t/* Trick#4: It is still not OK... But will it be useful to delay\n\t * recovery more?\n\t */\n\tpackets_out = tp->packets_out;\n\tif (packets_out <= tp->reordering &&\n\t    tp->sacked_out >= max_t(__u32, packets_out/2, tcp_reordering) &&\n\t    !tcp_may_send_now(sk)) {\n\t\t/* We have nothing to send. This connection is limited\n\t\t * either by receiver window or by application.\n\t\t */\n\t\treturn true;\n\t}\n\n\t/* If a thin stream is detected, retransmit after first\n\t * received dupack. Employ only if SACK is supported in order\n\t * to avoid possible corner-case series of spurious retransmissions\n\t * Use only if there are no unsent data.\n\t */\n\tif ((tp->thin_dupack || sysctl_tcp_thin_dupack) &&\n\t    tcp_stream_is_thin(tp) && tcp_dupack_heuristics(tp) > 1 &&\n\t    tcp_is_sack(tp) && !tcp_send_head(sk))\n\t\treturn true;\n\n\t/* Trick#6: TCP early retransmit, per RFC5827.  To avoid spurious\n\t * retransmissions due to small network reorderings, we implement\n\t * Mitigation A.3 in the RFC and delay the retransmission for a short\n\t * interval if appropriate.\n\t */\n\tif (tp->do_early_retrans && !tp->retrans_out && tp->sacked_out &&\n\t    (tp->packets_out >= (tp->sacked_out + 1) && tp->packets_out < 4) &&\n\t    !tcp_may_send_now(sk))\n\t\treturn !tcp_pause_early_retransmit(sk, flag);\n\n\treturn false;\n}\n\n/* Detect loss in event \"A\" above by marking head of queue up as lost.\n * For FACK or non-SACK(Reno) senders, the first \"packets\" number of segments\n * are considered lost. For RFC3517 SACK, a segment is considered lost if it\n * has at least tp->reordering SACKed seqments above it; \"packets\" refers to\n * the maximum SACKed segments to pass before reaching this limit.\n */\nstatic void tcp_mark_head_lost(struct sock *sk, int packets, int mark_head)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint cnt, oldcnt, lost;\n\tunsigned int mss;\n\t/* Use SACK to deduce losses of new sequences sent during recovery */\n\tconst u32 loss_high = tcp_is_sack(tp) ?  tp->snd_nxt : tp->high_seq;\n\n\tWARN_ON(packets > tp->packets_out);\n\tif (tp->lost_skb_hint) {\n\t\tskb = tp->lost_skb_hint;\n\t\tcnt = tp->lost_cnt_hint;\n\t\t/* Head already handled? */\n\t\tif (mark_head && skb != tcp_write_queue_head(sk))\n\t\t\treturn;\n\t} else {\n\t\tskb = tcp_write_queue_head(sk);\n\t\tcnt = 0;\n\t}\n\n\ttcp_for_write_queue_from(skb, sk) {\n\t\tif (skb == tcp_send_head(sk))\n\t\t\tbreak;\n\t\t/* TODO: do this better */\n\t\t/* this is not the most efficient way to do this... */\n\t\ttp->lost_skb_hint = skb;\n\t\ttp->lost_cnt_hint = cnt;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, loss_high))\n\t\t\tbreak;\n\n\t\toldcnt = cnt;\n\t\tif (tcp_is_fack(tp) || tcp_is_reno(tp) ||\n\t\t    (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED))\n\t\t\tcnt += tcp_skb_pcount(skb);\n\n\t\tif (cnt > packets) {\n\t\t\tif ((tcp_is_sack(tp) && !tcp_is_fack(tp)) ||\n\t\t\t    (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED) ||\n\t\t\t    (oldcnt >= packets))\n\t\t\t\tbreak;\n\n\t\t\tmss = tcp_skb_mss(skb);\n\t\t\t/* If needed, chop off the prefix to mark as lost. */\n\t\t\tlost = (packets - oldcnt) * mss;\n\t\t\tif (lost < skb->len &&\n\t\t\t    tcp_fragment(sk, skb, lost, mss, GFP_ATOMIC) < 0)\n\t\t\t\tbreak;\n\t\t\tcnt = packets;\n\t\t}\n\n\t\ttcp_skb_mark_lost(tp, skb);\n\n\t\tif (mark_head)\n\t\t\tbreak;\n\t}\n\ttcp_verify_left_out(tp);\n}\n\n/* Account newly detected lost packet(s) */\n\nstatic void tcp_update_scoreboard(struct sock *sk, int fast_rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_is_reno(tp)) {\n\t\ttcp_mark_head_lost(sk, 1, 1);\n\t} else if (tcp_is_fack(tp)) {\n\t\tint lost = tp->fackets_out - tp->reordering;\n\t\tif (lost <= 0)\n\t\t\tlost = 1;\n\t\ttcp_mark_head_lost(sk, lost, 0);\n\t} else {\n\t\tint sacked_upto = tp->sacked_out - tp->reordering;\n\t\tif (sacked_upto >= 0)\n\t\t\ttcp_mark_head_lost(sk, sacked_upto, 0);\n\t\telse if (fast_rexmit)\n\t\t\ttcp_mark_head_lost(sk, 1, 1);\n\t}\n}\n\nstatic bool tcp_tsopt_ecr_before(const struct tcp_sock *tp, u32 when)\n{\n\treturn tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t       before(tp->rx_opt.rcv_tsecr, when);\n}\n\n/* skb is spurious retransmitted if the returned timestamp echo\n * reply is prior to the skb transmission time\n */\nstatic bool tcp_skb_spurious_retrans(const struct tcp_sock *tp,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn (TCP_SKB_CB(skb)->sacked & TCPCB_RETRANS) &&\n\t       tcp_tsopt_ecr_before(tp, tcp_skb_timestamp(skb));\n}\n\n/* Nothing was retransmitted or returned timestamp is less\n * than timestamp of the first retransmission.\n */\nstatic inline bool tcp_packet_delayed(const struct tcp_sock *tp)\n{\n\treturn !tp->retrans_stamp ||\n\t       tcp_tsopt_ecr_before(tp, tp->retrans_stamp);\n}\n\n/* Undo procedures. */\n\n/* We can clear retrans_stamp when there are no retransmissions in the\n * window. It would seem that it is trivially available for us in\n * tp->retrans_out, however, that kind of assumptions doesn't consider\n * what will happen if errors occur when sending retransmission for the\n * second time. ...It could the that such segment has only\n * TCPCB_EVER_RETRANS set at the present time. It seems that checking\n * the head skb is enough except for some reneging corner cases that\n * are not worth the effort.\n *\n * Main reason for all this complexity is the fact that connection dying\n * time now depends on the validity of the retrans_stamp, in particular,\n * that successive retransmissions of a segment must not advance\n * retrans_stamp under any conditions.\n */\nstatic bool tcp_any_retrans_done(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (tp->retrans_out)\n\t\treturn true;\n\n\tskb = tcp_write_queue_head(sk);\n\tif (unlikely(skb && TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS))\n\t\treturn true;\n\n\treturn false;\n}\n\n#if FASTRETRANS_DEBUG > 1\nstatic void DBGUNDO(struct sock *sk, const char *msg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (sk->sk_family == AF_INET) {\n\t\tpr_debug(\"Undo %s %pI4/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &inet->inet_daddr, ntohs(inet->inet_dport),\n\t\t\t tp->snd_cwnd, tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (sk->sk_family == AF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\t\tpr_debug(\"Undo %s %pI6/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &np->daddr, ntohs(inet->inet_dport),\n\t\t\t tp->snd_cwnd, tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#endif\n}\n#else\n#define DBGUNDO(x...) do { } while (0)\n#endif\n\nstatic void tcp_undo_cwnd_reduction(struct sock *sk, bool unmark_loss)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unmark_loss) {\n\t\tstruct sk_buff *skb;\n\n\t\ttcp_for_write_queue(skb, sk) {\n\t\t\tif (skb == tcp_send_head(sk))\n\t\t\t\tbreak;\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_LOST;\n\t\t}\n\t\ttp->lost_out = 0;\n\t\ttcp_clear_all_retrans_hints(tp);\n\t}\n\n\tif (tp->prior_ssthresh) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\tif (icsk->icsk_ca_ops->undo_cwnd)\n\t\t\ttp->snd_cwnd = icsk->icsk_ca_ops->undo_cwnd(sk);\n\t\telse\n\t\t\ttp->snd_cwnd = max(tp->snd_cwnd, tp->snd_ssthresh << 1);\n\n\t\tif (tp->prior_ssthresh > tp->snd_ssthresh) {\n\t\t\ttp->snd_ssthresh = tp->prior_ssthresh;\n\t\t\ttcp_ecn_withdraw_cwr(tp);\n\t\t}\n\t}\n\ttp->snd_cwnd_stamp = tcp_time_stamp;\n\ttp->undo_marker = 0;\n}\n\nstatic inline bool tcp_may_undo(const struct tcp_sock *tp)\n{\n\treturn tp->undo_marker && (!tp->undo_retrans || tcp_packet_delayed(tp));\n}\n\n/* People celebrate: \"We love our President!\" */\nstatic bool tcp_try_undo_recovery(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_may_undo(tp)) {\n\t\tint mib_idx;\n\n\t\t/* Happy end! We did not retransmit anything\n\t\t * or our original transmission succeeded.\n\t\t */\n\t\tDBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? \"loss\" : \"retrans\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)\n\t\t\tmib_idx = LINUX_MIB_TCPLOSSUNDO;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPFULLUNDO;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t}\n\tif (tp->snd_una == tp->high_seq && tcp_is_reno(tp)) {\n\t\t/* Hold old state until something *above* high_seq\n\t\t * is ACKed. For Reno it is MUST to prevent false\n\t\t * fast retransmits (RFC2582). SACK TCP is safe. */\n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\t\treturn true;\n\t}\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\treturn false;\n}\n\n/* Try to undo cwnd reduction, because D-SACKs acked all retransmitted data */\nstatic bool tcp_try_undo_dsack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && !tp->undo_retrans) {\n\t\tDBGUNDO(sk, \"D-SACK\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKUNDO);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Undo during loss recovery after partial ACK or using F-RTO. */\nstatic bool tcp_try_undo_loss(struct sock *sk, bool frto_undo)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (frto_undo || tcp_may_undo(tp)) {\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\n\t\tDBGUNDO(sk, \"partial loss\");\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSUNDO);\n\t\tif (frto_undo)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPSPURIOUSRTOS);\n\t\tinet_csk(sk)->icsk_retransmits = 0;\n\t\tif (frto_undo || tcp_is_sack(tp))\n\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* The cwnd reduction in CWR and Recovery uses the PRR algorithm in RFC 6937.\n * It computes the number of packets to send (sndcnt) based on packets newly\n * delivered:\n *   1) If the packets in flight is larger than ssthresh, PRR spreads the\n *\tcwnd reductions across a full RTT.\n *   2) Otherwise PRR uses packet conservation to send as much as delivered.\n *      But when the retransmits are acked without further losses, PRR\n *      slow starts cwnd up to ssthresh to speed up the recovery.\n */\nstatic void tcp_init_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->high_seq = tp->snd_nxt;\n\ttp->tlp_high_seq = 0;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->prior_cwnd = tp->snd_cwnd;\n\ttp->prr_delivered = 0;\n\ttp->prr_out = 0;\n\ttp->snd_ssthresh = inet_csk(sk)->icsk_ca_ops->ssthresh(sk);\n\ttcp_ecn_queue_cwr(tp);\n}\n\nstatic void tcp_cwnd_reduction(struct sock *sk, int newly_acked_sacked,\n\t\t\t       int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint sndcnt = 0;\n\tint delta = tp->snd_ssthresh - tcp_packets_in_flight(tp);\n\n\tif (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))\n\t\treturn;\n\n\ttp->prr_delivered += newly_acked_sacked;\n\tif (delta < 0) {\n\t\tu64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +\n\t\t\t       tp->prior_cwnd - 1;\n\t\tsndcnt = div_u64(dividend, tp->prior_cwnd) - tp->prr_out;\n\t} else if ((flag & FLAG_RETRANS_DATA_ACKED) &&\n\t\t   !(flag & FLAG_LOST_RETRANS)) {\n\t\tsndcnt = min_t(int, delta,\n\t\t\t       max_t(int, tp->prr_delivered - tp->prr_out,\n\t\t\t\t     newly_acked_sacked) + 1);\n\t} else {\n\t\tsndcnt = min(delta, newly_acked_sacked);\n\t}\n\t/* Force a fast retransmit upon entering fast recovery */\n\tsndcnt = max(sndcnt, (tp->prr_out ? 0 : 1));\n\ttp->snd_cwnd = tcp_packets_in_flight(tp) + sndcnt;\n}\n\nstatic inline void tcp_end_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Reset cwnd to ssthresh in CWR or Recovery (unless it's undone) */\n\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_CWR ||\n\t    (tp->undo_marker && tp->snd_ssthresh < TCP_INFINITE_SSTHRESH)) {\n\t\ttp->snd_cwnd = tp->snd_ssthresh;\n\t\ttp->snd_cwnd_stamp = tcp_time_stamp;\n\t}\n\ttcp_ca_event(sk, CA_EVENT_COMPLETE_CWR);\n}\n\n/* Enter CWR state. Disable cwnd undo since congestion is proven with ECN */\nvoid tcp_enter_cwr(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->prior_ssthresh = 0;\n\tif (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {\n\t\ttp->undo_marker = 0;\n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t}\n}\nEXPORT_SYMBOL(tcp_enter_cwr);\n\nstatic void tcp_try_keep_open(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint state = TCP_CA_Open;\n\n\tif (tcp_left_out(tp) || tcp_any_retrans_done(sk))\n\t\tstate = TCP_CA_Disorder;\n\n\tif (inet_csk(sk)->icsk_ca_state != state) {\n\t\ttcp_set_ca_state(sk, state);\n\t\ttp->high_seq = tp->snd_nxt;\n\t}\n}\n\nstatic void tcp_try_to_open(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_verify_left_out(tp);\n\n\tif (!tcp_any_retrans_done(sk))\n\t\ttp->retrans_stamp = 0;\n\n\tif (flag & FLAG_ECE)\n\t\ttcp_enter_cwr(sk);\n\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_CWR) {\n\t\ttcp_try_keep_open(sk);\n\t}\n}\n\nstatic void tcp_mtup_probe_failed(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_mtup.search_high = icsk->icsk_mtup.probe_size - 1;\n\ticsk->icsk_mtup.probe_size = 0;\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPFAIL);\n}\n\nstatic void tcp_mtup_probe_success(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t/* FIXME: breaks with very large cwnd */\n\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\ttp->snd_cwnd = tp->snd_cwnd *\n\t\t       tcp_mss_to_mtu(sk, tp->mss_cache) /\n\t\t       icsk->icsk_mtup.probe_size;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->snd_cwnd_stamp = tcp_time_stamp;\n\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\n\ticsk->icsk_mtup.search_low = icsk->icsk_mtup.probe_size;\n\ticsk->icsk_mtup.probe_size = 0;\n\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPSUCCESS);\n}\n\n/* Do a simple retransmit without using the backoff mechanisms in\n * tcp_timer. This is used for path mtu discovery.\n * The socket is already locked here.\n */\nvoid tcp_simple_retransmit(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int mss = tcp_current_mss(sk);\n\tu32 prior_lost = tp->lost_out;\n\n\ttcp_for_write_queue(skb, sk) {\n\t\tif (skb == tcp_send_head(sk))\n\t\t\tbreak;\n\t\tif (tcp_skb_seglen(skb) > mss &&\n\t\t    !(TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)) {\n\t\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_RETRANS;\n\t\t\t\ttp->retrans_out -= tcp_skb_pcount(skb);\n\t\t\t}\n\t\t\ttcp_skb_mark_lost_uncond_verify(tp, skb);\n\t\t}\n\t}\n\n\ttcp_clear_retrans_hints_partial(tp);\n\n\tif (prior_lost == tp->lost_out)\n\t\treturn;\n\n\tif (tcp_is_reno(tp))\n\t\ttcp_limit_reno_sacked(tp);\n\n\ttcp_verify_left_out(tp);\n\n\t/* Don't muck with the congestion window here.\n\t * Reason is that we do not increase amount of _data_\n\t * in network, but units changed and effective\n\t * cwnd/ssthresh really reduced now.\n\t */\n\tif (icsk->icsk_ca_state != TCP_CA_Loss) {\n\t\ttp->high_seq = tp->snd_nxt;\n\t\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->prior_ssthresh = 0;\n\t\ttp->undo_marker = 0;\n\t\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\nEXPORT_SYMBOL(tcp_simple_retransmit);\n\nstatic void tcp_enter_recovery(struct sock *sk, bool ece_ack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mib_idx;\n\n\tif (tcp_is_reno(tp))\n\t\tmib_idx = LINUX_MIB_TCPRENORECOVERY;\n\telse\n\t\tmib_idx = LINUX_MIB_TCPSACKRECOVERY;\n\n\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\ttp->prior_ssthresh = 0;\n\ttcp_init_undo(tp);\n\n\tif (!tcp_in_cwnd_reduction(sk)) {\n\t\tif (!ece_ack)\n\t\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttcp_init_cwnd_reduction(sk);\n\t}\n\ttcp_set_ca_state(sk, TCP_CA_Recovery);\n}\n\n/* Process an ACK in CA_Loss state. Move to CA_Open if lost data are\n * recovered or spurious. Otherwise retransmits more on partial ACKs.\n */\nstatic void tcp_process_loss(struct sock *sk, int flag, bool is_dupack,\n\t\t\t     int *rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool recovered = !before(tp->snd_una, tp->high_seq);\n\n\tif ((flag & FLAG_SND_UNA_ADVANCED) &&\n\t    tcp_try_undo_loss(sk, false))\n\t\treturn;\n\n\tif (tp->frto) { /* F-RTO RFC5682 sec 3.1 (sack enhanced version). */\n\t\t/* Step 3.b. A timeout is spurious if not all data are\n\t\t * lost, i.e., never-retransmitted data are (s)acked.\n\t\t */\n\t\tif ((flag & FLAG_ORIG_SACK_ACKED) &&\n\t\t    tcp_try_undo_loss(sk, true))\n\t\t\treturn;\n\n\t\tif (after(tp->snd_nxt, tp->high_seq)) {\n\t\t\tif (flag & FLAG_DATA_SACKED || is_dupack)\n\t\t\t\ttp->frto = 0; /* Step 3.a. loss was real */\n\t\t} else if (flag & FLAG_SND_UNA_ADVANCED && !recovered) {\n\t\t\ttp->high_seq = tp->snd_nxt;\n\t\t\t/* Step 2.b. Try send new data (but deferred until cwnd\n\t\t\t * is updated in tcp_ack()). Otherwise fall back to\n\t\t\t * the conventional recovery.\n\t\t\t */\n\t\t\tif (tcp_send_head(sk) &&\n\t\t\t    after(tcp_wnd_end(tp), tp->snd_nxt)) {\n\t\t\t\t*rexmit = REXMIT_NEW;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\ttp->frto = 0;\n\t\t}\n\t}\n\n\tif (recovered) {\n\t\t/* F-RTO RFC5682 sec 3.1 step 2.a and 1st part of step 3.a */\n\t\ttcp_try_undo_recovery(sk);\n\t\treturn;\n\t}\n\tif (tcp_is_reno(tp)) {\n\t\t/* A Reno DUPACK means new data in F-RTO step 2.b above are\n\t\t * delivered. Lower inflight to clock out (re)tranmissions.\n\t\t */\n\t\tif (after(tp->snd_nxt, tp->high_seq) && is_dupack)\n\t\t\ttcp_add_reno_sack(sk);\n\t\telse if (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\ttcp_reset_reno_sack(tp);\n\t}\n\t*rexmit = REXMIT_LOST;\n}\n\n/* Undo during fast recovery after partial ACK. */\nstatic bool tcp_try_undo_partial(struct sock *sk, const int acked)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && tcp_packet_delayed(tp)) {\n\t\t/* Plain luck! Hole if filled with delayed\n\t\t * packet, rather than with a retransmit.\n\t\t */\n\t\ttcp_update_reordering(sk, tcp_fackets_out(tp) + acked, 1);\n\n\t\t/* We are getting evidence that the reordering degree is higher\n\t\t * than we realized. If there are no retransmits out then we\n\t\t * can undo. Otherwise we clock out new packets but do not\n\t\t * mark more packets lost or retransmit more.\n\t\t */\n\t\tif (tp->retrans_out)\n\t\t\treturn true;\n\n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\n\t\tDBGUNDO(sk, \"partial recovery\");\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPARTIALUNDO);\n\t\ttcp_try_keep_open(sk);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Process an event, which can update packets-in-flight not trivially.\n * Main goal of this function is to calculate new estimate for left_out,\n * taking into account both packets sitting in receiver's buffer and\n * packets lost by network.\n *\n * Besides that it updates the congestion state when packet loss or ECN\n * is detected. But it does not reduce the cwnd, it is done by the\n * congestion control later.\n *\n * It does _not_ decide what to send, it is made in function\n * tcp_xmit_retransmit_queue().\n */\nstatic void tcp_fastretrans_alert(struct sock *sk, const int acked,\n\t\t\t\t  bool is_dupack, int *ack_flag, int *rexmit)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint fast_rexmit = 0, flag = *ack_flag;\n\tbool do_lost = is_dupack || ((flag & FLAG_DATA_SACKED) &&\n\t\t\t\t    (tcp_fackets_out(tp) > tp->reordering));\n\n\tif (WARN_ON(!tp->packets_out && tp->sacked_out))\n\t\ttp->sacked_out = 0;\n\tif (WARN_ON(!tp->sacked_out && tp->fackets_out))\n\t\ttp->fackets_out = 0;\n\n\t/* Now state machine starts.\n\t * A. ECE, hence prohibit cwnd undoing, the reduction is required. */\n\tif (flag & FLAG_ECE)\n\t\ttp->prior_ssthresh = 0;\n\n\t/* B. In all the states check for reneging SACKs. */\n\tif (tcp_check_sack_reneging(sk, flag))\n\t\treturn;\n\n\t/* C. Check consistency of the current state. */\n\ttcp_verify_left_out(tp);\n\n\t/* D. Check state exit conditions. State can be terminated\n\t *    when high_seq is ACKed. */\n\tif (icsk->icsk_ca_state == TCP_CA_Open) {\n\t\tWARN_ON(tp->retrans_out != 0);\n\t\ttp->retrans_stamp = 0;\n\t} else if (!before(tp->snd_una, tp->high_seq)) {\n\t\tswitch (icsk->icsk_ca_state) {\n\t\tcase TCP_CA_CWR:\n\t\t\t/* CWR is to be held something *above* high_seq\n\t\t\t * is ACKed for CWR bit to reach receiver. */\n\t\t\tif (tp->snd_una != tp->high_seq) {\n\t\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_CA_Recovery:\n\t\t\tif (tcp_is_reno(tp))\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\tif (tcp_try_undo_recovery(sk))\n\t\t\t\treturn;\n\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Use RACK to detect loss */\n\tif (sysctl_tcp_recovery & TCP_RACK_LOST_RETRANS &&\n\t    tcp_rack_mark_lost(sk)) {\n\t\tflag |= FLAG_LOST_RETRANS;\n\t\t*ack_flag |= FLAG_LOST_RETRANS;\n\t}\n\n\t/* E. Process state. */\n\tswitch (icsk->icsk_ca_state) {\n\tcase TCP_CA_Recovery:\n\t\tif (!(flag & FLAG_SND_UNA_ADVANCED)) {\n\t\t\tif (tcp_is_reno(tp) && is_dupack)\n\t\t\t\ttcp_add_reno_sack(sk);\n\t\t} else {\n\t\t\tif (tcp_try_undo_partial(sk, acked))\n\t\t\t\treturn;\n\t\t\t/* Partial ACK arrived. Force fast retransmit. */\n\t\t\tdo_lost = tcp_is_reno(tp) ||\n\t\t\t\t  tcp_fackets_out(tp) > tp->reordering;\n\t\t}\n\t\tif (tcp_try_undo_dsack(sk)) {\n\t\t\ttcp_try_keep_open(sk);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase TCP_CA_Loss:\n\t\ttcp_process_loss(sk, flag, is_dupack, rexmit);\n\t\tif (icsk->icsk_ca_state != TCP_CA_Open &&\n\t\t    !(flag & FLAG_LOST_RETRANS))\n\t\t\treturn;\n\t\t/* Change state if cwnd is undone or retransmits are lost */\n\tdefault:\n\t\tif (tcp_is_reno(tp)) {\n\t\t\tif (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\tif (is_dupack)\n\t\t\t\ttcp_add_reno_sack(sk);\n\t\t}\n\n\t\tif (icsk->icsk_ca_state <= TCP_CA_Disorder)\n\t\t\ttcp_try_undo_dsack(sk);\n\n\t\tif (!tcp_time_to_recover(sk, flag)) {\n\t\t\ttcp_try_to_open(sk, flag);\n\t\t\treturn;\n\t\t}\n\n\t\t/* MTU probe failure: don't reduce cwnd */\n\t\tif (icsk->icsk_ca_state < TCP_CA_CWR &&\n\t\t    icsk->icsk_mtup.probe_size &&\n\t\t    tp->snd_una == tp->mtu_probe.probe_seq_start) {\n\t\t\ttcp_mtup_probe_failed(sk);\n\t\t\t/* Restores the reduction we did in tcp_mtup_probe() */\n\t\t\ttp->snd_cwnd++;\n\t\t\ttcp_simple_retransmit(sk);\n\t\t\treturn;\n\t\t}\n\n\t\t/* Otherwise enter Recovery state */\n\t\ttcp_enter_recovery(sk, (flag & FLAG_ECE));\n\t\tfast_rexmit = 1;\n\t}\n\n\tif (do_lost)\n\t\ttcp_update_scoreboard(sk, fast_rexmit);\n\t*rexmit = REXMIT_LOST;\n}\n\n/* Kathleen Nichols' algorithm for tracking the minimum value of\n * a data stream over some fixed time interval. (E.g., the minimum\n * RTT over the past five minutes.) It uses constant space and constant\n * time per update yet almost always delivers the same minimum as an\n * implementation that has to keep all the data in the window.\n *\n * The algorithm keeps track of the best, 2nd best & 3rd best min\n * values, maintaining an invariant that the measurement time of the\n * n'th best >= n-1'th best. It also makes sure that the three values\n * are widely separated in the time window since that bounds the worse\n * case error when that data is monotonically increasing over the window.\n *\n * Upon getting a new min, we can forget everything earlier because it\n * has no value - the new min is <= everything else in the window by\n * definition and it's the most recent. So we restart fresh on every new min\n * and overwrites 2nd & 3rd choices. The same property holds for 2nd & 3rd\n * best.\n */\nstatic void tcp_update_rtt_min(struct sock *sk, u32 rtt_us)\n{\n\tconst u32 now = tcp_time_stamp, wlen = sysctl_tcp_min_rtt_wlen * HZ;\n\tstruct rtt_meas *m = tcp_sk(sk)->rtt_min;\n\tstruct rtt_meas rttm = {\n\t\t.rtt = likely(rtt_us) ? rtt_us : jiffies_to_usecs(1),\n\t\t.ts = now,\n\t};\n\tu32 elapsed;\n\n\t/* Check if the new measurement updates the 1st, 2nd, or 3rd choices */\n\tif (unlikely(rttm.rtt <= m[0].rtt))\n\t\tm[0] = m[1] = m[2] = rttm;\n\telse if (rttm.rtt <= m[1].rtt)\n\t\tm[1] = m[2] = rttm;\n\telse if (rttm.rtt <= m[2].rtt)\n\t\tm[2] = rttm;\n\n\telapsed = now - m[0].ts;\n\tif (unlikely(elapsed > wlen)) {\n\t\t/* Passed entire window without a new min so make 2nd choice\n\t\t * the new min & 3rd choice the new 2nd. So forth and so on.\n\t\t */\n\t\tm[0] = m[1];\n\t\tm[1] = m[2];\n\t\tm[2] = rttm;\n\t\tif (now - m[0].ts > wlen) {\n\t\t\tm[0] = m[1];\n\t\t\tm[1] = rttm;\n\t\t\tif (now - m[0].ts > wlen)\n\t\t\t\tm[0] = rttm;\n\t\t}\n\t} else if (m[1].ts == m[0].ts && elapsed > wlen / 4) {\n\t\t/* Passed a quarter of the window without a new min so\n\t\t * take 2nd choice from the 2nd quarter of the window.\n\t\t */\n\t\tm[2] = m[1] = rttm;\n\t} else if (m[2].ts == m[1].ts && elapsed > wlen / 2) {\n\t\t/* Passed half the window without a new min so take the 3rd\n\t\t * choice from the last half of the window.\n\t\t */\n\t\tm[2] = rttm;\n\t}\n}\n\nstatic inline bool tcp_ack_update_rtt(struct sock *sk, const int flag,\n\t\t\t\t      long seq_rtt_us, long sack_rtt_us,\n\t\t\t\t      long ca_rtt_us)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Prefer RTT measured from ACK's timing to TS-ECR. This is because\n\t * broken middle-boxes or peers may corrupt TS-ECR fields. But\n\t * Karn's algorithm forbids taking RTT if some retransmitted data\n\t * is acked (RFC6298).\n\t */\n\tif (seq_rtt_us < 0)\n\t\tseq_rtt_us = sack_rtt_us;\n\n\t/* RTTM Rule: A TSecr value received in a segment is used to\n\t * update the averaged RTT measurement only if the segment\n\t * acknowledges some new data, i.e., only if it advances the\n\t * left edge of the send window.\n\t * See draft-ietf-tcplw-high-performance-00, section 3.3.\n\t */\n\tif (seq_rtt_us < 0 && tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t    flag & FLAG_ACKED)\n\t\tseq_rtt_us = ca_rtt_us = jiffies_to_usecs(tcp_time_stamp -\n\t\t\t\t\t\t\t  tp->rx_opt.rcv_tsecr);\n\tif (seq_rtt_us < 0)\n\t\treturn false;\n\n\t/* ca_rtt_us >= 0 is counting on the invariant that ca_rtt_us is\n\t * always taken together with ACK, SACK, or TS-opts. Any negative\n\t * values will be skipped with the seq_rtt_us < 0 check above.\n\t */\n\ttcp_update_rtt_min(sk, ca_rtt_us);\n\ttcp_rtt_estimator(sk, seq_rtt_us);\n\ttcp_set_rto(sk);\n\n\t/* RFC6298: only reset backoff on valid RTT measurement. */\n\tinet_csk(sk)->icsk_backoff = 0;\n\treturn true;\n}\n\n/* Compute time elapsed between (last) SYNACK and the ACK completing 3WHS. */\nvoid tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req)\n{\n\tlong rtt_us = -1L;\n\n\tif (req && !req->num_retrans && tcp_rsk(req)->snt_synack.v64) {\n\t\tstruct skb_mstamp now;\n\n\t\tskb_mstamp_get(&now);\n\t\trtt_us = skb_mstamp_us_delta(&now, &tcp_rsk(req)->snt_synack);\n\t}\n\n\ttcp_ack_update_rtt(sk, FLAG_SYN_ACKED, rtt_us, -1L, rtt_us);\n}\n\n\nstatic void tcp_cong_avoid(struct sock *sk, u32 ack, u32 acked)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_ca_ops->cong_avoid(sk, ack, acked);\n\ttcp_sk(sk)->snd_cwnd_stamp = tcp_time_stamp;\n}\n\n/* Restart timer after forward progress on connection.\n * RFC2988 recommends to restart timer to now+rto.\n */\nvoid tcp_rearm_rto(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If the retrans timer is currently being used by Fast Open\n\t * for SYN-ACK retrans purpose, stay put.\n\t */\n\tif (tp->fastopen_rsk)\n\t\treturn;\n\n\tif (!tp->packets_out) {\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);\n\t} else {\n\t\tu32 rto = inet_csk(sk)->icsk_rto;\n\t\t/* Offset the time elapsed after installing regular RTO */\n\t\tif (icsk->icsk_pending == ICSK_TIME_EARLY_RETRANS ||\n\t\t    icsk->icsk_pending == ICSK_TIME_LOSS_PROBE) {\n\t\t\tstruct sk_buff *skb = tcp_write_queue_head(sk);\n\t\t\tconst u32 rto_time_stamp =\n\t\t\t\ttcp_skb_timestamp(skb) + rto;\n\t\t\ts32 delta = (s32)(rto_time_stamp - tcp_time_stamp);\n\t\t\t/* delta may not be positive if the socket is locked\n\t\t\t * when the retrans timer fires and is rescheduled.\n\t\t\t */\n\t\t\tif (delta > 0)\n\t\t\t\trto = delta;\n\t\t}\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS, rto,\n\t\t\t\t\t  TCP_RTO_MAX);\n\t}\n}\n\n/* This function is called when the delayed ER timer fires. TCP enters\n * fast recovery and performs fast-retransmit.\n */\nvoid tcp_resume_early_retransmit(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_rearm_rto(sk);\n\n\t/* Stop if ER is disabled after the delayed ER timer is scheduled */\n\tif (!tp->do_early_retrans)\n\t\treturn;\n\n\ttcp_enter_recovery(sk, false);\n\ttcp_update_scoreboard(sk, 1);\n\ttcp_xmit_retransmit_queue(sk);\n}\n\n/* If we get here, the whole TSO packet has not been acked. */\nstatic u32 tcp_tso_acked(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 packets_acked;\n\n\tBUG_ON(!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una));\n\n\tpackets_acked = tcp_skb_pcount(skb);\n\tif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\n\t\treturn 0;\n\tpackets_acked -= tcp_skb_pcount(skb);\n\n\tif (packets_acked) {\n\t\tBUG_ON(tcp_skb_pcount(skb) == 0);\n\t\tBUG_ON(!before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq));\n\t}\n\n\treturn packets_acked;\n}\n\nstatic void tcp_ack_tstamp(struct sock *sk, struct sk_buff *skb,\n\t\t\t   u32 prior_snd_una)\n{\n\tconst struct skb_shared_info *shinfo;\n\n\t/* Avoid cache line misses to get skb_shinfo() and shinfo->tx_flags */\n\tif (likely(!TCP_SKB_CB(skb)->txstamp_ack))\n\t\treturn;\n\n\tshinfo = skb_shinfo(skb);\n\tif (!before(shinfo->tskey, prior_snd_una) &&\n\t    before(shinfo->tskey, tcp_sk(sk)->snd_una))\n\t\t__skb_tstamp_tx(skb, NULL, sk, SCM_TSTAMP_ACK);\n}\n\n/* Remove acknowledged frames from the retransmission queue. If our packet\n * is before the ack sequence we can discard it as it's confirmed to have\n * arrived at the other end.\n */\nstatic int tcp_clean_rtx_queue(struct sock *sk, int prior_fackets,\n\t\t\t       u32 prior_snd_una, int *acked,\n\t\t\t       struct tcp_sacktag_state *sack)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct skb_mstamp first_ackt, last_ackt, now;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 prior_sacked = tp->sacked_out;\n\tu32 reord = tp->packets_out;\n\tbool fully_acked = true;\n\tlong sack_rtt_us = -1L;\n\tlong seq_rtt_us = -1L;\n\tlong ca_rtt_us = -1L;\n\tstruct sk_buff *skb;\n\tu32 pkts_acked = 0;\n\tbool rtt_update;\n\tint flag = 0;\n\n\tfirst_ackt.v64 = 0;\n\n\twhile ((skb = tcp_write_queue_head(sk)) && skb != tcp_send_head(sk)) {\n\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\n\t\tu8 sacked = scb->sacked;\n\t\tu32 acked_pcount;\n\n\t\ttcp_ack_tstamp(sk, skb, prior_snd_una);\n\n\t\t/* Determine how many packets and what bytes were acked, tso and else */\n\t\tif (after(scb->end_seq, tp->snd_una)) {\n\t\t\tif (tcp_skb_pcount(skb) == 1 ||\n\t\t\t    !after(tp->snd_una, scb->seq))\n\t\t\t\tbreak;\n\n\t\t\tacked_pcount = tcp_tso_acked(sk, skb);\n\t\t\tif (!acked_pcount)\n\t\t\t\tbreak;\n\n\t\t\tfully_acked = false;\n\t\t} else {\n\t\t\t/* Speedup tcp_unlink_write_queue() and next loop */\n\t\t\tprefetchw(skb->next);\n\t\t\tacked_pcount = tcp_skb_pcount(skb);\n\t\t}\n\n\t\tif (unlikely(sacked & TCPCB_RETRANS)) {\n\t\t\tif (sacked & TCPCB_SACKED_RETRANS)\n\t\t\t\ttp->retrans_out -= acked_pcount;\n\t\t\tflag |= FLAG_RETRANS_DATA_ACKED;\n\t\t} else if (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\t\tlast_ackt = skb->skb_mstamp;\n\t\t\tWARN_ON_ONCE(last_ackt.v64 == 0);\n\t\t\tif (!first_ackt.v64)\n\t\t\t\tfirst_ackt = last_ackt;\n\n\t\t\treord = min(pkts_acked, reord);\n\t\t\tif (!after(scb->end_seq, tp->high_seq))\n\t\t\t\tflag |= FLAG_ORIG_SACK_ACKED;\n\t\t}\n\n\t\tif (sacked & TCPCB_SACKED_ACKED) {\n\t\t\ttp->sacked_out -= acked_pcount;\n\t\t} else if (tcp_is_sack(tp)) {\n\t\t\ttp->delivered += acked_pcount;\n\t\t\tif (!tcp_skb_spurious_retrans(tp, skb))\n\t\t\t\ttcp_rack_advance(tp, &skb->skb_mstamp, sacked);\n\t\t}\n\t\tif (sacked & TCPCB_LOST)\n\t\t\ttp->lost_out -= acked_pcount;\n\n\t\ttp->packets_out -= acked_pcount;\n\t\tpkts_acked += acked_pcount;\n\n\t\t/* Initial outgoing SYN's get put onto the write_queue\n\t\t * just like anything else we transmit.  It is not\n\t\t * true data, and if we misinform our callers that\n\t\t * this ACK acks real data, we will erroneously exit\n\t\t * connection startup slow start one packet too\n\t\t * quickly.  This is severely frowned upon behavior.\n\t\t */\n\t\tif (likely(!(scb->tcp_flags & TCPHDR_SYN))) {\n\t\t\tflag |= FLAG_DATA_ACKED;\n\t\t} else {\n\t\t\tflag |= FLAG_SYN_ACKED;\n\t\t\ttp->retrans_stamp = 0;\n\t\t}\n\n\t\tif (!fully_acked)\n\t\t\tbreak;\n\n\t\ttcp_unlink_write_queue(skb, sk);\n\t\tsk_wmem_free_skb(sk, skb);\n\t\tif (unlikely(skb == tp->retransmit_skb_hint))\n\t\t\ttp->retransmit_skb_hint = NULL;\n\t\tif (unlikely(skb == tp->lost_skb_hint))\n\t\t\ttp->lost_skb_hint = NULL;\n\t}\n\n\tif (likely(between(tp->snd_up, prior_snd_una, tp->snd_una)))\n\t\ttp->snd_up = tp->snd_una;\n\n\tif (skb && (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED))\n\t\tflag |= FLAG_SACK_RENEGING;\n\n\tskb_mstamp_get(&now);\n\tif (likely(first_ackt.v64) && !(flag & FLAG_RETRANS_DATA_ACKED)) {\n\t\tseq_rtt_us = skb_mstamp_us_delta(&now, &first_ackt);\n\t\tca_rtt_us = skb_mstamp_us_delta(&now, &last_ackt);\n\t}\n\tif (sack->first_sackt.v64) {\n\t\tsack_rtt_us = skb_mstamp_us_delta(&now, &sack->first_sackt);\n\t\tca_rtt_us = skb_mstamp_us_delta(&now, &sack->last_sackt);\n\t}\n\n\trtt_update = tcp_ack_update_rtt(sk, flag, seq_rtt_us, sack_rtt_us,\n\t\t\t\t\tca_rtt_us);\n\n\tif (flag & FLAG_ACKED) {\n\t\ttcp_rearm_rto(sk);\n\t\tif (unlikely(icsk->icsk_mtup.probe_size &&\n\t\t\t     !after(tp->mtu_probe.probe_seq_end, tp->snd_una))) {\n\t\t\ttcp_mtup_probe_success(sk);\n\t\t}\n\n\t\tif (tcp_is_reno(tp)) {\n\t\t\ttcp_remove_reno_sacks(sk, pkts_acked);\n\t\t} else {\n\t\t\tint delta;\n\n\t\t\t/* Non-retransmitted hole got filled? That's reordering */\n\t\t\tif (reord < prior_fackets)\n\t\t\t\ttcp_update_reordering(sk, tp->fackets_out - reord, 0);\n\n\t\t\tdelta = tcp_is_fack(tp) ? pkts_acked :\n\t\t\t\t\t\t  prior_sacked - tp->sacked_out;\n\t\t\ttp->lost_cnt_hint -= min(tp->lost_cnt_hint, delta);\n\t\t}\n\n\t\ttp->fackets_out -= min(pkts_acked, tp->fackets_out);\n\n\t} else if (skb && rtt_update && sack_rtt_us >= 0 &&\n\t\t   sack_rtt_us > skb_mstamp_us_delta(&now, &skb->skb_mstamp)) {\n\t\t/* Do not re-arm RTO if the sack RTT is measured from data sent\n\t\t * after when the head was last (re)transmitted. Otherwise the\n\t\t * timeout may continue to extend in loss recovery.\n\t\t */\n\t\ttcp_rearm_rto(sk);\n\t}\n\n\tif (icsk->icsk_ca_ops->pkts_acked) {\n\t\tstruct ack_sample sample = { .pkts_acked = pkts_acked,\n\t\t\t\t\t     .rtt_us = ca_rtt_us };\n\n\t\ticsk->icsk_ca_ops->pkts_acked(sk, &sample);\n\t}\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tif (!tp->packets_out && tcp_is_sack(tp)) {\n\t\ticsk = inet_csk(sk);\n\t\tif (tp->lost_out) {\n\t\t\tpr_debug(\"Leak l=%u %d\\n\",\n\t\t\t\t tp->lost_out, icsk->icsk_ca_state);\n\t\t\ttp->lost_out = 0;\n\t\t}\n\t\tif (tp->sacked_out) {\n\t\t\tpr_debug(\"Leak s=%u %d\\n\",\n\t\t\t\t tp->sacked_out, icsk->icsk_ca_state);\n\t\t\ttp->sacked_out = 0;\n\t\t}\n\t\tif (tp->retrans_out) {\n\t\t\tpr_debug(\"Leak r=%u %d\\n\",\n\t\t\t\t tp->retrans_out, icsk->icsk_ca_state);\n\t\t\ttp->retrans_out = 0;\n\t\t}\n\t}\n#endif\n\t*acked = pkts_acked;\n\treturn flag;\n}\n\nstatic void tcp_ack_probe(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t/* Was it a usable window open? */\n\n\tif (!after(TCP_SKB_CB(tcp_send_head(sk))->end_seq, tcp_wnd_end(tp))) {\n\t\ticsk->icsk_backoff = 0;\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_PROBE0);\n\t\t/* Socket must be waked up by subsequent tcp_data_snd_check().\n\t\t * This function is not for random using!\n\t\t */\n\t} else {\n\t\tunsigned long when = tcp_probe0_when(sk, TCP_RTO_MAX);\n\n\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\n\t\t\t\t\t  when, TCP_RTO_MAX);\n\t}\n}\n\nstatic inline bool tcp_ack_is_dubious(const struct sock *sk, const int flag)\n{\n\treturn !(flag & FLAG_NOT_DUP) || (flag & FLAG_CA_ALERT) ||\n\t\tinet_csk(sk)->icsk_ca_state != TCP_CA_Open;\n}\n\n/* Decide wheather to run the increase function of congestion control. */\nstatic inline bool tcp_may_raise_cwnd(const struct sock *sk, const int flag)\n{\n\t/* If reordering is high then always grow cwnd whenever data is\n\t * delivered regardless of its ordering. Otherwise stay conservative\n\t * and only grow cwnd on in-order delivery (RFC5681). A stretched ACK w/\n\t * new SACK or ECE mark may first advance cwnd here and later reduce\n\t * cwnd in tcp_fastretrans_alert() based on more states.\n\t */\n\tif (tcp_sk(sk)->reordering > sock_net(sk)->ipv4.sysctl_tcp_reordering)\n\t\treturn flag & FLAG_FORWARD_PROGRESS;\n\n\treturn flag & FLAG_DATA_ACKED;\n}\n\n/* The \"ultimate\" congestion control function that aims to replace the rigid\n * cwnd increase and decrease control (tcp_cong_avoid,tcp_*cwnd_reduction).\n * It's called toward the end of processing an ACK with precise rate\n * information. All transmission or retransmission are delayed afterwards.\n */\nstatic void tcp_cong_control(struct sock *sk, u32 ack, u32 acked_sacked,\n\t\t\t     int flag)\n{\n\tif (tcp_in_cwnd_reduction(sk)) {\n\t\t/* Reduce cwnd if state mandates */\n\t\ttcp_cwnd_reduction(sk, acked_sacked, flag);\n\t} else if (tcp_may_raise_cwnd(sk, flag)) {\n\t\t/* Advance cwnd if state allows */\n\t\ttcp_cong_avoid(sk, ack, acked_sacked);\n\t}\n\ttcp_update_pacing_rate(sk);\n}\n\n/* Check that window update is acceptable.\n * The function assumes that snd_una<=ack<=snd_next.\n */\nstatic inline bool tcp_may_update_window(const struct tcp_sock *tp,\n\t\t\t\t\tconst u32 ack, const u32 ack_seq,\n\t\t\t\t\tconst u32 nwin)\n{\n\treturn\tafter(ack, tp->snd_una) ||\n\t\tafter(ack_seq, tp->snd_wl1) ||\n\t\t(ack_seq == tp->snd_wl1 && nwin > tp->snd_wnd);\n}\n\n/* If we update tp->snd_una, also update tp->bytes_acked */\nstatic void tcp_snd_una_update(struct tcp_sock *tp, u32 ack)\n{\n\tu32 delta = ack - tp->snd_una;\n\n\tsock_owned_by_me((struct sock *)tp);\n\tu64_stats_update_begin_raw(&tp->syncp);\n\ttp->bytes_acked += delta;\n\tu64_stats_update_end_raw(&tp->syncp);\n\ttp->snd_una = ack;\n}\n\n/* If we update tp->rcv_nxt, also update tp->bytes_received */\nstatic void tcp_rcv_nxt_update(struct tcp_sock *tp, u32 seq)\n{\n\tu32 delta = seq - tp->rcv_nxt;\n\n\tsock_owned_by_me((struct sock *)tp);\n\tu64_stats_update_begin_raw(&tp->syncp);\n\ttp->bytes_received += delta;\n\tu64_stats_update_end_raw(&tp->syncp);\n\ttp->rcv_nxt = seq;\n}\n\n/* Update our send window.\n *\n * Window update algorithm, described in RFC793/RFC1122 (used in linux-2.2\n * and in FreeBSD. NetBSD's one is even worse.) is wrong.\n */\nstatic int tcp_ack_update_window(struct sock *sk, const struct sk_buff *skb, u32 ack,\n\t\t\t\t u32 ack_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint flag = 0;\n\tu32 nwin = ntohs(tcp_hdr(skb)->window);\n\n\tif (likely(!tcp_hdr(skb)->syn))\n\t\tnwin <<= tp->rx_opt.snd_wscale;\n\n\tif (tcp_may_update_window(tp, ack, ack_seq, nwin)) {\n\t\tflag |= FLAG_WIN_UPDATE;\n\t\ttcp_update_wl(tp, ack_seq);\n\n\t\tif (tp->snd_wnd != nwin) {\n\t\t\ttp->snd_wnd = nwin;\n\n\t\t\t/* Note, it is the only place, where\n\t\t\t * fast path is recovered for sending TCP.\n\t\t\t */\n\t\t\ttp->pred_flags = 0;\n\t\t\ttcp_fast_path_check(sk);\n\n\t\t\tif (tcp_send_head(sk))\n\t\t\t\ttcp_slow_start_after_idle_check(sk);\n\n\t\t\tif (nwin > tp->max_window) {\n\t\t\t\ttp->max_window = nwin;\n\t\t\t\ttcp_sync_mss(sk, inet_csk(sk)->icsk_pmtu_cookie);\n\t\t\t}\n\t\t}\n\t}\n\n\ttcp_snd_una_update(tp, ack);\n\n\treturn flag;\n}\n\n/* Return true if we're currently rate-limiting out-of-window ACKs and\n * thus shouldn't send a dupack right now. We rate-limit dupacks in\n * response to out-of-window SYNs or ACKs to mitigate ACK loops or DoS\n * attacks that send repeated SYNs or ACKs for the same connection. To\n * do this, we do not send a duplicate SYNACK or ACK if the remote\n * endpoint is sending out-of-window SYNs or pure ACKs at a high rate.\n */\nbool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,\n\t\t\t  int mib_idx, u32 *last_oow_ack_time)\n{\n\t/* Data packets without SYNs are not likely part of an ACK loop. */\n\tif ((TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq) &&\n\t    !tcp_hdr(skb)->syn)\n\t\tgoto not_rate_limited;\n\n\tif (*last_oow_ack_time) {\n\t\ts32 elapsed = (s32)(tcp_time_stamp - *last_oow_ack_time);\n\n\t\tif (0 <= elapsed && elapsed < sysctl_tcp_invalid_ratelimit) {\n\t\t\tNET_INC_STATS(net, mib_idx);\n\t\t\treturn true;\t/* rate-limited: don't send yet! */\n\t\t}\n\t}\n\n\t*last_oow_ack_time = tcp_time_stamp;\n\nnot_rate_limited:\n\treturn false;\t/* not rate-limited: go ahead, send dupack now! */\n}\n\n/* RFC 5961 7 [ACK Throttling] */\nstatic void tcp_send_challenge_ack(struct sock *sk, const struct sk_buff *skb)\n{\n\t/* unprotected vars, we dont care of overwrites */\n\tstatic u32 challenge_timestamp;\n\tstatic unsigned int challenge_count;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 count, now;\n\n\t/* First check our per-socket dupack rate limit. */\n\tif (tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t LINUX_MIB_TCPACKSKIPPEDCHALLENGE,\n\t\t\t\t &tp->last_oow_ack_time))\n\t\treturn;\n\n\t/* Then check host-wide RFC 5961 rate limit. */\n\tnow = jiffies / HZ;\n\tif (now != challenge_timestamp) {\n\t\tu32 half = (sysctl_tcp_challenge_ack_limit + 1) >> 1;\n\n\t\tchallenge_timestamp = now;\n\t\tWRITE_ONCE(challenge_count, half +\n\t\t\t   prandom_u32_max(sysctl_tcp_challenge_ack_limit));\n\t}\n\tcount = READ_ONCE(challenge_count);\n\tif (count > 0) {\n\t\tWRITE_ONCE(challenge_count, count - 1);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPCHALLENGEACK);\n\t\ttcp_send_ack(sk);\n\t}\n}\n\nstatic void tcp_store_ts_recent(struct tcp_sock *tp)\n{\n\ttp->rx_opt.ts_recent = tp->rx_opt.rcv_tsval;\n\ttp->rx_opt.ts_recent_stamp = get_seconds();\n}\n\nstatic void tcp_replace_ts_recent(struct tcp_sock *tp, u32 seq)\n{\n\tif (tp->rx_opt.saw_tstamp && !after(seq, tp->rcv_wup)) {\n\t\t/* PAWS bug workaround wrt. ACK frames, the PAWS discard\n\t\t * extra check below makes sure this can only happen\n\t\t * for pure ACK frames.  -DaveM\n\t\t *\n\t\t * Not only, also it occurs for expired timestamps.\n\t\t */\n\n\t\tif (tcp_paws_check(&tp->rx_opt, 0))\n\t\t\ttcp_store_ts_recent(tp);\n\t}\n}\n\n/* This routine deals with acks during a TLP episode.\n * We mark the end of a TLP episode on receiving TLP dupack or when\n * ack is after tlp_high_seq.\n * Ref: loss detection algorithm in draft-dukkipati-tcpm-tcp-loss-probe.\n */\nstatic void tcp_process_tlp_ack(struct sock *sk, u32 ack, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (before(ack, tp->tlp_high_seq))\n\t\treturn;\n\n\tif (flag & FLAG_DSACKING_ACK) {\n\t\t/* This DSACK means original and TLP probe arrived; no loss */\n\t\ttp->tlp_high_seq = 0;\n\t} else if (after(ack, tp->tlp_high_seq)) {\n\t\t/* ACK advances: there was a loss, so reduce cwnd. Reset\n\t\t * tlp_high_seq in tcp_init_cwnd_reduction()\n\t\t */\n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t\ttcp_end_cwnd_reduction(sk);\n\t\ttcp_try_keep_open(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPLOSSPROBERECOVERY);\n\t} else if (!(flag & (FLAG_SND_UNA_ADVANCED |\n\t\t\t     FLAG_NOT_DUP | FLAG_DATA_SACKED))) {\n\t\t/* Pure dupack: original and TLP probe arrived; no loss */\n\t\ttp->tlp_high_seq = 0;\n\t}\n}\n\nstatic inline void tcp_in_ack_event(struct sock *sk, u32 flags)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->in_ack_event)\n\t\ticsk->icsk_ca_ops->in_ack_event(sk, flags);\n}\n\n/* Congestion control has updated the cwnd already. So if we're in\n * loss recovery then now we do any new sends (for FRTO) or\n * retransmits (for CA_Loss or CA_recovery) that make sense.\n */\nstatic void tcp_xmit_recovery(struct sock *sk, int rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (rexmit == REXMIT_NONE)\n\t\treturn;\n\n\tif (unlikely(rexmit == 2)) {\n\t\t__tcp_push_pending_frames(sk, tcp_current_mss(sk),\n\t\t\t\t\t  TCP_NAGLE_OFF);\n\t\tif (after(tp->snd_nxt, tp->high_seq))\n\t\t\treturn;\n\t\ttp->frto = 0;\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\n\n/* This routine deals with incoming acks, but not outgoing ones. */\nstatic int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sacktag_state sack_state;\n\tu32 prior_snd_una = tp->snd_una;\n\tu32 ack_seq = TCP_SKB_CB(skb)->seq;\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\tbool is_dupack = false;\n\tu32 prior_fackets;\n\tint prior_packets = tp->packets_out;\n\tu32 prior_delivered = tp->delivered;\n\tint acked = 0; /* Number of packets newly acked */\n\tint rexmit = REXMIT_NONE; /* Flag to (re)transmit to recover losses */\n\n\tsack_state.first_sackt.v64 = 0;\n\n\t/* We very likely will need to access write queue head. */\n\tprefetchw(sk->sk_write_queue.next);\n\n\t/* If the ack is older than previous acks\n\t * then we can probably ignore it.\n\t */\n\tif (before(ack, prior_snd_una)) {\n\t\t/* RFC 5961 5.2 [Blind Data Injection Attack].[Mitigation] */\n\t\tif (before(ack, prior_snd_una - tp->max_window)) {\n\t\t\ttcp_send_challenge_ack(sk, skb);\n\t\t\treturn -1;\n\t\t}\n\t\tgoto old_ack;\n\t}\n\n\t/* If the ack includes data we haven't sent yet, discard\n\t * this segment (RFC793 Section 3.9).\n\t */\n\tif (after(ack, tp->snd_nxt))\n\t\tgoto invalid_ack;\n\n\tif (icsk->icsk_pending == ICSK_TIME_EARLY_RETRANS ||\n\t    icsk->icsk_pending == ICSK_TIME_LOSS_PROBE)\n\t\ttcp_rearm_rto(sk);\n\n\tif (after(ack, prior_snd_una)) {\n\t\tflag |= FLAG_SND_UNA_ADVANCED;\n\t\ticsk->icsk_retransmits = 0;\n\t}\n\n\tprior_fackets = tp->fackets_out;\n\n\t/* ts_recent update must be made after we are sure that the packet\n\t * is in window.\n\t */\n\tif (flag & FLAG_UPDATE_TS_RECENT)\n\t\ttcp_replace_ts_recent(tp, TCP_SKB_CB(skb)->seq);\n\n\tif (!(flag & FLAG_SLOWPATH) && after(ack, prior_snd_una)) {\n\t\t/* Window is constant, pure forward advance.\n\t\t * No more checks are required.\n\t\t * Note, we use the fact that SND.UNA>=SND.WL2.\n\t\t */\n\t\ttcp_update_wl(tp, ack_seq);\n\t\ttcp_snd_una_update(tp, ack);\n\t\tflag |= FLAG_WIN_UPDATE;\n\n\t\ttcp_in_ack_event(sk, CA_ACK_WIN_UPDATE);\n\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPACKS);\n\t} else {\n\t\tu32 ack_ev_flags = CA_ACK_SLOWPATH;\n\n\t\tif (ack_seq != TCP_SKB_CB(skb)->end_seq)\n\t\t\tflag |= FLAG_DATA;\n\t\telse\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPUREACKS);\n\n\t\tflag |= tcp_ack_update_window(sk, skb, ack, ack_seq);\n\n\t\tif (TCP_SKB_CB(skb)->sacked)\n\t\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t\t&sack_state);\n\n\t\tif (tcp_ecn_rcv_ecn_echo(tp, tcp_hdr(skb))) {\n\t\t\tflag |= FLAG_ECE;\n\t\t\tack_ev_flags |= CA_ACK_ECE;\n\t\t}\n\n\t\tif (flag & FLAG_WIN_UPDATE)\n\t\t\tack_ev_flags |= CA_ACK_WIN_UPDATE;\n\n\t\ttcp_in_ack_event(sk, ack_ev_flags);\n\t}\n\n\t/* We passed data and got it acked, remove any soft error\n\t * log. Something worked...\n\t */\n\tsk->sk_err_soft = 0;\n\ticsk->icsk_probes_out = 0;\n\ttp->rcv_tstamp = tcp_time_stamp;\n\tif (!prior_packets)\n\t\tgoto no_queue;\n\n\t/* See if we can take anything off of the retransmit queue. */\n\tflag |= tcp_clean_rtx_queue(sk, prior_fackets, prior_snd_una, &acked,\n\t\t\t\t    &sack_state);\n\n\tif (tcp_ack_is_dubious(sk, flag)) {\n\t\tis_dupack = !(flag & (FLAG_SND_UNA_ADVANCED | FLAG_NOT_DUP));\n\t\ttcp_fastretrans_alert(sk, acked, is_dupack, &flag, &rexmit);\n\t}\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\n\tif ((flag & FLAG_FORWARD_PROGRESS) || !(flag & FLAG_NOT_DUP)) {\n\t\tstruct dst_entry *dst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tdst_confirm(dst);\n\t}\n\n\tif (icsk->icsk_pending == ICSK_TIME_RETRANS)\n\t\ttcp_schedule_loss_probe(sk);\n\ttcp_cong_control(sk, ack, tp->delivered - prior_delivered, flag);\n\ttcp_xmit_recovery(sk, rexmit);\n\treturn 1;\n\nno_queue:\n\t/* If data was DSACKed, see if we can undo a cwnd reduction. */\n\tif (flag & FLAG_DSACKING_ACK)\n\t\ttcp_fastretrans_alert(sk, acked, is_dupack, &flag, &rexmit);\n\t/* If this ack opens up a zero window, clear backoff.  It was\n\t * being used to time the probes, and is probably far higher than\n\t * it needs to be for normal retransmission.\n\t */\n\tif (tcp_send_head(sk))\n\t\ttcp_ack_probe(sk);\n\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\treturn 1;\n\ninvalid_ack:\n\tSOCK_DEBUG(sk, \"Ack %u after %u:%u\\n\", ack, tp->snd_una, tp->snd_nxt);\n\treturn -1;\n\nold_ack:\n\t/* If data was SACKed, tag it and see if we should send more data.\n\t * If data was DSACKed, see if we can undo a cwnd reduction.\n\t */\n\tif (TCP_SKB_CB(skb)->sacked) {\n\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t&sack_state);\n\t\ttcp_fastretrans_alert(sk, acked, is_dupack, &flag, &rexmit);\n\t\ttcp_xmit_recovery(sk, rexmit);\n\t}\n\n\tSOCK_DEBUG(sk, \"Ack %u before %u:%u\\n\", ack, tp->snd_una, tp->snd_nxt);\n\treturn 0;\n}\n\nstatic void tcp_parse_fastopen_option(int len, const unsigned char *cookie,\n\t\t\t\t      bool syn, struct tcp_fastopen_cookie *foc,\n\t\t\t\t      bool exp_opt)\n{\n\t/* Valid only in SYN or SYN-ACK with an even length.  */\n\tif (!foc || !syn || len < 0 || (len & 1))\n\t\treturn;\n\n\tif (len >= TCP_FASTOPEN_COOKIE_MIN &&\n\t    len <= TCP_FASTOPEN_COOKIE_MAX)\n\t\tmemcpy(foc->val, cookie, len);\n\telse if (len != 0)\n\t\tlen = -1;\n\tfoc->len = len;\n\tfoc->exp = exp_opt;\n}\n\n/* Look for tcp options. Normally only called on SYN and SYNACK packets.\n * But, this can also be called on packets in the established flow when\n * the fast version below fails.\n */\nvoid tcp_parse_options(const struct sk_buff *skb,\n\t\t       struct tcp_options_received *opt_rx, int estab,\n\t\t       struct tcp_fastopen_cookie *foc)\n{\n\tconst unsigned char *ptr;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tint length = (th->doff * 4) - sizeof(struct tcphdr);\n\n\tptr = (const unsigned char *)(th + 1);\n\topt_rx->saw_tstamp = 0;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn;\n\t\tcase TCPOPT_NOP:\t/* Ref: RFC 793 section 3.1 */\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2) /* \"silly options\" */\n\t\t\t\treturn;\n\t\t\tif (opsize > length)\n\t\t\t\treturn;\t/* don't parse partial options */\n\t\t\tswitch (opcode) {\n\t\t\tcase TCPOPT_MSS:\n\t\t\t\tif (opsize == TCPOLEN_MSS && th->syn && !estab) {\n\t\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);\n\t\t\t\t\tif (in_mss) {\n\t\t\t\t\t\tif (opt_rx->user_mss &&\n\t\t\t\t\t\t    opt_rx->user_mss < in_mss)\n\t\t\t\t\t\t\tin_mss = opt_rx->user_mss;\n\t\t\t\t\t\topt_rx->mss_clamp = in_mss;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_WINDOW:\n\t\t\t\tif (opsize == TCPOLEN_WINDOW && th->syn &&\n\t\t\t\t    !estab && sysctl_tcp_window_scaling) {\n\t\t\t\t\t__u8 snd_wscale = *(__u8 *)ptr;\n\t\t\t\t\topt_rx->wscale_ok = 1;\n\t\t\t\t\tif (snd_wscale > 14) {\n\t\t\t\t\t\tnet_info_ratelimited(\"%s: Illegal window scaling value %d >14 received\\n\",\n\t\t\t\t\t\t\t\t     __func__,\n\t\t\t\t\t\t\t\t     snd_wscale);\n\t\t\t\t\t\tsnd_wscale = 14;\n\t\t\t\t\t}\n\t\t\t\t\topt_rx->snd_wscale = snd_wscale;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_TIMESTAMP:\n\t\t\t\tif ((opsize == TCPOLEN_TIMESTAMP) &&\n\t\t\t\t    ((estab && opt_rx->tstamp_ok) ||\n\t\t\t\t     (!estab && sysctl_tcp_timestamps))) {\n\t\t\t\t\topt_rx->saw_tstamp = 1;\n\t\t\t\t\topt_rx->rcv_tsval = get_unaligned_be32(ptr);\n\t\t\t\t\topt_rx->rcv_tsecr = get_unaligned_be32(ptr + 4);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_SACK_PERM:\n\t\t\t\tif (opsize == TCPOLEN_SACK_PERM && th->syn &&\n\t\t\t\t    !estab && sysctl_tcp_sack) {\n\t\t\t\t\topt_rx->sack_ok = TCP_SACK_SEEN;\n\t\t\t\t\ttcp_sack_reset(opt_rx);\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_SACK:\n\t\t\t\tif ((opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK)) &&\n\t\t\t\t   !((opsize - TCPOLEN_SACK_BASE) % TCPOLEN_SACK_PERBLOCK) &&\n\t\t\t\t   opt_rx->sack_ok) {\n\t\t\t\t\tTCP_SKB_CB(skb)->sacked = (ptr - 2) - (unsigned char *)th;\n\t\t\t\t}\n\t\t\t\tbreak;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\tcase TCPOPT_MD5SIG:\n\t\t\t\t/*\n\t\t\t\t * The MD5 Hash has already been\n\t\t\t\t * checked (see tcp_v{4,6}_do_rcv()).\n\t\t\t\t */\n\t\t\t\tbreak;\n#endif\n\t\t\tcase TCPOPT_FASTOPEN:\n\t\t\t\ttcp_parse_fastopen_option(\n\t\t\t\t\topsize - TCPOLEN_FASTOPEN_BASE,\n\t\t\t\t\tptr, th->syn, foc, false);\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_EXP:\n\t\t\t\t/* Fast Open option shares code 254 using a\n\t\t\t\t * 16 bits magic number.\n\t\t\t\t */\n\t\t\t\tif (opsize >= TCPOLEN_EXP_FASTOPEN_BASE &&\n\t\t\t\t    get_unaligned_be16(ptr) ==\n\t\t\t\t    TCPOPT_FASTOPEN_MAGIC)\n\t\t\t\t\ttcp_parse_fastopen_option(opsize -\n\t\t\t\t\t\tTCPOLEN_EXP_FASTOPEN_BASE,\n\t\t\t\t\t\tptr + 2, th->syn, foc, true);\n\t\t\t\tbreak;\n\n\t\t\t}\n\t\t\tptr += opsize-2;\n\t\t\tlength -= opsize;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(tcp_parse_options);\n\nstatic bool tcp_parse_aligned_timestamp(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tconst __be32 *ptr = (const __be32 *)(th + 1);\n\n\tif (*ptr == htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16)\n\t\t\t  | (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP)) {\n\t\ttp->rx_opt.saw_tstamp = 1;\n\t\t++ptr;\n\t\ttp->rx_opt.rcv_tsval = ntohl(*ptr);\n\t\t++ptr;\n\t\tif (*ptr)\n\t\t\ttp->rx_opt.rcv_tsecr = ntohl(*ptr) - tp->tsoffset;\n\t\telse\n\t\t\ttp->rx_opt.rcv_tsecr = 0;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Fast parse options. This hopes to only see timestamps.\n * If it is wrong it falls back on tcp_parse_options().\n */\nstatic bool tcp_fast_parse_options(const struct sk_buff *skb,\n\t\t\t\t   const struct tcphdr *th, struct tcp_sock *tp)\n{\n\t/* In the spirit of fast parsing, compare doff directly to constant\n\t * values.  Because equality is used, short doff can be ignored here.\n\t */\n\tif (th->doff == (sizeof(*th) / 4)) {\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\treturn false;\n\t} else if (tp->rx_opt.tstamp_ok &&\n\t\t   th->doff == ((sizeof(*th) + TCPOLEN_TSTAMP_ALIGNED) / 4)) {\n\t\tif (tcp_parse_aligned_timestamp(tp, th))\n\t\t\treturn true;\n\t}\n\n\ttcp_parse_options(skb, &tp->rx_opt, 1, NULL);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\treturn true;\n}\n\n#ifdef CONFIG_TCP_MD5SIG\n/*\n * Parse MD5 Signature option\n */\nconst u8 *tcp_parse_md5sig_option(const struct tcphdr *th)\n{\n\tint length = (th->doff << 2) - sizeof(*th);\n\tconst u8 *ptr = (const u8 *)(th + 1);\n\n\t/* If the TCP option is too short, we can short cut */\n\tif (length < TCPOLEN_MD5SIG)\n\t\treturn NULL;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn NULL;\n\t\tcase TCPOPT_NOP:\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2 || opsize > length)\n\t\t\t\treturn NULL;\n\t\t\tif (opcode == TCPOPT_MD5SIG)\n\t\t\t\treturn opsize == TCPOLEN_MD5SIG ? ptr : NULL;\n\t\t}\n\t\tptr += opsize - 2;\n\t\tlength -= opsize;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_parse_md5sig_option);\n#endif\n\n/* Sorry, PAWS as specified is broken wrt. pure-ACKs -DaveM\n *\n * It is not fatal. If this ACK does _not_ change critical state (seqs, window)\n * it can pass through stack. So, the following predicate verifies that\n * this segment is not used for anything but congestion avoidance or\n * fast retransmit. Moreover, we even are able to eliminate most of such\n * second order effects, if we apply some small \"replay\" window (~RTO)\n * to timestamp space.\n *\n * All these measures still do not guarantee that we reject wrapped ACKs\n * on networks with high bandwidth, when sequence space is recycled fastly,\n * but it guarantees that such events will be very rare and do not affect\n * connection seriously. This doesn't look nice, but alas, PAWS is really\n * buggy extension.\n *\n * [ Later note. Even worse! It is buggy for segments _with_ data. RFC\n * states that events when retransmit arrives after original data are rare.\n * It is a blatant lie. VJ forgot about fast retransmit! 8)8) It is\n * the biggest problem on large power networks even with minor reordering.\n * OK, let's give it small replay window. If peer clock is even 1hz, it is safe\n * up to bandwidth of 18Gigabit/sec. 8) ]\n */\n\nstatic int tcp_disordered_ack(const struct sock *sk, const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tu32 seq = TCP_SKB_CB(skb)->seq;\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\n\treturn (/* 1. Pure ACK with correct sequence number. */\n\t\t(th->ack && seq == TCP_SKB_CB(skb)->end_seq && seq == tp->rcv_nxt) &&\n\n\t\t/* 2. ... and duplicate ACK. */\n\t\tack == tp->snd_una &&\n\n\t\t/* 3. ... and does not update window. */\n\t\t!tcp_may_update_window(tp, ack, seq, ntohs(th->window) << tp->rx_opt.snd_wscale) &&\n\n\t\t/* 4. ... and sits in replay window. */\n\t\t(s32)(tp->rx_opt.ts_recent - tp->rx_opt.rcv_tsval) <= (inet_csk(sk)->icsk_rto * 1024) / HZ);\n}\n\nstatic inline bool tcp_paws_discard(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\treturn !tcp_paws_check(&tp->rx_opt, TCP_PAWS_WINDOW) &&\n\t       !tcp_disordered_ack(sk, skb);\n}\n\n/* Check segment sequence number for validity.\n *\n * Segment controls are considered valid, if the segment\n * fits to the window after truncation to the window. Acceptability\n * of data (and SYN, FIN, of course) is checked separately.\n * See tcp_data_queue(), for example.\n *\n * Also, controls (RST is main one) are accepted using RCV.WUP instead\n * of RCV.NXT. Peer still did not advance his SND.UNA when we\n * delayed ACK, so that hisSND.UNA<=ourRCV.WUP.\n * (borrowed from freebsd)\n */\n\nstatic inline bool tcp_sequence(const struct tcp_sock *tp, u32 seq, u32 end_seq)\n{\n\treturn\t!before(end_seq, tp->rcv_wup) &&\n\t\t!after(seq, tp->rcv_nxt + tcp_receive_window(tp));\n}\n\n/* When we get a reset we do this. */\nvoid tcp_reset(struct sock *sk)\n{\n\t/* We want the right error as BSD sees it (and indeed as we do). */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\t\tsk->sk_err = ECONNREFUSED;\n\t\tbreak;\n\tcase TCP_CLOSE_WAIT:\n\t\tsk->sk_err = EPIPE;\n\t\tbreak;\n\tcase TCP_CLOSE:\n\t\treturn;\n\tdefault:\n\t\tsk->sk_err = ECONNRESET;\n\t}\n\t/* This barrier is coupled with smp_rmb() in tcp_poll() */\n\tsmp_wmb();\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_error_report(sk);\n\n\ttcp_done(sk);\n}\n\n/*\n * \tProcess the FIN bit. This now behaves as it is supposed to work\n *\tand the FIN takes effect when it is validly part of sequence\n *\tspace. Not before when we get holes.\n *\n *\tIf we are ESTABLISHED, a received fin moves us to CLOSE-WAIT\n *\t(and thence onto LAST-ACK and finally, CLOSE, we never enter\n *\tTIME-WAIT)\n *\n *\tIf we are in FINWAIT-1, a received FIN indicates simultaneous\n *\tclose and we go into CLOSING (and later onto TIME-WAIT)\n *\n *\tIf we are in FINWAIT-2, a received FIN moves us to TIME-WAIT.\n */\nvoid tcp_fin(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tinet_csk_schedule_ack(sk);\n\n\tsk->sk_shutdown |= RCV_SHUTDOWN;\n\tsock_set_flag(sk, SOCK_DONE);\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_ESTABLISHED:\n\t\t/* Move to CLOSE_WAIT */\n\t\ttcp_set_state(sk, TCP_CLOSE_WAIT);\n\t\tinet_csk(sk)->icsk_ack.pingpong = 1;\n\t\tbreak;\n\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\t\t/* Received a retransmission of the FIN, do\n\t\t * nothing.\n\t\t */\n\t\tbreak;\n\tcase TCP_LAST_ACK:\n\t\t/* RFC793: Remain in the LAST-ACK state. */\n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1:\n\t\t/* This case occurs when a simultaneous close\n\t\t * happens, we must ack the received FIN and\n\t\t * enter the CLOSING state.\n\t\t */\n\t\ttcp_send_ack(sk);\n\t\ttcp_set_state(sk, TCP_CLOSING);\n\t\tbreak;\n\tcase TCP_FIN_WAIT2:\n\t\t/* Received a FIN -- send ACK and enter TIME_WAIT. */\n\t\ttcp_send_ack(sk);\n\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\tbreak;\n\tdefault:\n\t\t/* Only TCP_LISTEN and TCP_CLOSE are left, in these\n\t\t * cases we should never reach this piece of code.\n\t\t */\n\t\tpr_err(\"%s: Impossible, sk->sk_state=%d\\n\",\n\t\t       __func__, sk->sk_state);\n\t\tbreak;\n\t}\n\n\t/* It _is_ possible, that we have something out-of-order _after_ FIN.\n\t * Probably, we should reset in this case. For now drop them.\n\t */\n\t__skb_queue_purge(&tp->out_of_order_queue);\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_reset(&tp->rx_opt);\n\tsk_mem_reclaim(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\n\t\t/* Do not send POLL_HUP for half duplex close. */\n\t\tif (sk->sk_shutdown == SHUTDOWN_MASK ||\n\t\t    sk->sk_state == TCP_CLOSE)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_HUP);\n\t\telse\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\t}\n}\n\nstatic inline bool tcp_sack_extend(struct tcp_sack_block *sp, u32 seq,\n\t\t\t\t  u32 end_seq)\n{\n\tif (!after(seq, sp->end_seq) && !after(sp->start_seq, end_seq)) {\n\t\tif (before(seq, sp->start_seq))\n\t\t\tsp->start_seq = seq;\n\t\tif (after(end_seq, sp->end_seq))\n\t\t\tsp->end_seq = end_seq;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_dsack_set(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_is_sack(tp) && sysctl_tcp_dsack) {\n\t\tint mib_idx;\n\n\t\tif (before(seq, tp->rcv_nxt))\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOLDSENT;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOFOSENT;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\t\ttp->rx_opt.dsack = 1;\n\t\ttp->duplicate_sack[0].start_seq = seq;\n\t\ttp->duplicate_sack[0].end_seq = end_seq;\n\t}\n}\n\nstatic void tcp_dsack_extend(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->rx_opt.dsack)\n\t\ttcp_dsack_set(sk, seq, end_seq);\n\telse\n\t\ttcp_sack_extend(tp->duplicate_sack, seq, end_seq);\n}\n\nstatic void tcp_send_dupack(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t    before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_enter_quickack_mode(sk);\n\n\t\tif (tcp_is_sack(tp) && sysctl_tcp_dsack) {\n\t\t\tu32 end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t\tif (after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))\n\t\t\t\tend_seq = tp->rcv_nxt;\n\t\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, end_seq);\n\t\t}\n\t}\n\n\ttcp_send_ack(sk);\n}\n\n/* These routines update the SACK block as out-of-order packets arrive or\n * in-order packets close up the sequence space.\n */\nstatic void tcp_sack_maybe_coalesce(struct tcp_sock *tp)\n{\n\tint this_sack;\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tstruct tcp_sack_block *swalk = sp + 1;\n\n\t/* See if the recent change to the first SACK eats into\n\t * or hits the sequence space of other SACK blocks, if so coalesce.\n\t */\n\tfor (this_sack = 1; this_sack < tp->rx_opt.num_sacks;) {\n\t\tif (tcp_sack_extend(sp, swalk->start_seq, swalk->end_seq)) {\n\t\t\tint i;\n\n\t\t\t/* Zap SWALK, by moving every further SACK up by one slot.\n\t\t\t * Decrease num_sacks.\n\t\t\t */\n\t\t\ttp->rx_opt.num_sacks--;\n\t\t\tfor (i = this_sack; i < tp->rx_opt.num_sacks; i++)\n\t\t\t\tsp[i] = sp[i + 1];\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++, swalk++;\n\t}\n}\n\nstatic void tcp_sack_new_ofo_skb(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint cur_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\tif (!cur_sacks)\n\t\tgoto new_sack;\n\n\tfor (this_sack = 0; this_sack < cur_sacks; this_sack++, sp++) {\n\t\tif (tcp_sack_extend(sp, seq, end_seq)) {\n\t\t\t/* Rotate this_sack to the first one. */\n\t\t\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t\t\tswap(*sp, *(sp - 1));\n\t\t\tif (cur_sacks > 1)\n\t\t\t\ttcp_sack_maybe_coalesce(tp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* Could not find an adjacent existing SACK, build a new one,\n\t * put it at the front, and shift everyone else down.  We\n\t * always know there is at least one SACK present already here.\n\t *\n\t * If the sack array is full, forget about the last one.\n\t */\n\tif (this_sack >= TCP_NUM_SACKS) {\n\t\tthis_sack--;\n\t\ttp->rx_opt.num_sacks--;\n\t\tsp--;\n\t}\n\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t*sp = *(sp - 1);\n\nnew_sack:\n\t/* Build the new head SACK, and we're done. */\n\tsp->start_seq = seq;\n\tsp->end_seq = end_seq;\n\ttp->rx_opt.num_sacks++;\n}\n\n/* RCV.NXT advances, some SACKs should be eaten. */\n\nstatic void tcp_sack_remove(struct tcp_sock *tp)\n{\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint num_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\t/* Empty ofo queue, hence, all the SACKs are eaten. Clear. */\n\tif (skb_queue_empty(&tp->out_of_order_queue)) {\n\t\ttp->rx_opt.num_sacks = 0;\n\t\treturn;\n\t}\n\n\tfor (this_sack = 0; this_sack < num_sacks;) {\n\t\t/* Check if the start of the sack is covered by RCV.NXT. */\n\t\tif (!before(tp->rcv_nxt, sp->start_seq)) {\n\t\t\tint i;\n\n\t\t\t/* RCV.NXT must cover all the block! */\n\t\t\tWARN_ON(before(tp->rcv_nxt, sp->end_seq));\n\n\t\t\t/* Zap this SACK, by moving forward any other SACKS. */\n\t\t\tfor (i = this_sack+1; i < num_sacks; i++)\n\t\t\t\ttp->selective_acks[i-1] = tp->selective_acks[i];\n\t\t\tnum_sacks--;\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++;\n\t\tsp++;\n\t}\n\ttp->rx_opt.num_sacks = num_sacks;\n}\n\n/**\n * tcp_try_coalesce - try to merge skb to prior one\n * @sk: socket\n * @to: prior buffer\n * @from: buffer to add in queue\n * @fragstolen: pointer to boolean\n *\n * Before queueing skb @from after @to, try to merge them\n * to reduce overall memory use and queue lengths, if cost is small.\n * Packets in ofo or receive queues can stay a long time.\n * Better try to coalesce them right now to avoid future collapses.\n * Returns true if caller should free @from instead of queueing it\n */\nstatic bool tcp_try_coalesce(struct sock *sk,\n\t\t\t     struct sk_buff *to,\n\t\t\t     struct sk_buff *from,\n\t\t\t     bool *fragstolen)\n{\n\tint delta;\n\n\t*fragstolen = false;\n\n\t/* Its possible this segment overlaps with prior segment in queue */\n\tif (TCP_SKB_CB(from)->seq != TCP_SKB_CB(to)->end_seq)\n\t\treturn false;\n\n\tif (!skb_try_coalesce(to, from, fragstolen, &delta))\n\t\treturn false;\n\n\tatomic_add(delta, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, delta);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOALESCE);\n\tTCP_SKB_CB(to)->end_seq = TCP_SKB_CB(from)->end_seq;\n\tTCP_SKB_CB(to)->ack_seq = TCP_SKB_CB(from)->ack_seq;\n\tTCP_SKB_CB(to)->tcp_flags |= TCP_SKB_CB(from)->tcp_flags;\n\treturn true;\n}\n\nstatic void tcp_drop(struct sock *sk, struct sk_buff *skb)\n{\n\tsk_drops_add(sk, skb);\n\t__kfree_skb(skb);\n}\n\n/* This one checks to see if we can put data from the\n * out_of_order queue into the receive_queue.\n */\nstatic void tcp_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u32 dsack_high = tp->rcv_nxt;\n\tstruct sk_buff *skb, *tail;\n\tbool fragstolen, eaten;\n\n\twhile ((skb = skb_peek(&tp->out_of_order_queue)) != NULL) {\n\t\tif (after(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))\n\t\t\tbreak;\n\n\t\tif (before(TCP_SKB_CB(skb)->seq, dsack_high)) {\n\t\t\t__u32 dsack = dsack_high;\n\t\t\tif (before(TCP_SKB_CB(skb)->end_seq, dsack_high))\n\t\t\t\tdsack_high = TCP_SKB_CB(skb)->end_seq;\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb)->seq, dsack);\n\t\t}\n\n\t\t__skb_unlink(skb, &tp->out_of_order_queue);\n\t\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt)) {\n\t\t\tSOCK_DEBUG(sk, \"ofo packet was already received\\n\");\n\t\t\ttcp_drop(sk, skb);\n\t\t\tcontinue;\n\t\t}\n\t\tSOCK_DEBUG(sk, \"ofo requeuing : rcv_next %X seq %X - %X\\n\",\n\t\t\t   tp->rcv_nxt, TCP_SKB_CB(skb)->seq,\n\t\t\t   TCP_SKB_CB(skb)->end_seq);\n\n\t\ttail = skb_peek_tail(&sk->sk_receive_queue);\n\t\teaten = tail && tcp_try_coalesce(sk, tail, skb, &fragstolen);\n\t\ttcp_rcv_nxt_update(tp, TCP_SKB_CB(skb)->end_seq);\n\t\tif (!eaten)\n\t\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\ttcp_fin(sk);\n\t\tif (eaten)\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\t}\n}\n\nstatic bool tcp_prune_ofo_queue(struct sock *sk);\nstatic int tcp_prune_queue(struct sock *sk);\n\nstatic int tcp_try_rmem_schedule(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t unsigned int size)\n{\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t    !sk_rmem_schedule(sk, skb, size)) {\n\n\t\tif (tcp_prune_queue(sk) < 0)\n\t\t\treturn -1;\n\n\t\tif (!sk_rmem_schedule(sk, skb, size)) {\n\t\t\tif (!tcp_prune_ofo_queue(sk))\n\t\t\t\treturn -1;\n\n\t\t\tif (!sk_rmem_schedule(sk, skb, size))\n\t\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb1;\n\tu32 seq, end_seq;\n\n\ttcp_ecn_check_ce(tp, skb);\n\n\tif (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);\n\t\ttcp_drop(sk, skb);\n\t\treturn;\n\t}\n\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n\tinet_csk_schedule_ack(sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);\n\tSOCK_DEBUG(sk, \"out of order segment: rcv_next %X seq %X - %X\\n\",\n\t\t   tp->rcv_nxt, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);\n\n\tskb1 = skb_peek_tail(&tp->out_of_order_queue);\n\tif (!skb1) {\n\t\t/* Initial out of order segment, build 1 SACK. */\n\t\tif (tcp_is_sack(tp)) {\n\t\t\ttp->rx_opt.num_sacks = 1;\n\t\t\ttp->selective_acks[0].start_seq = TCP_SKB_CB(skb)->seq;\n\t\t\ttp->selective_acks[0].end_seq =\n\t\t\t\t\t\tTCP_SKB_CB(skb)->end_seq;\n\t\t}\n\t\t__skb_queue_head(&tp->out_of_order_queue, skb);\n\t\tgoto end;\n\t}\n\n\tseq = TCP_SKB_CB(skb)->seq;\n\tend_seq = TCP_SKB_CB(skb)->end_seq;\n\n\tif (seq == TCP_SKB_CB(skb1)->end_seq) {\n\t\tbool fragstolen;\n\n\t\tif (!tcp_try_coalesce(sk, skb1, skb, &fragstolen)) {\n\t\t\t__skb_queue_after(&tp->out_of_order_queue, skb1, skb);\n\t\t} else {\n\t\t\ttcp_grow_window(sk, skb);\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\t\tskb = NULL;\n\t\t}\n\n\t\tif (!tp->rx_opt.num_sacks ||\n\t\t    tp->selective_acks[0].end_seq != seq)\n\t\t\tgoto add_sack;\n\n\t\t/* Common case: data arrive in order after hole. */\n\t\ttp->selective_acks[0].end_seq = end_seq;\n\t\tgoto end;\n\t}\n\n\t/* Find place to insert this segment. */\n\twhile (1) {\n\t\tif (!after(TCP_SKB_CB(skb1)->seq, seq))\n\t\t\tbreak;\n\t\tif (skb_queue_is_first(&tp->out_of_order_queue, skb1)) {\n\t\t\tskb1 = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tskb1 = skb_queue_prev(&tp->out_of_order_queue, skb1);\n\t}\n\n\t/* Do skb overlap to previous one? */\n\tif (skb1 && before(seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\t/* All the bits are present. Drop. */\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\n\t\t\ttcp_drop(sk, skb);\n\t\t\tskb = NULL;\n\t\t\ttcp_dsack_set(sk, seq, end_seq);\n\t\t\tgoto add_sack;\n\t\t}\n\t\tif (after(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\t/* Partial overlap. */\n\t\t\ttcp_dsack_set(sk, seq,\n\t\t\t\t      TCP_SKB_CB(skb1)->end_seq);\n\t\t} else {\n\t\t\tif (skb_queue_is_first(&tp->out_of_order_queue,\n\t\t\t\t\t       skb1))\n\t\t\t\tskb1 = NULL;\n\t\t\telse\n\t\t\t\tskb1 = skb_queue_prev(\n\t\t\t\t\t&tp->out_of_order_queue,\n\t\t\t\t\tskb1);\n\t\t}\n\t}\n\tif (!skb1)\n\t\t__skb_queue_head(&tp->out_of_order_queue, skb);\n\telse\n\t\t__skb_queue_after(&tp->out_of_order_queue, skb1, skb);\n\n\t/* And clean segments covered by new one as whole. */\n\twhile (!skb_queue_is_last(&tp->out_of_order_queue, skb)) {\n\t\tskb1 = skb_queue_next(&tp->out_of_order_queue, skb);\n\n\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tbreak;\n\t\tif (before(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t end_seq);\n\t\t\tbreak;\n\t\t}\n\t\t__skb_unlink(skb1, &tp->out_of_order_queue);\n\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\n\t\ttcp_drop(sk, skb1);\n\t}\n\nadd_sack:\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_new_ofo_skb(sk, seq, end_seq);\nend:\n\tif (skb) {\n\t\ttcp_grow_window(sk, skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n}\n\nstatic int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb, int hdrlen,\n\t\t  bool *fragstolen)\n{\n\tint eaten;\n\tstruct sk_buff *tail = skb_peek_tail(&sk->sk_receive_queue);\n\n\t__skb_pull(skb, hdrlen);\n\teaten = (tail &&\n\t\t tcp_try_coalesce(sk, tail, skb, fragstolen)) ? 1 : 0;\n\ttcp_rcv_nxt_update(tcp_sk(sk), TCP_SKB_CB(skb)->end_seq);\n\tif (!eaten) {\n\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n\treturn eaten;\n}\n\nint tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\tint data_len = 0;\n\tbool fragstolen;\n\n\tif (size == 0)\n\t\treturn 0;\n\n\tif (size > PAGE_SIZE) {\n\t\tint npages = min_t(size_t, size >> PAGE_SHIFT, MAX_SKB_FRAGS);\n\n\t\tdata_len = npages << PAGE_SHIFT;\n\t\tsize = data_len + (size & ~PAGE_MASK);\n\t}\n\tskb = alloc_skb_with_frags(size - data_len, data_len,\n\t\t\t\t   PAGE_ALLOC_COSTLY_ORDER,\n\t\t\t\t   &err, sk->sk_allocation);\n\tif (!skb)\n\t\tgoto err;\n\n\tskb_put(skb, size - data_len);\n\tskb->data_len = data_len;\n\tskb->len = size;\n\n\tif (tcp_try_rmem_schedule(sk, skb, skb->truesize))\n\t\tgoto err_free;\n\n\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);\n\tif (err)\n\t\tgoto err_free;\n\n\tTCP_SKB_CB(skb)->seq = tcp_sk(sk)->rcv_nxt;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(skb)->seq + size;\n\tTCP_SKB_CB(skb)->ack_seq = tcp_sk(sk)->snd_una - 1;\n\n\tif (tcp_queue_rcv(sk, skb, 0, &fragstolen)) {\n\t\tWARN_ON_ONCE(fragstolen); /* should not happen */\n\t\t__kfree_skb(skb);\n\t}\n\treturn size;\n\nerr_free:\n\tkfree_skb(skb);\nerr:\n\treturn err;\n\n}\n\nstatic void tcp_data_queue(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool fragstolen = false;\n\tint eaten = -1;\n\n\tif (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\tskb_dst_drop(skb);\n\t__skb_pull(skb, tcp_hdr(skb)->doff * 4);\n\n\ttcp_ecn_accept_cwr(tp, skb);\n\n\ttp->rx_opt.dsack = 0;\n\n\t/*  Queue data for delivery to the user.\n\t *  Packets in sequence go to the receive queue.\n\t *  Out of sequence packets to the out_of_order_queue.\n\t */\n\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt) {\n\t\tif (tcp_receive_window(tp) == 0)\n\t\t\tgoto out_of_window;\n\n\t\t/* Ok. In sequence. In window. */\n\t\tif (tp->ucopy.task == current &&\n\t\t    tp->copied_seq == tp->rcv_nxt && tp->ucopy.len &&\n\t\t    sock_owned_by_user(sk) && !tp->urg_data) {\n\t\t\tint chunk = min_t(unsigned int, skb->len,\n\t\t\t\t\t  tp->ucopy.len);\n\n\t\t\t__set_current_state(TASK_RUNNING);\n\n\t\t\tif (!skb_copy_datagram_msg(skb, 0, tp->ucopy.msg, chunk)) {\n\t\t\t\ttp->ucopy.len -= chunk;\n\t\t\t\ttp->copied_seq += chunk;\n\t\t\t\teaten = (chunk == skb->len);\n\t\t\t\ttcp_rcv_space_adjust(sk);\n\t\t\t}\n\t\t}\n\n\t\tif (eaten <= 0) {\nqueue_and_out:\n\t\t\tif (eaten < 0) {\n\t\t\t\tif (skb_queue_len(&sk->sk_receive_queue) == 0)\n\t\t\t\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t\t\telse if (tcp_try_rmem_schedule(sk, skb, skb->truesize))\n\t\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\teaten = tcp_queue_rcv(sk, skb, 0, &fragstolen);\n\t\t}\n\t\ttcp_rcv_nxt_update(tp, TCP_SKB_CB(skb)->end_seq);\n\t\tif (skb->len)\n\t\t\ttcp_event_data_recv(sk, skb);\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\ttcp_fin(sk);\n\n\t\tif (!skb_queue_empty(&tp->out_of_order_queue)) {\n\t\t\ttcp_ofo_queue(sk);\n\n\t\t\t/* RFC2581. 4.2. SHOULD send immediate ACK, when\n\t\t\t * gap in queue is filled.\n\t\t\t */\n\t\t\tif (skb_queue_empty(&tp->out_of_order_queue))\n\t\t\t\tinet_csk(sk)->icsk_ack.pingpong = 0;\n\t\t}\n\n\t\tif (tp->rx_opt.num_sacks)\n\t\t\ttcp_sack_remove(tp);\n\n\t\ttcp_fast_path_check(sk);\n\n\t\tif (eaten > 0)\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_data_ready(sk);\n\t\treturn;\n\t}\n\n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt)) {\n\t\t/* A retransmit, 2nd most common case.  Force an immediate ack. */\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);\n\nout_of_window:\n\t\ttcp_enter_quickack_mode(sk);\n\t\tinet_csk_schedule_ack(sk);\ndrop:\n\t\ttcp_drop(sk, skb);\n\t\treturn;\n\t}\n\n\t/* Out of window. F.e. zero window probe. */\n\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt + tcp_receive_window(tp)))\n\t\tgoto out_of_window;\n\n\ttcp_enter_quickack_mode(sk);\n\n\tif (before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\t/* Partial packet, seq < rcv_next < end_seq */\n\t\tSOCK_DEBUG(sk, \"partial packet: rcv_next %X seq %X - %X\\n\",\n\t\t\t   tp->rcv_nxt, TCP_SKB_CB(skb)->seq,\n\t\t\t   TCP_SKB_CB(skb)->end_seq);\n\n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, tp->rcv_nxt);\n\n\t\t/* If window is closed, drop tail of packet. But after\n\t\t * remembering D-SACK for its head made in previous line.\n\t\t */\n\t\tif (!tcp_receive_window(tp))\n\t\t\tgoto out_of_window;\n\t\tgoto queue_and_out;\n\t}\n\n\ttcp_data_queue_ofo(sk, skb);\n}\n\nstatic struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\tstruct sk_buff_head *list)\n{\n\tstruct sk_buff *next = NULL;\n\n\tif (!skb_queue_is_last(list, skb))\n\t\tnext = skb_queue_next(list, skb);\n\n\t__skb_unlink(skb, list);\n\t__kfree_skb(skb);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOLLAPSED);\n\n\treturn next;\n}\n\n/* Collapse contiguous sequence of skbs head..tail with\n * sequence numbers start..end.\n *\n * If tail is NULL, this means until the end of the list.\n *\n * Segments with FIN/SYN are not collapsed (only because this\n * simplifies code)\n */\nstatic void\ntcp_collapse(struct sock *sk, struct sk_buff_head *list,\n\t     struct sk_buff *head, struct sk_buff *tail,\n\t     u32 start, u32 end)\n{\n\tstruct sk_buff *skb, *n;\n\tbool end_of_skbs;\n\n\t/* First, check that queue is collapsible and find\n\t * the point where collapsing can be useful. */\n\tskb = head;\nrestart:\n\tend_of_skbs = true;\n\tskb_queue_walk_from_safe(list, skb, n) {\n\t\tif (skb == tail)\n\t\t\tbreak;\n\t\t/* No new bits? It is possible on ofo queue. */\n\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\tskb = tcp_collapse_one(sk, skb, list);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\tgoto restart;\n\t\t}\n\n\t\t/* The first skb to collapse is:\n\t\t * - not SYN/FIN and\n\t\t * - bloated or contains data before \"start\" or\n\t\t *   overlaps to the next one.\n\t\t */\n\t\tif (!(TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)) &&\n\t\t    (tcp_win_from_space(skb->truesize) > skb->len ||\n\t\t     before(TCP_SKB_CB(skb)->seq, start))) {\n\t\t\tend_of_skbs = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!skb_queue_is_last(list, skb)) {\n\t\t\tstruct sk_buff *next = skb_queue_next(list, skb);\n\t\t\tif (next != tail &&\n\t\t\t    TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(next)->seq) {\n\t\t\t\tend_of_skbs = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Decided to skip this, advance start seq. */\n\t\tstart = TCP_SKB_CB(skb)->end_seq;\n\t}\n\tif (end_of_skbs ||\n\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)))\n\t\treturn;\n\n\twhile (before(start, end)) {\n\t\tint copy = min_t(int, SKB_MAX_ORDER(0, 0), end - start);\n\t\tstruct sk_buff *nskb;\n\n\t\tnskb = alloc_skb(copy, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn;\n\n\t\tmemcpy(nskb->cb, skb->cb, sizeof(skb->cb));\n\t\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(nskb)->end_seq = start;\n\t\t__skb_queue_before(list, skb, nskb);\n\t\tskb_set_owner_r(nskb, sk);\n\n\t\t/* Copy data, releasing collapsed skbs. */\n\t\twhile (copy > 0) {\n\t\t\tint offset = start - TCP_SKB_CB(skb)->seq;\n\t\t\tint size = TCP_SKB_CB(skb)->end_seq - start;\n\n\t\t\tBUG_ON(offset < 0);\n\t\t\tif (size > 0) {\n\t\t\t\tsize = min(copy, size);\n\t\t\t\tif (skb_copy_bits(skb, offset, skb_put(nskb, size), size))\n\t\t\t\t\tBUG();\n\t\t\t\tTCP_SKB_CB(nskb)->end_seq += size;\n\t\t\t\tcopy -= size;\n\t\t\t\tstart += size;\n\t\t\t}\n\t\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t\tskb = tcp_collapse_one(sk, skb, list);\n\t\t\t\tif (!skb ||\n\t\t\t\t    skb == tail ||\n\t\t\t\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)))\n\t\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}\n\n/* Collapse ofo queue. Algorithm: select contiguous sequence of skbs\n * and tcp_collapse() them until all the queue is collapsed.\n */\nstatic void tcp_collapse_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb = skb_peek(&tp->out_of_order_queue);\n\tstruct sk_buff *head;\n\tu32 start, end;\n\n\tif (!skb)\n\t\treturn;\n\n\tstart = TCP_SKB_CB(skb)->seq;\n\tend = TCP_SKB_CB(skb)->end_seq;\n\thead = skb;\n\n\tfor (;;) {\n\t\tstruct sk_buff *next = NULL;\n\n\t\tif (!skb_queue_is_last(&tp->out_of_order_queue, skb))\n\t\t\tnext = skb_queue_next(&tp->out_of_order_queue, skb);\n\t\tskb = next;\n\n\t\t/* Segment is terminated when we see gap or when\n\t\t * we are at the end of all the queue. */\n\t\tif (!skb ||\n\t\t    after(TCP_SKB_CB(skb)->seq, end) ||\n\t\t    before(TCP_SKB_CB(skb)->end_seq, start)) {\n\t\t\ttcp_collapse(sk, &tp->out_of_order_queue,\n\t\t\t\t     head, skb, start, end);\n\t\t\thead = skb;\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\t/* Start new segment */\n\t\t\tstart = TCP_SKB_CB(skb)->seq;\n\t\t\tend = TCP_SKB_CB(skb)->end_seq;\n\t\t} else {\n\t\t\tif (before(TCP_SKB_CB(skb)->seq, start))\n\t\t\t\tstart = TCP_SKB_CB(skb)->seq;\n\t\t\tif (after(TCP_SKB_CB(skb)->end_seq, end))\n\t\t\t\tend = TCP_SKB_CB(skb)->end_seq;\n\t\t}\n\t}\n}\n\n/*\n * Purge the out-of-order queue.\n * Return true if queue was pruned.\n */\nstatic bool tcp_prune_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool res = false;\n\n\tif (!skb_queue_empty(&tp->out_of_order_queue)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\t\t__skb_queue_purge(&tp->out_of_order_queue);\n\n\t\t/* Reset SACK state.  A conforming SACK implementation will\n\t\t * do the same at a timeout based retransmit.  When a connection\n\t\t * is in a sad state like this, we care only about integrity\n\t\t * of the connection not performance.\n\t\t */\n\t\tif (tp->rx_opt.sack_ok)\n\t\t\ttcp_sack_reset(&tp->rx_opt);\n\t\tsk_mem_reclaim(sk);\n\t\tres = true;\n\t}\n\treturn res;\n}\n\n/* Reduce allocated memory if we can, trying to get\n * the socket within its memory limits again.\n *\n * Return less than zero if we should start dropping frames\n * until the socket owning process reads some of the data\n * to stabilize the situation.\n */\nstatic int tcp_prune_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tSOCK_DEBUG(sk, \"prune_queue: c=%x\\n\", tp->copied_seq);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\ttcp_clamp_window(sk);\n\telse if (tcp_under_memory_pressure(sk))\n\t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, 4U * tp->advmss);\n\n\ttcp_collapse_ofo_queue(sk);\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\ttcp_collapse(sk, &sk->sk_receive_queue,\n\t\t\t     skb_peek(&sk->sk_receive_queue),\n\t\t\t     NULL,\n\t\t\t     tp->copied_seq, tp->rcv_nxt);\n\tsk_mem_reclaim(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* Collapsing did not help, destructive actions follow.\n\t * This must not ever occur. */\n\n\ttcp_prune_ofo_queue(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* If we are really being abused, tell the caller to silently\n\t * drop receive data on the floor.  It will get retransmitted\n\t * and hopefully then we'll have sufficient space.\n\t */\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);\n\n\t/* Massive buffer overcommit. */\n\ttp->pred_flags = 0;\n\treturn -1;\n}\n\nstatic bool tcp_should_expand_sndbuf(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If the user specified a specific send buffer setting, do\n\t * not modify it.\n\t */\n\tif (sk->sk_userlocks & SOCK_SNDBUF_LOCK)\n\t\treturn false;\n\n\t/* If we are under global TCP memory pressure, do not expand.  */\n\tif (tcp_under_memory_pressure(sk))\n\t\treturn false;\n\n\t/* If we are under soft global TCP memory pressure, do not expand.  */\n\tif (sk_memory_allocated(sk) >= sk_prot_mem_limits(sk, 0))\n\t\treturn false;\n\n\t/* If we filled the congestion window, do not expand.  */\n\tif (tcp_packets_in_flight(tp) >= tp->snd_cwnd)\n\t\treturn false;\n\n\treturn true;\n}\n\n/* When incoming ACK allowed to free some skb from write_queue,\n * we remember this event in flag SOCK_QUEUE_SHRUNK and wake up socket\n * on the exit from tcp input handler.\n *\n * PROBLEM: sndbuf expansion does not work well with largesend.\n */\nstatic void tcp_new_space(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_should_expand_sndbuf(sk)) {\n\t\ttcp_sndbuf_expand(sk);\n\t\ttp->snd_cwnd_stamp = tcp_time_stamp;\n\t}\n\n\tsk->sk_write_space(sk);\n}\n\nstatic void tcp_check_space(struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_QUEUE_SHRUNK)) {\n\t\tsock_reset_flag(sk, SOCK_QUEUE_SHRUNK);\n\t\t/* pairs with tcp_poll() */\n\t\tsmp_mb__after_atomic();\n\t\tif (sk->sk_socket &&\n\t\t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags))\n\t\t\ttcp_new_space(sk);\n\t}\n}\n\nstatic inline void tcp_data_snd_check(struct sock *sk)\n{\n\ttcp_push_pending_frames(sk);\n\ttcp_check_space(sk);\n}\n\n/*\n * Check if sending an ack is needed.\n */\nstatic void __tcp_ack_snd_check(struct sock *sk, int ofo_possible)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t    /* More than one full frame received... */\n\tif (((tp->rcv_nxt - tp->rcv_wup) > inet_csk(sk)->icsk_ack.rcv_mss &&\n\t     /* ... and right edge of window advances far enough.\n\t      * (tcp_recvmsg() will send ACK otherwise). Or...\n\t      */\n\t     __tcp_select_window(sk) >= tp->rcv_wnd) ||\n\t    /* We ACK each frame or... */\n\t    tcp_in_quickack_mode(sk) ||\n\t    /* We have out of order data. */\n\t    (ofo_possible && skb_peek(&tp->out_of_order_queue))) {\n\t\t/* Then ack it now */\n\t\ttcp_send_ack(sk);\n\t} else {\n\t\t/* Else, send delayed ack. */\n\t\ttcp_send_delayed_ack(sk);\n\t}\n}\n\nstatic inline void tcp_ack_snd_check(struct sock *sk)\n{\n\tif (!inet_csk_ack_scheduled(sk)) {\n\t\t/* We sent a data segment already. */\n\t\treturn;\n\t}\n\t__tcp_ack_snd_check(sk, 1);\n}\n\n/*\n *\tThis routine is only called when we have urgent data\n *\tsignaled. Its the 'slow' part of tcp_urg. It could be\n *\tmoved inline now as tcp_urg is only called from one\n *\tplace. We handle URGent data wrong. We have to - as\n *\tBSD still doesn't use the correction from RFC961.\n *\tFor 1003.1g we should support a new option TCP_STDURG to permit\n *\teither form (or just set the sysctl tcp_stdurg).\n */\n\nstatic void tcp_check_urg(struct sock *sk, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 ptr = ntohs(th->urg_ptr);\n\n\tif (ptr && !sysctl_tcp_stdurg)\n\t\tptr--;\n\tptr += ntohl(th->seq);\n\n\t/* Ignore urgent data that we've already seen and read. */\n\tif (after(tp->copied_seq, ptr))\n\t\treturn;\n\n\t/* Do not replay urg ptr.\n\t *\n\t * NOTE: interesting situation not covered by specs.\n\t * Misbehaving sender may send urg ptr, pointing to segment,\n\t * which we already have in ofo queue. We are not able to fetch\n\t * such data and will stay in TCP_URG_NOTYET until will be eaten\n\t * by recvmsg(). Seems, we are not obliged to handle such wicked\n\t * situations. But it is worth to think about possibility of some\n\t * DoSes using some hypothetical application level deadlock.\n\t */\n\tif (before(ptr, tp->rcv_nxt))\n\t\treturn;\n\n\t/* Do we already have a newer (or duplicate) urgent pointer? */\n\tif (tp->urg_data && !after(ptr, tp->urg_seq))\n\t\treturn;\n\n\t/* Tell the world about our new urgent pointer. */\n\tsk_send_sigurg(sk);\n\n\t/* We may be adding urgent data when the last byte read was\n\t * urgent. To do this requires some care. We cannot just ignore\n\t * tp->copied_seq since we would read the last urgent byte again\n\t * as data, nor can we alter copied_seq until this data arrives\n\t * or we break the semantics of SIOCATMARK (and thus sockatmark())\n\t *\n\t * NOTE. Double Dutch. Rendering to plain English: author of comment\n\t * above did something sort of \tsend(\"A\", MSG_OOB); send(\"B\", MSG_OOB);\n\t * and expect that both A and B disappear from stream. This is _wrong_.\n\t * Though this happens in BSD with high probability, this is occasional.\n\t * Any application relying on this is buggy. Note also, that fix \"works\"\n\t * only in this artificial test. Insert some normal data between A and B and we will\n\t * decline of BSD again. Verdict: it is better to remove to trap\n\t * buggy users.\n\t */\n\tif (tp->urg_seq == tp->copied_seq && tp->urg_data &&\n\t    !sock_flag(sk, SOCK_URGINLINE) && tp->copied_seq != tp->rcv_nxt) {\n\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\t\ttp->copied_seq++;\n\t\tif (skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t__skb_unlink(skb, &sk->sk_receive_queue);\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t}\n\n\ttp->urg_data = TCP_URG_NOTYET;\n\ttp->urg_seq = ptr;\n\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n}\n\n/* This is the 'fast' part of urgent handling. */\nstatic void tcp_urg(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Check if we get a new urgent pointer - normally not. */\n\tif (th->urg)\n\t\ttcp_check_urg(sk, th);\n\n\t/* Do we wait for any urgent data? - normally not... */\n\tif (tp->urg_data == TCP_URG_NOTYET) {\n\t\tu32 ptr = tp->urg_seq - ntohl(th->seq) + (th->doff * 4) -\n\t\t\t  th->syn;\n\n\t\t/* Is the urgent pointer pointing into this packet? */\n\t\tif (ptr < skb->len) {\n\t\t\tu8 tmp;\n\t\t\tif (skb_copy_bits(skb, ptr, &tmp, 1))\n\t\t\t\tBUG();\n\t\t\ttp->urg_data = TCP_URG_VALID | tmp;\n\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\tsk->sk_data_ready(sk);\n\t\t}\n\t}\n}\n\nstatic int tcp_copy_to_iovec(struct sock *sk, struct sk_buff *skb, int hlen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint chunk = skb->len - hlen;\n\tint err;\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_msg(skb, hlen, tp->ucopy.msg, chunk);\n\telse\n\t\terr = skb_copy_and_csum_datagram_msg(skb, hlen, tp->ucopy.msg);\n\n\tif (!err) {\n\t\ttp->ucopy.len -= chunk;\n\t\ttp->copied_seq += chunk;\n\t\ttcp_rcv_space_adjust(sk);\n\t}\n\n\treturn err;\n}\n\n/* Does PAWS and seqno based validation of an incoming segment, flags will\n * play significant role here.\n */\nstatic bool tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  const struct tcphdr *th, int syn_inerr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* RFC1323: H1. Apply PAWS check first. */\n\tif (tcp_fast_parse_options(skb, th, tp) && tp->rx_opt.saw_tstamp &&\n\t    tcp_paws_discard(sk, skb)) {\n\t\tif (!th->rst) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);\n\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDPAWS,\n\t\t\t\t\t\t  &tp->last_oow_ack_time))\n\t\t\t\ttcp_send_dupack(sk, skb);\n\t\t\tgoto discard;\n\t\t}\n\t\t/* Reset is accepted even if it did not pass PAWS. */\n\t}\n\n\t/* Step 1: check sequence number */\n\tif (!tcp_sequence(tp, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t/* RFC793, page 37: \"In all states except SYN-SENT, all reset\n\t\t * (RST) segments are validated by checking their SEQ-fields.\"\n\t\t * And page 69: \"If an incoming segment is not acceptable,\n\t\t * an acknowledgment should be sent in reply (unless the RST\n\t\t * bit is set, if so drop the segment and return)\".\n\t\t */\n\t\tif (!th->rst) {\n\t\t\tif (th->syn)\n\t\t\t\tgoto syn_challenge;\n\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDSEQ,\n\t\t\t\t\t\t  &tp->last_oow_ack_time))\n\t\t\t\ttcp_send_dupack(sk, skb);\n\t\t}\n\t\tgoto discard;\n\t}\n\n\t/* Step 2: check RST bit */\n\tif (th->rst) {\n\t\t/* RFC 5961 3.2 :\n\t\t * If sequence number exactly matches RCV.NXT, then\n\t\t *     RESET the connection\n\t\t * else\n\t\t *     Send a challenge ACK\n\t\t */\n\t\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt)\n\t\t\ttcp_reset(sk);\n\t\telse\n\t\t\ttcp_send_challenge_ack(sk, skb);\n\t\tgoto discard;\n\t}\n\n\t/* step 3: check security and precedence [ignored] */\n\n\t/* step 4: Check for a SYN\n\t * RFC 5961 4.2 : Send a challenge ack\n\t */\n\tif (th->syn) {\nsyn_challenge:\n\t\tif (syn_inerr)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNCHALLENGE);\n\t\ttcp_send_challenge_ack(sk, skb);\n\t\tgoto discard;\n\t}\n\n\treturn true;\n\ndiscard:\n\ttcp_drop(sk, skb);\n\treturn false;\n}\n\n/*\n *\tTCP receive function for the ESTABLISHED state.\n *\n *\tIt is split into a fast path and a slow path. The fast path is\n * \tdisabled when:\n *\t- A zero window was announced from us - zero window probing\n *        is only handled properly in the slow path.\n *\t- Out of order segments arrived.\n *\t- Urgent data is expected.\n *\t- There is no buffer space left\n *\t- Unexpected TCP flags/window values/header lengths are received\n *\t  (detected by checking the TCP header against pred_flags)\n *\t- Data is sent in both directions. Fast path only supports pure senders\n *\t  or pure receivers (this means either the sequence number or the ack\n *\t  value must stay constant)\n *\t- Unexpected TCP option.\n *\n *\tWhen these conditions are not satisfied it drops into a standard\n *\treceive procedure patterned after RFC793 to handle all cases.\n *\tThe first three cases are guaranteed by proper pred_flags setting,\n *\tthe rest is checked inline. Fast processing is turned on in\n *\ttcp_data_queue when everything is OK.\n */\nvoid tcp_rcv_established(struct sock *sk, struct sk_buff *skb,\n\t\t\t const struct tcphdr *th, unsigned int len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unlikely(!sk->sk_rx_dst))\n\t\tinet_csk(sk)->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t/*\n\t *\tHeader prediction.\n\t *\tThe code loosely follows the one in the famous\n\t *\t\"30 instruction TCP receive\" Van Jacobson mail.\n\t *\n\t *\tVan's trick is to deposit buffers into socket queue\n\t *\ton a device interrupt, to call tcp_recv function\n\t *\ton the receive process context and checksum and copy\n\t *\tthe buffer to user space. smart...\n\t *\n\t *\tOur current scheme is not silly either but we take the\n\t *\textra cost of the net_bh soft interrupt processing...\n\t *\tWe do checksum and copy also but from device to kernel.\n\t */\n\n\ttp->rx_opt.saw_tstamp = 0;\n\n\t/*\tpred_flags is 0xS?10 << 16 + snd_wnd\n\t *\tif header_prediction is to be made\n\t *\t'S' will always be tp->tcp_header_len >> 2\n\t *\t'?' will be 0 for the fast path, otherwise pred_flags is 0 to\n\t *  turn it off\t(when there are holes in the receive\n\t *\t space for instance)\n\t *\tPSH flag is ignored.\n\t */\n\n\tif ((tcp_flag_word(th) & TCP_HP_BITS) == tp->pred_flags &&\n\t    TCP_SKB_CB(skb)->seq == tp->rcv_nxt &&\n\t    !after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {\n\t\tint tcp_header_len = tp->tcp_header_len;\n\n\t\t/* Timestamp header prediction: tcp_header_len\n\t\t * is automatically equal to th->doff*4 due to pred_flags\n\t\t * match.\n\t\t */\n\n\t\t/* Check timestamp */\n\t\tif (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) {\n\t\t\t/* No? Slow path! */\n\t\t\tif (!tcp_parse_aligned_timestamp(tp, th))\n\t\t\t\tgoto slow_path;\n\n\t\t\t/* If PAWS failed, check it more carefully in slow path */\n\t\t\tif ((s32)(tp->rx_opt.rcv_tsval - tp->rx_opt.ts_recent) < 0)\n\t\t\t\tgoto slow_path;\n\n\t\t\t/* DO NOT update ts_recent here, if checksum fails\n\t\t\t * and timestamp was corrupted part, it will result\n\t\t\t * in a hung connection since we will drop all\n\t\t\t * future packets due to the PAWS test.\n\t\t\t */\n\t\t}\n\n\t\tif (len <= tcp_header_len) {\n\t\t\t/* Bulk data transfer: sender */\n\t\t\tif (len == tcp_header_len) {\n\t\t\t\t/* Predicted packet is in window by definition.\n\t\t\t\t * seq == rcv_nxt and rcv_wup <= rcv_nxt.\n\t\t\t\t * Hence, check seq<=rcv_wup reduces to:\n\t\t\t\t */\n\t\t\t\tif (tcp_header_len ==\n\t\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\t\ttcp_store_ts_recent(tp);\n\n\t\t\t\t/* We know that such packets are checksummed\n\t\t\t\t * on entry.\n\t\t\t\t */\n\t\t\t\ttcp_ack(sk, skb, 0);\n\t\t\t\t__kfree_skb(skb);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\treturn;\n\t\t\t} else { /* Header too small */\n\t\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t} else {\n\t\t\tint eaten = 0;\n\t\t\tbool fragstolen = false;\n\n\t\t\tif (tp->ucopy.task == current &&\n\t\t\t    tp->copied_seq == tp->rcv_nxt &&\n\t\t\t    len - tcp_header_len <= tp->ucopy.len &&\n\t\t\t    sock_owned_by_user(sk)) {\n\t\t\t\t__set_current_state(TASK_RUNNING);\n\n\t\t\t\tif (!tcp_copy_to_iovec(sk, skb, tcp_header_len)) {\n\t\t\t\t\t/* Predicted packet is in window by definition.\n\t\t\t\t\t * seq == rcv_nxt and rcv_wup <= rcv_nxt.\n\t\t\t\t\t * Hence, check seq<=rcv_wup reduces to:\n\t\t\t\t\t */\n\t\t\t\t\tif (tcp_header_len ==\n\t\t\t\t\t    (sizeof(struct tcphdr) +\n\t\t\t\t\t     TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\t\t\ttcp_store_ts_recent(tp);\n\n\t\t\t\t\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t\t\t\t\t__skb_pull(skb, tcp_header_len);\n\t\t\t\t\ttcp_rcv_nxt_update(tp, TCP_SKB_CB(skb)->end_seq);\n\t\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t\t\tLINUX_MIB_TCPHPHITSTOUSER);\n\t\t\t\t\teaten = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!eaten) {\n\t\t\t\tif (tcp_checksum_complete(skb))\n\t\t\t\t\tgoto csum_error;\n\n\t\t\t\tif ((int)skb->truesize > sk->sk_forward_alloc)\n\t\t\t\t\tgoto step5;\n\n\t\t\t\t/* Predicted packet is in window by definition.\n\t\t\t\t * seq == rcv_nxt and rcv_wup <= rcv_nxt.\n\t\t\t\t * Hence, check seq<=rcv_wup reduces to:\n\t\t\t\t */\n\t\t\t\tif (tcp_header_len ==\n\t\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\t\ttcp_store_ts_recent(tp);\n\n\t\t\t\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPHITS);\n\n\t\t\t\t/* Bulk data transfer: receiver */\n\t\t\t\teaten = tcp_queue_rcv(sk, skb, tcp_header_len,\n\t\t\t\t\t\t      &fragstolen);\n\t\t\t}\n\n\t\t\ttcp_event_data_recv(sk, skb);\n\n\t\t\tif (TCP_SKB_CB(skb)->ack_seq != tp->snd_una) {\n\t\t\t\t/* Well, only one small jumplet in fast path... */\n\t\t\t\ttcp_ack(sk, skb, FLAG_DATA);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\tif (!inet_csk_ack_scheduled(sk))\n\t\t\t\t\tgoto no_ack;\n\t\t\t}\n\n\t\t\t__tcp_ack_snd_check(sk, 0);\nno_ack:\n\t\t\tif (eaten)\n\t\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\t\tsk->sk_data_ready(sk);\n\t\t\treturn;\n\t\t}\n\t}\n\nslow_path:\n\tif (len < (th->doff << 2) || tcp_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (!th->ack && !th->rst && !th->syn)\n\t\tgoto discard;\n\n\t/*\n\t *\tStandard slow path.\n\t */\n\n\tif (!tcp_validate_incoming(sk, skb, th, 1))\n\t\treturn;\n\nstep5:\n\tif (tcp_ack(sk, skb, FLAG_SLOWPATH | FLAG_UPDATE_TS_RECENT) < 0)\n\t\tgoto discard;\n\n\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t/* Process urgent data. */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\ttcp_data_queue(sk, skb);\n\n\ttcp_data_snd_check(sk);\n\ttcp_ack_snd_check(sk);\n\treturn;\n\ncsum_error:\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\ndiscard:\n\ttcp_drop(sk, skb);\n}\nEXPORT_SYMBOL(tcp_rcv_established);\n\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_set_state(sk, TCP_ESTABLISHED);\n\n\tif (skb) {\n\t\ticsk->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t\tsecurity_inet_conn_established(sk, skb);\n\t}\n\n\t/* Make sure socket is routed, for correct metrics.  */\n\ticsk->icsk_af_ops->rebuild_header(sk);\n\n\ttcp_init_metrics(sk);\n\n\ttcp_init_congestion_control(sk);\n\n\t/* Prevent spurious tcp_cwnd_restart() on first data\n\t * packet.\n\t */\n\ttp->lsndtime = tcp_time_stamp;\n\n\ttcp_init_buffer_space(sk);\n\n\tif (sock_flag(sk, SOCK_KEEPOPEN))\n\t\tinet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));\n\n\tif (!tp->rx_opt.snd_wscale)\n\t\t__tcp_fast_path_on(tp, tp->snd_wnd);\n\telse\n\t\ttp->pred_flags = 0;\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\t}\n}\n\nstatic bool tcp_rcv_fastopen_synack(struct sock *sk, struct sk_buff *synack,\n\t\t\t\t    struct tcp_fastopen_cookie *cookie)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *data = tp->syn_data ? tcp_write_queue_head(sk) : NULL;\n\tu16 mss = tp->rx_opt.mss_clamp, try_exp = 0;\n\tbool syn_drop = false;\n\n\tif (mss == tp->rx_opt.user_mss) {\n\t\tstruct tcp_options_received opt;\n\n\t\t/* Get original SYNACK MSS value if user MSS sets mss_clamp */\n\t\ttcp_clear_options(&opt);\n\t\topt.user_mss = opt.mss_clamp = 0;\n\t\ttcp_parse_options(synack, &opt, 0, NULL);\n\t\tmss = opt.mss_clamp;\n\t}\n\n\tif (!tp->syn_fastopen) {\n\t\t/* Ignore an unsolicited cookie */\n\t\tcookie->len = -1;\n\t} else if (tp->total_retrans) {\n\t\t/* SYN timed out and the SYN-ACK neither has a cookie nor\n\t\t * acknowledges data. Presumably the remote received only\n\t\t * the retransmitted (regular) SYNs: either the original\n\t\t * SYN-data or the corresponding SYN-ACK was dropped.\n\t\t */\n\t\tsyn_drop = (cookie->len < 0 && data);\n\t} else if (cookie->len < 0 && !tp->syn_data) {\n\t\t/* We requested a cookie but didn't get it. If we did not use\n\t\t * the (old) exp opt format then try so next time (try_exp=1).\n\t\t * Otherwise we go back to use the RFC7413 opt (try_exp=2).\n\t\t */\n\t\ttry_exp = tp->syn_fastopen_exp ? 2 : 1;\n\t}\n\n\ttcp_fastopen_cache_set(sk, mss, cookie, syn_drop, try_exp);\n\n\tif (data) { /* Retransmit unacked data in SYN */\n\t\ttcp_for_write_queue_from(data, sk) {\n\t\t\tif (data == tcp_send_head(sk) ||\n\t\t\t    __tcp_retransmit_skb(sk, data, 1))\n\t\t\t\tbreak;\n\t\t}\n\t\ttcp_rearm_rto(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPFASTOPENACTIVEFAIL);\n\t\treturn true;\n\t}\n\ttp->syn_data_acked = tp->syn_data;\n\tif (tp->syn_data_acked)\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPFASTOPENACTIVE);\n\n\ttcp_fastopen_add_skb(sk, synack);\n\n\treturn false;\n}\n\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t const struct tcphdr *th)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\tint saved_clamp = tp->rx_opt.mss_clamp;\n\n\ttcp_parse_options(skb, &tp->rx_opt, 0, &foc);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\tif (th->ack) {\n\t\t/* rfc793:\n\t\t * \"If the state is SYN-SENT then\n\t\t *    first check the ACK bit\n\t\t *      If the ACK bit is set\n\t\t *\t  If SEG.ACK =< ISS, or SEG.ACK > SND.NXT, send\n\t\t *        a reset (unless the RST bit is set, if so drop\n\t\t *        the segment and return)\"\n\t\t */\n\t\tif (!after(TCP_SKB_CB(skb)->ack_seq, tp->snd_una) ||\n\t\t    after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt))\n\t\t\tgoto reset_and_undo;\n\n\t\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t\t    !between(tp->rx_opt.rcv_tsecr, tp->retrans_stamp,\n\t\t\t     tcp_time_stamp)) {\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_PAWSACTIVEREJECTED);\n\t\t\tgoto reset_and_undo;\n\t\t}\n\n\t\t/* Now ACK is acceptable.\n\t\t *\n\t\t * \"If the RST bit is set\n\t\t *    If the ACK was acceptable then signal the user \"error:\n\t\t *    connection reset\", drop the segment, enter CLOSED state,\n\t\t *    delete TCB, and return.\"\n\t\t */\n\n\t\tif (th->rst) {\n\t\t\ttcp_reset(sk);\n\t\t\tgoto discard;\n\t\t}\n\n\t\t/* rfc793:\n\t\t *   \"fifth, if neither of the SYN or RST bits is set then\n\t\t *    drop the segment and return.\"\n\t\t *\n\t\t *    See note below!\n\t\t *                                        --ANK(990513)\n\t\t */\n\t\tif (!th->syn)\n\t\t\tgoto discard_and_undo;\n\n\t\t/* rfc793:\n\t\t *   \"If the SYN bit is on ...\n\t\t *    are acceptable then ...\n\t\t *    (our SYN has been ACKed), change the connection\n\t\t *    state to ESTABLISHED...\"\n\t\t */\n\n\t\ttcp_ecn_rcv_synack(tp, th);\n\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\t\ttcp_ack(sk, skb, FLAG_SLOWPATH);\n\n\t\t/* Ok.. it's good. Set up sequence numbers and\n\t\t * move to established.\n\t\t */\n\t\ttp->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments is\n\t\t * never scaled.\n\t\t */\n\t\ttp->snd_wnd = ntohs(th->window);\n\n\t\tif (!tp->rx_opt.wscale_ok) {\n\t\t\ttp->rx_opt.snd_wscale = tp->rx_opt.rcv_wscale = 0;\n\t\t\ttp->window_clamp = min(tp->window_clamp, 65535U);\n\t\t}\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok\t   = 1;\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttp->advmss\t    -= TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\tif (tcp_is_sack(tp) && sysctl_tcp_fack)\n\t\t\ttcp_enable_fack(tp);\n\n\t\ttcp_mtup_init(sk);\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\t/* Remember, tcp_poll() does not lock socket!\n\t\t * Change state from SYN-SENT only after copied_seq\n\t\t * is initialized. */\n\t\ttp->copied_seq = tp->rcv_nxt;\n\n\t\tsmp_mb();\n\n\t\ttcp_finish_connect(sk, skb);\n\n\t\tif ((tp->syn_fastopen || tp->syn_data) &&\n\t\t    tcp_rcv_fastopen_synack(sk, skb, &foc))\n\t\t\treturn -1;\n\n\t\tif (sk->sk_write_pending ||\n\t\t    icsk->icsk_accept_queue.rskq_defer_accept ||\n\t\t    icsk->icsk_ack.pingpong) {\n\t\t\t/* Save one ACK. Data will be ready after\n\t\t\t * several ticks, if write_pending is set.\n\t\t\t *\n\t\t\t * It may be deleted, but with this feature tcpdumps\n\t\t\t * look so _wonderfully_ clever, that I was not able\n\t\t\t * to stand against the temptation 8)     --ANK\n\t\t\t */\n\t\t\tinet_csk_schedule_ack(sk);\n\t\t\ticsk->icsk_ack.lrcvtime = tcp_time_stamp;\n\t\t\ttcp_enter_quickack_mode(sk);\n\t\t\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,\n\t\t\t\t\t\t  TCP_DELACK_MAX, TCP_RTO_MAX);\n\ndiscard:\n\t\t\ttcp_drop(sk, skb);\n\t\t\treturn 0;\n\t\t} else {\n\t\t\ttcp_send_ack(sk);\n\t\t}\n\t\treturn -1;\n\t}\n\n\t/* No ACK in the segment */\n\n\tif (th->rst) {\n\t\t/* rfc793:\n\t\t * \"If the RST bit is set\n\t\t *\n\t\t *      Otherwise (no ACK) drop the segment and return.\"\n\t\t */\n\n\t\tgoto discard_and_undo;\n\t}\n\n\t/* PAWS check. */\n\tif (tp->rx_opt.ts_recent_stamp && tp->rx_opt.saw_tstamp &&\n\t    tcp_paws_reject(&tp->rx_opt, 0))\n\t\tgoto discard_and_undo;\n\n\tif (th->syn) {\n\t\t/* We see SYN without ACK. It is attempt of\n\t\t * simultaneous connect with crossed SYNs.\n\t\t * Particularly, it can be connect to self.\n\t\t */\n\t\ttcp_set_state(sk, TCP_SYN_RECV);\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok = 1;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\ttp->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;\n\t\ttp->copied_seq = tp->rcv_nxt;\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments is\n\t\t * never scaled.\n\t\t */\n\t\ttp->snd_wnd    = ntohs(th->window);\n\t\ttp->snd_wl1    = TCP_SKB_CB(skb)->seq;\n\t\ttp->max_window = tp->snd_wnd;\n\n\t\ttcp_ecn_rcv_syn(tp, th);\n\n\t\ttcp_mtup_init(sk);\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\ttcp_send_synack(sk);\n#if 0\n\t\t/* Note, we could accept data and URG from this segment.\n\t\t * There are no obstacles to make this (except that we must\n\t\t * either change tcp_recvmsg() to prevent it from returning data\n\t\t * before 3WHS completes per RFC793, or employ TCP Fast Open).\n\t\t *\n\t\t * However, if we ignore data in ACKless segments sometimes,\n\t\t * we have no reasons to accept it sometimes.\n\t\t * Also, seems the code doing it in step6 of tcp_rcv_state_process\n\t\t * is not flawless. So, discard packet for sanity.\n\t\t * Uncomment this return to process the data.\n\t\t */\n\t\treturn -1;\n#else\n\t\tgoto discard;\n#endif\n\t}\n\t/* \"fifth, if neither of the SYN or RST bits is set then\n\t * drop the segment and return.\"\n\t */\n\ndiscard_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\tgoto discard;\n\nreset_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\treturn 1;\n}\n\n/*\n *\tThis function implements the receiving procedure of RFC 793 for\n *\tall states except ESTABLISHED and TIME_WAIT.\n *\tIt's called from both tcp_v4_rcv and tcp_v6_rcv and should be\n *\taddress independent.\n */\n\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct request_sock *req;\n\tint queued = 0;\n\tbool acceptable;\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\tgoto discard;\n\n\tcase TCP_LISTEN:\n\t\tif (th->ack)\n\t\t\treturn 1;\n\n\t\tif (th->rst)\n\t\t\tgoto discard;\n\n\t\tif (th->syn) {\n\t\t\tif (th->fin)\n\t\t\t\tgoto discard;\n\t\t\tif (icsk->icsk_af_ops->conn_request(sk, skb) < 0)\n\t\t\t\treturn 1;\n\n\t\t\tconsume_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t\tgoto discard;\n\n\tcase TCP_SYN_SENT:\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th);\n\t\tif (queued >= 0)\n\t\t\treturn queued;\n\n\t\t/* Do step6 onward by hand. */\n\t\ttcp_urg(sk, skb, th);\n\t\t__kfree_skb(skb);\n\t\ttcp_data_snd_check(sk);\n\t\treturn 0;\n\t}\n\n\ttp->rx_opt.saw_tstamp = 0;\n\treq = tp->fastopen_rsk;\n\tif (req) {\n\t\tWARN_ON_ONCE(sk->sk_state != TCP_SYN_RECV &&\n\t\t    sk->sk_state != TCP_FIN_WAIT1);\n\n\t\tif (!tcp_check_req(sk, skb, req, true))\n\t\t\tgoto discard;\n\t}\n\n\tif (!th->ack && !th->rst && !th->syn)\n\t\tgoto discard;\n\n\tif (!tcp_validate_incoming(sk, skb, th, 0))\n\t\treturn 0;\n\n\t/* step 5: check the ACK field */\n\tacceptable = tcp_ack(sk, skb, FLAG_SLOWPATH |\n\t\t\t\t      FLAG_UPDATE_TS_RECENT) > 0;\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\t\tif (!acceptable)\n\t\t\treturn 1;\n\n\t\tif (!tp->srtt_us)\n\t\t\ttcp_synack_rtt_meas(sk, req);\n\n\t\t/* Once we leave TCP_SYN_RECV, we no longer need req\n\t\t * so release it.\n\t\t */\n\t\tif (req) {\n\t\t\ttp->total_retrans = req->num_retrans;\n\t\t\treqsk_fastopen_remove(sk, req, false);\n\t\t} else {\n\t\t\t/* Make sure socket is routed, for correct metrics. */\n\t\t\ticsk->icsk_af_ops->rebuild_header(sk);\n\t\t\ttcp_init_congestion_control(sk);\n\n\t\t\ttcp_mtup_init(sk);\n\t\t\ttp->copied_seq = tp->rcv_nxt;\n\t\t\ttcp_init_buffer_space(sk);\n\t\t}\n\t\tsmp_mb();\n\t\ttcp_set_state(sk, TCP_ESTABLISHED);\n\t\tsk->sk_state_change(sk);\n\n\t\t/* Note, that this wakeup is only for marginal crossed SYN case.\n\t\t * Passively open sockets are not waked up, because\n\t\t * sk->sk_sleep == NULL and sk->sk_socket == NULL.\n\t\t */\n\t\tif (sk->sk_socket)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\n\t\ttp->snd_una = TCP_SKB_CB(skb)->ack_seq;\n\t\ttp->snd_wnd = ntohs(th->window) << tp->rx_opt.snd_wscale;\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\n\t\tif (tp->rx_opt.tstamp_ok)\n\t\t\ttp->advmss -= TCPOLEN_TSTAMP_ALIGNED;\n\n\t\tif (req) {\n\t\t\t/* Re-arm the timer because data may have been sent out.\n\t\t\t * This is similar to the regular data transmission case\n\t\t\t * when new data has just been ack'ed.\n\t\t\t *\n\t\t\t * (TFO) - we could try to be more aggressive and\n\t\t\t * retransmitting any data sooner based on when they\n\t\t\t * are sent out.\n\t\t\t */\n\t\t\ttcp_rearm_rto(sk);\n\t\t} else\n\t\t\ttcp_init_metrics(sk);\n\n\t\ttcp_update_pacing_rate(sk);\n\n\t\t/* Prevent spurious tcp_cwnd_restart() on first data packet */\n\t\ttp->lsndtime = tcp_time_stamp;\n\n\t\ttcp_initialize_rcv_mss(sk);\n\t\ttcp_fast_path_on(tp);\n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1: {\n\t\tstruct dst_entry *dst;\n\t\tint tmo;\n\n\t\t/* If we enter the TCP_FIN_WAIT1 state and we are a\n\t\t * Fast Open socket and this is the first acceptable\n\t\t * ACK we have received, this would have acknowledged\n\t\t * our SYNACK so stop the SYNACK timer.\n\t\t */\n\t\tif (req) {\n\t\t\t/* Return RST if ack_seq is invalid.\n\t\t\t * Note that RFC793 only says to generate a\n\t\t\t * DUPACK for it but for TCP Fast Open it seems\n\t\t\t * better to treat this case like TCP_SYN_RECV\n\t\t\t * above.\n\t\t\t */\n\t\t\tif (!acceptable)\n\t\t\t\treturn 1;\n\t\t\t/* We no longer need the request sock. */\n\t\t\treqsk_fastopen_remove(sk, req, false);\n\t\t\ttcp_rearm_rto(sk);\n\t\t}\n\t\tif (tp->snd_una != tp->write_seq)\n\t\t\tbreak;\n\n\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\n\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\n\t\tdst = __sk_dst_get(sk);\n\t\tif (dst)\n\t\t\tdst_confirm(dst);\n\n\t\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\t\t/* Wake up lingering close() */\n\t\t\tsk->sk_state_change(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tp->linger2 < 0 ||\n\t\t    (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t     after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt))) {\n\t\t\ttcp_done(sk);\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\treturn 1;\n\t\t}\n\n\t\ttmo = tcp_fin_time(sk);\n\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\n\t\t} else if (th->fin || sock_owned_by_user(sk)) {\n\t\t\t/* Bad case. We could lose such FIN otherwise.\n\t\t\t * It is not a big problem, but it looks confusing\n\t\t\t * and not so rare event. We still can lose it now,\n\t\t\t * if it spins in bh_lock_sock(), but it is really\n\t\t\t * marginal case.\n\t\t\t */\n\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\n\t\t} else {\n\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\tgoto discard;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase TCP_CLOSING:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\t\tgoto discard;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_LAST_ACK:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_update_metrics(sk);\n\t\t\ttcp_done(sk);\n\t\t\tgoto discard;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/* step 6: check the URG bit */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\tcase TCP_LAST_ACK:\n\t\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))\n\t\t\tbreak;\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_FIN_WAIT2:\n\t\t/* RFC 793 says to queue data in these states,\n\t\t * RFC 1122 says we MUST send a reset.\n\t\t * BSD 4.4 also does reset.\n\t\t */\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\ttcp_reset(sk);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t\t/* Fall through */\n\tcase TCP_ESTABLISHED:\n\t\ttcp_data_queue(sk, skb);\n\t\tqueued = 1;\n\t\tbreak;\n\t}\n\n\t/* tcp_data could move socket to TIME-WAIT */\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\ttcp_data_snd_check(sk);\n\t\ttcp_ack_snd_check(sk);\n\t}\n\n\tif (!queued) {\ndiscard:\n\t\ttcp_drop(sk, skb);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_rcv_state_process);\n\nstatic inline void pr_drop_req(struct request_sock *req, __u16 port, int family)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\tif (family == AF_INET)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI4/%u\\n\",\n\t\t\t\t    &ireq->ir_rmt_addr, port);\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (family == AF_INET6)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI6/%u\\n\",\n\t\t\t\t    &ireq->ir_v6_rmt_addr, port);\n#endif\n}\n\n/* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set\n *\n * If we receive a SYN packet with these bits set, it means a\n * network is playing bad games with TOS bits. In order to\n * avoid possible false congestion notifications, we disable\n * TCP ECN negotiation.\n *\n * Exception: tcp_ca wants ECN. This is required for DCTCP\n * congestion control: Linux DCTCP asserts ECT on all packets,\n * including SYN, which is most optimal solution; however,\n * others, such as FreeBSD do not.\n */\nstatic void tcp_ecn_create_request(struct request_sock *req,\n\t\t\t\t   const struct sk_buff *skb,\n\t\t\t\t   const struct sock *listen_sk,\n\t\t\t\t   const struct dst_entry *dst)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tconst struct net *net = sock_net(listen_sk);\n\tbool th_ecn = th->ece && th->cwr;\n\tbool ect, ecn_ok;\n\tu32 ecn_ok_dst;\n\n\tif (!th_ecn)\n\t\treturn;\n\n\tect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);\n\tecn_ok_dst = dst_feature(dst, DST_FEATURE_ECN_MASK);\n\tecn_ok = net->ipv4.sysctl_tcp_ecn || ecn_ok_dst;\n\n\tif ((!ect && ecn_ok) || tcp_ca_needs_ecn(listen_sk) ||\n\t    (ecn_ok_dst & DST_FEATURE_ECN_CA))\n\t\tinet_rsk(req)->ecn_ok = 1;\n}\n\nstatic void tcp_openreq_init(struct request_sock *req,\n\t\t\t     const struct tcp_options_received *rx_opt,\n\t\t\t     struct sk_buff *skb, const struct sock *sk)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\treq->rsk_rcv_wnd = 0;\t\t/* So that tcp_send_synack() knows! */\n\treq->cookie_ts = 0;\n\ttcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;\n\ttcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;\n\tskb_mstamp_get(&tcp_rsk(req)->snt_synack);\n\ttcp_rsk(req)->last_oow_ack_time = 0;\n\treq->mss = rx_opt->mss_clamp;\n\treq->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;\n\tireq->tstamp_ok = rx_opt->tstamp_ok;\n\tireq->sack_ok = rx_opt->sack_ok;\n\tireq->snd_wscale = rx_opt->snd_wscale;\n\tireq->wscale_ok = rx_opt->wscale_ok;\n\tireq->acked = 0;\n\tireq->ecn_ok = 0;\n\tireq->ir_rmt_port = tcp_hdr(skb)->source;\n\tireq->ir_num = ntohs(tcp_hdr(skb)->dest);\n\tireq->ir_mark = inet_request_mark(sk, skb);\n}\n\nstruct request_sock *inet_reqsk_alloc(const struct request_sock_ops *ops,\n\t\t\t\t      struct sock *sk_listener,\n\t\t\t\t      bool attach_listener)\n{\n\tstruct request_sock *req = reqsk_alloc(ops, sk_listener,\n\t\t\t\t\t       attach_listener);\n\n\tif (req) {\n\t\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\t\tkmemcheck_annotate_bitfield(ireq, flags);\n\t\tireq->opt = NULL;\n\t\tatomic64_set(&ireq->ir_cookie, 0);\n\t\tireq->ireq_state = TCP_NEW_SYN_RECV;\n\t\twrite_pnet(&ireq->ireq_net, sock_net(sk_listener));\n\t\tireq->ireq_family = sk_listener->sk_family;\n\t}\n\n\treturn req;\n}\nEXPORT_SYMBOL(inet_reqsk_alloc);\n\n/*\n * Return true if a syncookie should be sent\n */\nstatic bool tcp_syn_flood_action(const struct sock *sk,\n\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t const char *proto)\n{\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\tconst char *msg = \"Dropping request\";\n\tbool want_cookie = false;\n\tstruct net *net = sock_net(sk);\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (net->ipv4.sysctl_tcp_syncookies) {\n\t\tmsg = \"Sending cookies\";\n\t\twant_cookie = true;\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDOCOOKIES);\n\t} else\n#endif\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDROP);\n\n\tif (!queue->synflood_warned &&\n\t    net->ipv4.sysctl_tcp_syncookies != 2 &&\n\t    xchg(&queue->synflood_warned, 1) == 0)\n\t\tpr_info(\"%s: Possible SYN flooding on port %d. %s.  Check SNMP counters.\\n\",\n\t\t\tproto, ntohs(tcp_hdr(skb)->dest), msg);\n\n\treturn want_cookie;\n}\n\nstatic void tcp_reqsk_record_syn(const struct sock *sk,\n\t\t\t\t struct request_sock *req,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tif (tcp_sk(sk)->save_syn) {\n\t\tu32 len = skb_network_header_len(skb) + tcp_hdrlen(skb);\n\t\tu32 *copy;\n\n\t\tcopy = kmalloc(len + sizeof(u32), GFP_ATOMIC);\n\t\tif (copy) {\n\t\t\tcopy[0] = len;\n\t\t\tmemcpy(&copy[1], skb_network_header(skb), len);\n\t\t\treq->saved_syn = copy;\n\t\t}\n\t}\n}\n\nint tcp_conn_request(struct request_sock_ops *rsk_ops,\n\t\t     const struct tcp_request_sock_ops *af_ops,\n\t\t     struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\t__u32 isn = TCP_SKB_CB(skb)->tcp_tw_isn;\n\tstruct tcp_options_received tmp_opt;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct sock *fastopen_sk = NULL;\n\tstruct dst_entry *dst = NULL;\n\tstruct request_sock *req;\n\tbool want_cookie = false;\n\tstruct flowi fl;\n\n\t/* TW buckets are converted to open requests without\n\t * limitations, they conserve resources and peer is\n\t * evidently real one.\n\t */\n\tif ((net->ipv4.sysctl_tcp_syncookies == 2 ||\n\t     inet_csk_reqsk_queue_is_full(sk)) && !isn) {\n\t\twant_cookie = tcp_syn_flood_action(sk, skb, rsk_ops->slab_name);\n\t\tif (!want_cookie)\n\t\t\tgoto drop;\n\t}\n\n\n\t/* Accept backlog is full. If we have already queued enough\n\t * of warm entries in syn queue, drop request. It is better than\n\t * clogging syn queue with openreqs with exponentially increasing\n\t * timeout.\n\t */\n\tif (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\n\t\tgoto drop;\n\t}\n\n\treq = inet_reqsk_alloc(rsk_ops, sk, !want_cookie);\n\tif (!req)\n\t\tgoto drop;\n\n\ttcp_rsk(req)->af_specific = af_ops;\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = af_ops->mss_clamp;\n\ttmp_opt.user_mss  = tp->rx_opt.user_mss;\n\ttcp_parse_options(skb, &tmp_opt, 0, want_cookie ? NULL : &foc);\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb, sk);\n\n\t/* Note: tcp_v6_init_req() might override ir_iif for link locals */\n\tinet_rsk(req)->ir_iif = inet_request_bound_dev_if(sk, skb);\n\n\taf_ops->init_req(req, sk, skb);\n\n\tif (security_inet_conn_request(sk, skb, req))\n\t\tgoto drop_and_free;\n\n\tif (!want_cookie && !isn) {\n\t\t/* VJ's idea. We save last timestamp seen\n\t\t * from the destination in peer table, when entering\n\t\t * state TIME-WAIT, and check against it before\n\t\t * accepting new connection request.\n\t\t *\n\t\t * If \"isn\" is not zero, this request hit alive\n\t\t * timewait bucket, so that all the necessary checks\n\t\t * are made in the function processing timewait state.\n\t\t */\n\t\tif (tcp_death_row.sysctl_tw_recycle) {\n\t\t\tbool strict;\n\n\t\t\tdst = af_ops->route_req(sk, &fl, req, &strict);\n\n\t\t\tif (dst && strict &&\n\t\t\t    !tcp_peer_is_proven(req, dst, true,\n\t\t\t\t\t\ttmp_opt.saw_tstamp)) {\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSPASSIVEREJECTED);\n\t\t\t\tgoto drop_and_release;\n\t\t\t}\n\t\t}\n\t\t/* Kill the following clause, if you dislike this way. */\n\t\telse if (!net->ipv4.sysctl_tcp_syncookies &&\n\t\t\t (sysctl_max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t\t  (sysctl_max_syn_backlog >> 2)) &&\n\t\t\t !tcp_peer_is_proven(req, dst, false,\n\t\t\t\t\t     tmp_opt.saw_tstamp)) {\n\t\t\t/* Without syncookies last quarter of\n\t\t\t * backlog is filled with destinations,\n\t\t\t * proven to be alive.\n\t\t\t * It means that we continue to communicate\n\t\t\t * to destinations, already remembered\n\t\t\t * to the moment of synflood.\n\t\t\t */\n\t\t\tpr_drop_req(req, ntohs(tcp_hdr(skb)->source),\n\t\t\t\t    rsk_ops->family);\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = af_ops->init_seq(skb);\n\t}\n\tif (!dst) {\n\t\tdst = af_ops->route_req(sk, &fl, req, NULL);\n\t\tif (!dst)\n\t\t\tgoto drop_and_free;\n\t}\n\n\ttcp_ecn_create_request(req, skb, sk, dst);\n\n\tif (want_cookie) {\n\t\tisn = cookie_init_sequence(af_ops, sk, skb, &req->mss);\n\t\treq->cookie_ts = tmp_opt.tstamp_ok;\n\t\tif (!tmp_opt.tstamp_ok)\n\t\t\tinet_rsk(req)->ecn_ok = 0;\n\t}\n\n\ttcp_rsk(req)->snt_isn = isn;\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\ttcp_openreq_init_rwin(req, sk, dst);\n\tif (!want_cookie) {\n\t\ttcp_reqsk_record_syn(sk, req, skb);\n\t\tfastopen_sk = tcp_try_fastopen(sk, skb, req, &foc, dst);\n\t}\n\tif (fastopen_sk) {\n\t\taf_ops->send_synack(fastopen_sk, dst, &fl, req,\n\t\t\t\t    &foc, TCP_SYNACK_FASTOPEN);\n\t\t/* Add the child socket directly into the accept queue */\n\t\tinet_csk_reqsk_queue_add(sk, req, fastopen_sk);\n\t\tsk->sk_data_ready(sk);\n\t\tbh_unlock_sock(fastopen_sk);\n\t\tsock_put(fastopen_sk);\n\t} else {\n\t\ttcp_rsk(req)->tfo_listener = false;\n\t\tif (!want_cookie)\n\t\t\tinet_csk_reqsk_queue_hash_add(sk, req, TCP_TIMEOUT_INIT);\n\t\taf_ops->send_synack(sk, dst, &fl, req, &foc,\n\t\t\t\t    !want_cookie ? TCP_SYNACK_NORMAL :\n\t\t\t\t\t\t   TCP_SYNACK_COOKIE);\n\t\tif (want_cookie) {\n\t\t\treqsk_free(req);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treqsk_put(req);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\treqsk_free(req);\ndrop:\n\ttcp_listendrop(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_conn_request);\n", "code_before": "// SPDX-License-Identifier: GPL-2.0\n/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n */\n\n/*\n * Changes:\n *\t\tPedro Roque\t:\tFast Retransmit/Recovery.\n *\t\t\t\t\tTwo receive queues.\n *\t\t\t\t\tRetransmit queue handled by TCP.\n *\t\t\t\t\tBetter retransmit timer handling.\n *\t\t\t\t\tNew congestion avoidance.\n *\t\t\t\t\tHeader prediction.\n *\t\t\t\t\tVariable renaming.\n *\n *\t\tEric\t\t:\tFast Retransmit.\n *\t\tRandy Scott\t:\tMSS option defines.\n *\t\tEric Schenk\t:\tFixes to slow start algorithm.\n *\t\tEric Schenk\t:\tYet another double ACK bug.\n *\t\tEric Schenk\t:\tDelayed ACK bug fixes.\n *\t\tEric Schenk\t:\tFloyd style fast retrans war avoidance.\n *\t\tDavid S. Miller\t:\tDon't allow zero congestion window.\n *\t\tEric Schenk\t:\tFix retransmitter so that it sends\n *\t\t\t\t\tnext packet on ack of previous packet.\n *\t\tAndi Kleen\t:\tMoved open_request checking here\n *\t\t\t\t\tand process RSTs for open_requests.\n *\t\tAndi Kleen\t:\tBetter prune_queue, and other fixes.\n *\t\tAndrey Savochkin:\tFix RTT measurements in the presence of\n *\t\t\t\t\ttimestamps.\n *\t\tAndrey Savochkin:\tCheck sequence numbers correctly when\n *\t\t\t\t\tremoving SACKs due to in sequence incoming\n *\t\t\t\t\tdata segments.\n *\t\tAndi Kleen:\t\tMake sure we never ack data there is not\n *\t\t\t\t\tenough room for. Also make this condition\n *\t\t\t\t\ta fatal error if it might still happen.\n *\t\tAndi Kleen:\t\tAdd tcp_measure_rcv_mss to make\n *\t\t\t\t\tconnections with MSS<min(MTU,ann. MSS)\n *\t\t\t\t\twork without delayed acks.\n *\t\tAndi Kleen:\t\tProcess packets with PSH set in the\n *\t\t\t\t\tfast path.\n *\t\tJ Hadi Salim:\t\tECN support\n *\t \tAndrei Gurtov,\n *\t\tPasi Sarolahti,\n *\t\tPanu Kuhlberg:\t\tExperimental audit of TCP (re)transmission\n *\t\t\t\t\tengine. Lots of bugs are found.\n *\t\tPasi Sarolahti:\t\tF-RTO for dealing with spurious RTOs\n */\n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/sysctl.h>\n#include <linux/kernel.h>\n#include <linux/prefetch.h>\n#include <linux/bitops.h>\n#include <net/dst.h>\n#include <net/tcp.h>\n#include <net/tcp_ecn.h>\n#include <net/proto_memory.h>\n#include <net/inet_common.h>\n#include <linux/ipsec.h>\n#include <linux/unaligned.h>\n#include <linux/errqueue.h>\n#include <trace/events/tcp.h>\n#include <linux/jump_label_ratelimit.h>\n#include <net/busy_poll.h>\n#include <net/mptcp.h>\n\nint sysctl_tcp_max_orphans __read_mostly = NR_FILE;\n\n#define FLAG_DATA\t\t0x01 /* Incoming frame contained data.\t\t*/\n#define FLAG_WIN_UPDATE\t\t0x02 /* Incoming ACK was a window update.\t*/\n#define FLAG_DATA_ACKED\t\t0x04 /* This ACK acknowledged new data.\t\t*/\n#define FLAG_RETRANS_DATA_ACKED\t0x08 /* \"\" \"\" some of which was retransmitted.\t*/\n#define FLAG_SYN_ACKED\t\t0x10 /* This ACK acknowledged SYN.\t\t*/\n#define FLAG_DATA_SACKED\t0x20 /* New SACK.\t\t\t\t*/\n#define FLAG_ECE\t\t0x40 /* ECE in this ACK\t\t\t\t*/\n#define FLAG_LOST_RETRANS\t0x80 /* This ACK marks some retransmission lost */\n#define FLAG_SLOWPATH\t\t0x100 /* Do not skip RFC checks for window update.*/\n#define FLAG_ORIG_SACK_ACKED\t0x200 /* Never retransmitted data are (s)acked\t*/\n#define FLAG_SND_UNA_ADVANCED\t0x400 /* Snd_una was changed (!= FLAG_DATA_ACKED) */\n#define FLAG_DSACKING_ACK\t0x800 /* SACK blocks contained D-SACK info */\n#define FLAG_SET_XMIT_TIMER\t0x1000 /* Set TLP or RTO timer */\n#define FLAG_SACK_RENEGING\t0x2000 /* snd_una advanced to a sacked seq */\n#define FLAG_UPDATE_TS_RECENT\t0x4000 /* tcp_replace_ts_recent() */\n#define FLAG_NO_CHALLENGE_ACK\t0x8000 /* do not call tcp_send_challenge_ack()\t*/\n#define FLAG_ACK_MAYBE_DELAYED\t0x10000 /* Likely a delayed ACK */\n#define FLAG_DSACK_TLP\t\t0x20000 /* DSACK for tail loss probe */\n#define FLAG_TS_PROGRESS\t0x40000 /* Positive timestamp delta */\n\n#define FLAG_ACKED\t\t(FLAG_DATA_ACKED|FLAG_SYN_ACKED)\n#define FLAG_NOT_DUP\t\t(FLAG_DATA|FLAG_WIN_UPDATE|FLAG_ACKED)\n#define FLAG_CA_ALERT\t\t(FLAG_DATA_SACKED|FLAG_ECE|FLAG_DSACKING_ACK)\n#define FLAG_FORWARD_PROGRESS\t(FLAG_ACKED|FLAG_DATA_SACKED)\n\n#define TCP_REMNANT (TCP_FLAG_FIN|TCP_FLAG_URG|TCP_FLAG_SYN|TCP_FLAG_PSH)\n#define TCP_HP_BITS (~(TCP_RESERVED_BITS|TCP_FLAG_PSH))\n\n#define REXMIT_NONE\t0 /* no loss recovery to do */\n#define REXMIT_LOST\t1 /* retransmit packets marked lost */\n#define REXMIT_NEW\t2 /* FRTO-style transmit of unsent/new packets */\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\nstatic DEFINE_STATIC_KEY_DEFERRED_FALSE(clean_acked_data_enabled, HZ);\n\nvoid clean_acked_data_enable(struct tcp_sock *tp,\n\t\t\t     void (*cad)(struct sock *sk, u32 ack_seq))\n{\n\ttp->tcp_clean_acked = cad;\n\tstatic_branch_deferred_inc(&clean_acked_data_enabled);\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_enable);\n\nvoid clean_acked_data_disable(struct tcp_sock *tp)\n{\n\tstatic_branch_slow_dec_deferred(&clean_acked_data_enabled);\n\ttp->tcp_clean_acked = NULL;\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_disable);\n\nvoid clean_acked_data_flush(void)\n{\n\tstatic_key_deferred_flush(&clean_acked_data_enabled);\n}\nEXPORT_SYMBOL_GPL(clean_acked_data_flush);\n#endif\n\n#ifdef CONFIG_CGROUP_BPF\nstatic void bpf_skops_parse_hdr(struct sock *sk, struct sk_buff *skb)\n{\n\tbool unknown_opt = tcp_sk(sk)->rx_opt.saw_unknown &&\n\t\tBPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk),\n\t\t\t\t       BPF_SOCK_OPS_PARSE_UNKNOWN_HDR_OPT_CB_FLAG);\n\tbool parse_all_opt = BPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk),\n\t\t\t\t\t\t    BPF_SOCK_OPS_PARSE_ALL_HDR_OPT_CB_FLAG);\n\tstruct bpf_sock_ops_kern sock_ops;\n\n\tif (likely(!unknown_opt && !parse_all_opt))\n\t\treturn;\n\n\t/* The skb will be handled in the\n\t * bpf_skops_established() or\n\t * bpf_skops_write_hdr_opt().\n\t */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_SYN_SENT:\n\tcase TCP_LISTEN:\n\t\treturn;\n\t}\n\n\tsock_owned_by_me(sk);\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tsock_ops.op = BPF_SOCK_OPS_PARSE_HDR_OPT_CB;\n\tsock_ops.is_fullsock = 1;\n\tsock_ops.is_locked_tcp_sock = 1;\n\tsock_ops.sk = sk;\n\tbpf_skops_init_skb(&sock_ops, skb, tcp_hdrlen(skb));\n\n\tBPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);\n}\n\nstatic void bpf_skops_established(struct sock *sk, int bpf_op,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tstruct bpf_sock_ops_kern sock_ops;\n\n\tsock_owned_by_me(sk);\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tsock_ops.op = bpf_op;\n\tsock_ops.is_fullsock = 1;\n\tsock_ops.is_locked_tcp_sock = 1;\n\tsock_ops.sk = sk;\n\t/* sk with TCP_REPAIR_ON does not have skb in tcp_finish_connect */\n\tif (skb)\n\t\tbpf_skops_init_skb(&sock_ops, skb, tcp_hdrlen(skb));\n\n\tBPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);\n}\n#else\nstatic void bpf_skops_parse_hdr(struct sock *sk, struct sk_buff *skb)\n{\n}\n\nstatic void bpf_skops_established(struct sock *sk, int bpf_op,\n\t\t\t\t  struct sk_buff *skb)\n{\n}\n#endif\n\nstatic __cold void tcp_gro_dev_warn(const struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t    unsigned int len)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), skb->skb_iif);\n\tif (!dev || len >= READ_ONCE(dev->mtu))\n\t\tpr_warn(\"%s: Driver has suspect GRO implementation, TCP performance may be compromised.\\n\",\n\t\t\tdev ? dev->name : \"Unknown driver\");\n\trcu_read_unlock();\n}\n\n/* Adapt the MSS value used to make delayed ack decision to the\n * real world.\n */\nstatic void tcp_measure_rcv_mss(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst unsigned int lss = icsk->icsk_ack.last_seg_size;\n\tunsigned int len;\n\n\ticsk->icsk_ack.last_seg_size = 0;\n\n\t/* skb->len may jitter because of SACKs, even if peer\n\t * sends good full-sized frames.\n\t */\n\tlen = skb_shinfo(skb)->gso_size ? : skb->len;\n\tif (len >= icsk->icsk_ack.rcv_mss) {\n\t\t/* Note: divides are still a bit expensive.\n\t\t * For the moment, only adjust scaling_ratio\n\t\t * when we update icsk_ack.rcv_mss.\n\t\t */\n\t\tif (unlikely(len != icsk->icsk_ack.rcv_mss)) {\n\t\t\tu64 val = (u64)skb->len << TCP_RMEM_TO_WIN_SCALE;\n\t\t\tu8 old_ratio = tcp_sk(sk)->scaling_ratio;\n\n\t\t\tdo_div(val, skb->truesize);\n\t\t\ttcp_sk(sk)->scaling_ratio = val ? val : 1;\n\n\t\t\tif (old_ratio != tcp_sk(sk)->scaling_ratio) {\n\t\t\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\t\t\tval = tcp_win_from_space(sk, sk->sk_rcvbuf);\n\t\t\t\ttcp_set_window_clamp(sk, val);\n\n\t\t\t\tif (tp->window_clamp < tp->rcvq_space.space)\n\t\t\t\t\ttp->rcvq_space.space = tp->window_clamp;\n\t\t\t}\n\t\t}\n\t\ticsk->icsk_ack.rcv_mss = min_t(unsigned int, len,\n\t\t\t\t\t       tcp_sk(sk)->advmss);\n\t\t/* Account for possibly-removed options */\n\t\tDO_ONCE_LITE_IF(len > icsk->icsk_ack.rcv_mss + MAX_TCP_OPTION_SPACE,\n\t\t\t\ttcp_gro_dev_warn, sk, skb, len);\n\t\t/* If the skb has a len of exactly 1*MSS and has the PSH bit\n\t\t * set then it is likely the end of an application write. So\n\t\t * more data may not be arriving soon, and yet the data sender\n\t\t * may be waiting for an ACK if cwnd-bound or using TX zero\n\t\t * copy. So we set ICSK_ACK_PUSHED here so that\n\t\t * tcp_cleanup_rbuf() will send an ACK immediately if the app\n\t\t * reads all of the data and is not ping-pong. If len > MSS\n\t\t * then this logic does not matter (and does not hurt) because\n\t\t * tcp_cleanup_rbuf() will always ACK immediately if the app\n\t\t * reads data and there is more than an MSS of unACKed data.\n\t\t */\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_PSH)\n\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t} else {\n\t\t/* Otherwise, we make more careful check taking into account,\n\t\t * that SACKs block is variable.\n\t\t *\n\t\t * \"len\" is invariant segment length, including TCP header.\n\t\t */\n\t\tlen += skb->data - skb_transport_header(skb);\n\t\tif (len >= TCP_MSS_DEFAULT + sizeof(struct tcphdr) ||\n\t\t    /* If PSH is not set, packet should be\n\t\t     * full sized, provided peer TCP is not badly broken.\n\t\t     * This observation (if it is correct 8)) allows\n\t\t     * to handle super-low mtu links fairly.\n\t\t     */\n\t\t    (len >= TCP_MIN_MSS + sizeof(struct tcphdr) &&\n\t\t     !(tcp_flag_word(tcp_hdr(skb)) & TCP_REMNANT))) {\n\t\t\t/* Subtract also invariant (if peer is RFC compliant),\n\t\t\t * tcp header plus fixed timestamp option length.\n\t\t\t * Resulting \"len\" is MSS free of SACK jitter.\n\t\t\t */\n\t\t\tlen -= tcp_sk(sk)->tcp_header_len;\n\t\t\ticsk->icsk_ack.last_seg_size = len;\n\t\t\tif (len == lss) {\n\t\t\t\ticsk->icsk_ack.rcv_mss = len;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tif (icsk->icsk_ack.pending & ICSK_ACK_PUSHED)\n\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED2;\n\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t}\n}\n\nstatic void tcp_incr_quickack(struct sock *sk, unsigned int max_quickacks)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tunsigned int quickacks = tcp_sk(sk)->rcv_wnd / (2 * icsk->icsk_ack.rcv_mss);\n\n\tif (quickacks == 0)\n\t\tquickacks = 2;\n\tquickacks = min(quickacks, max_quickacks);\n\tif (quickacks > icsk->icsk_ack.quick)\n\t\ticsk->icsk_ack.quick = quickacks;\n}\n\nstatic void tcp_enter_quickack_mode(struct sock *sk, unsigned int max_quickacks)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_incr_quickack(sk, max_quickacks);\n\tinet_csk_exit_pingpong_mode(sk);\n\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n}\n\n/* Send ACKs quickly, if \"quick\" count is not exhausted\n * and the session is not interactive.\n */\n\nstatic bool tcp_in_quickack_mode(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\treturn icsk->icsk_ack.dst_quick_ack ||\n\t\t(icsk->icsk_ack.quick && !inet_csk_in_pingpong_mode(sk));\n}\n\nstatic void tcp_data_ecn_check(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_ecn_disabled(tp))\n\t\treturn;\n\n\tswitch (TCP_SKB_CB(skb)->ip_dsfield & INET_ECN_MASK) {\n\tcase INET_ECN_NOT_ECT:\n\t\t/* Funny extension: if ECT is not set on a segment,\n\t\t * and we already seen ECT on a previous segment,\n\t\t * it is probably a retransmit.\n\t\t */\n\t\tif (tp->ecn_flags & TCP_ECN_SEEN)\n\t\t\ttcp_enter_quickack_mode(sk, 2);\n\t\tbreak;\n\tcase INET_ECN_CE:\n\t\tif (tcp_ca_needs_ecn(sk))\n\t\t\ttcp_ca_event(sk, CA_EVENT_ECN_IS_CE);\n\n\t\tif (!(tp->ecn_flags & TCP_ECN_DEMAND_CWR) &&\n\t\t    tcp_ecn_mode_rfc3168(tp)) {\n\t\t\t/* Better not delay acks, sender can have a very low cwnd */\n\t\t\ttcp_enter_quickack_mode(sk, 2);\n\t\t\ttp->ecn_flags |= TCP_ECN_DEMAND_CWR;\n\t\t}\n\t\t/* As for RFC3168 ECN, the TCP_ECN_SEEN flag is set by\n\t\t * tcp_data_ecn_check() when the ECN codepoint of\n\t\t * received TCP data contains ECT(0), ECT(1), or CE.\n\t\t */\n\t\tif (!tcp_ecn_mode_rfc3168(tp))\n\t\t\tbreak;\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\tdefault:\n\t\tif (tcp_ca_needs_ecn(sk))\n\t\t\ttcp_ca_event(sk, CA_EVENT_ECN_NO_CE);\n\t\tif (!tcp_ecn_mode_rfc3168(tp))\n\t\t\tbreak;\n\t\ttp->ecn_flags |= TCP_ECN_SEEN;\n\t\tbreak;\n\t}\n}\n\n/* Returns true if the byte counters can be used */\nstatic bool tcp_accecn_process_option(struct tcp_sock *tp,\n\t\t\t\t      const struct sk_buff *skb,\n\t\t\t\t      u32 delivered_bytes, int flag)\n{\n\tu8 estimate_ecnfield = tp->est_ecnfield;\n\tbool ambiguous_ecn_bytes_incr = false;\n\tbool first_changed = false;\n\tunsigned int optlen;\n\tbool order1, res;\n\tunsigned int i;\n\tu8 *ptr;\n\n\tif (tcp_accecn_opt_fail_recv(tp))\n\t\treturn false;\n\n\tif (!(flag & FLAG_SLOWPATH) || !tp->rx_opt.accecn) {\n\t\tif (!tp->saw_accecn_opt) {\n\t\t\t/* Too late to enable after this point due to\n\t\t\t * potential counter wraps\n\t\t\t */\n\t\t\tif (tp->bytes_sent >= (1 << 23) - 1) {\n\t\t\t\tu8 saw_opt = TCP_ACCECN_OPT_FAIL_SEEN;\n\n\t\t\t\ttcp_accecn_saw_opt_fail_recv(tp, saw_opt);\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tif (estimate_ecnfield) {\n\t\t\tu8 ecnfield = estimate_ecnfield - 1;\n\n\t\t\ttp->delivered_ecn_bytes[ecnfield] += delivered_bytes;\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n\tptr = skb_transport_header(skb) + tp->rx_opt.accecn;\n\toptlen = ptr[1] - 2;\n\tif (WARN_ON_ONCE(ptr[0] != TCPOPT_ACCECN0 && ptr[0] != TCPOPT_ACCECN1))\n\t\treturn false;\n\torder1 = (ptr[0] == TCPOPT_ACCECN1);\n\tptr += 2;\n\n\tif (tp->saw_accecn_opt < TCP_ACCECN_OPT_COUNTER_SEEN) {\n\t\ttp->saw_accecn_opt = tcp_accecn_option_init(skb,\n\t\t\t\t\t\t\t    tp->rx_opt.accecn);\n\t\tif (tp->saw_accecn_opt == TCP_ACCECN_OPT_FAIL_SEEN)\n\t\t\ttcp_accecn_fail_mode_set(tp, TCP_ACCECN_OPT_FAIL_RECV);\n\t}\n\n\tres = !!estimate_ecnfield;\n\tfor (i = 0; i < 3; i++) {\n\t\tu32 init_offset;\n\t\tu8 ecnfield;\n\t\ts32 delta;\n\t\tu32 *cnt;\n\n\t\tif (optlen < TCPOLEN_ACCECN_PERFIELD)\n\t\t\tbreak;\n\n\t\tecnfield = tcp_accecn_optfield_to_ecnfield(i, order1);\n\t\tinit_offset = tcp_accecn_field_init_offset(ecnfield);\n\t\tcnt = &tp->delivered_ecn_bytes[ecnfield - 1];\n\t\tdelta = tcp_update_ecn_bytes(cnt, ptr, init_offset);\n\t\tif (delta && delta < 0) {\n\t\t\tres = false;\n\t\t\tambiguous_ecn_bytes_incr = true;\n\t\t}\n\t\tif (delta && ecnfield != estimate_ecnfield) {\n\t\t\tif (!first_changed) {\n\t\t\t\ttp->est_ecnfield = ecnfield;\n\t\t\t\tfirst_changed = true;\n\t\t\t} else {\n\t\t\t\tres = false;\n\t\t\t\tambiguous_ecn_bytes_incr = true;\n\t\t\t}\n\t\t}\n\n\t\toptlen -= TCPOLEN_ACCECN_PERFIELD;\n\t\tptr += TCPOLEN_ACCECN_PERFIELD;\n\t}\n\tif (ambiguous_ecn_bytes_incr)\n\t\ttp->est_ecnfield = 0;\n\n\treturn res;\n}\n\nstatic void tcp_count_delivered_ce(struct tcp_sock *tp, u32 ecn_count)\n{\n\ttp->delivered_ce += ecn_count;\n}\n\n/* Updates the delivered and delivered_ce counts */\nstatic void tcp_count_delivered(struct tcp_sock *tp, u32 delivered,\n\t\t\t\tbool ece_ack)\n{\n\ttp->delivered += delivered;\n\tif (tcp_ecn_mode_rfc3168(tp) && ece_ack)\n\t\ttcp_count_delivered_ce(tp, delivered);\n}\n\n/* Returns the ECN CE delta */\nstatic u32 __tcp_accecn_process(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\tu32 delivered_pkts, u32 delivered_bytes,\n\t\t\t\tint flag)\n{\n\tu32 old_ceb = tcp_sk(sk)->delivered_ecn_bytes[INET_ECN_CE - 1];\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 delta, safe_delta, d_ceb;\n\tbool opt_deltas_valid;\n\tu32 corrected_ace;\n\n\t/* Reordered ACK or uncertain due to lack of data to send and ts */\n\tif (!(flag & (FLAG_FORWARD_PROGRESS | FLAG_TS_PROGRESS)))\n\t\treturn 0;\n\n\topt_deltas_valid = tcp_accecn_process_option(tp, skb,\n\t\t\t\t\t\t     delivered_bytes, flag);\n\n\tif (!(flag & FLAG_SLOWPATH)) {\n\t\t/* AccECN counter might overflow on large ACKs */\n\t\tif (delivered_pkts <= TCP_ACCECN_CEP_ACE_MASK)\n\t\t\treturn 0;\n\t}\n\n\t/* ACE field is not available during handshake */\n\tif (flag & FLAG_SYN_ACKED)\n\t\treturn 0;\n\n\tif (tp->received_ce_pending >= TCP_ACCECN_ACE_MAX_DELTA)\n\t\tinet_csk(sk)->icsk_ack.pending |= ICSK_ACK_NOW;\n\n\tcorrected_ace = tcp_accecn_ace(th) - TCP_ACCECN_CEP_INIT_OFFSET;\n\tdelta = (corrected_ace - tp->delivered_ce) & TCP_ACCECN_CEP_ACE_MASK;\n\tif (delivered_pkts <= TCP_ACCECN_CEP_ACE_MASK)\n\t\treturn delta;\n\n\tsafe_delta = delivered_pkts -\n\t\t     ((delivered_pkts - delta) & TCP_ACCECN_CEP_ACE_MASK);\n\n\tif (opt_deltas_valid) {\n\t\td_ceb = tp->delivered_ecn_bytes[INET_ECN_CE - 1] - old_ceb;\n\t\tif (!d_ceb)\n\t\t\treturn delta;\n\n\t\tif ((delivered_pkts >= (TCP_ACCECN_CEP_ACE_MASK + 1) * 2) &&\n\t\t    (tcp_is_sack(tp) ||\n\t\t     ((1 << inet_csk(sk)->icsk_ca_state) &\n\t\t      (TCPF_CA_Open | TCPF_CA_CWR)))) {\n\t\t\tu32 est_d_cep;\n\n\t\t\tif (delivered_bytes <= d_ceb)\n\t\t\t\treturn safe_delta;\n\n\t\t\test_d_cep = DIV_ROUND_UP_ULL((u64)d_ceb *\n\t\t\t\t\t\t     delivered_pkts,\n\t\t\t\t\t\t     delivered_bytes);\n\t\t\treturn min(safe_delta,\n\t\t\t\t   delta +\n\t\t\t\t   (est_d_cep & ~TCP_ACCECN_CEP_ACE_MASK));\n\t\t}\n\n\t\tif (d_ceb > delta * tp->mss_cache)\n\t\t\treturn safe_delta;\n\t\tif (d_ceb <\n\t\t    safe_delta * tp->mss_cache >> TCP_ACCECN_SAFETY_SHIFT)\n\t\t\treturn delta;\n\t}\n\n\treturn safe_delta;\n}\n\nstatic u32 tcp_accecn_process(struct sock *sk, const struct sk_buff *skb,\n\t\t\t      u32 delivered_pkts, u32 delivered_bytes,\n\t\t\t      int *flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 delta;\n\n\tdelta = __tcp_accecn_process(sk, skb, delivered_pkts,\n\t\t\t\t     delivered_bytes, *flag);\n\tif (delta > 0) {\n\t\ttcp_count_delivered_ce(tp, delta);\n\t\t*flag |= FLAG_ECE;\n\t\t/* Recalculate header predictor */\n\t\tif (tp->pred_flags)\n\t\t\ttcp_fast_path_on(tp);\n\t}\n\treturn delta;\n}\n\n/* Buffer size and advertised window tuning.\n *\n * 1. Tuning sk->sk_sndbuf, when connection enters established state.\n */\n\nstatic void tcp_sndbuf_expand(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tint sndmem, per_mss;\n\tu32 nr_segs;\n\n\t/* Worst case is non GSO/TSO : each frame consumes one skb\n\t * and skb->head is kmalloced using power of two area of memory\n\t */\n\tper_mss = max_t(u32, tp->rx_opt.mss_clamp, tp->mss_cache) +\n\t\t  MAX_TCP_HEADER +\n\t\t  SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tper_mss = roundup_pow_of_two(per_mss) +\n\t\t  SKB_DATA_ALIGN(sizeof(struct sk_buff));\n\n\tnr_segs = max_t(u32, TCP_INIT_CWND, tcp_snd_cwnd(tp));\n\tnr_segs = max_t(u32, nr_segs, tp->reordering + 1);\n\n\t/* Fast Recovery (RFC 5681 3.2) :\n\t * Cubic needs 1.7 factor, rounded to 2 to include\n\t * extra cushion (application might react slowly to EPOLLOUT)\n\t */\n\tsndmem = ca_ops->sndbuf_expand ? ca_ops->sndbuf_expand(sk) : 2;\n\tsndmem *= nr_segs * per_mss;\n\n\tif (sk->sk_sndbuf < sndmem)\n\t\tWRITE_ONCE(sk->sk_sndbuf,\n\t\t\t   min(sndmem, READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_wmem[2])));\n}\n\n/* 2. Tuning advertised window (window_clamp, rcv_ssthresh)\n *\n * All tcp_full_space() is split to two parts: \"network\" buffer, allocated\n * forward and advertised in receiver window (tp->rcv_wnd) and\n * \"application buffer\", required to isolate scheduling/application\n * latencies from network.\n * window_clamp is maximal advertised window. It can be less than\n * tcp_full_space(), in this case tcp_full_space() - window_clamp\n * is reserved for \"application\" buffer. The less window_clamp is\n * the smoother our behaviour from viewpoint of network, but the lower\n * throughput and the higher sensitivity of the connection to losses. 8)\n *\n * rcv_ssthresh is more strict window_clamp used at \"slow start\"\n * phase to predict further behaviour of this connection.\n * It is used for two goals:\n * - to enforce header prediction at sender, even when application\n *   requires some significant \"application buffer\". It is check #1.\n * - to prevent pruning of receive queue because of misprediction\n *   of receiver window. Check #2.\n *\n * The scheme does not work when sender sends good segments opening\n * window and then starts to feed us spaghetti. But it should work\n * in common situations. Otherwise, we have to rely on queue collapsing.\n */\n\n/* Slow part of check#2. */\nstatic int __tcp_grow_window(const struct sock *sk, const struct sk_buff *skb,\n\t\t\t     unsigned int skbtruesize)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t/* Optimize this! */\n\tint truesize = tcp_win_from_space(sk, skbtruesize) >> 1;\n\tint window = tcp_win_from_space(sk, READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_rmem[2])) >> 1;\n\n\twhile (tp->rcv_ssthresh <= window) {\n\t\tif (truesize <= skb->len)\n\t\t\treturn 2 * inet_csk(sk)->icsk_ack.rcv_mss;\n\n\t\ttruesize >>= 1;\n\t\twindow >>= 1;\n\t}\n\treturn 0;\n}\n\n/* Even if skb appears to have a bad len/truesize ratio, TCP coalescing\n * can play nice with us, as sk_buff and skb->head might be either\n * freed or shared with up to MAX_SKB_FRAGS segments.\n * Only give a boost to drivers using page frag(s) to hold the frame(s),\n * and if no payload was pulled in skb->head before reaching us.\n */\nstatic u32 truesize_adjust(bool adjust, const struct sk_buff *skb)\n{\n\tu32 truesize = skb->truesize;\n\n\tif (adjust && !skb_headlen(skb)) {\n\t\ttruesize -= SKB_TRUESIZE(skb_end_offset(skb));\n\t\t/* paranoid check, some drivers might be buggy */\n\t\tif (unlikely((int)truesize < (int)skb->len))\n\t\t\ttruesize = skb->truesize;\n\t}\n\treturn truesize;\n}\n\nstatic void tcp_grow_window(struct sock *sk, const struct sk_buff *skb,\n\t\t\t    bool adjust)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint room;\n\n\troom = min_t(int, tp->window_clamp, tcp_space(sk)) - tp->rcv_ssthresh;\n\n\tif (room <= 0)\n\t\treturn;\n\n\t/* Check #1 */\n\tif (!tcp_under_memory_pressure(sk)) {\n\t\tunsigned int truesize = truesize_adjust(adjust, skb);\n\t\tint incr;\n\n\t\t/* Check #2. Increase window, if skb with such overhead\n\t\t * will fit to rcvbuf in future.\n\t\t */\n\t\tif (tcp_win_from_space(sk, truesize) <= skb->len)\n\t\t\tincr = 2 * tp->advmss;\n\t\telse\n\t\t\tincr = __tcp_grow_window(sk, skb, truesize);\n\n\t\tif (incr) {\n\t\t\tincr = max_t(int, incr, 2 * skb->len);\n\t\t\ttp->rcv_ssthresh += min(room, incr);\n\t\t\tinet_csk(sk)->icsk_ack.quick |= 1;\n\t\t}\n\t} else {\n\t\t/* Under pressure:\n\t\t * Adjust rcv_ssthresh according to reserved mem\n\t\t */\n\t\ttcp_adjust_rcv_ssthresh(sk);\n\t}\n}\n\n/* 3. Try to fixup all. It is made immediately after connection enters\n *    established state.\n */\nstatic void tcp_init_buffer_space(struct sock *sk)\n{\n\tint tcp_app_win = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_app_win);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint maxwin;\n\n\tif (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK))\n\t\ttcp_sndbuf_expand(sk);\n\n\ttcp_mstamp_refresh(tp);\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n\ttp->rcvq_space.seq = tp->copied_seq;\n\n\tmaxwin = tcp_full_space(sk);\n\n\tif (tp->window_clamp >= maxwin) {\n\t\tWRITE_ONCE(tp->window_clamp, maxwin);\n\n\t\tif (tcp_app_win && maxwin > 4 * tp->advmss)\n\t\t\tWRITE_ONCE(tp->window_clamp,\n\t\t\t\t   max(maxwin - (maxwin >> tcp_app_win),\n\t\t\t\t       4 * tp->advmss));\n\t}\n\n\t/* Force reservation of one segment. */\n\tif (tcp_app_win &&\n\t    tp->window_clamp > 2 * tp->advmss &&\n\t    tp->window_clamp + tp->advmss > maxwin)\n\t\tWRITE_ONCE(tp->window_clamp,\n\t\t\t   max(2 * tp->advmss, maxwin - tp->advmss));\n\n\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, tp->window_clamp);\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->rcvq_space.space = min3(tp->rcv_ssthresh, tp->rcv_wnd,\n\t\t\t\t    (u32)TCP_INIT_CWND * tp->advmss);\n}\n\n/* 4. Recalculate window clamp after socket hit its memory bounds. */\nstatic void tcp_clamp_window(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\tint rmem2;\n\n\ticsk->icsk_ack.quick = 0;\n\trmem2 = READ_ONCE(net->ipv4.sysctl_tcp_rmem[2]);\n\n\tif (sk->sk_rcvbuf < rmem2 &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK) &&\n\t    !tcp_under_memory_pressure(sk) &&\n\t    sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)) {\n\t\tWRITE_ONCE(sk->sk_rcvbuf,\n\t\t\t   min(atomic_read(&sk->sk_rmem_alloc), rmem2));\n\t}\n\tif (atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf)\n\t\ttp->rcv_ssthresh = min(tp->window_clamp, 2U * tp->advmss);\n}\n\n/* Initialize RCV_MSS value.\n * RCV_MSS is an our guess about MSS used by the peer.\n * We haven't any direct information about the MSS.\n * It's better to underestimate the RCV_MSS rather than overestimate.\n * Overestimations make us ACKing less frequently than needed.\n * Underestimations are more easy to detect and fix by tcp_measure_rcv_mss().\n */\nvoid tcp_initialize_rcv_mss(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int hint = min_t(unsigned int, tp->advmss, tp->mss_cache);\n\n\thint = min(hint, tp->rcv_wnd / 2);\n\thint = min(hint, TCP_MSS_DEFAULT);\n\thint = max(hint, TCP_MIN_MSS);\n\n\tinet_csk(sk)->icsk_ack.rcv_mss = hint;\n}\nEXPORT_IPV6_MOD(tcp_initialize_rcv_mss);\n\n/* Receiver \"autotuning\" code.\n *\n * The algorithm for RTT estimation w/o timestamps is based on\n * Dynamic Right-Sizing (DRS) by Wu Feng and Mike Fisk of LANL.\n * <https://public.lanl.gov/radiant/pubs.html#DRS>\n *\n * More detail on this code can be found at\n * <http://staff.psc.edu/jheffner/>,\n * though this reference is out of date.  A new paper\n * is pending.\n */\nstatic void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)\n{\n\tu32 new_sample, old_sample = tp->rcv_rtt_est.rtt_us;\n\tlong m = sample << 3;\n\n\tif (old_sample == 0 || m < old_sample) {\n\t\tnew_sample = m;\n\t} else {\n\t\t/* If we sample in larger samples in the non-timestamp\n\t\t * case, we could grossly overestimate the RTT especially\n\t\t * with chatty applications or bulk transfer apps which\n\t\t * are stalled on filesystem I/O.\n\t\t *\n\t\t * Also, since we are only going for a minimum in the\n\t\t * non-timestamp case, we do not smooth things out\n\t\t * else with timestamps disabled convergence takes too\n\t\t * long.\n\t\t */\n\t\tif (win_dep)\n\t\t\treturn;\n\t\t/* Do not use this sample if receive queue is not empty. */\n\t\tif (tp->rcv_nxt != tp->copied_seq)\n\t\t\treturn;\n\t\tnew_sample = old_sample - (old_sample >> 3) + sample;\n\t}\n\n\ttp->rcv_rtt_est.rtt_us = new_sample;\n}\n\nstatic inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)\n{\n\tu32 delta_us;\n\n\tif (tp->rcv_rtt_est.time == 0)\n\t\tgoto new_measure;\n\tif (before(tp->rcv_nxt, tp->rcv_rtt_est.seq))\n\t\treturn;\n\tdelta_us = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcv_rtt_est.time);\n\tif (!delta_us)\n\t\tdelta_us = 1;\n\ttcp_rcv_rtt_update(tp, delta_us, 1);\n\nnew_measure:\n\ttp->rcv_rtt_est.seq = tp->rcv_nxt + tp->rcv_wnd;\n\ttp->rcv_rtt_est.time = tp->tcp_mstamp;\n}\n\nstatic s32 tcp_rtt_tsopt_us(const struct tcp_sock *tp, u32 min_delta)\n{\n\tu32 delta, delta_us;\n\n\tdelta = tcp_time_stamp_ts(tp) - tp->rx_opt.rcv_tsecr;\n\tif (tp->tcp_usec_ts)\n\t\treturn delta;\n\n\tif (likely(delta < INT_MAX / (USEC_PER_SEC / TCP_TS_HZ))) {\n\t\tif (!delta)\n\t\t\tdelta = min_delta;\n\t\tdelta_us = delta * (USEC_PER_SEC / TCP_TS_HZ);\n\t\treturn delta_us;\n\t}\n\treturn -1;\n}\n\nstatic inline void tcp_rcv_rtt_measure_ts(struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->rx_opt.rcv_tsecr == tp->rcv_rtt_last_tsecr)\n\t\treturn;\n\ttp->rcv_rtt_last_tsecr = tp->rx_opt.rcv_tsecr;\n\n\tif (TCP_SKB_CB(skb)->end_seq -\n\t    TCP_SKB_CB(skb)->seq >= inet_csk(sk)->icsk_ack.rcv_mss) {\n\t\ts32 delta = tcp_rtt_tsopt_us(tp, 0);\n\n\t\tif (delta > 0)\n\t\t\ttcp_rcv_rtt_update(tp, delta, 0);\n\t}\n}\n\nvoid tcp_rcvbuf_grow(struct sock *sk)\n{\n\tconst struct net *net = sock_net(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint rcvwin, rcvbuf, cap;\n\n\tif (!READ_ONCE(net->ipv4.sysctl_tcp_moderate_rcvbuf) ||\n\t    (sk->sk_userlocks & SOCK_RCVBUF_LOCK))\n\t\treturn;\n\n\t/* slow start: allow the sender to double its rate. */\n\trcvwin = tp->rcvq_space.space << 1;\n\n\tif (!RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\trcvwin += TCP_SKB_CB(tp->ooo_last_skb)->end_seq - tp->rcv_nxt;\n\n\tcap = READ_ONCE(net->ipv4.sysctl_tcp_rmem[2]);\n\n\trcvbuf = min_t(u32, tcp_space_from_win(sk, rcvwin), cap);\n\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\tWRITE_ONCE(sk->sk_rcvbuf, rcvbuf);\n\t\t/* Make the window clamp follow along.  */\n\t\tWRITE_ONCE(tp->window_clamp,\n\t\t\t   tcp_win_from_space(sk, rcvbuf));\n\t}\n}\n/*\n * This function should be called every time data is copied to user space.\n * It calculates the appropriate TCP receive buffer space.\n */\nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint time, inq, copied;\n\n\ttrace_tcp_rcv_space_adjust(sk);\n\n\ttcp_mstamp_refresh(tp);\n\ttime = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcvq_space.time);\n\tif (time < (tp->rcv_rtt_est.rtt_us >> 3) || tp->rcv_rtt_est.rtt_us == 0)\n\t\treturn;\n\n\t/* Number of bytes copied to user in last RTT */\n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\t/* Number of bytes in receive queue. */\n\tinq = tp->rcv_nxt - tp->copied_seq;\n\tcopied -= inq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\ttrace_tcp_rcvbuf_grow(sk, time);\n\n\ttp->rcvq_space.space = copied;\n\n\ttcp_rcvbuf_grow(sk);\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n}\n\nstatic void tcp_save_lrcv_flowlabel(struct sock *sk, const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (skb->protocol == htons(ETH_P_IPV6))\n\t\ticsk->icsk_ack.lrcv_flowlabel = ntohl(ip6_flowlabel(ipv6_hdr(skb)));\n#endif\n}\n\n/* There is something which you must keep in mind when you analyze the\n * behavior of the tp->ato delayed ack timeout interval.  When a\n * connection starts up, we want to ack as quickly as possible.  The\n * problem is that \"good\" TCP's do slow start at the beginning of data\n * transmission.  The means that until we send the first few ACK's the\n * sender will sit on his end and only queue most of his data, because\n * he can only send snd_cwnd unacked packets at any given time.  For\n * each ACK we send, he increments snd_cwnd and transmits more of his\n * queue.  -DaveM\n */\nstatic void tcp_event_data_recv(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now;\n\n\tinet_csk_schedule_ack(sk);\n\n\ttcp_measure_rcv_mss(sk, skb);\n\n\ttcp_rcv_rtt_measure(tp);\n\n\tnow = tcp_jiffies32;\n\n\tif (!icsk->icsk_ack.ato) {\n\t\t/* The _first_ data packet received, initialize\n\t\t * delayed ACK engine.\n\t\t */\n\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\n\t\ticsk->icsk_ack.ato = TCP_ATO_MIN;\n\t} else {\n\t\tint m = now - icsk->icsk_ack.lrcvtime;\n\n\t\tif (m <= TCP_ATO_MIN / 2) {\n\t\t\t/* The fastest case is the first. */\n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + TCP_ATO_MIN / 2;\n\t\t} else if (m < icsk->icsk_ack.ato) {\n\t\t\ticsk->icsk_ack.ato = (icsk->icsk_ack.ato >> 1) + m;\n\t\t\tif (icsk->icsk_ack.ato > icsk->icsk_rto)\n\t\t\t\ticsk->icsk_ack.ato = icsk->icsk_rto;\n\t\t} else if (m > icsk->icsk_rto) {\n\t\t\t/* Too long gap. Apparently sender failed to\n\t\t\t * restart window, so that we send ACKs quickly.\n\t\t\t */\n\t\t\ttcp_incr_quickack(sk, TCP_MAX_QUICKACKS);\n\t\t}\n\t}\n\ticsk->icsk_ack.lrcvtime = now;\n\ttcp_save_lrcv_flowlabel(sk, skb);\n\n\ttcp_data_ecn_check(sk, skb);\n\n\tif (skb->len >= 128)\n\t\ttcp_grow_window(sk, skb, true);\n}\n\n/* Called to compute a smoothed rtt estimate. The data fed to this\n * routine either comes from timestamps, or from segments that were\n * known _not_ to have been retransmitted [see Karn/Partridge\n * Proceedings SIGCOMM 87]. The algorithm is from the SIGCOMM 88\n * piece by Van Jacobson.\n * NOTE: the next three routines used to be one big routine.\n * To save cycles in the RFC 1323 implementation it was better to break\n * it up into three procedures. -- erics\n */\nstatic void tcp_rtt_estimator(struct sock *sk, long mrtt_us)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tlong m = mrtt_us; /* RTT */\n\tu32 srtt = tp->srtt_us;\n\n\t/*\tThe following amusing code comes from Jacobson's\n\t *\tarticle in SIGCOMM '88.  Note that rtt and mdev\n\t *\tare scaled versions of rtt and mean deviation.\n\t *\tThis is designed to be as fast as possible\n\t *\tm stands for \"measurement\".\n\t *\n\t *\tOn a 1990 paper the rto value is changed to:\n\t *\tRTO = rtt + 4 * mdev\n\t *\n\t * Funny. This algorithm seems to be very broken.\n\t * These formulae increase RTO, when it should be decreased, increase\n\t * too slowly, when it should be increased quickly, decrease too quickly\n\t * etc. I guess in BSD RTO takes ONE value, so that it is absolutely\n\t * does not matter how to _calculate_ it. Seems, it was trap\n\t * that VJ failed to avoid. 8)\n\t */\n\tif (srtt != 0) {\n\t\tm -= (srtt >> 3);\t/* m is now error in rtt est */\n\t\tsrtt += m;\t\t/* rtt = 7/8 rtt + 1/8 new */\n\t\tif (m < 0) {\n\t\t\tm = -m;\t\t/* m is now abs(error) */\n\t\t\tm -= (tp->mdev_us >> 2);   /* similar update on mdev */\n\t\t\t/* This is similar to one of Eifel findings.\n\t\t\t * Eifel blocks mdev updates when rtt decreases.\n\t\t\t * This solution is a bit different: we use finer gain\n\t\t\t * for mdev in this case (alpha*beta).\n\t\t\t * Like Eifel it also prevents growth of rto,\n\t\t\t * but also it limits too fast rto decreases,\n\t\t\t * happening in pure Eifel.\n\t\t\t */\n\t\t\tif (m > 0)\n\t\t\t\tm >>= 3;\n\t\t} else {\n\t\t\tm -= (tp->mdev_us >> 2);   /* similar update on mdev */\n\t\t}\n\t\ttp->mdev_us += m;\t\t/* mdev = 3/4 mdev + 1/4 new */\n\t\tif (tp->mdev_us > tp->mdev_max_us) {\n\t\t\ttp->mdev_max_us = tp->mdev_us;\n\t\t\tif (tp->mdev_max_us > tp->rttvar_us)\n\t\t\t\ttp->rttvar_us = tp->mdev_max_us;\n\t\t}\n\t\tif (after(tp->snd_una, tp->rtt_seq)) {\n\t\t\tif (tp->mdev_max_us < tp->rttvar_us)\n\t\t\t\ttp->rttvar_us -= (tp->rttvar_us - tp->mdev_max_us) >> 2;\n\t\t\ttp->rtt_seq = tp->snd_nxt;\n\t\t\ttp->mdev_max_us = tcp_rto_min_us(sk);\n\n\t\t\ttcp_bpf_rtt(sk, mrtt_us, srtt);\n\t\t}\n\t} else {\n\t\t/* no previous measure. */\n\t\tsrtt = m << 3;\t\t/* take the measured time to be rtt */\n\t\ttp->mdev_us = m << 1;\t/* make sure rto = 3*rtt */\n\t\ttp->rttvar_us = max(tp->mdev_us, tcp_rto_min_us(sk));\n\t\ttp->mdev_max_us = tp->rttvar_us;\n\t\ttp->rtt_seq = tp->snd_nxt;\n\n\t\ttcp_bpf_rtt(sk, mrtt_us, srtt);\n\t}\n\ttp->srtt_us = max(1U, srtt);\n}\n\nstatic void tcp_update_pacing_rate(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tu64 rate;\n\n\t/* set sk_pacing_rate to 200 % of current rate (mss * cwnd / srtt) */\n\trate = (u64)tp->mss_cache * ((USEC_PER_SEC / 100) << 3);\n\n\t/* current rate is (cwnd * mss) / srtt\n\t * In Slow Start [1], set sk_pacing_rate to 200 % the current rate.\n\t * In Congestion Avoidance phase, set it to 120 % the current rate.\n\t *\n\t * [1] : Normal Slow Start condition is (tp->snd_cwnd < tp->snd_ssthresh)\n\t *\t If snd_cwnd >= (tp->snd_ssthresh / 2), we are approaching\n\t *\t end of slow start and should slow down.\n\t */\n\tif (tcp_snd_cwnd(tp) < tp->snd_ssthresh / 2)\n\t\trate *= READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_pacing_ss_ratio);\n\telse\n\t\trate *= READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_pacing_ca_ratio);\n\n\trate *= max(tcp_snd_cwnd(tp), tp->packets_out);\n\n\tif (likely(tp->srtt_us))\n\t\tdo_div(rate, tp->srtt_us);\n\n\t/* WRITE_ONCE() is needed because sch_fq fetches sk_pacing_rate\n\t * without any lock. We want to make sure compiler wont store\n\t * intermediate values in this location.\n\t */\n\tWRITE_ONCE(sk->sk_pacing_rate,\n\t\t   min_t(u64, rate, READ_ONCE(sk->sk_max_pacing_rate)));\n}\n\n/* Calculate rto without backoff.  This is the second half of Van Jacobson's\n * routine referred to above.\n */\nstatic void tcp_set_rto(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\t/* Old crap is replaced with new one. 8)\n\t *\n\t * More seriously:\n\t * 1. If rtt variance happened to be less 50msec, it is hallucination.\n\t *    It cannot be less due to utterly erratic ACK generation made\n\t *    at least by solaris and freebsd. \"Erratic ACKs\" has _nothing_\n\t *    to do with delayed acks, because at cwnd>2 true delack timeout\n\t *    is invisible. Actually, Linux-2.4 also generates erratic\n\t *    ACKs in some circumstances.\n\t */\n\tinet_csk(sk)->icsk_rto = __tcp_set_rto(tp);\n\n\t/* 2. Fixups made earlier cannot be right.\n\t *    If we do not estimate RTO correctly without them,\n\t *    all the algo is pure shit and should be replaced\n\t *    with correct one. It is exactly, which we pretend to do.\n\t */\n\n\t/* NOTE: clamping at TCP_RTO_MIN is not required, current algo\n\t * guarantees that rto is higher.\n\t */\n\ttcp_bound_rto(sk);\n}\n\n__u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst)\n{\n\t__u32 cwnd = (dst ? dst_metric(dst, RTAX_INITCWND) : 0);\n\n\tif (!cwnd)\n\t\tcwnd = TCP_INIT_CWND;\n\treturn min_t(__u32, cwnd, tp->snd_cwnd_clamp);\n}\n\nstruct tcp_sacktag_state {\n\t/* Timestamps for earliest and latest never-retransmitted segment\n\t * that was SACKed. RTO needs the earliest RTT to stay conservative,\n\t * but congestion control should still get an accurate delay signal.\n\t */\n\tu64\tfirst_sackt;\n\tu64\tlast_sackt;\n\tu32\treord;\n\tu32\tsack_delivered;\n\tu32\tdelivered_bytes;\n\tint\tflag;\n\tunsigned int mss_now;\n\tstruct rate_sample *rate;\n};\n\n/* Take a notice that peer is sending D-SACKs. Skip update of data delivery\n * and spurious retransmission information if this DSACK is unlikely caused by\n * sender's action:\n * - DSACKed sequence range is larger than maximum receiver's window.\n * - Total no. of DSACKed segments exceed the total no. of retransmitted segs.\n */\nstatic u32 tcp_dsack_seen(struct tcp_sock *tp, u32 start_seq,\n\t\t\t  u32 end_seq, struct tcp_sacktag_state *state)\n{\n\tu32 seq_len, dup_segs = 1;\n\n\tif (!before(start_seq, end_seq))\n\t\treturn 0;\n\n\tseq_len = end_seq - start_seq;\n\t/* Dubious DSACK: DSACKed range greater than maximum advertised rwnd */\n\tif (seq_len > tp->max_window)\n\t\treturn 0;\n\tif (seq_len > tp->mss_cache)\n\t\tdup_segs = DIV_ROUND_UP(seq_len, tp->mss_cache);\n\telse if (tp->tlp_high_seq && tp->tlp_high_seq == end_seq)\n\t\tstate->flag |= FLAG_DSACK_TLP;\n\n\ttp->dsack_dups += dup_segs;\n\t/* Skip the DSACK if dup segs weren't retransmitted by sender */\n\tif (tp->dsack_dups > tp->total_retrans)\n\t\treturn 0;\n\n\ttp->rx_opt.sack_ok |= TCP_DSACK_SEEN;\n\t/* We increase the RACK ordering window in rounds where we receive\n\t * DSACKs that may have been due to reordering causing RACK to trigger\n\t * a spurious fast recovery. Thus RACK ignores DSACKs that happen\n\t * without having seen reordering, or that match TLP probes (TLP\n\t * is timer-driven, not triggered by RACK).\n\t */\n\tif (tp->reord_seen && !(state->flag & FLAG_DSACK_TLP))\n\t\ttp->rack.dsack_seen = 1;\n\n\tstate->flag |= FLAG_DSACKING_ACK;\n\t/* A spurious retransmission is delivered */\n\tstate->sack_delivered += dup_segs;\n\n\treturn dup_segs;\n}\n\n/* It's reordering when higher sequence was delivered (i.e. sacked) before\n * some lower never-retransmitted sequence (\"low_seq\"). The maximum reordering\n * distance is approximated in full-mss packet distance (\"reordering\").\n */\nstatic void tcp_check_sack_reordering(struct sock *sk, const u32 low_seq,\n\t\t\t\t      const int ts)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst u32 mss = tp->mss_cache;\n\tu32 fack, metric;\n\n\tfack = tcp_highest_sack_seq(tp);\n\tif (!before(low_seq, fack))\n\t\treturn;\n\n\tmetric = fack - low_seq;\n\tif ((metric > tp->reordering * mss) && mss) {\n#if FASTRETRANS_DEBUG > 1\n\t\tpr_debug(\"Disorder%d %d %u f%u s%u rr%d\\n\",\n\t\t\t tp->rx_opt.sack_ok, inet_csk(sk)->icsk_ca_state,\n\t\t\t tp->reordering,\n\t\t\t 0,\n\t\t\t tp->sacked_out,\n\t\t\t tp->undo_marker ? tp->undo_retrans : 0);\n#endif\n\t\ttp->reordering = min_t(u32, (metric + mss - 1) / mss,\n\t\t\t\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_max_reordering));\n\t}\n\n\t/* This exciting event is worth to be remembered. 8) */\n\ttp->reord_seen++;\n\tNET_INC_STATS(sock_net(sk),\n\t\t      ts ? LINUX_MIB_TCPTSREORDER : LINUX_MIB_TCPSACKREORDER);\n}\n\n /* This must be called before lost_out or retrans_out are updated\n  * on a new loss, because we want to know if all skbs previously\n  * known to be lost have already been retransmitted, indicating\n  * that this newly lost skb is our next skb to retransmit.\n  */\nstatic void tcp_verify_retransmit_hint(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tif ((!tp->retransmit_skb_hint && tp->retrans_out >= tp->lost_out) ||\n\t    (tp->retransmit_skb_hint &&\n\t     before(TCP_SKB_CB(skb)->seq,\n\t\t    TCP_SKB_CB(tp->retransmit_skb_hint)->seq)))\n\t\ttp->retransmit_skb_hint = skb;\n}\n\n/* Sum the number of packets on the wire we have marked as lost, and\n * notify the congestion control module that the given skb was marked lost.\n */\nstatic void tcp_notify_skb_loss_event(struct tcp_sock *tp, const struct sk_buff *skb)\n{\n\ttp->lost += tcp_skb_pcount(skb);\n}\n\nvoid tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb)\n{\n\t__u8 sacked = TCP_SKB_CB(skb)->sacked;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (sacked & TCPCB_SACKED_ACKED)\n\t\treturn;\n\n\ttcp_verify_retransmit_hint(tp, skb);\n\tif (sacked & TCPCB_LOST) {\n\t\tif (sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t/* Account for retransmits that are lost again */\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_RETRANS;\n\t\t\ttp->retrans_out -= tcp_skb_pcount(skb);\n\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPLOSTRETRANSMIT,\n\t\t\t\t      tcp_skb_pcount(skb));\n\t\t\ttcp_notify_skb_loss_event(tp, skb);\n\t\t}\n\t} else {\n\t\ttp->lost_out += tcp_skb_pcount(skb);\n\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_LOST;\n\t\ttcp_notify_skb_loss_event(tp, skb);\n\t}\n}\n\n/* This procedure tags the retransmission queue when SACKs arrive.\n *\n * We have three tag bits: SACKED(S), RETRANS(R) and LOST(L).\n * Packets in queue with these bits set are counted in variables\n * sacked_out, retrans_out and lost_out, correspondingly.\n *\n * Valid combinations are:\n * Tag  InFlight\tDescription\n * 0\t1\t\t- orig segment is in flight.\n * S\t0\t\t- nothing flies, orig reached receiver.\n * L\t0\t\t- nothing flies, orig lost by net.\n * R\t2\t\t- both orig and retransmit are in flight.\n * L|R\t1\t\t- orig is lost, retransmit is in flight.\n * S|R  1\t\t- orig reached receiver, retrans is still in flight.\n * (L|S|R is logically valid, it could occur when L|R is sacked,\n *  but it is equivalent to plain S and code short-circuits it to S.\n *  L|S is logically invalid, it would mean -1 packet in flight 8))\n *\n * These 6 states form finite state machine, controlled by the following events:\n * 1. New ACK (+SACK) arrives. (tcp_sacktag_write_queue())\n * 2. Retransmission. (tcp_retransmit_skb(), tcp_xmit_retransmit_queue())\n * 3. Loss detection event of two flavors:\n *\tA. Scoreboard estimator decided the packet is lost.\n *\t   A'. Reno \"three dupacks\" marks head of queue lost.\n *\tB. SACK arrives sacking SND.NXT at the moment, when the\n *\t   segment was retransmitted.\n * 4. D-SACK added new rule: D-SACK changes any tag to S.\n *\n * It is pleasant to note, that state diagram turns out to be commutative,\n * so that we are allowed not to be bothered by order of our actions,\n * when multiple events arrive simultaneously. (see the function below).\n *\n * Reordering detection.\n * --------------------\n * Reordering metric is maximal distance, which a packet can be displaced\n * in packet stream. With SACKs we can estimate it:\n *\n * 1. SACK fills old hole and the corresponding segment was not\n *    ever retransmitted -> reordering. Alas, we cannot use it\n *    when segment was retransmitted.\n * 2. The last flaw is solved with D-SACK. D-SACK arrives\n *    for retransmitted and already SACKed segment -> reordering..\n * Both of these heuristics are not used in Loss state, when we cannot\n * account for retransmits accurately.\n *\n * SACK block validation.\n * ----------------------\n *\n * SACK block range validation checks that the received SACK block fits to\n * the expected sequence limits, i.e., it is between SND.UNA and SND.NXT.\n * Note that SND.UNA is not included to the range though being valid because\n * it means that the receiver is rather inconsistent with itself reporting\n * SACK reneging when it should advance SND.UNA. Such SACK block this is\n * perfectly valid, however, in light of RFC2018 which explicitly states\n * that \"SACK block MUST reflect the newest segment.  Even if the newest\n * segment is going to be discarded ...\", not that it looks very clever\n * in case of head skb. Due to potentional receiver driven attacks, we\n * choose to avoid immediate execution of a walk in write queue due to\n * reneging and defer head skb's loss recovery to standard loss recovery\n * procedure that will eventually trigger (nothing forbids us doing this).\n *\n * Implements also blockage to start_seq wrap-around. Problem lies in the\n * fact that though start_seq (s) is before end_seq (i.e., not reversed),\n * there's no guarantee that it will be before snd_nxt (n). The problem\n * happens when start_seq resides between end_seq wrap (e_w) and snd_nxt\n * wrap (s_w):\n *\n *         <- outs wnd ->                          <- wrapzone ->\n *         u     e      n                         u_w   e_w  s n_w\n *         |     |      |                          |     |   |  |\n * |<------------+------+----- TCP seqno space --------------+---------->|\n * ...-- <2^31 ->|                                           |<--------...\n * ...---- >2^31 ------>|                                    |<--------...\n *\n * Current code wouldn't be vulnerable but it's better still to discard such\n * crazy SACK blocks. Doing this check for start_seq alone closes somewhat\n * similar case (end_seq after snd_nxt wrap) as earlier reversed check in\n * snd_nxt wrap -> snd_una region will then become \"well defined\", i.e.,\n * equal to the ideal case (infinite seqno space without wrap caused issues).\n *\n * With D-SACK the lower bound is extended to cover sequence space below\n * SND.UNA down to undo_marker, which is the last point of interest. Yet\n * again, D-SACK block must not to go across snd_una (for the same reason as\n * for the normal SACK blocks, explained above). But there all simplicity\n * ends, TCP might receive valid D-SACKs below that. As long as they reside\n * fully below undo_marker they do not affect behavior in anyway and can\n * therefore be safely ignored. In rare cases (which are more or less\n * theoretical ones), the D-SACK will nicely cross that boundary due to skb\n * fragmentation and packet reordering past skb's retransmission. To consider\n * them correctly, the acceptable range must be extended even more though\n * the exact amount is rather hard to quantify. However, tp->max_window can\n * be used as an exaggerated estimate.\n */\nstatic bool tcp_is_sackblock_valid(struct tcp_sock *tp, bool is_dsack,\n\t\t\t\t   u32 start_seq, u32 end_seq)\n{\n\t/* Too far in future, or reversed (interpretation is ambiguous) */\n\tif (after(end_seq, tp->snd_nxt) || !before(start_seq, end_seq))\n\t\treturn false;\n\n\t/* Nasty start_seq wrap-around check (see comments above) */\n\tif (!before(start_seq, tp->snd_nxt))\n\t\treturn false;\n\n\t/* In outstanding window? ...This is valid exit for D-SACKs too.\n\t * start_seq == snd_una is non-sensical (see comments above)\n\t */\n\tif (after(start_seq, tp->snd_una))\n\t\treturn true;\n\n\tif (!is_dsack || !tp->undo_marker)\n\t\treturn false;\n\n\t/* ...Then it's D-SACK, and must reside below snd_una completely */\n\tif (after(end_seq, tp->snd_una))\n\t\treturn false;\n\n\tif (!before(start_seq, tp->undo_marker))\n\t\treturn true;\n\n\t/* Too old */\n\tif (!after(end_seq, tp->undo_marker))\n\t\treturn false;\n\n\t/* Undo_marker boundary crossing (overestimates a lot). Known already:\n\t *   start_seq < undo_marker and end_seq >= undo_marker.\n\t */\n\treturn !before(start_seq, end_seq - tp->max_window);\n}\n\nstatic bool tcp_check_dsack(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\t    struct tcp_sack_block_wire *sp, int num_sacks,\n\t\t\t    u32 prior_snd_una, struct tcp_sacktag_state *state)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 start_seq_0 = get_unaligned_be32(&sp[0].start_seq);\n\tu32 end_seq_0 = get_unaligned_be32(&sp[0].end_seq);\n\tu32 dup_segs;\n\n\tif (before(start_seq_0, TCP_SKB_CB(ack_skb)->ack_seq)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECV);\n\t} else if (num_sacks > 1) {\n\t\tu32 end_seq_1 = get_unaligned_be32(&sp[1].end_seq);\n\t\tu32 start_seq_1 = get_unaligned_be32(&sp[1].start_seq);\n\n\t\tif (after(end_seq_0, end_seq_1) || before(start_seq_0, start_seq_1))\n\t\t\treturn false;\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKOFORECV);\n\t} else {\n\t\treturn false;\n\t}\n\n\tdup_segs = tcp_dsack_seen(tp, start_seq_0, end_seq_0, state);\n\tif (!dup_segs) {\t/* Skip dubious DSACK */\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKIGNOREDDUBIOUS);\n\t\treturn false;\n\t}\n\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDSACKRECVSEGS, dup_segs);\n\n\t/* D-SACK for already forgotten data... Do dumb counting. */\n\tif (tp->undo_marker && tp->undo_retrans > 0 &&\n\t    !after(end_seq_0, prior_snd_una) &&\n\t    after(end_seq_0, tp->undo_marker))\n\t\ttp->undo_retrans = max_t(int, 0, tp->undo_retrans - dup_segs);\n\n\treturn true;\n}\n\n/* Check if skb is fully within the SACK block. In presence of GSO skbs,\n * the incoming SACK may not exactly match but we can find smaller MSS\n * aligned portion of it that matches. Therefore we might need to fragment\n * which may fail and creates some hassle (caller must handle error case\n * returns).\n *\n * FIXME: this could be merged to shift decision code\n */\nstatic int tcp_match_skb_to_sack(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  u32 start_seq, u32 end_seq)\n{\n\tint err;\n\tbool in_sack;\n\tunsigned int pkt_len;\n\tunsigned int mss;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (tcp_skb_pcount(skb) > 1 && !in_sack &&\n\t    after(TCP_SKB_CB(skb)->end_seq, start_seq)) {\n\t\tmss = tcp_skb_mss(skb);\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\n\t\tif (!in_sack) {\n\t\t\tpkt_len = start_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\tpkt_len = mss;\n\t\t} else {\n\t\t\tpkt_len = end_seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (pkt_len < mss)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* Round if necessary so that SACKs cover only full MSSes\n\t\t * and/or the remaining small portion (if present)\n\t\t */\n\t\tif (pkt_len > mss) {\n\t\t\tunsigned int new_len = (pkt_len / mss) * mss;\n\t\t\tif (!in_sack && new_len < pkt_len)\n\t\t\t\tnew_len += mss;\n\t\t\tpkt_len = new_len;\n\t\t}\n\n\t\tif (pkt_len >= skb->len && !in_sack)\n\t\t\treturn 0;\n\n\t\terr = tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t   pkt_len, mss, GFP_ATOMIC);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn in_sack;\n}\n\n/* Mark the given newly-SACKed range as such, adjusting counters and hints. */\nstatic u8 tcp_sacktag_one(struct sock *sk,\n\t\t\t  struct tcp_sacktag_state *state, u8 sacked,\n\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t  int dup_sack, int pcount, u32 plen,\n\t\t\t  u64 xmit_time)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Account D-SACK for retransmitted packet. */\n\tif (dup_sack && (sacked & TCPCB_RETRANS)) {\n\t\tif (tp->undo_marker && tp->undo_retrans > 0 &&\n\t\t    after(end_seq, tp->undo_marker))\n\t\t\ttp->undo_retrans = max_t(int, 0, tp->undo_retrans - pcount);\n\t\tif ((sacked & TCPCB_SACKED_ACKED) &&\n\t\t    before(start_seq, state->reord))\n\t\t\t\tstate->reord = start_seq;\n\t}\n\n\t/* Nothing to do; acked frame is about to be dropped (was ACKed). */\n\tif (!after(end_seq, tp->snd_una))\n\t\treturn sacked;\n\n\tif (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\ttcp_rack_advance(tp, sacked, end_seq, xmit_time);\n\n\t\tif (sacked & TCPCB_SACKED_RETRANS) {\n\t\t\t/* If the segment is not tagged as lost,\n\t\t\t * we do not clear RETRANS, believing\n\t\t\t * that retransmission is still in flight.\n\t\t\t */\n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~(TCPCB_LOST|TCPCB_SACKED_RETRANS);\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t\ttp->retrans_out -= pcount;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!(sacked & TCPCB_RETRANS)) {\n\t\t\t\t/* New sack for not retransmitted frame,\n\t\t\t\t * which was in hole. It is reordering.\n\t\t\t\t */\n\t\t\t\tif (before(start_seq,\n\t\t\t\t\t   tcp_highest_sack_seq(tp)) &&\n\t\t\t\t    before(start_seq, state->reord))\n\t\t\t\t\tstate->reord = start_seq;\n\n\t\t\t\tif (!after(end_seq, tp->high_seq))\n\t\t\t\t\tstate->flag |= FLAG_ORIG_SACK_ACKED;\n\t\t\t\tif (state->first_sackt == 0)\n\t\t\t\t\tstate->first_sackt = xmit_time;\n\t\t\t\tstate->last_sackt = xmit_time;\n\t\t\t}\n\n\t\t\tif (sacked & TCPCB_LOST) {\n\t\t\t\tsacked &= ~TCPCB_LOST;\n\t\t\t\ttp->lost_out -= pcount;\n\t\t\t}\n\t\t}\n\n\t\tsacked |= TCPCB_SACKED_ACKED;\n\t\tstate->flag |= FLAG_DATA_SACKED;\n\t\ttp->sacked_out += pcount;\n\t\t/* Out-of-order packets delivered */\n\t\tstate->sack_delivered += pcount;\n\t\tstate->delivered_bytes += plen;\n\t}\n\n\t/* D-SACK. We can detect redundant retransmission in S|R and plain R\n\t * frames and clear it. undo_retrans is decreased above, L|R frames\n\t * are accounted above as well.\n\t */\n\tif (dup_sack && (sacked & TCPCB_SACKED_RETRANS)) {\n\t\tsacked &= ~TCPCB_SACKED_RETRANS;\n\t\ttp->retrans_out -= pcount;\n\t}\n\n\treturn sacked;\n}\n\n/* Shift newly-SACKed bytes from this skb to the immediately previous\n * already-SACKed sk_buff. Mark the newly-SACKed bytes as such.\n */\nstatic bool tcp_shifted_skb(struct sock *sk, struct sk_buff *prev,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    struct tcp_sacktag_state *state,\n\t\t\t    unsigned int pcount, int shifted, int mss,\n\t\t\t    bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 start_seq = TCP_SKB_CB(skb)->seq;\t/* start of newly-SACKed */\n\tu32 end_seq = start_seq + shifted;\t/* end of newly-SACKed */\n\n\tBUG_ON(!pcount);\n\n\t/* Adjust counters and hints for the newly sacked sequence\n\t * range but discard the return value since prev is already\n\t * marked. We must tag the range first because the seq\n\t * advancement below implicitly advances\n\t * tcp_highest_sack_seq() when skb is highest_sack.\n\t */\n\ttcp_sacktag_one(sk, state, TCP_SKB_CB(skb)->sacked,\n\t\t\tstart_seq, end_seq, dup_sack, pcount, skb->len,\n\t\t\ttcp_skb_timestamp_us(skb));\n\ttcp_rate_skb_delivered(sk, skb, state->rate);\n\n\tTCP_SKB_CB(prev)->end_seq += shifted;\n\tTCP_SKB_CB(skb)->seq += shifted;\n\n\ttcp_skb_pcount_add(prev, pcount);\n\tWARN_ON_ONCE(tcp_skb_pcount(skb) < pcount);\n\ttcp_skb_pcount_add(skb, -pcount);\n\n\t/* When we're adding to gso_segs == 1, gso_size will be zero,\n\t * in theory this shouldn't be necessary but as long as DSACK\n\t * code can come after this skb later on it's better to keep\n\t * setting gso_size to something.\n\t */\n\tif (!TCP_SKB_CB(prev)->tcp_gso_size)\n\t\tTCP_SKB_CB(prev)->tcp_gso_size = mss;\n\n\t/* CHECKME: To clear or not to clear? Mimics normal skb currently */\n\tif (tcp_skb_pcount(skb) <= 1)\n\t\tTCP_SKB_CB(skb)->tcp_gso_size = 0;\n\n\t/* Difference in this won't matter, both ACKed by the same cumul. ACK */\n\tTCP_SKB_CB(prev)->sacked |= (TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS);\n\n\tif (skb->len > 0) {\n\t\tBUG_ON(!tcp_skb_pcount(skb));\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTED);\n\t\treturn false;\n\t}\n\n\t/* Whole SKB was eaten :-) */\n\n\tif (skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = prev;\n\n\tTCP_SKB_CB(prev)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(prev)->eor = TCP_SKB_CB(skb)->eor;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\tTCP_SKB_CB(prev)->end_seq++;\n\n\tif (skb == tcp_highest_sack(sk))\n\t\ttcp_advance_highest_sack(sk, skb);\n\n\ttcp_skb_collapse_tstamp(prev, skb);\n\tif (unlikely(TCP_SKB_CB(prev)->tx.delivered_mstamp))\n\t\tTCP_SKB_CB(prev)->tx.delivered_mstamp = 0;\n\n\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKMERGED);\n\n\treturn true;\n}\n\n/* I wish gso_size would have a bit more sane initialization than\n * something-or-zero which complicates things\n */\nstatic int tcp_skb_seglen(const struct sk_buff *skb)\n{\n\treturn tcp_skb_pcount(skb) == 1 ? skb->len : tcp_skb_mss(skb);\n}\n\n/* Shifting pages past head area doesn't work */\nstatic int skb_can_shift(const struct sk_buff *skb)\n{\n\treturn !skb_headlen(skb) && skb_is_nonlinear(skb);\n}\n\nint tcp_skb_shift(struct sk_buff *to, struct sk_buff *from,\n\t\t  int pcount, int shiftlen)\n{\n\t/* TCP min gso_size is 8 bytes (TCP_MIN_GSO_SIZE)\n\t * Since TCP_SKB_CB(skb)->tcp_gso_segs is 16 bits, we need\n\t * to make sure not storing more than 65535 * 8 bytes per skb,\n\t * even if current MSS is bigger.\n\t */\n\tif (unlikely(to->len + shiftlen >= 65535 * TCP_MIN_GSO_SIZE))\n\t\treturn 0;\n\tif (unlikely(tcp_skb_pcount(to) + pcount > 65535))\n\t\treturn 0;\n\treturn skb_shift(to, from, shiftlen);\n}\n\n/* Try collapsing SACK blocks spanning across multiple skbs to a single\n * skb.\n */\nstatic struct sk_buff *tcp_shift_skb_data(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t  struct tcp_sacktag_state *state,\n\t\t\t\t\t  u32 start_seq, u32 end_seq,\n\t\t\t\t\t  bool dup_sack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *prev;\n\tint mss;\n\tint pcount = 0;\n\tint len;\n\tint in_sack;\n\n\t/* Normally R but no L won't result in plain S */\n\tif (!dup_sack &&\n\t    (TCP_SKB_CB(skb)->sacked & (TCPCB_LOST|TCPCB_SACKED_RETRANS)) == TCPCB_SACKED_RETRANS)\n\t\tgoto fallback;\n\tif (!skb_can_shift(skb))\n\t\tgoto fallback;\n\t/* This frame is about to be dropped (was ACKed). */\n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una))\n\t\tgoto fallback;\n\n\t/* Can only happen with delayed DSACK + discard craziness */\n\tprev = skb_rb_prev(skb);\n\tif (!prev)\n\t\tgoto fallback;\n\n\tif ((TCP_SKB_CB(prev)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED)\n\t\tgoto fallback;\n\n\tif (!tcp_skb_can_collapse(prev, skb))\n\t\tgoto fallback;\n\n\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq) &&\n\t\t  !before(end_seq, TCP_SKB_CB(skb)->end_seq);\n\n\tif (in_sack) {\n\t\tlen = skb->len;\n\t\tpcount = tcp_skb_pcount(skb);\n\t\tmss = tcp_skb_seglen(skb);\n\n\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\n\t\t * drop this restriction as unnecessary\n\t\t */\n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\t} else {\n\t\tif (!after(TCP_SKB_CB(skb)->end_seq, start_seq))\n\t\t\tgoto noop;\n\t\t/* CHECKME: This is non-MSS split case only?, this will\n\t\t * cause skipped skbs due to advancing loop btw, original\n\t\t * has that feature too\n\t\t */\n\t\tif (tcp_skb_pcount(skb) <= 1)\n\t\t\tgoto noop;\n\n\t\tin_sack = !after(start_seq, TCP_SKB_CB(skb)->seq);\n\t\tif (!in_sack) {\n\t\t\t/* TODO: head merge to next could be attempted here\n\t\t\t * if (!after(TCP_SKB_CB(skb)->end_seq, end_seq)),\n\t\t\t * though it might not be worth of the additional hassle\n\t\t\t *\n\t\t\t * ...we can probably just fallback to what was done\n\t\t\t * previously. We could try merging non-SACKed ones\n\t\t\t * as well but it probably isn't going to buy off\n\t\t\t * because later SACKs might again split them, and\n\t\t\t * it would make skb timestamp tracking considerably\n\t\t\t * harder problem.\n\t\t\t */\n\t\t\tgoto fallback;\n\t\t}\n\n\t\tlen = end_seq - TCP_SKB_CB(skb)->seq;\n\t\tBUG_ON(len < 0);\n\t\tBUG_ON(len > skb->len);\n\n\t\t/* MSS boundaries should be honoured or else pcount will\n\t\t * severely break even though it makes things bit trickier.\n\t\t * Optimize common case to avoid most of the divides\n\t\t */\n\t\tmss = tcp_skb_mss(skb);\n\n\t\t/* TODO: Fix DSACKs to not fragment already SACKed and we can\n\t\t * drop this restriction as unnecessary\n\t\t */\n\t\tif (mss != tcp_skb_seglen(prev))\n\t\t\tgoto fallback;\n\n\t\tif (len == mss) {\n\t\t\tpcount = 1;\n\t\t} else if (len < mss) {\n\t\t\tgoto noop;\n\t\t} else {\n\t\t\tpcount = len / mss;\n\t\t\tlen = pcount * mss;\n\t\t}\n\t}\n\n\t/* tcp_sacktag_one() won't SACK-tag ranges below snd_una */\n\tif (!after(TCP_SKB_CB(skb)->seq + len, tp->snd_una))\n\t\tgoto fallback;\n\n\tif (!tcp_skb_shift(prev, skb, pcount, len))\n\t\tgoto fallback;\n\tif (!tcp_shifted_skb(sk, prev, skb, state, pcount, len, mss, dup_sack))\n\t\tgoto out;\n\n\t/* Hole filled allows collapsing with the next as well, this is very\n\t * useful when hole on every nth skb pattern happens\n\t */\n\tskb = skb_rb_next(prev);\n\tif (!skb)\n\t\tgoto out;\n\n\tif (!skb_can_shift(skb) ||\n\t    ((TCP_SKB_CB(skb)->sacked & TCPCB_TAGBITS) != TCPCB_SACKED_ACKED) ||\n\t    (mss != tcp_skb_seglen(skb)))\n\t\tgoto out;\n\n\tif (!tcp_skb_can_collapse(prev, skb))\n\t\tgoto out;\n\tlen = skb->len;\n\tpcount = tcp_skb_pcount(skb);\n\tif (tcp_skb_shift(prev, skb, pcount, len))\n\t\ttcp_shifted_skb(sk, prev, skb, state, pcount,\n\t\t\t\tlen, mss, 0);\n\nout:\n\treturn prev;\n\nnoop:\n\treturn skb;\n\nfallback:\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_SACKSHIFTFALLBACK);\n\treturn NULL;\n}\n\nstatic struct sk_buff *tcp_sacktag_walk(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\tu32 start_seq, u32 end_seq,\n\t\t\t\t\tbool dup_sack_in)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *tmp;\n\n\tskb_rbtree_walk_from(skb) {\n\t\tint in_sack = 0;\n\t\tbool dup_sack = dup_sack_in;\n\n\t\t/* queue is in-order => we can short-circuit the walk early */\n\t\tif (!before(TCP_SKB_CB(skb)->seq, end_seq))\n\t\t\tbreak;\n\n\t\tif (next_dup  &&\n\t\t    before(TCP_SKB_CB(skb)->seq, next_dup->end_seq)) {\n\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\tnext_dup->start_seq,\n\t\t\t\t\t\t\tnext_dup->end_seq);\n\t\t\tif (in_sack > 0)\n\t\t\t\tdup_sack = true;\n\t\t}\n\n\t\t/* skb reference here is a bit tricky to get right, since\n\t\t * shifting can eat and free both this skb and the next,\n\t\t * so not even _safe variant of the loop is enough.\n\t\t */\n\t\tif (in_sack <= 0) {\n\t\t\ttmp = tcp_shift_skb_data(sk, skb, state,\n\t\t\t\t\t\t start_seq, end_seq, dup_sack);\n\t\t\tif (tmp) {\n\t\t\t\tif (tmp != skb) {\n\t\t\t\t\tskb = tmp;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tin_sack = 0;\n\t\t\t} else {\n\t\t\t\tin_sack = tcp_match_skb_to_sack(sk, skb,\n\t\t\t\t\t\t\t\tstart_seq,\n\t\t\t\t\t\t\t\tend_seq);\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(in_sack < 0))\n\t\t\tbreak;\n\n\t\tif (in_sack) {\n\t\t\tTCP_SKB_CB(skb)->sacked =\n\t\t\t\ttcp_sacktag_one(sk,\n\t\t\t\t\t\tstate,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->sacked,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->seq,\n\t\t\t\t\t\tTCP_SKB_CB(skb)->end_seq,\n\t\t\t\t\t\tdup_sack,\n\t\t\t\t\t\ttcp_skb_pcount(skb),\n\t\t\t\t\t\tskb->len,\n\t\t\t\t\t\ttcp_skb_timestamp_us(skb));\n\t\t\ttcp_rate_skb_delivered(sk, skb, state->rate);\n\t\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\t\tlist_del_init(&skb->tcp_tsorted_anchor);\n\n\t\t\tif (!before(TCP_SKB_CB(skb)->seq,\n\t\t\t\t    tcp_highest_sack_seq(tp)))\n\t\t\t\ttcp_advance_highest_sack(sk, skb);\n\t\t}\n\t}\n\treturn skb;\n}\n\nstatic struct sk_buff *tcp_sacktag_bsearch(struct sock *sk, u32 seq)\n{\n\tstruct rb_node *parent, **p = &sk->tcp_rtx_queue.rb_node;\n\tstruct sk_buff *skb;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!before(seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\tp = &parent->rb_right;\n\t\t\tcontinue;\n\t\t}\n\t\treturn skb;\n\t}\n\treturn NULL;\n}\n\nstatic struct sk_buff *tcp_sacktag_skip(struct sk_buff *skb, struct sock *sk,\n\t\t\t\t\tu32 skip_to_seq)\n{\n\tif (skb && after(TCP_SKB_CB(skb)->seq, skip_to_seq))\n\t\treturn skb;\n\n\treturn tcp_sacktag_bsearch(sk, skip_to_seq);\n}\n\nstatic struct sk_buff *tcp_maybe_skipping_dsack(struct sk_buff *skb,\n\t\t\t\t\t\tstruct sock *sk,\n\t\t\t\t\t\tstruct tcp_sack_block *next_dup,\n\t\t\t\t\t\tstruct tcp_sacktag_state *state,\n\t\t\t\t\t\tu32 skip_to_seq)\n{\n\tif (!next_dup)\n\t\treturn skb;\n\n\tif (before(next_dup->start_seq, skip_to_seq)) {\n\t\tskb = tcp_sacktag_skip(skb, sk, next_dup->start_seq);\n\t\tskb = tcp_sacktag_walk(skb, sk, NULL, state,\n\t\t\t\t       next_dup->start_seq, next_dup->end_seq,\n\t\t\t\t       1);\n\t}\n\n\treturn skb;\n}\n\nstatic int tcp_sack_cache_ok(const struct tcp_sock *tp, const struct tcp_sack_block *cache)\n{\n\treturn cache < tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n}\n\nstatic int\ntcp_sacktag_write_queue(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\tu32 prior_snd_una, struct tcp_sacktag_state *state)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tconst unsigned char *ptr = (skb_transport_header(ack_skb) +\n\t\t\t\t    TCP_SKB_CB(ack_skb)->sacked);\n\tstruct tcp_sack_block_wire *sp_wire = (struct tcp_sack_block_wire *)(ptr+2);\n\tstruct tcp_sack_block sp[TCP_NUM_SACKS];\n\tstruct tcp_sack_block *cache;\n\tstruct sk_buff *skb;\n\tint num_sacks = min(TCP_NUM_SACKS, (ptr[1] - TCPOLEN_SACK_BASE) >> 3);\n\tint used_sacks;\n\tbool found_dup_sack = false;\n\tint i, j;\n\tint first_sack_index;\n\n\tstate->flag = 0;\n\tstate->reord = tp->snd_nxt;\n\n\tif (!tp->sacked_out)\n\t\ttcp_highest_sack_reset(sk);\n\n\tfound_dup_sack = tcp_check_dsack(sk, ack_skb, sp_wire,\n\t\t\t\t\t num_sacks, prior_snd_una, state);\n\n\t/* Eliminate too old ACKs, but take into\n\t * account more or less fresh ones, they can\n\t * contain valid SACK info.\n\t */\n\tif (before(TCP_SKB_CB(ack_skb)->ack_seq, prior_snd_una - tp->max_window))\n\t\treturn 0;\n\n\tif (!tp->packets_out)\n\t\tgoto out;\n\n\tused_sacks = 0;\n\tfirst_sack_index = 0;\n\tfor (i = 0; i < num_sacks; i++) {\n\t\tbool dup_sack = !i && found_dup_sack;\n\n\t\tsp[used_sacks].start_seq = get_unaligned_be32(&sp_wire[i].start_seq);\n\t\tsp[used_sacks].end_seq = get_unaligned_be32(&sp_wire[i].end_seq);\n\n\t\tif (!tcp_is_sackblock_valid(tp, dup_sack,\n\t\t\t\t\t    sp[used_sacks].start_seq,\n\t\t\t\t\t    sp[used_sacks].end_seq)) {\n\t\t\tint mib_idx;\n\n\t\t\tif (dup_sack) {\n\t\t\t\tif (!tp->undo_marker)\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDNOUNDO;\n\t\t\t\telse\n\t\t\t\t\tmib_idx = LINUX_MIB_TCPDSACKIGNOREDOLD;\n\t\t\t} else {\n\t\t\t\t/* Don't count olds caused by ACK reordering */\n\t\t\t\tif ((TCP_SKB_CB(ack_skb)->ack_seq != tp->snd_una) &&\n\t\t\t\t    !after(sp[used_sacks].end_seq, tp->snd_una))\n\t\t\t\t\tcontinue;\n\t\t\t\tmib_idx = LINUX_MIB_TCPSACKDISCARD;\n\t\t\t}\n\n\t\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t\t\tif (i == 0)\n\t\t\t\tfirst_sack_index = -1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Ignore very old stuff early */\n\t\tif (!after(sp[used_sacks].end_seq, prior_snd_una)) {\n\t\t\tif (i == 0)\n\t\t\t\tfirst_sack_index = -1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tused_sacks++;\n\t}\n\n\t/* order SACK blocks to allow in order walk of the retrans queue */\n\tfor (i = used_sacks - 1; i > 0; i--) {\n\t\tfor (j = 0; j < i; j++) {\n\t\t\tif (after(sp[j].start_seq, sp[j + 1].start_seq)) {\n\t\t\t\tswap(sp[j], sp[j + 1]);\n\n\t\t\t\t/* Track where the first SACK block goes to */\n\t\t\t\tif (j == first_sack_index)\n\t\t\t\t\tfirst_sack_index = j + 1;\n\t\t\t}\n\t\t}\n\t}\n\n\tstate->mss_now = tcp_current_mss(sk);\n\tskb = NULL;\n\ti = 0;\n\n\tif (!tp->sacked_out) {\n\t\t/* It's already past, so skip checking against it */\n\t\tcache = tp->recv_sack_cache + ARRAY_SIZE(tp->recv_sack_cache);\n\t} else {\n\t\tcache = tp->recv_sack_cache;\n\t\t/* Skip empty blocks in at head of the cache */\n\t\twhile (tcp_sack_cache_ok(tp, cache) && !cache->start_seq &&\n\t\t       !cache->end_seq)\n\t\t\tcache++;\n\t}\n\n\twhile (i < used_sacks) {\n\t\tu32 start_seq = sp[i].start_seq;\n\t\tu32 end_seq = sp[i].end_seq;\n\t\tbool dup_sack = (found_dup_sack && (i == first_sack_index));\n\t\tstruct tcp_sack_block *next_dup = NULL;\n\n\t\tif (found_dup_sack && ((i + 1) == first_sack_index))\n\t\t\tnext_dup = &sp[i + 1];\n\n\t\t/* Skip too early cached blocks */\n\t\twhile (tcp_sack_cache_ok(tp, cache) &&\n\t\t       !before(start_seq, cache->end_seq))\n\t\t\tcache++;\n\n\t\t/* Can skip some work by looking recv_sack_cache? */\n\t\tif (tcp_sack_cache_ok(tp, cache) && !dup_sack &&\n\t\t    after(end_seq, cache->start_seq)) {\n\n\t\t\t/* Head todo? */\n\t\t\tif (before(start_seq, cache->start_seq)) {\n\t\t\t\tskb = tcp_sacktag_skip(skb, sk, start_seq);\n\t\t\t\tskb = tcp_sacktag_walk(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       start_seq,\n\t\t\t\t\t\t       cache->start_seq,\n\t\t\t\t\t\t       dup_sack);\n\t\t\t}\n\n\t\t\t/* Rest of the block already fully processed? */\n\t\t\tif (!after(end_seq, cache->end_seq))\n\t\t\t\tgoto advance_sp;\n\n\t\t\tskb = tcp_maybe_skipping_dsack(skb, sk, next_dup,\n\t\t\t\t\t\t       state,\n\t\t\t\t\t\t       cache->end_seq);\n\n\t\t\t/* ...tail remains todo... */\n\t\t\tif (tcp_highest_sack_seq(tp) == cache->end_seq) {\n\t\t\t\t/* ...but better entrypoint exists! */\n\t\t\t\tskb = tcp_highest_sack(sk);\n\t\t\t\tif (!skb)\n\t\t\t\t\tbreak;\n\t\t\t\tcache++;\n\t\t\t\tgoto walk;\n\t\t\t}\n\n\t\t\tskb = tcp_sacktag_skip(skb, sk, cache->end_seq);\n\t\t\t/* Check overlap against next cached too (past this one already) */\n\t\t\tcache++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!before(start_seq, tcp_highest_sack_seq(tp))) {\n\t\t\tskb = tcp_highest_sack(sk);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t}\n\t\tskb = tcp_sacktag_skip(skb, sk, start_seq);\n\nwalk:\n\t\tskb = tcp_sacktag_walk(skb, sk, next_dup, state,\n\t\t\t\t       start_seq, end_seq, dup_sack);\n\nadvance_sp:\n\t\ti++;\n\t}\n\n\t/* Clear the head of the cache sack blocks so we can skip it next time */\n\tfor (i = 0; i < ARRAY_SIZE(tp->recv_sack_cache) - used_sacks; i++) {\n\t\ttp->recv_sack_cache[i].start_seq = 0;\n\t\ttp->recv_sack_cache[i].end_seq = 0;\n\t}\n\tfor (j = 0; j < used_sacks; j++)\n\t\ttp->recv_sack_cache[i++] = sp[j];\n\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_Loss || tp->undo_marker)\n\t\ttcp_check_sack_reordering(sk, state->reord, 0);\n\n\ttcp_verify_left_out(tp);\nout:\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tWARN_ON((int)tcp_packets_in_flight(tp) < 0);\n#endif\n\treturn state->flag;\n}\n\n/* Limits sacked_out so that sum with lost_out isn't ever larger than\n * packets_out. Returns false if sacked_out adjustement wasn't necessary.\n */\nstatic bool tcp_limit_reno_sacked(struct tcp_sock *tp)\n{\n\tu32 holes;\n\n\tholes = max(tp->lost_out, 1U);\n\tholes = min(holes, tp->packets_out);\n\n\tif ((tp->sacked_out + holes) > tp->packets_out) {\n\t\ttp->sacked_out = tp->packets_out - holes;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* If we receive more dupacks than we expected counting segments\n * in assumption of absent reordering, interpret this as reordering.\n * The only another reason could be bug in receiver TCP.\n */\nstatic void tcp_check_reno_reordering(struct sock *sk, const int addend)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tcp_limit_reno_sacked(tp))\n\t\treturn;\n\n\ttp->reordering = min_t(u32, tp->packets_out + addend,\n\t\t\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_max_reordering));\n\ttp->reord_seen++;\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRENOREORDER);\n}\n\n/* Emulate SACKs for SACKless connection: account for a new dupack. */\n\nstatic void tcp_add_reno_sack(struct sock *sk, int num_dupack, bool ece_ack)\n{\n\tif (num_dupack) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tu32 prior_sacked = tp->sacked_out;\n\t\ts32 delivered;\n\n\t\ttp->sacked_out += num_dupack;\n\t\ttcp_check_reno_reordering(sk, 0);\n\t\tdelivered = tp->sacked_out - prior_sacked;\n\t\tif (delivered > 0)\n\t\t\ttcp_count_delivered(tp, delivered, ece_ack);\n\t\ttcp_verify_left_out(tp);\n\t}\n}\n\n/* Account for ACK, ACKing some data in Reno Recovery phase. */\n\nstatic void tcp_remove_reno_sacks(struct sock *sk, int acked, bool ece_ack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (acked > 0) {\n\t\t/* One ACK acked hole. The rest eat duplicate ACKs. */\n\t\ttcp_count_delivered(tp, max_t(int, acked - tp->sacked_out, 1),\n\t\t\t\t    ece_ack);\n\t\tif (acked - 1 >= tp->sacked_out)\n\t\t\ttp->sacked_out = 0;\n\t\telse\n\t\t\ttp->sacked_out -= acked - 1;\n\t}\n\ttcp_check_reno_reordering(sk, acked);\n\ttcp_verify_left_out(tp);\n}\n\nstatic inline void tcp_reset_reno_sack(struct tcp_sock *tp)\n{\n\ttp->sacked_out = 0;\n}\n\nvoid tcp_clear_retrans(struct tcp_sock *tp)\n{\n\ttp->retrans_out = 0;\n\ttp->lost_out = 0;\n\ttp->undo_marker = 0;\n\ttp->undo_retrans = -1;\n\ttp->sacked_out = 0;\n\ttp->rto_stamp = 0;\n\ttp->total_rto = 0;\n\ttp->total_rto_recoveries = 0;\n\ttp->total_rto_time = 0;\n}\n\nstatic inline void tcp_init_undo(struct tcp_sock *tp)\n{\n\ttp->undo_marker = tp->snd_una;\n\n\t/* Retransmission still in flight may cause DSACKs later. */\n\t/* First, account for regular retransmits in flight: */\n\ttp->undo_retrans = tp->retrans_out;\n\t/* Next, account for TLP retransmits in flight: */\n\tif (tp->tlp_high_seq && tp->tlp_retrans)\n\t\ttp->undo_retrans++;\n\t/* Finally, avoid 0, because undo_retrans==0 means \"can undo now\": */\n\tif (!tp->undo_retrans)\n\t\ttp->undo_retrans = -1;\n}\n\n/* If we detect SACK reneging, forget all SACK information\n * and reset tags completely, otherwise preserve SACKs. If receiver\n * dropped its ofo queue, we will know this due to reneging detection.\n */\nstatic void tcp_timeout_mark_lost(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *head;\n\tbool is_reneg;\t\t\t/* is receiver reneging on SACKs? */\n\n\thead = tcp_rtx_queue_head(sk);\n\tis_reneg = head && (TCP_SKB_CB(head)->sacked & TCPCB_SACKED_ACKED);\n\tif (is_reneg) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSACKRENEGING);\n\t\ttp->sacked_out = 0;\n\t\t/* Mark SACK reneging until we recover from this loss event. */\n\t\ttp->is_sack_reneg = 1;\n\t} else if (tcp_is_reno(tp)) {\n\t\ttcp_reset_reno_sack(tp);\n\t}\n\n\tskb = head;\n\tskb_rbtree_walk_from(skb) {\n\t\tif (is_reneg)\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_SACKED_ACKED;\n\t\telse if (skb != head && tcp_rack_skb_timeout(tp, skb, 0) > 0)\n\t\t\tcontinue; /* Don't mark recently sent ones lost yet */\n\t\ttcp_mark_skb_lost(sk, skb);\n\t}\n\ttcp_verify_left_out(tp);\n\ttcp_clear_all_retrans_hints(tp);\n}\n\n/* Enter Loss state. */\nvoid tcp_enter_loss(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tbool new_recovery = icsk->icsk_ca_state < TCP_CA_Recovery;\n\tu8 reordering;\n\n\ttcp_timeout_mark_lost(sk);\n\n\t/* Reduce ssthresh if it has not yet been made inside this window. */\n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder ||\n\t    !after(tp->high_seq, tp->snd_una) ||\n\t    (icsk->icsk_ca_state == TCP_CA_Loss && !icsk->icsk_retransmits)) {\n\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->prior_cwnd = tcp_snd_cwnd(tp);\n\t\ttp->snd_ssthresh = icsk->icsk_ca_ops->ssthresh(sk);\n\t\ttcp_ca_event(sk, CA_EVENT_LOSS);\n\t\ttcp_init_undo(tp);\n\t}\n\ttcp_snd_cwnd_set(tp, tcp_packets_in_flight(tp) + 1);\n\ttp->snd_cwnd_cnt   = 0;\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\n\t/* Timeout in disordered state after receiving substantial DUPACKs\n\t * suggests that the degree of reordering is over-estimated.\n\t */\n\treordering = READ_ONCE(net->ipv4.sysctl_tcp_reordering);\n\tif (icsk->icsk_ca_state <= TCP_CA_Disorder &&\n\t    tp->sacked_out >= reordering)\n\t\ttp->reordering = min_t(unsigned int, tp->reordering,\n\t\t\t\t       reordering);\n\n\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\ttp->high_seq = tp->snd_nxt;\n\ttp->tlp_high_seq = 0;\n\ttcp_ecn_queue_cwr(tp);\n\n\t/* F-RTO RFC5682 sec 3.1 step 1: retransmit SND.UNA if no previous\n\t * loss recovery is underway except recurring timeout(s) on\n\t * the same SND.UNA (sec 3.2). Disable F-RTO on path MTU probing\n\t */\n\ttp->frto = READ_ONCE(net->ipv4.sysctl_tcp_frto) &&\n\t\t   (new_recovery || icsk->icsk_retransmits) &&\n\t\t   !inet_csk(sk)->icsk_mtup.probe_size;\n}\n\n/* If ACK arrived pointing to a remembered SACK, it means that our\n * remembered SACKs do not reflect real state of receiver i.e.\n * receiver _host_ is heavily congested (or buggy).\n *\n * To avoid big spurious retransmission bursts due to transient SACK\n * scoreboard oddities that look like reneging, we give the receiver a\n * little time (max(RTT/2, 10ms)) to send us some more ACKs that will\n * restore sanity to the SACK scoreboard. If the apparent reneging\n * persists until this RTO then we'll clear the SACK scoreboard.\n */\nstatic bool tcp_check_sack_reneging(struct sock *sk, int *ack_flag)\n{\n\tif (*ack_flag & FLAG_SACK_RENEGING &&\n\t    *ack_flag & FLAG_SND_UNA_ADVANCED) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tunsigned long delay = max(usecs_to_jiffies(tp->srtt_us >> 4),\n\t\t\t\t\t  msecs_to_jiffies(10));\n\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS, delay, false);\n\t\t*ack_flag &= ~FLAG_SET_XMIT_TIMER;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Linux NewReno/SACK/ECN state machine.\n * --------------------------------------\n *\n * \"Open\"\tNormal state, no dubious events, fast path.\n * \"Disorder\"   In all the respects it is \"Open\",\n *\t\tbut requires a bit more attention. It is entered when\n *\t\twe see some SACKs or dupacks. It is split of \"Open\"\n *\t\tmainly to move some processing from fast path to slow one.\n * \"CWR\"\tCWND was reduced due to some Congestion Notification event.\n *\t\tIt can be ECN, ICMP source quench, local device congestion.\n * \"Recovery\"\tCWND was reduced, we are fast-retransmitting.\n * \"Loss\"\tCWND was reduced due to RTO timeout or SACK reneging.\n *\n * tcp_fastretrans_alert() is entered:\n * - each incoming ACK, if state is not \"Open\"\n * - when arrived ACK is unusual, namely:\n *\t* SACK\n *\t* Duplicate ACK.\n *\t* ECN ECE.\n *\n * Counting packets in flight is pretty simple.\n *\n *\tin_flight = packets_out - left_out + retrans_out\n *\n *\tpackets_out is SND.NXT-SND.UNA counted in packets.\n *\n *\tretrans_out is number of retransmitted segments.\n *\n *\tleft_out is number of segments left network, but not ACKed yet.\n *\n *\t\tleft_out = sacked_out + lost_out\n *\n *     sacked_out: Packets, which arrived to receiver out of order\n *\t\t   and hence not ACKed. With SACKs this number is simply\n *\t\t   amount of SACKed data. Even without SACKs\n *\t\t   it is easy to give pretty reliable estimate of this number,\n *\t\t   counting duplicate ACKs.\n *\n *       lost_out: Packets lost by network. TCP has no explicit\n *\t\t   \"loss notification\" feedback from network (for now).\n *\t\t   It means that this number can be only _guessed_.\n *\t\t   Actually, it is the heuristics to predict lossage that\n *\t\t   distinguishes different algorithms.\n *\n *\tF.e. after RTO, when all the queue is considered as lost,\n *\tlost_out = packets_out and in_flight = retrans_out.\n *\n *\t\tEssentially, we have now a few algorithms detecting\n *\t\tlost packets.\n *\n *\t\tIf the receiver supports SACK:\n *\n *\t\tRACK (RFC8985): RACK is a newer loss detection algorithm\n *\t\t(2017-) that checks timing instead of counting DUPACKs.\n *\t\tEssentially a packet is considered lost if it's not S/ACKed\n *\t\tafter RTT + reordering_window, where both metrics are\n *\t\tdynamically measured and adjusted. This is implemented in\n *\t\ttcp_rack_mark_lost.\n *\n *\t\tIf the receiver does not support SACK:\n *\n *\t\tNewReno (RFC6582): in Recovery we assume that one segment\n *\t\tis lost (classic Reno). While we are in Recovery and\n *\t\ta partial ACK arrives, we assume that one more packet\n *\t\tis lost (NewReno). This heuristics are the same in NewReno\n *\t\tand SACK.\n *\n * The really tricky (and requiring careful tuning) part of the algorithm\n * is hidden in the RACK code in tcp_recovery.c and tcp_xmit_retransmit_queue().\n * The first determines the moment _when_ we should reduce CWND and,\n * hence, slow down forward transmission. In fact, it determines the moment\n * when we decide that hole is caused by loss, rather than by a reorder.\n *\n * tcp_xmit_retransmit_queue() decides, _what_ we should retransmit to fill\n * holes, caused by lost packets.\n *\n * And the most logically complicated part of algorithm is undo\n * heuristics. We detect false retransmits due to both too early\n * fast retransmit (reordering) and underestimated RTO, analyzing\n * timestamps and D-SACKs. When we detect that some segments were\n * retransmitted by mistake and CWND reduction was wrong, we undo\n * window reduction and abort recovery phase. This logic is hidden\n * inside several functions named tcp_try_undo_<something>.\n */\n\n/* This function decides, when we should leave Disordered state\n * and enter Recovery phase, reducing congestion window.\n *\n * Main question: may we further continue forward transmission\n * with the same cwnd?\n */\nstatic bool tcp_time_to_recover(const struct tcp_sock *tp)\n{\n\t/* Has loss detection marked at least one packet lost? */\n\treturn tp->lost_out != 0;\n}\n\nstatic bool tcp_tsopt_ecr_before(const struct tcp_sock *tp, u32 when)\n{\n\treturn tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t       before(tp->rx_opt.rcv_tsecr, when);\n}\n\n/* skb is spurious retransmitted if the returned timestamp echo\n * reply is prior to the skb transmission time\n */\nstatic bool tcp_skb_spurious_retrans(const struct tcp_sock *tp,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn (TCP_SKB_CB(skb)->sacked & TCPCB_RETRANS) &&\n\t       tcp_tsopt_ecr_before(tp, tcp_skb_timestamp_ts(tp->tcp_usec_ts, skb));\n}\n\n/* Nothing was retransmitted or returned timestamp is less\n * than timestamp of the first retransmission.\n */\nstatic inline bool tcp_packet_delayed(const struct tcp_sock *tp)\n{\n\tconst struct sock *sk = (const struct sock *)tp;\n\n\t/* Received an echoed timestamp before the first retransmission? */\n\tif (tp->retrans_stamp)\n\t\treturn tcp_tsopt_ecr_before(tp, tp->retrans_stamp);\n\n\t/* We set tp->retrans_stamp upon the first retransmission of a loss\n\t * recovery episode, so normally if tp->retrans_stamp is 0 then no\n\t * retransmission has happened yet (likely due to TSQ, which can cause\n\t * fast retransmits to be delayed). So if snd_una advanced while\n\t * (tp->retrans_stamp is 0 then apparently a packet was merely delayed,\n\t * not lost. But there are exceptions where we retransmit but then\n\t * clear tp->retrans_stamp, so we check for those exceptions.\n\t */\n\n\t/* (1) For non-SACK connections, tcp_is_non_sack_preventing_reopen()\n\t * clears tp->retrans_stamp when snd_una == high_seq.\n\t */\n\tif (!tcp_is_sack(tp) && !before(tp->snd_una, tp->high_seq))\n\t\treturn false;\n\n\t/* (2) In TCP_SYN_SENT tcp_clean_rtx_queue() clears tp->retrans_stamp\n\t * when setting FLAG_SYN_ACKED is set, even if the SYN was\n\t * retransmitted.\n\t */\n\tif (sk->sk_state == TCP_SYN_SENT)\n\t\treturn false;\n\n\treturn true;\t/* tp->retrans_stamp is zero; no retransmit yet */\n}\n\n/* Undo procedures. */\n\n/* We can clear retrans_stamp when there are no retransmissions in the\n * window. It would seem that it is trivially available for us in\n * tp->retrans_out, however, that kind of assumptions doesn't consider\n * what will happen if errors occur when sending retransmission for the\n * second time. ...It could the that such segment has only\n * TCPCB_EVER_RETRANS set at the present time. It seems that checking\n * the head skb is enough except for some reneging corner cases that\n * are not worth the effort.\n *\n * Main reason for all this complexity is the fact that connection dying\n * time now depends on the validity of the retrans_stamp, in particular,\n * that successive retransmissions of a segment must not advance\n * retrans_stamp under any conditions.\n */\nstatic bool tcp_any_retrans_done(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (tp->retrans_out)\n\t\treturn true;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (unlikely(skb && TCP_SKB_CB(skb)->sacked & TCPCB_EVER_RETRANS))\n\t\treturn true;\n\n\treturn false;\n}\n\n/* If loss recovery is finished and there are no retransmits out in the\n * network, then we clear retrans_stamp so that upon the next loss recovery\n * retransmits_timed_out() and timestamp-undo are using the correct value.\n */\nstatic void tcp_retrans_stamp_cleanup(struct sock *sk)\n{\n\tif (!tcp_any_retrans_done(sk))\n\t\ttcp_sk(sk)->retrans_stamp = 0;\n}\n\nstatic void DBGUNDO(struct sock *sk, const char *msg)\n{\n#if FASTRETRANS_DEBUG > 1\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\n\tif (sk->sk_family == AF_INET) {\n\t\tpr_debug(\"Undo %s %pI4/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &inet->inet_daddr, ntohs(inet->inet_dport),\n\t\t\t tcp_snd_cwnd(tp), tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (sk->sk_family == AF_INET6) {\n\t\tpr_debug(\"Undo %s %pI6/%u c%u l%u ss%u/%u p%u\\n\",\n\t\t\t msg,\n\t\t\t &sk->sk_v6_daddr, ntohs(inet->inet_dport),\n\t\t\t tcp_snd_cwnd(tp), tcp_left_out(tp),\n\t\t\t tp->snd_ssthresh, tp->prior_ssthresh,\n\t\t\t tp->packets_out);\n\t}\n#endif\n#endif\n}\n\nstatic void tcp_undo_cwnd_reduction(struct sock *sk, bool unmark_loss)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (unmark_loss) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb_rbtree_walk(skb, &sk->tcp_rtx_queue) {\n\t\t\tTCP_SKB_CB(skb)->sacked &= ~TCPCB_LOST;\n\t\t}\n\t\ttp->lost_out = 0;\n\t\ttcp_clear_all_retrans_hints(tp);\n\t}\n\n\tif (tp->prior_ssthresh) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\ttcp_snd_cwnd_set(tp, icsk->icsk_ca_ops->undo_cwnd(sk));\n\n\t\tif (tp->prior_ssthresh > tp->snd_ssthresh) {\n\t\t\ttp->snd_ssthresh = tp->prior_ssthresh;\n\t\t\ttcp_ecn_withdraw_cwr(tp);\n\t\t}\n\t}\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->undo_marker = 0;\n\ttp->rack.advanced = 1; /* Force RACK to re-exam losses */\n}\n\nstatic inline bool tcp_may_undo(const struct tcp_sock *tp)\n{\n\treturn tp->undo_marker && (!tp->undo_retrans || tcp_packet_delayed(tp));\n}\n\nstatic bool tcp_is_non_sack_preventing_reopen(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->snd_una == tp->high_seq && tcp_is_reno(tp)) {\n\t\t/* Hold old state until something *above* high_seq\n\t\t * is ACKed. For Reno it is MUST to prevent false\n\t\t * fast retransmits (RFC2582). SACK TCP is safe. */\n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* People celebrate: \"We love our President!\" */\nstatic bool tcp_try_undo_recovery(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_may_undo(tp)) {\n\t\tint mib_idx;\n\n\t\t/* Happy end! We did not retransmit anything\n\t\t * or our original transmission succeeded.\n\t\t */\n\t\tDBGUNDO(sk, inet_csk(sk)->icsk_ca_state == TCP_CA_Loss ? \"loss\" : \"retrans\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss)\n\t\t\tmib_idx = LINUX_MIB_TCPLOSSUNDO;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPFULLUNDO;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\t} else if (tp->rack.reo_wnd_persist) {\n\t\ttp->rack.reo_wnd_persist--;\n\t}\n\tif (tcp_is_non_sack_preventing_reopen(sk))\n\t\treturn true;\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttp->is_sack_reneg = 0;\n\treturn false;\n}\n\n/* Try to undo cwnd reduction, because D-SACKs acked all retransmitted data */\nstatic bool tcp_try_undo_dsack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && !tp->undo_retrans) {\n\t\ttp->rack.reo_wnd_persist = min(TCP_RACK_RECOVERY_THRESH,\n\t\t\t\t\t       tp->rack.reo_wnd_persist + 1);\n\t\tDBGUNDO(sk, \"D-SACK\");\n\t\ttcp_undo_cwnd_reduction(sk, false);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDSACKUNDO);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Undo during loss recovery after partial ACK or using F-RTO. */\nstatic bool tcp_try_undo_loss(struct sock *sk, bool frto_undo)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (frto_undo || tcp_may_undo(tp)) {\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\n\t\tDBGUNDO(sk, \"partial loss\");\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSUNDO);\n\t\tif (frto_undo)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPSPURIOUSRTOS);\n\t\tWRITE_ONCE(inet_csk(sk)->icsk_retransmits, 0);\n\t\tif (tcp_is_non_sack_preventing_reopen(sk))\n\t\t\treturn true;\n\t\tif (frto_undo || tcp_is_sack(tp)) {\n\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\t\ttp->is_sack_reneg = 0;\n\t\t}\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* The cwnd reduction in CWR and Recovery uses the PRR algorithm in RFC 6937.\n * It computes the number of packets to send (sndcnt) based on packets newly\n * delivered:\n *   1) If the packets in flight is larger than ssthresh, PRR spreads the\n *\tcwnd reductions across a full RTT.\n *   2) Otherwise PRR uses packet conservation to send as much as delivered.\n *      But when SND_UNA is acked without further losses,\n *      slow starts cwnd up to ssthresh to speed up the recovery.\n */\nstatic void tcp_init_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->high_seq = tp->snd_nxt;\n\ttp->tlp_high_seq = 0;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->prior_cwnd = tcp_snd_cwnd(tp);\n\ttp->prr_delivered = 0;\n\ttp->prr_out = 0;\n\ttp->snd_ssthresh = inet_csk(sk)->icsk_ca_ops->ssthresh(sk);\n\ttcp_ecn_queue_cwr(tp);\n}\n\nvoid tcp_cwnd_reduction(struct sock *sk, int newly_acked_sacked, int newly_lost, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint sndcnt = 0;\n\tint delta = tp->snd_ssthresh - tcp_packets_in_flight(tp);\n\n\tif (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))\n\t\treturn;\n\n\ttrace_tcp_cwnd_reduction_tp(sk, newly_acked_sacked, newly_lost, flag);\n\n\ttp->prr_delivered += newly_acked_sacked;\n\tif (delta < 0) {\n\t\tu64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +\n\t\t\t       tp->prior_cwnd - 1;\n\t\tsndcnt = div_u64(dividend, tp->prior_cwnd) - tp->prr_out;\n\t} else {\n\t\tsndcnt = max_t(int, tp->prr_delivered - tp->prr_out,\n\t\t\t       newly_acked_sacked);\n\t\tif (flag & FLAG_SND_UNA_ADVANCED && !newly_lost)\n\t\t\tsndcnt++;\n\t\tsndcnt = min(delta, sndcnt);\n\t}\n\t/* Force a fast retransmit upon entering fast recovery */\n\tsndcnt = max(sndcnt, (tp->prr_out ? 0 : 1));\n\ttcp_snd_cwnd_set(tp, tcp_packets_in_flight(tp) + sndcnt);\n}\n\nstatic inline void tcp_end_cwnd_reduction(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (inet_csk(sk)->icsk_ca_ops->cong_control)\n\t\treturn;\n\n\t/* Reset cwnd to ssthresh in CWR or Recovery (unless it's undone) */\n\tif (tp->snd_ssthresh < TCP_INFINITE_SSTHRESH &&\n\t    (inet_csk(sk)->icsk_ca_state == TCP_CA_CWR || tp->undo_marker)) {\n\t\ttcp_snd_cwnd_set(tp, tp->snd_ssthresh);\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t}\n\ttcp_ca_event(sk, CA_EVENT_COMPLETE_CWR);\n}\n\n/* Enter CWR state. Disable cwnd undo since congestion is proven with ECN */\nvoid tcp_enter_cwr(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->prior_ssthresh = 0;\n\tif (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {\n\t\ttp->undo_marker = 0;\n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t}\n}\nEXPORT_SYMBOL(tcp_enter_cwr);\n\nstatic void tcp_try_keep_open(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint state = TCP_CA_Open;\n\n\tif (tcp_left_out(tp) || tcp_any_retrans_done(sk))\n\t\tstate = TCP_CA_Disorder;\n\n\tif (inet_csk(sk)->icsk_ca_state != state) {\n\t\ttcp_set_ca_state(sk, state);\n\t\ttp->high_seq = tp->snd_nxt;\n\t}\n}\n\nstatic void tcp_try_to_open(struct sock *sk, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_verify_left_out(tp);\n\n\tif (!tcp_any_retrans_done(sk))\n\t\ttp->retrans_stamp = 0;\n\n\tif (flag & FLAG_ECE)\n\t\ttcp_enter_cwr(sk);\n\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_CWR) {\n\t\ttcp_try_keep_open(sk);\n\t}\n}\n\nstatic void tcp_mtup_probe_failed(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_mtup.search_high = icsk->icsk_mtup.probe_size - 1;\n\ticsk->icsk_mtup.probe_size = 0;\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPFAIL);\n}\n\nstatic void tcp_mtup_probe_success(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tu64 val;\n\n\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\n\tval = (u64)tcp_snd_cwnd(tp) * tcp_mss_to_mtu(sk, tp->mss_cache);\n\tdo_div(val, icsk->icsk_mtup.probe_size);\n\tDEBUG_NET_WARN_ON_ONCE((u32)val != val);\n\ttcp_snd_cwnd_set(tp, max_t(u32, 1U, val));\n\n\ttp->snd_cwnd_cnt = 0;\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\n\ticsk->icsk_mtup.search_low = icsk->icsk_mtup.probe_size;\n\ticsk->icsk_mtup.probe_size = 0;\n\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMTUPSUCCESS);\n}\n\n/* Sometimes we deduce that packets have been dropped due to reasons other than\n * congestion, like path MTU reductions or failed client TFO attempts. In these\n * cases we call this function to retransmit as many packets as cwnd allows,\n * without reducing cwnd. Given that retransmits will set retrans_stamp to a\n * non-zero value (and may do so in a later calling context due to TSQ), we\n * also enter CA_Loss so that we track when all retransmitted packets are ACKed\n * and clear retrans_stamp when that happens (to ensure later recurring RTOs\n * are using the correct retrans_stamp and don't declare ETIMEDOUT\n * prematurely).\n */\nstatic void tcp_non_congestion_loss_retransmit(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (icsk->icsk_ca_state != TCP_CA_Loss) {\n\t\ttp->high_seq = tp->snd_nxt;\n\t\ttp->snd_ssthresh = tcp_current_ssthresh(sk);\n\t\ttp->prior_ssthresh = 0;\n\t\ttp->undo_marker = 0;\n\t\ttcp_set_ca_state(sk, TCP_CA_Loss);\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\n\n/* Do a simple retransmit without using the backoff mechanisms in\n * tcp_timer. This is used for path mtu discovery.\n * The socket is already locked here.\n */\nvoid tcp_simple_retransmit(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint mss;\n\n\t/* A fastopen SYN request is stored as two separate packets within\n\t * the retransmit queue, this is done by tcp_send_syn_data().\n\t * As a result simply checking the MSS of the frames in the queue\n\t * will not work for the SYN packet.\n\t *\n\t * Us being here is an indication of a path MTU issue so we can\n\t * assume that the fastopen SYN was lost and just mark all the\n\t * frames in the retransmit queue as lost. We will use an MSS of\n\t * -1 to mark all frames as lost, otherwise compute the current MSS.\n\t */\n\tif (tp->syn_data && sk->sk_state == TCP_SYN_SENT)\n\t\tmss = -1;\n\telse\n\t\tmss = tcp_current_mss(sk);\n\n\tskb_rbtree_walk(skb, &sk->tcp_rtx_queue) {\n\t\tif (tcp_skb_seglen(skb) > mss)\n\t\t\ttcp_mark_skb_lost(sk, skb);\n\t}\n\n\tif (!tp->lost_out)\n\t\treturn;\n\n\tif (tcp_is_reno(tp))\n\t\ttcp_limit_reno_sacked(tp);\n\n\ttcp_verify_left_out(tp);\n\n\t/* Don't muck with the congestion window here.\n\t * Reason is that we do not increase amount of _data_\n\t * in network, but units changed and effective\n\t * cwnd/ssthresh really reduced now.\n\t */\n\ttcp_non_congestion_loss_retransmit(sk);\n}\nEXPORT_IPV6_MOD(tcp_simple_retransmit);\n\nvoid tcp_enter_recovery(struct sock *sk, bool ece_ack)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mib_idx;\n\n\t/* Start the clock with our fast retransmit, for undo and ETIMEDOUT. */\n\ttcp_retrans_stamp_cleanup(sk);\n\n\tif (tcp_is_reno(tp))\n\t\tmib_idx = LINUX_MIB_TCPRENORECOVERY;\n\telse\n\t\tmib_idx = LINUX_MIB_TCPSACKRECOVERY;\n\n\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\ttp->prior_ssthresh = 0;\n\ttcp_init_undo(tp);\n\n\tif (!tcp_in_cwnd_reduction(sk)) {\n\t\tif (!ece_ack)\n\t\t\ttp->prior_ssthresh = tcp_current_ssthresh(sk);\n\t\ttcp_init_cwnd_reduction(sk);\n\t}\n\ttcp_set_ca_state(sk, TCP_CA_Recovery);\n}\n\nstatic void tcp_update_rto_time(struct tcp_sock *tp)\n{\n\tif (tp->rto_stamp) {\n\t\ttp->total_rto_time += tcp_time_stamp_ms(tp) - tp->rto_stamp;\n\t\ttp->rto_stamp = 0;\n\t}\n}\n\n/* Process an ACK in CA_Loss state. Move to CA_Open if lost data are\n * recovered or spurious. Otherwise retransmits more on partial ACKs.\n */\nstatic void tcp_process_loss(struct sock *sk, int flag, int num_dupack,\n\t\t\t     int *rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool recovered = !before(tp->snd_una, tp->high_seq);\n\n\tif ((flag & FLAG_SND_UNA_ADVANCED || rcu_access_pointer(tp->fastopen_rsk)) &&\n\t    tcp_try_undo_loss(sk, false))\n\t\treturn;\n\n\tif (tp->frto) { /* F-RTO RFC5682 sec 3.1 (sack enhanced version). */\n\t\t/* Step 3.b. A timeout is spurious if not all data are\n\t\t * lost, i.e., never-retransmitted data are (s)acked.\n\t\t */\n\t\tif ((flag & FLAG_ORIG_SACK_ACKED) &&\n\t\t    tcp_try_undo_loss(sk, true))\n\t\t\treturn;\n\n\t\tif (after(tp->snd_nxt, tp->high_seq)) {\n\t\t\tif (flag & FLAG_DATA_SACKED || num_dupack)\n\t\t\t\ttp->frto = 0; /* Step 3.a. loss was real */\n\t\t} else if (flag & FLAG_SND_UNA_ADVANCED && !recovered) {\n\t\t\ttp->high_seq = tp->snd_nxt;\n\t\t\t/* Step 2.b. Try send new data (but deferred until cwnd\n\t\t\t * is updated in tcp_ack()). Otherwise fall back to\n\t\t\t * the conventional recovery.\n\t\t\t */\n\t\t\tif (!tcp_write_queue_empty(sk) &&\n\t\t\t    after(tcp_wnd_end(tp), tp->snd_nxt)) {\n\t\t\t\t*rexmit = REXMIT_NEW;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\ttp->frto = 0;\n\t\t}\n\t}\n\n\tif (recovered) {\n\t\t/* F-RTO RFC5682 sec 3.1 step 2.a and 1st part of step 3.a */\n\t\ttcp_try_undo_recovery(sk);\n\t\treturn;\n\t}\n\tif (tcp_is_reno(tp)) {\n\t\t/* A Reno DUPACK means new data in F-RTO step 2.b above are\n\t\t * delivered. Lower inflight to clock out (re)transmissions.\n\t\t */\n\t\tif (after(tp->snd_nxt, tp->high_seq) && num_dupack)\n\t\t\ttcp_add_reno_sack(sk, num_dupack, flag & FLAG_ECE);\n\t\telse if (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\ttcp_reset_reno_sack(tp);\n\t}\n\t*rexmit = REXMIT_LOST;\n}\n\n/* Undo during fast recovery after partial ACK. */\nstatic bool tcp_try_undo_partial(struct sock *sk, u32 prior_snd_una)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->undo_marker && tcp_packet_delayed(tp)) {\n\t\t/* Plain luck! Hole if filled with delayed\n\t\t * packet, rather than with a retransmit. Check reordering.\n\t\t */\n\t\ttcp_check_sack_reordering(sk, prior_snd_una, 1);\n\n\t\t/* We are getting evidence that the reordering degree is higher\n\t\t * than we realized. If there are no retransmits out then we\n\t\t * can undo. Otherwise we clock out new packets but do not\n\t\t * mark more packets lost or retransmit more.\n\t\t */\n\t\tif (tp->retrans_out)\n\t\t\treturn true;\n\n\t\tif (!tcp_any_retrans_done(sk))\n\t\t\ttp->retrans_stamp = 0;\n\n\t\tDBGUNDO(sk, \"partial recovery\");\n\t\ttcp_undo_cwnd_reduction(sk, true);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPARTIALUNDO);\n\t\ttcp_try_keep_open(sk);\n\t}\n\treturn false;\n}\n\nstatic void tcp_identify_packet_loss(struct sock *sk, int *ack_flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_rtx_queue_empty(sk))\n\t\treturn;\n\n\tif (unlikely(tcp_is_reno(tp))) {\n\t\ttcp_newreno_mark_lost(sk, *ack_flag & FLAG_SND_UNA_ADVANCED);\n\t} else {\n\t\tu32 prior_retrans = tp->retrans_out;\n\n\t\tif (tcp_rack_mark_lost(sk))\n\t\t\t*ack_flag &= ~FLAG_SET_XMIT_TIMER;\n\t\tif (prior_retrans > tp->retrans_out)\n\t\t\t*ack_flag |= FLAG_LOST_RETRANS;\n\t}\n}\n\n/* Process an event, which can update packets-in-flight not trivially.\n * Main goal of this function is to calculate new estimate for left_out,\n * taking into account both packets sitting in receiver's buffer and\n * packets lost by network.\n *\n * Besides that it updates the congestion state when packet loss or ECN\n * is detected. But it does not reduce the cwnd, it is done by the\n * congestion control later.\n *\n * It does _not_ decide what to send, it is made in function\n * tcp_xmit_retransmit_queue().\n */\nstatic void tcp_fastretrans_alert(struct sock *sk, const u32 prior_snd_una,\n\t\t\t\t  int num_dupack, int *ack_flag, int *rexmit)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint flag = *ack_flag;\n\tbool ece_ack = flag & FLAG_ECE;\n\n\tif (!tp->packets_out && tp->sacked_out)\n\t\ttp->sacked_out = 0;\n\n\t/* Now state machine starts.\n\t * A. ECE, hence prohibit cwnd undoing, the reduction is required. */\n\tif (ece_ack)\n\t\ttp->prior_ssthresh = 0;\n\n\t/* B. In all the states check for reneging SACKs. */\n\tif (tcp_check_sack_reneging(sk, ack_flag))\n\t\treturn;\n\n\t/* C. Check consistency of the current state. */\n\ttcp_verify_left_out(tp);\n\n\t/* D. Check state exit conditions. State can be terminated\n\t *    when high_seq is ACKed. */\n\tif (icsk->icsk_ca_state == TCP_CA_Open) {\n\t\tWARN_ON(tp->retrans_out != 0 && !tp->syn_data);\n\t\ttp->retrans_stamp = 0;\n\t} else if (!before(tp->snd_una, tp->high_seq)) {\n\t\tswitch (icsk->icsk_ca_state) {\n\t\tcase TCP_CA_CWR:\n\t\t\t/* CWR is to be held something *above* high_seq\n\t\t\t * is ACKed for CWR bit to reach receiver. */\n\t\t\tif (tp->snd_una != tp->high_seq) {\n\t\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\t\ttcp_set_ca_state(sk, TCP_CA_Open);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_CA_Recovery:\n\t\t\tif (tcp_is_reno(tp))\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\tif (tcp_try_undo_recovery(sk))\n\t\t\t\treturn;\n\t\t\ttcp_end_cwnd_reduction(sk);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* E. Process state. */\n\tswitch (icsk->icsk_ca_state) {\n\tcase TCP_CA_Recovery:\n\t\tif (!(flag & FLAG_SND_UNA_ADVANCED)) {\n\t\t\tif (tcp_is_reno(tp))\n\t\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\n\t\t} else if (tcp_try_undo_partial(sk, prior_snd_una))\n\t\t\treturn;\n\n\t\tif (tcp_try_undo_dsack(sk))\n\t\t\ttcp_try_to_open(sk, flag);\n\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (icsk->icsk_ca_state != TCP_CA_Recovery) {\n\t\t\tif (!tcp_time_to_recover(tp))\n\t\t\t\treturn;\n\t\t\t/* Undo reverts the recovery state. If loss is evident,\n\t\t\t * starts a new recovery (e.g. reordering then loss);\n\t\t\t */\n\t\t\ttcp_enter_recovery(sk, ece_ack);\n\t\t}\n\t\tbreak;\n\tcase TCP_CA_Loss:\n\t\ttcp_process_loss(sk, flag, num_dupack, rexmit);\n\t\tif (icsk->icsk_ca_state != TCP_CA_Loss)\n\t\t\ttcp_update_rto_time(tp);\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (!(icsk->icsk_ca_state == TCP_CA_Open ||\n\t\t      (*ack_flag & FLAG_LOST_RETRANS)))\n\t\t\treturn;\n\t\t/* Change state if cwnd is undone or retransmits are lost */\n\t\tfallthrough;\n\tdefault:\n\t\tif (tcp_is_reno(tp)) {\n\t\t\tif (flag & FLAG_SND_UNA_ADVANCED)\n\t\t\t\ttcp_reset_reno_sack(tp);\n\t\t\ttcp_add_reno_sack(sk, num_dupack, ece_ack);\n\t\t}\n\n\t\tif (icsk->icsk_ca_state <= TCP_CA_Disorder)\n\t\t\ttcp_try_undo_dsack(sk);\n\n\t\ttcp_identify_packet_loss(sk, ack_flag);\n\t\tif (!tcp_time_to_recover(tp)) {\n\t\t\ttcp_try_to_open(sk, flag);\n\t\t\treturn;\n\t\t}\n\n\t\t/* MTU probe failure: don't reduce cwnd */\n\t\tif (icsk->icsk_ca_state < TCP_CA_CWR &&\n\t\t    icsk->icsk_mtup.probe_size &&\n\t\t    tp->snd_una == tp->mtu_probe.probe_seq_start) {\n\t\t\ttcp_mtup_probe_failed(sk);\n\t\t\t/* Restores the reduction we did in tcp_mtup_probe() */\n\t\t\ttcp_snd_cwnd_set(tp, tcp_snd_cwnd(tp) + 1);\n\t\t\ttcp_simple_retransmit(sk);\n\t\t\treturn;\n\t\t}\n\n\t\t/* Otherwise enter Recovery state */\n\t\ttcp_enter_recovery(sk, ece_ack);\n\t}\n\n\t*rexmit = REXMIT_LOST;\n}\n\nstatic void tcp_update_rtt_min(struct sock *sk, u32 rtt_us, const int flag)\n{\n\tu32 wlen = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_min_rtt_wlen) * HZ;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif ((flag & FLAG_ACK_MAYBE_DELAYED) && rtt_us > tcp_min_rtt(tp)) {\n\t\t/* If the remote keeps returning delayed ACKs, eventually\n\t\t * the min filter would pick it up and overestimate the\n\t\t * prop. delay when it expires. Skip suspected delayed ACKs.\n\t\t */\n\t\treturn;\n\t}\n\tminmax_running_min(&tp->rtt_min, wlen, tcp_jiffies32,\n\t\t\t   rtt_us ? : jiffies_to_usecs(1));\n}\n\nstatic bool tcp_ack_update_rtt(struct sock *sk, const int flag,\n\t\t\t       long seq_rtt_us, long sack_rtt_us,\n\t\t\t       long ca_rtt_us, struct rate_sample *rs)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Prefer RTT measured from ACK's timing to TS-ECR. This is because\n\t * broken middle-boxes or peers may corrupt TS-ECR fields. But\n\t * Karn's algorithm forbids taking RTT if some retransmitted data\n\t * is acked (RFC6298).\n\t */\n\tif (seq_rtt_us < 0)\n\t\tseq_rtt_us = sack_rtt_us;\n\n\t/* RTTM Rule: A TSecr value received in a segment is used to\n\t * update the averaged RTT measurement only if the segment\n\t * acknowledges some new data, i.e., only if it advances the\n\t * left edge of the send window.\n\t * See draft-ietf-tcplw-high-performance-00, section 3.3.\n\t */\n\tif (seq_rtt_us < 0 && tp->rx_opt.saw_tstamp &&\n\t    tp->rx_opt.rcv_tsecr && flag & FLAG_ACKED)\n\t\tseq_rtt_us = ca_rtt_us = tcp_rtt_tsopt_us(tp, 1);\n\n\trs->rtt_us = ca_rtt_us; /* RTT of last (S)ACKed packet (or -1) */\n\tif (seq_rtt_us < 0)\n\t\treturn false;\n\n\t/* ca_rtt_us >= 0 is counting on the invariant that ca_rtt_us is\n\t * always taken together with ACK, SACK, or TS-opts. Any negative\n\t * values will be skipped with the seq_rtt_us < 0 check above.\n\t */\n\ttcp_update_rtt_min(sk, ca_rtt_us, flag);\n\ttcp_rtt_estimator(sk, seq_rtt_us);\n\ttcp_set_rto(sk);\n\n\t/* RFC6298: only reset backoff on valid RTT measurement. */\n\tinet_csk(sk)->icsk_backoff = 0;\n\treturn true;\n}\n\n/* Compute time elapsed between (last) SYNACK and the ACK completing 3WHS. */\nvoid tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req)\n{\n\tstruct rate_sample rs;\n\tlong rtt_us = -1L;\n\n\tif (req && !req->num_retrans && tcp_rsk(req)->snt_synack)\n\t\trtt_us = tcp_stamp_us_delta(tcp_clock_us(), tcp_rsk(req)->snt_synack);\n\n\ttcp_ack_update_rtt(sk, FLAG_SYN_ACKED, rtt_us, -1L, rtt_us, &rs);\n}\n\n\nstatic void tcp_cong_avoid(struct sock *sk, u32 ack, u32 acked)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\ticsk->icsk_ca_ops->cong_avoid(sk, ack, acked);\n\ttcp_sk(sk)->snd_cwnd_stamp = tcp_jiffies32;\n}\n\n/* Restart timer after forward progress on connection.\n * RFC2988 recommends to restart timer to now+rto.\n */\nvoid tcp_rearm_rto(struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If the retrans timer is currently being used by Fast Open\n\t * for SYN-ACK retrans purpose, stay put.\n\t */\n\tif (rcu_access_pointer(tp->fastopen_rsk))\n\t\treturn;\n\n\tif (!tp->packets_out) {\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);\n\t} else {\n\t\tu32 rto = inet_csk(sk)->icsk_rto;\n\t\t/* Offset the time elapsed after installing regular RTO */\n\t\tif (icsk->icsk_pending == ICSK_TIME_REO_TIMEOUT ||\n\t\t    icsk->icsk_pending == ICSK_TIME_LOSS_PROBE) {\n\t\t\ts64 delta_us = tcp_rto_delta_us(sk);\n\t\t\t/* delta_us may not be positive if the socket is locked\n\t\t\t * when the retrans timer fires and is rescheduled.\n\t\t\t */\n\t\t\trto = usecs_to_jiffies(max_t(int, delta_us, 1));\n\t\t}\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS, rto, true);\n\t}\n}\n\n/* Try to schedule a loss probe; if that doesn't work, then schedule an RTO. */\nstatic void tcp_set_xmit_timer(struct sock *sk)\n{\n\tif (!tcp_schedule_loss_probe(sk, true))\n\t\ttcp_rearm_rto(sk);\n}\n\n/* If we get here, the whole TSO packet has not been acked. */\nstatic u32 tcp_tso_acked(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 packets_acked;\n\n\tBUG_ON(!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una));\n\n\tpackets_acked = tcp_skb_pcount(skb);\n\tif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\n\t\treturn 0;\n\tpackets_acked -= tcp_skb_pcount(skb);\n\n\tif (packets_acked) {\n\t\tBUG_ON(tcp_skb_pcount(skb) == 0);\n\t\tBUG_ON(!before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq));\n\t}\n\n\treturn packets_acked;\n}\n\nstatic void tcp_ack_tstamp(struct sock *sk, struct sk_buff *skb,\n\t\t\t   const struct sk_buff *ack_skb, u32 prior_snd_una)\n{\n\tconst struct skb_shared_info *shinfo;\n\n\t/* Avoid cache line misses to get skb_shinfo() and shinfo->tx_flags */\n\tif (likely(!TCP_SKB_CB(skb)->txstamp_ack))\n\t\treturn;\n\n\tshinfo = skb_shinfo(skb);\n\tif (!before(shinfo->tskey, prior_snd_una) &&\n\t    before(shinfo->tskey, tcp_sk(sk)->snd_una)) {\n\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t__skb_tstamp_tx(skb, ack_skb, NULL, sk, SCM_TSTAMP_ACK);\n\t\t} tcp_skb_tsorted_restore(skb);\n\t}\n}\n\n/* Remove acknowledged frames from the retransmission queue. If our packet\n * is before the ack sequence we can discard it as it's confirmed to have\n * arrived at the other end.\n */\nstatic int tcp_clean_rtx_queue(struct sock *sk, const struct sk_buff *ack_skb,\n\t\t\t       u32 prior_fack, u32 prior_snd_una,\n\t\t\t       struct tcp_sacktag_state *sack, bool ece_ack)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu64 first_ackt, last_ackt;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 prior_sacked = tp->sacked_out;\n\tu32 reord = tp->snd_nxt; /* lowest acked un-retx un-sacked seq */\n\tstruct sk_buff *skb, *next;\n\tbool fully_acked = true;\n\tlong sack_rtt_us = -1L;\n\tlong seq_rtt_us = -1L;\n\tlong ca_rtt_us = -1L;\n\tu32 pkts_acked = 0;\n\tbool rtt_update;\n\tint flag = 0;\n\n\tfirst_ackt = 0;\n\n\tfor (skb = skb_rb_first(&sk->tcp_rtx_queue); skb; skb = next) {\n\t\tstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\n\t\tconst u32 start_seq = scb->seq;\n\t\tu8 sacked = scb->sacked;\n\t\tu32 acked_pcount;\n\n\t\t/* Determine how many packets and what bytes were acked, tso and else */\n\t\tif (after(scb->end_seq, tp->snd_una)) {\n\t\t\tif (tcp_skb_pcount(skb) == 1 ||\n\t\t\t    !after(tp->snd_una, scb->seq))\n\t\t\t\tbreak;\n\n\t\t\tacked_pcount = tcp_tso_acked(sk, skb);\n\t\t\tif (!acked_pcount)\n\t\t\t\tbreak;\n\t\t\tfully_acked = false;\n\t\t} else {\n\t\t\tacked_pcount = tcp_skb_pcount(skb);\n\t\t}\n\n\t\tif (unlikely(sacked & TCPCB_RETRANS)) {\n\t\t\tif (sacked & TCPCB_SACKED_RETRANS)\n\t\t\t\ttp->retrans_out -= acked_pcount;\n\t\t\tflag |= FLAG_RETRANS_DATA_ACKED;\n\t\t} else if (!(sacked & TCPCB_SACKED_ACKED)) {\n\t\t\tlast_ackt = tcp_skb_timestamp_us(skb);\n\t\t\tWARN_ON_ONCE(last_ackt == 0);\n\t\t\tif (!first_ackt)\n\t\t\t\tfirst_ackt = last_ackt;\n\n\t\t\tif (before(start_seq, reord))\n\t\t\t\treord = start_seq;\n\t\t\tif (!after(scb->end_seq, tp->high_seq))\n\t\t\t\tflag |= FLAG_ORIG_SACK_ACKED;\n\t\t}\n\n\t\tif (sacked & TCPCB_SACKED_ACKED) {\n\t\t\ttp->sacked_out -= acked_pcount;\n\t\t\t/* snd_una delta covers these skbs */\n\t\t\tsack->delivered_bytes -= skb->len;\n\t\t} else if (tcp_is_sack(tp)) {\n\t\t\ttcp_count_delivered(tp, acked_pcount, ece_ack);\n\t\t\tif (!tcp_skb_spurious_retrans(tp, skb))\n\t\t\t\ttcp_rack_advance(tp, sacked, scb->end_seq,\n\t\t\t\t\t\t tcp_skb_timestamp_us(skb));\n\t\t}\n\t\tif (sacked & TCPCB_LOST)\n\t\t\ttp->lost_out -= acked_pcount;\n\n\t\ttp->packets_out -= acked_pcount;\n\t\tpkts_acked += acked_pcount;\n\t\ttcp_rate_skb_delivered(sk, skb, sack->rate);\n\n\t\t/* Initial outgoing SYN's get put onto the write_queue\n\t\t * just like anything else we transmit.  It is not\n\t\t * true data, and if we misinform our callers that\n\t\t * this ACK acks real data, we will erroneously exit\n\t\t * connection startup slow start one packet too\n\t\t * quickly.  This is severely frowned upon behavior.\n\t\t */\n\t\tif (likely(!(scb->tcp_flags & TCPHDR_SYN))) {\n\t\t\tflag |= FLAG_DATA_ACKED;\n\t\t} else {\n\t\t\tflag |= FLAG_SYN_ACKED;\n\t\t\ttp->retrans_stamp = 0;\n\t\t}\n\n\t\tif (!fully_acked)\n\t\t\tbreak;\n\n\t\ttcp_ack_tstamp(sk, skb, ack_skb, prior_snd_una);\n\n\t\tnext = skb_rb_next(skb);\n\t\tif (unlikely(skb == tp->retransmit_skb_hint))\n\t\t\ttp->retransmit_skb_hint = NULL;\n\t\ttcp_highest_sack_replace(sk, skb, next);\n\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t}\n\n\tif (!skb)\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_BUSY);\n\n\tif (likely(between(tp->snd_up, prior_snd_una, tp->snd_una)))\n\t\ttp->snd_up = tp->snd_una;\n\n\tif (skb) {\n\t\ttcp_ack_tstamp(sk, skb, ack_skb, prior_snd_una);\n\t\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\t\tflag |= FLAG_SACK_RENEGING;\n\t}\n\n\tif (likely(first_ackt) && !(flag & FLAG_RETRANS_DATA_ACKED)) {\n\t\tseq_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, first_ackt);\n\t\tca_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, last_ackt);\n\n\t\tif (pkts_acked == 1 && fully_acked && !prior_sacked &&\n\t\t    (tp->snd_una - prior_snd_una) < tp->mss_cache &&\n\t\t    sack->rate->prior_delivered + 1 == tp->delivered &&\n\t\t    !(flag & (FLAG_CA_ALERT | FLAG_SYN_ACKED))) {\n\t\t\t/* Conservatively mark a delayed ACK. It's typically\n\t\t\t * from a lone runt packet over the round trip to\n\t\t\t * a receiver w/o out-of-order or CE events.\n\t\t\t */\n\t\t\tflag |= FLAG_ACK_MAYBE_DELAYED;\n\t\t}\n\t}\n\tif (sack->first_sackt) {\n\t\tsack_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, sack->first_sackt);\n\t\tca_rtt_us = tcp_stamp_us_delta(tp->tcp_mstamp, sack->last_sackt);\n\t}\n\trtt_update = tcp_ack_update_rtt(sk, flag, seq_rtt_us, sack_rtt_us,\n\t\t\t\t\tca_rtt_us, sack->rate);\n\n\tif (flag & FLAG_ACKED) {\n\t\tflag |= FLAG_SET_XMIT_TIMER;  /* set TLP or RTO timer */\n\t\tif (unlikely(icsk->icsk_mtup.probe_size &&\n\t\t\t     !after(tp->mtu_probe.probe_seq_end, tp->snd_una))) {\n\t\t\ttcp_mtup_probe_success(sk);\n\t\t}\n\n\t\tif (tcp_is_reno(tp)) {\n\t\t\ttcp_remove_reno_sacks(sk, pkts_acked, ece_ack);\n\n\t\t\t/* If any of the cumulatively ACKed segments was\n\t\t\t * retransmitted, non-SACK case cannot confirm that\n\t\t\t * progress was due to original transmission due to\n\t\t\t * lack of TCPCB_SACKED_ACKED bits even if some of\n\t\t\t * the packets may have been never retransmitted.\n\t\t\t */\n\t\t\tif (flag & FLAG_RETRANS_DATA_ACKED)\n\t\t\t\tflag &= ~FLAG_ORIG_SACK_ACKED;\n\t\t} else {\n\t\t\t/* Non-retransmitted hole got filled? That's reordering */\n\t\t\tif (before(reord, prior_fack))\n\t\t\t\ttcp_check_sack_reordering(sk, reord, 0);\n\t\t}\n\n\t\tsack->delivered_bytes = (skb ?\n\t\t\t\t\t TCP_SKB_CB(skb)->seq : tp->snd_una) -\n\t\t\t\t\t prior_snd_una;\n\t} else if (skb && rtt_update && sack_rtt_us >= 0 &&\n\t\t   sack_rtt_us > tcp_stamp_us_delta(tp->tcp_mstamp,\n\t\t\t\t\t\t    tcp_skb_timestamp_us(skb))) {\n\t\t/* Do not re-arm RTO if the sack RTT is measured from data sent\n\t\t * after when the head was last (re)transmitted. Otherwise the\n\t\t * timeout may continue to extend in loss recovery.\n\t\t */\n\t\tflag |= FLAG_SET_XMIT_TIMER;  /* set TLP or RTO timer */\n\t}\n\n\tif (icsk->icsk_ca_ops->pkts_acked) {\n\t\tstruct ack_sample sample = { .pkts_acked = pkts_acked,\n\t\t\t\t\t     .rtt_us = sack->rate->rtt_us };\n\n\t\tsample.in_flight = tp->mss_cache *\n\t\t\t(tp->delivered - sack->rate->prior_delivered);\n\t\ticsk->icsk_ca_ops->pkts_acked(sk, &sample);\n\t}\n\n#if FASTRETRANS_DEBUG > 0\n\tWARN_ON((int)tp->sacked_out < 0);\n\tWARN_ON((int)tp->lost_out < 0);\n\tWARN_ON((int)tp->retrans_out < 0);\n\tif (!tp->packets_out && tcp_is_sack(tp)) {\n\t\ticsk = inet_csk(sk);\n\t\tif (tp->lost_out) {\n\t\t\tpr_debug(\"Leak l=%u %d\\n\",\n\t\t\t\t tp->lost_out, icsk->icsk_ca_state);\n\t\t\ttp->lost_out = 0;\n\t\t}\n\t\tif (tp->sacked_out) {\n\t\t\tpr_debug(\"Leak s=%u %d\\n\",\n\t\t\t\t tp->sacked_out, icsk->icsk_ca_state);\n\t\t\ttp->sacked_out = 0;\n\t\t}\n\t\tif (tp->retrans_out) {\n\t\t\tpr_debug(\"Leak r=%u %d\\n\",\n\t\t\t\t tp->retrans_out, icsk->icsk_ca_state);\n\t\t\ttp->retrans_out = 0;\n\t\t}\n\t}\n#endif\n\treturn flag;\n}\n\nstatic void tcp_ack_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct sk_buff *head = tcp_send_head(sk);\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Was it a usable window open? */\n\tif (!head)\n\t\treturn;\n\tif (!after(TCP_SKB_CB(head)->end_seq, tcp_wnd_end(tp))) {\n\t\ticsk->icsk_backoff = 0;\n\t\ticsk->icsk_probes_tstamp = 0;\n\t\tinet_csk_clear_xmit_timer(sk, ICSK_TIME_PROBE0);\n\t\t/* Socket must be waked up by subsequent tcp_data_snd_check().\n\t\t * This function is not for random using!\n\t\t */\n\t} else {\n\t\tunsigned long when = tcp_probe0_when(sk, tcp_rto_max(sk));\n\n\t\twhen = tcp_clamp_probe0_to_user_timeout(sk, when);\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, when, true);\n\t}\n}\n\nstatic inline bool tcp_ack_is_dubious(const struct sock *sk, const int flag)\n{\n\treturn !(flag & FLAG_NOT_DUP) || (flag & FLAG_CA_ALERT) ||\n\t\tinet_csk(sk)->icsk_ca_state != TCP_CA_Open;\n}\n\n/* Decide wheather to run the increase function of congestion control. */\nstatic inline bool tcp_may_raise_cwnd(const struct sock *sk, const int flag)\n{\n\t/* If reordering is high then always grow cwnd whenever data is\n\t * delivered regardless of its ordering. Otherwise stay conservative\n\t * and only grow cwnd on in-order delivery (RFC5681). A stretched ACK w/\n\t * new SACK or ECE mark may first advance cwnd here and later reduce\n\t * cwnd in tcp_fastretrans_alert() based on more states.\n\t */\n\tif (tcp_sk(sk)->reordering >\n\t    READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_reordering))\n\t\treturn flag & FLAG_FORWARD_PROGRESS;\n\n\treturn flag & FLAG_DATA_ACKED;\n}\n\n/* The \"ultimate\" congestion control function that aims to replace the rigid\n * cwnd increase and decrease control (tcp_cong_avoid,tcp_*cwnd_reduction).\n * It's called toward the end of processing an ACK with precise rate\n * information. All transmission or retransmission are delayed afterwards.\n */\nstatic void tcp_cong_control(struct sock *sk, u32 ack, u32 acked_sacked,\n\t\t\t     int flag, const struct rate_sample *rs)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->cong_control) {\n\t\ticsk->icsk_ca_ops->cong_control(sk, ack, flag, rs);\n\t\treturn;\n\t}\n\n\tif (tcp_in_cwnd_reduction(sk)) {\n\t\t/* Reduce cwnd if state mandates */\n\t\ttcp_cwnd_reduction(sk, acked_sacked, rs->losses, flag);\n\t} else if (tcp_may_raise_cwnd(sk, flag)) {\n\t\t/* Advance cwnd if state allows */\n\t\ttcp_cong_avoid(sk, ack, acked_sacked);\n\t}\n\ttcp_update_pacing_rate(sk);\n}\n\n/* Check that window update is acceptable.\n * The function assumes that snd_una<=ack<=snd_next.\n */\nstatic inline bool tcp_may_update_window(const struct tcp_sock *tp,\n\t\t\t\t\tconst u32 ack, const u32 ack_seq,\n\t\t\t\t\tconst u32 nwin)\n{\n\treturn\tafter(ack, tp->snd_una) ||\n\t\tafter(ack_seq, tp->snd_wl1) ||\n\t\t(ack_seq == tp->snd_wl1 && (nwin > tp->snd_wnd || !nwin));\n}\n\nstatic void tcp_snd_sne_update(struct tcp_sock *tp, u32 ack)\n{\n#ifdef CONFIG_TCP_AO\n\tstruct tcp_ao_info *ao;\n\n\tif (!static_branch_unlikely(&tcp_ao_needed.key))\n\t\treturn;\n\n\tao = rcu_dereference_protected(tp->ao_info,\n\t\t\t\t       lockdep_sock_is_held((struct sock *)tp));\n\tif (ao && ack < tp->snd_una) {\n\t\tao->snd_sne++;\n\t\ttrace_tcp_ao_snd_sne_update((struct sock *)tp, ao->snd_sne);\n\t}\n#endif\n}\n\n/* If we update tp->snd_una, also update tp->bytes_acked */\nstatic void tcp_snd_una_update(struct tcp_sock *tp, u32 ack)\n{\n\tu32 delta = ack - tp->snd_una;\n\n\tsock_owned_by_me((struct sock *)tp);\n\ttp->bytes_acked += delta;\n\ttcp_snd_sne_update(tp, ack);\n\ttp->snd_una = ack;\n}\n\nstatic void tcp_rcv_sne_update(struct tcp_sock *tp, u32 seq)\n{\n#ifdef CONFIG_TCP_AO\n\tstruct tcp_ao_info *ao;\n\n\tif (!static_branch_unlikely(&tcp_ao_needed.key))\n\t\treturn;\n\n\tao = rcu_dereference_protected(tp->ao_info,\n\t\t\t\t       lockdep_sock_is_held((struct sock *)tp));\n\tif (ao && seq < tp->rcv_nxt) {\n\t\tao->rcv_sne++;\n\t\ttrace_tcp_ao_rcv_sne_update((struct sock *)tp, ao->rcv_sne);\n\t}\n#endif\n}\n\n/* If we update tp->rcv_nxt, also update tp->bytes_received */\nstatic void tcp_rcv_nxt_update(struct tcp_sock *tp, u32 seq)\n{\n\tu32 delta = seq - tp->rcv_nxt;\n\n\tsock_owned_by_me((struct sock *)tp);\n\ttp->bytes_received += delta;\n\ttcp_rcv_sne_update(tp, seq);\n\tWRITE_ONCE(tp->rcv_nxt, seq);\n}\n\n/* Update our send window.\n *\n * Window update algorithm, described in RFC793/RFC1122 (used in linux-2.2\n * and in FreeBSD. NetBSD's one is even worse.) is wrong.\n */\nstatic int tcp_ack_update_window(struct sock *sk, const struct sk_buff *skb, u32 ack,\n\t\t\t\t u32 ack_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint flag = 0;\n\tu32 nwin = ntohs(tcp_hdr(skb)->window);\n\n\tif (likely(!tcp_hdr(skb)->syn))\n\t\tnwin <<= tp->rx_opt.snd_wscale;\n\n\tif (tcp_may_update_window(tp, ack, ack_seq, nwin)) {\n\t\tflag |= FLAG_WIN_UPDATE;\n\t\ttcp_update_wl(tp, ack_seq);\n\n\t\tif (tp->snd_wnd != nwin) {\n\t\t\ttp->snd_wnd = nwin;\n\n\t\t\t/* Note, it is the only place, where\n\t\t\t * fast path is recovered for sending TCP.\n\t\t\t */\n\t\t\ttp->pred_flags = 0;\n\t\t\ttcp_fast_path_check(sk);\n\n\t\t\tif (!tcp_write_queue_empty(sk))\n\t\t\t\ttcp_slow_start_after_idle_check(sk);\n\n\t\t\tif (nwin > tp->max_window) {\n\t\t\t\ttp->max_window = nwin;\n\t\t\t\ttcp_sync_mss(sk, inet_csk(sk)->icsk_pmtu_cookie);\n\t\t\t}\n\t\t}\n\t}\n\n\ttcp_snd_una_update(tp, ack);\n\n\treturn flag;\n}\n\nstatic bool __tcp_oow_rate_limited(struct net *net, int mib_idx,\n\t\t\t\t   u32 *last_oow_ack_time)\n{\n\t/* Paired with the WRITE_ONCE() in this function. */\n\tu32 val = READ_ONCE(*last_oow_ack_time);\n\n\tif (val) {\n\t\ts32 elapsed = (s32)(tcp_jiffies32 - val);\n\n\t\tif (0 <= elapsed &&\n\t\t    elapsed < READ_ONCE(net->ipv4.sysctl_tcp_invalid_ratelimit)) {\n\t\t\tNET_INC_STATS(net, mib_idx);\n\t\t\treturn true;\t/* rate-limited: don't send yet! */\n\t\t}\n\t}\n\n\t/* Paired with the prior READ_ONCE() and with itself,\n\t * as we might be lockless.\n\t */\n\tWRITE_ONCE(*last_oow_ack_time, tcp_jiffies32);\n\n\treturn false;\t/* not rate-limited: go ahead, send dupack now! */\n}\n\n/* Return true if we're currently rate-limiting out-of-window ACKs and\n * thus shouldn't send a dupack right now. We rate-limit dupacks in\n * response to out-of-window SYNs or ACKs to mitigate ACK loops or DoS\n * attacks that send repeated SYNs or ACKs for the same connection. To\n * do this, we do not send a duplicate SYNACK or ACK if the remote\n * endpoint is sending out-of-window SYNs or pure ACKs at a high rate.\n */\nbool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,\n\t\t\t  int mib_idx, u32 *last_oow_ack_time)\n{\n\t/* Data packets without SYNs are not likely part of an ACK loop. */\n\tif ((TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq) &&\n\t    !tcp_hdr(skb)->syn)\n\t\treturn false;\n\n\treturn __tcp_oow_rate_limited(net, mib_idx, last_oow_ack_time);\n}\n\nstatic void tcp_send_ack_reflect_ect(struct sock *sk, bool accecn_reflector)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu16 flags = 0;\n\n\tif (accecn_reflector)\n\t\tflags = tcp_accecn_reflector_flags(tp->syn_ect_rcv);\n\t__tcp_send_ack(sk, tp->rcv_nxt, flags);\n}\n\n/* RFC 5961 7 [ACK Throttling] */\nstatic void tcp_send_challenge_ack(struct sock *sk, bool accecn_reflector)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 count, now, ack_limit;\n\n\t/* First check our per-socket dupack rate limit. */\n\tif (__tcp_oow_rate_limited(net,\n\t\t\t\t   LINUX_MIB_TCPACKSKIPPEDCHALLENGE,\n\t\t\t\t   &tp->last_oow_ack_time))\n\t\treturn;\n\n\tack_limit = READ_ONCE(net->ipv4.sysctl_tcp_challenge_ack_limit);\n\tif (ack_limit == INT_MAX)\n\t\tgoto send_ack;\n\n\t/* Then check host-wide RFC 5961 rate limit. */\n\tnow = jiffies / HZ;\n\tif (now != READ_ONCE(net->ipv4.tcp_challenge_timestamp)) {\n\t\tu32 half = (ack_limit + 1) >> 1;\n\n\t\tWRITE_ONCE(net->ipv4.tcp_challenge_timestamp, now);\n\t\tWRITE_ONCE(net->ipv4.tcp_challenge_count,\n\t\t\t   get_random_u32_inclusive(half, ack_limit + half - 1));\n\t}\n\tcount = READ_ONCE(net->ipv4.tcp_challenge_count);\n\tif (count > 0) {\n\t\tWRITE_ONCE(net->ipv4.tcp_challenge_count, count - 1);\nsend_ack:\n\t\tNET_INC_STATS(net, LINUX_MIB_TCPCHALLENGEACK);\n\t\ttcp_send_ack_reflect_ect(sk, accecn_reflector);\n\t}\n}\n\nstatic void tcp_store_ts_recent(struct tcp_sock *tp)\n{\n\ttp->rx_opt.ts_recent = tp->rx_opt.rcv_tsval;\n\ttp->rx_opt.ts_recent_stamp = ktime_get_seconds();\n}\n\nstatic int __tcp_replace_ts_recent(struct tcp_sock *tp, s32 tstamp_delta)\n{\n\ttcp_store_ts_recent(tp);\n\treturn tstamp_delta > 0 ? FLAG_TS_PROGRESS : 0;\n}\n\nstatic int tcp_replace_ts_recent(struct tcp_sock *tp, u32 seq)\n{\n\ts32 delta;\n\n\tif (tp->rx_opt.saw_tstamp && !after(seq, tp->rcv_wup)) {\n\t\t/* PAWS bug workaround wrt. ACK frames, the PAWS discard\n\t\t * extra check below makes sure this can only happen\n\t\t * for pure ACK frames.  -DaveM\n\t\t *\n\t\t * Not only, also it occurs for expired timestamps.\n\t\t */\n\n\t\tif (tcp_paws_check(&tp->rx_opt, 0)) {\n\t\t\tdelta = tp->rx_opt.rcv_tsval - tp->rx_opt.ts_recent;\n\t\t\treturn __tcp_replace_ts_recent(tp, delta);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* This routine deals with acks during a TLP episode and ends an episode by\n * resetting tlp_high_seq. Ref: TLP algorithm in RFC8985\n */\nstatic void tcp_process_tlp_ack(struct sock *sk, u32 ack, int flag)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (before(ack, tp->tlp_high_seq))\n\t\treturn;\n\n\tif (!tp->tlp_retrans) {\n\t\t/* TLP of new data has been acknowledged */\n\t\ttp->tlp_high_seq = 0;\n\t} else if (flag & FLAG_DSACK_TLP) {\n\t\t/* This DSACK means original and TLP probe arrived; no loss */\n\t\ttp->tlp_high_seq = 0;\n\t} else if (after(ack, tp->tlp_high_seq)) {\n\t\t/* ACK advances: there was a loss, so reduce cwnd. Reset\n\t\t * tlp_high_seq in tcp_init_cwnd_reduction()\n\t\t */\n\t\ttcp_init_cwnd_reduction(sk);\n\t\ttcp_set_ca_state(sk, TCP_CA_CWR);\n\t\ttcp_end_cwnd_reduction(sk);\n\t\ttcp_try_keep_open(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPLOSSPROBERECOVERY);\n\t} else if (!(flag & (FLAG_SND_UNA_ADVANCED |\n\t\t\t     FLAG_NOT_DUP | FLAG_DATA_SACKED))) {\n\t\t/* Pure dupack: original and TLP probe arrived; no loss */\n\t\ttp->tlp_high_seq = 0;\n\t}\n}\n\nstatic void tcp_in_ack_event(struct sock *sk, int flag)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->in_ack_event) {\n\t\tu32 ack_ev_flags = 0;\n\n\t\tif (flag & FLAG_WIN_UPDATE)\n\t\t\tack_ev_flags |= CA_ACK_WIN_UPDATE;\n\t\tif (flag & FLAG_SLOWPATH) {\n\t\t\tack_ev_flags |= CA_ACK_SLOWPATH;\n\t\t\tif (flag & FLAG_ECE)\n\t\t\t\tack_ev_flags |= CA_ACK_ECE;\n\t\t}\n\n\t\ticsk->icsk_ca_ops->in_ack_event(sk, ack_ev_flags);\n\t}\n}\n\n/* Congestion control has updated the cwnd already. So if we're in\n * loss recovery then now we do any new sends (for FRTO) or\n * retransmits (for CA_Loss or CA_recovery) that make sense.\n */\nstatic void tcp_xmit_recovery(struct sock *sk, int rexmit)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (rexmit == REXMIT_NONE || sk->sk_state == TCP_SYN_SENT)\n\t\treturn;\n\n\tif (unlikely(rexmit == REXMIT_NEW)) {\n\t\t__tcp_push_pending_frames(sk, tcp_current_mss(sk),\n\t\t\t\t\t  TCP_NAGLE_OFF);\n\t\tif (after(tp->snd_nxt, tp->high_seq))\n\t\t\treturn;\n\t\ttp->frto = 0;\n\t}\n\ttcp_xmit_retransmit_queue(sk);\n}\n\n/* Returns the number of packets newly acked or sacked by the current ACK */\nstatic u32 tcp_newly_delivered(struct sock *sk, u32 prior_delivered,\n\t\t\t       u32 ecn_count, int flag)\n{\n\tconst struct net *net = sock_net(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 delivered;\n\n\tdelivered = tp->delivered - prior_delivered;\n\tNET_ADD_STATS(net, LINUX_MIB_TCPDELIVERED, delivered);\n\n\tif (flag & FLAG_ECE) {\n\t\tif (tcp_ecn_mode_rfc3168(tp))\n\t\t\tecn_count = delivered;\n\t\tNET_ADD_STATS(net, LINUX_MIB_TCPDELIVEREDCE, ecn_count);\n\t}\n\n\treturn delivered;\n}\n\n/* This routine deals with incoming acks, but not outgoing ones. */\nstatic int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sacktag_state sack_state;\n\tstruct rate_sample rs = { .prior_delivered = 0 };\n\tu32 prior_snd_una = tp->snd_una;\n\tbool is_sack_reneg = tp->is_sack_reneg;\n\tu32 ack_seq = TCP_SKB_CB(skb)->seq;\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\tint num_dupack = 0;\n\tint prior_packets = tp->packets_out;\n\tu32 delivered = tp->delivered;\n\tu32 lost = tp->lost;\n\tint rexmit = REXMIT_NONE; /* Flag to (re)transmit to recover losses */\n\tu32 ecn_count = 0;\t  /* Did we receive ECE/an AccECN ACE update? */\n\tu32 prior_fack;\n\n\tsack_state.first_sackt = 0;\n\tsack_state.rate = &rs;\n\tsack_state.sack_delivered = 0;\n\tsack_state.delivered_bytes = 0;\n\n\t/* We very likely will need to access rtx queue. */\n\tprefetch(sk->tcp_rtx_queue.rb_node);\n\n\t/* If the ack is older than previous acks\n\t * then we can probably ignore it.\n\t */\n\tif (before(ack, prior_snd_una)) {\n\t\tu32 max_window;\n\n\t\t/* do not accept ACK for bytes we never sent. */\n\t\tmax_window = min_t(u64, tp->max_window, tp->bytes_acked);\n\t\t/* RFC 5961 5.2 [Blind Data Injection Attack].[Mitigation] */\n\t\tif (before(ack, prior_snd_una - max_window)) {\n\t\t\tif (!(flag & FLAG_NO_CHALLENGE_ACK))\n\t\t\t\ttcp_send_challenge_ack(sk, false);\n\t\t\treturn -SKB_DROP_REASON_TCP_TOO_OLD_ACK;\n\t\t}\n\t\tgoto old_ack;\n\t}\n\n\t/* If the ack includes data we haven't sent yet, discard\n\t * this segment (RFC793 Section 3.9).\n\t */\n\tif (after(ack, tp->snd_nxt))\n\t\treturn -SKB_DROP_REASON_TCP_ACK_UNSENT_DATA;\n\n\tif (after(ack, prior_snd_una)) {\n\t\tflag |= FLAG_SND_UNA_ADVANCED;\n\t\tWRITE_ONCE(icsk->icsk_retransmits, 0);\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\n\t\tif (static_branch_unlikely(&clean_acked_data_enabled.key))\n\t\t\tif (tp->tcp_clean_acked)\n\t\t\t\ttp->tcp_clean_acked(sk, ack);\n#endif\n\t}\n\n\tprior_fack = tcp_is_sack(tp) ? tcp_highest_sack_seq(tp) : tp->snd_una;\n\trs.prior_in_flight = tcp_packets_in_flight(tp);\n\n\t/* ts_recent update must be made after we are sure that the packet\n\t * is in window.\n\t */\n\tif (flag & FLAG_UPDATE_TS_RECENT)\n\t\tflag |= tcp_replace_ts_recent(tp, TCP_SKB_CB(skb)->seq);\n\n\tif ((flag & (FLAG_SLOWPATH | FLAG_SND_UNA_ADVANCED)) ==\n\t    FLAG_SND_UNA_ADVANCED) {\n\t\t/* Window is constant, pure forward advance.\n\t\t * No more checks are required.\n\t\t * Note, we use the fact that SND.UNA>=SND.WL2.\n\t\t */\n\t\ttcp_update_wl(tp, ack_seq);\n\t\ttcp_snd_una_update(tp, ack);\n\t\tflag |= FLAG_WIN_UPDATE;\n\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPACKS);\n\t} else {\n\t\tif (ack_seq != TCP_SKB_CB(skb)->end_seq)\n\t\t\tflag |= FLAG_DATA;\n\t\telse\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPUREACKS);\n\n\t\tflag |= tcp_ack_update_window(sk, skb, ack, ack_seq);\n\n\t\tif (TCP_SKB_CB(skb)->sacked)\n\t\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t\t&sack_state);\n\n\t\tif (tcp_ecn_rcv_ecn_echo(tp, tcp_hdr(skb)))\n\t\t\tflag |= FLAG_ECE;\n\n\t\tif (sack_state.sack_delivered)\n\t\t\ttcp_count_delivered(tp, sack_state.sack_delivered,\n\t\t\t\t\t    flag & FLAG_ECE);\n\t}\n\n\t/* This is a deviation from RFC3168 since it states that:\n\t * \"When the TCP data sender is ready to set the CWR bit after reducing\n\t * the congestion window, it SHOULD set the CWR bit only on the first\n\t * new data packet that it transmits.\"\n\t * We accept CWR on pure ACKs to be more robust\n\t * with widely-deployed TCP implementations that do this.\n\t */\n\ttcp_ecn_accept_cwr(sk, skb);\n\n\t/* We passed data and got it acked, remove any soft error\n\t * log. Something worked...\n\t */\n\tif (READ_ONCE(sk->sk_err_soft))\n\t\tWRITE_ONCE(sk->sk_err_soft, 0);\n\tWRITE_ONCE(icsk->icsk_probes_out, 0);\n\ttp->rcv_tstamp = tcp_jiffies32;\n\tif (!prior_packets)\n\t\tgoto no_queue;\n\n\t/* See if we can take anything off of the retransmit queue. */\n\tflag |= tcp_clean_rtx_queue(sk, skb, prior_fack, prior_snd_una,\n\t\t\t\t    &sack_state, flag & FLAG_ECE);\n\n\ttcp_rack_update_reo_wnd(sk, &rs);\n\n\tif (tcp_ecn_mode_accecn(tp))\n\t\tecn_count = tcp_accecn_process(sk, skb,\n\t\t\t\t\t       tp->delivered - delivered,\n\t\t\t\t\t       sack_state.delivered_bytes,\n\t\t\t\t\t       &flag);\n\n\ttcp_in_ack_event(sk, flag);\n\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\n\tif (tcp_ack_is_dubious(sk, flag)) {\n\t\tif (!(flag & (FLAG_SND_UNA_ADVANCED |\n\t\t\t      FLAG_NOT_DUP | FLAG_DSACKING_ACK))) {\n\t\t\tnum_dupack = 1;\n\t\t\t/* Consider if pure acks were aggregated in tcp_add_backlog() */\n\t\t\tif (!(flag & FLAG_DATA))\n\t\t\t\tnum_dupack = max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\t\t}\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t}\n\n\t/* If needed, reset TLP/RTO timer when RACK doesn't set. */\n\tif (flag & FLAG_SET_XMIT_TIMER)\n\t\ttcp_set_xmit_timer(sk);\n\n\tif ((flag & FLAG_FORWARD_PROGRESS) || !(flag & FLAG_NOT_DUP))\n\t\tsk_dst_confirm(sk);\n\n\tdelivered = tcp_newly_delivered(sk, delivered, ecn_count, flag);\n\n\tlost = tp->lost - lost;\t\t\t/* freshly marked lost */\n\trs.is_ack_delayed = !!(flag & FLAG_ACK_MAYBE_DELAYED);\n\ttcp_rate_gen(sk, delivered, lost, is_sack_reneg, sack_state.rate);\n\ttcp_cong_control(sk, ack, delivered, flag, sack_state.rate);\n\ttcp_xmit_recovery(sk, rexmit);\n\treturn 1;\n\nno_queue:\n\tif (tcp_ecn_mode_accecn(tp))\n\t\tecn_count = tcp_accecn_process(sk, skb,\n\t\t\t\t\t       tp->delivered - delivered,\n\t\t\t\t\t       sack_state.delivered_bytes,\n\t\t\t\t\t       &flag);\n\ttcp_in_ack_event(sk, flag);\n\t/* If data was DSACKed, see if we can undo a cwnd reduction. */\n\tif (flag & FLAG_DSACKING_ACK) {\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t\ttcp_newly_delivered(sk, delivered, ecn_count, flag);\n\t}\n\t/* If this ack opens up a zero window, clear backoff.  It was\n\t * being used to time the probes, and is probably far higher than\n\t * it needs to be for normal retransmission.\n\t */\n\ttcp_ack_probe(sk);\n\n\tif (tp->tlp_high_seq)\n\t\ttcp_process_tlp_ack(sk, ack, flag);\n\treturn 1;\n\nold_ack:\n\t/* If data was SACKed, tag it and see if we should send more data.\n\t * If data was DSACKed, see if we can undo a cwnd reduction.\n\t */\n\tif (TCP_SKB_CB(skb)->sacked) {\n\t\tflag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,\n\t\t\t\t\t\t&sack_state);\n\t\ttcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &flag,\n\t\t\t\t      &rexmit);\n\t\ttcp_newly_delivered(sk, delivered, ecn_count, flag);\n\t\ttcp_xmit_recovery(sk, rexmit);\n\t}\n\n\treturn 0;\n}\n\nstatic void tcp_parse_fastopen_option(int len, const unsigned char *cookie,\n\t\t\t\t      bool syn, struct tcp_fastopen_cookie *foc,\n\t\t\t\t      bool exp_opt)\n{\n\t/* Valid only in SYN or SYN-ACK with an even length.  */\n\tif (!foc || !syn || len < 0 || (len & 1))\n\t\treturn;\n\n\tif (len >= TCP_FASTOPEN_COOKIE_MIN &&\n\t    len <= TCP_FASTOPEN_COOKIE_MAX)\n\t\tmemcpy(foc->val, cookie, len);\n\telse if (len != 0)\n\t\tlen = -1;\n\tfoc->len = len;\n\tfoc->exp = exp_opt;\n}\n\nstatic bool smc_parse_options(const struct tcphdr *th,\n\t\t\t      struct tcp_options_received *opt_rx,\n\t\t\t      const unsigned char *ptr,\n\t\t\t      int opsize)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (th->syn && !(opsize & 1) &&\n\t\t    opsize >= TCPOLEN_EXP_SMC_BASE &&\n\t\t    get_unaligned_be32(ptr) == TCPOPT_SMC_MAGIC) {\n\t\t\topt_rx->smc_ok = 1;\n\t\t\treturn true;\n\t\t}\n\t}\n#endif\n\treturn false;\n}\n\n/* Try to parse the MSS option from the TCP header. Return 0 on failure, clamped\n * value on success.\n */\nu16 tcp_parse_mss_option(const struct tcphdr *th, u16 user_mss)\n{\n\tconst unsigned char *ptr = (const unsigned char *)(th + 1);\n\tint length = (th->doff * 4) - sizeof(struct tcphdr);\n\tu16 mss = 0;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn mss;\n\t\tcase TCPOPT_NOP:\t/* Ref: RFC 793 section 3.1 */\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (length < 2)\n\t\t\t\treturn mss;\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2) /* \"silly options\" */\n\t\t\t\treturn mss;\n\t\t\tif (opsize > length)\n\t\t\t\treturn mss;\t/* fail on partial options */\n\t\t\tif (opcode == TCPOPT_MSS && opsize == TCPOLEN_MSS) {\n\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);\n\n\t\t\t\tif (in_mss) {\n\t\t\t\t\tif (user_mss && user_mss < in_mss)\n\t\t\t\t\t\tin_mss = user_mss;\n\t\t\t\t\tmss = in_mss;\n\t\t\t\t}\n\t\t\t}\n\t\t\tptr += opsize - 2;\n\t\t\tlength -= opsize;\n\t\t}\n\t}\n\treturn mss;\n}\n\n/* Look for tcp options. Normally only called on SYN and SYNACK packets.\n * But, this can also be called on packets in the established flow when\n * the fast version below fails.\n */\nvoid tcp_parse_options(const struct net *net,\n\t\t       const struct sk_buff *skb,\n\t\t       struct tcp_options_received *opt_rx, int estab,\n\t\t       struct tcp_fastopen_cookie *foc)\n{\n\tconst unsigned char *ptr;\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tint length = (th->doff * 4) - sizeof(struct tcphdr);\n\n\tptr = (const unsigned char *)(th + 1);\n\topt_rx->saw_tstamp = 0;\n\topt_rx->accecn = 0;\n\topt_rx->saw_unknown = 0;\n\n\twhile (length > 0) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn;\n\t\tcase TCPOPT_NOP:\t/* Ref: RFC 793 section 3.1 */\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (length < 2)\n\t\t\t\treturn;\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2) /* \"silly options\" */\n\t\t\t\treturn;\n\t\t\tif (opsize > length)\n\t\t\t\treturn;\t/* don't parse partial options */\n\t\t\tswitch (opcode) {\n\t\t\tcase TCPOPT_MSS:\n\t\t\t\tif (opsize == TCPOLEN_MSS && th->syn && !estab) {\n\t\t\t\t\tu16 in_mss = get_unaligned_be16(ptr);\n\t\t\t\t\tif (in_mss) {\n\t\t\t\t\t\tif (opt_rx->user_mss &&\n\t\t\t\t\t\t    opt_rx->user_mss < in_mss)\n\t\t\t\t\t\t\tin_mss = opt_rx->user_mss;\n\t\t\t\t\t\topt_rx->mss_clamp = in_mss;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_WINDOW:\n\t\t\t\tif (opsize == TCPOLEN_WINDOW && th->syn &&\n\t\t\t\t    !estab && READ_ONCE(net->ipv4.sysctl_tcp_window_scaling)) {\n\t\t\t\t\t__u8 snd_wscale = *(__u8 *)ptr;\n\t\t\t\t\topt_rx->wscale_ok = 1;\n\t\t\t\t\tif (snd_wscale > TCP_MAX_WSCALE) {\n\t\t\t\t\t\tnet_info_ratelimited(\"%s: Illegal window scaling value %d > %u received\\n\",\n\t\t\t\t\t\t\t\t     __func__,\n\t\t\t\t\t\t\t\t     snd_wscale,\n\t\t\t\t\t\t\t\t     TCP_MAX_WSCALE);\n\t\t\t\t\t\tsnd_wscale = TCP_MAX_WSCALE;\n\t\t\t\t\t}\n\t\t\t\t\topt_rx->snd_wscale = snd_wscale;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_TIMESTAMP:\n\t\t\t\tif ((opsize == TCPOLEN_TIMESTAMP) &&\n\t\t\t\t    ((estab && opt_rx->tstamp_ok) ||\n\t\t\t\t     (!estab && READ_ONCE(net->ipv4.sysctl_tcp_timestamps)))) {\n\t\t\t\t\topt_rx->saw_tstamp = 1;\n\t\t\t\t\topt_rx->rcv_tsval = get_unaligned_be32(ptr);\n\t\t\t\t\topt_rx->rcv_tsecr = get_unaligned_be32(ptr + 4);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase TCPOPT_SACK_PERM:\n\t\t\t\tif (opsize == TCPOLEN_SACK_PERM && th->syn &&\n\t\t\t\t    !estab && READ_ONCE(net->ipv4.sysctl_tcp_sack)) {\n\t\t\t\t\topt_rx->sack_ok = TCP_SACK_SEEN;\n\t\t\t\t\ttcp_sack_reset(opt_rx);\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_SACK:\n\t\t\t\tif ((opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK)) &&\n\t\t\t\t   !((opsize - TCPOLEN_SACK_BASE) % TCPOLEN_SACK_PERBLOCK) &&\n\t\t\t\t   opt_rx->sack_ok) {\n\t\t\t\t\tTCP_SKB_CB(skb)->sacked = (ptr - 2) - (unsigned char *)th;\n\t\t\t\t}\n\t\t\t\tbreak;\n#ifdef CONFIG_TCP_MD5SIG\n\t\t\tcase TCPOPT_MD5SIG:\n\t\t\t\t/* The MD5 Hash has already been\n\t\t\t\t * checked (see tcp_v{4,6}_rcv()).\n\t\t\t\t */\n\t\t\t\tbreak;\n#endif\n#ifdef CONFIG_TCP_AO\n\t\t\tcase TCPOPT_AO:\n\t\t\t\t/* TCP AO has already been checked\n\t\t\t\t * (see tcp_inbound_ao_hash()).\n\t\t\t\t */\n\t\t\t\tbreak;\n#endif\n\t\t\tcase TCPOPT_FASTOPEN:\n\t\t\t\ttcp_parse_fastopen_option(\n\t\t\t\t\topsize - TCPOLEN_FASTOPEN_BASE,\n\t\t\t\t\tptr, th->syn, foc, false);\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_ACCECN0:\n\t\t\tcase TCPOPT_ACCECN1:\n\t\t\t\t/* Save offset of AccECN option in TCP header */\n\t\t\t\topt_rx->accecn = (ptr - 2) - (__u8 *)th;\n\t\t\t\tbreak;\n\n\t\t\tcase TCPOPT_EXP:\n\t\t\t\t/* Fast Open option shares code 254 using a\n\t\t\t\t * 16 bits magic number.\n\t\t\t\t */\n\t\t\t\tif (opsize >= TCPOLEN_EXP_FASTOPEN_BASE &&\n\t\t\t\t    get_unaligned_be16(ptr) ==\n\t\t\t\t    TCPOPT_FASTOPEN_MAGIC) {\n\t\t\t\t\ttcp_parse_fastopen_option(opsize -\n\t\t\t\t\t\tTCPOLEN_EXP_FASTOPEN_BASE,\n\t\t\t\t\t\tptr + 2, th->syn, foc, true);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (smc_parse_options(th, opt_rx, ptr, opsize))\n\t\t\t\t\tbreak;\n\n\t\t\t\topt_rx->saw_unknown = 1;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\topt_rx->saw_unknown = 1;\n\t\t\t}\n\t\t\tptr += opsize-2;\n\t\t\tlength -= opsize;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(tcp_parse_options);\n\nstatic bool tcp_parse_aligned_timestamp(struct tcp_sock *tp, const struct tcphdr *th)\n{\n\tconst __be32 *ptr = (const __be32 *)(th + 1);\n\n\tif (*ptr == htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16)\n\t\t\t  | (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP)) {\n\t\ttp->rx_opt.saw_tstamp = 1;\n\t\t++ptr;\n\t\ttp->rx_opt.rcv_tsval = ntohl(*ptr);\n\t\t++ptr;\n\t\tif (*ptr)\n\t\t\ttp->rx_opt.rcv_tsecr = ntohl(*ptr) - tp->tsoffset;\n\t\telse\n\t\t\ttp->rx_opt.rcv_tsecr = 0;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/* Fast parse options. This hopes to only see timestamps.\n * If it is wrong it falls back on tcp_parse_options().\n */\nstatic bool tcp_fast_parse_options(const struct net *net,\n\t\t\t\t   const struct sk_buff *skb,\n\t\t\t\t   const struct tcphdr *th, struct tcp_sock *tp)\n{\n\t/* In the spirit of fast parsing, compare doff directly to constant\n\t * values.  Because equality is used, short doff can be ignored here.\n\t */\n\tif (th->doff == (sizeof(*th) / 4)) {\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\ttp->rx_opt.accecn = 0;\n\t\treturn false;\n\t} else if (tp->rx_opt.tstamp_ok &&\n\t\t   th->doff == ((sizeof(*th) + TCPOLEN_TSTAMP_ALIGNED) / 4)) {\n\t\tif (tcp_parse_aligned_timestamp(tp, th)) {\n\t\t\ttp->rx_opt.accecn = 0;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\ttcp_parse_options(net, skb, &tp->rx_opt, 1, NULL);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\treturn true;\n}\n\n#if defined(CONFIG_TCP_MD5SIG) || defined(CONFIG_TCP_AO)\n/*\n * Parse Signature options\n */\nint tcp_do_parse_auth_options(const struct tcphdr *th,\n\t\t\t      const u8 **md5_hash, const u8 **ao_hash)\n{\n\tint length = (th->doff << 2) - sizeof(*th);\n\tconst u8 *ptr = (const u8 *)(th + 1);\n\tunsigned int minlen = TCPOLEN_MD5SIG;\n\n\tif (IS_ENABLED(CONFIG_TCP_AO))\n\t\tminlen = sizeof(struct tcp_ao_hdr) + 1;\n\n\t*md5_hash = NULL;\n\t*ao_hash = NULL;\n\n\t/* If not enough data remaining, we can short cut */\n\twhile (length >= minlen) {\n\t\tint opcode = *ptr++;\n\t\tint opsize;\n\n\t\tswitch (opcode) {\n\t\tcase TCPOPT_EOL:\n\t\t\treturn 0;\n\t\tcase TCPOPT_NOP:\n\t\t\tlength--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\topsize = *ptr++;\n\t\t\tif (opsize < 2 || opsize > length)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (opcode == TCPOPT_MD5SIG) {\n\t\t\t\tif (opsize != TCPOLEN_MD5SIG)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (unlikely(*md5_hash || *ao_hash))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\t*md5_hash = ptr;\n\t\t\t} else if (opcode == TCPOPT_AO) {\n\t\t\t\tif (opsize <= sizeof(struct tcp_ao_hdr))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tif (unlikely(*md5_hash || *ao_hash))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\t*ao_hash = ptr;\n\t\t\t}\n\t\t}\n\t\tptr += opsize - 2;\n\t\tlength -= opsize;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_do_parse_auth_options);\n#endif\n\n/* Sorry, PAWS as specified is broken wrt. pure-ACKs -DaveM\n *\n * It is not fatal. If this ACK does _not_ change critical state (seqs, window)\n * it can pass through stack. So, the following predicate verifies that\n * this segment is not used for anything but congestion avoidance or\n * fast retransmit. Moreover, we even are able to eliminate most of such\n * second order effects, if we apply some small \"replay\" window (~RTO)\n * to timestamp space.\n *\n * All these measures still do not guarantee that we reject wrapped ACKs\n * on networks with high bandwidth, when sequence space is recycled fastly,\n * but it guarantees that such events will be very rare and do not affect\n * connection seriously. This doesn't look nice, but alas, PAWS is really\n * buggy extension.\n *\n * [ Later note. Even worse! It is buggy for segments _with_ data. RFC\n * states that events when retransmit arrives after original data are rare.\n * It is a blatant lie. VJ forgot about fast retransmit! 8)8) It is\n * the biggest problem on large power networks even with minor reordering.\n * OK, let's give it small replay window. If peer clock is even 1hz, it is safe\n * up to bandwidth of 18Gigabit/sec. 8) ]\n */\n\n/* Estimates max number of increments of remote peer TSval in\n * a replay window (based on our current RTO estimation).\n */\nstatic u32 tcp_tsval_replay(const struct sock *sk)\n{\n\t/* If we use usec TS resolution,\n\t * then expect the remote peer to use the same resolution.\n\t */\n\tif (tcp_sk(sk)->tcp_usec_ts)\n\t\treturn inet_csk(sk)->icsk_rto * (USEC_PER_SEC / HZ);\n\n\t/* RFC 7323 recommends a TSval clock between 1ms and 1sec.\n\t * We know that some OS (including old linux) can use 1200 Hz.\n\t */\n\treturn inet_csk(sk)->icsk_rto * 1200 / HZ;\n}\n\nstatic enum skb_drop_reason tcp_disordered_ack_check(const struct sock *sk,\n\t\t\t\t\t\t     const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tSKB_DR_INIT(reason, TCP_RFC7323_PAWS);\n\tu32 ack = TCP_SKB_CB(skb)->ack_seq;\n\tu32 seq = TCP_SKB_CB(skb)->seq;\n\n\t/* 1. Is this not a pure ACK ? */\n\tif (!th->ack || seq != TCP_SKB_CB(skb)->end_seq)\n\t\treturn reason;\n\n\t/* 2. Is its sequence not the expected one ? */\n\tif (seq != tp->rcv_nxt)\n\t\treturn before(seq, tp->rcv_nxt) ?\n\t\t\tSKB_DROP_REASON_TCP_RFC7323_PAWS_ACK :\n\t\t\treason;\n\n\t/* 3. Is this not a duplicate ACK ? */\n\tif (ack != tp->snd_una)\n\t\treturn reason;\n\n\t/* 4. Is this updating the window ? */\n\tif (tcp_may_update_window(tp, ack, seq, ntohs(th->window) <<\n\t\t\t\t\t\ttp->rx_opt.snd_wscale))\n\t\treturn reason;\n\n\t/* 5. Is this not in the replay window ? */\n\tif ((s32)(tp->rx_opt.ts_recent - tp->rx_opt.rcv_tsval) >\n\t    tcp_tsval_replay(sk))\n\t\treturn reason;\n\n\treturn 0;\n}\n\n/* Check segment sequence number for validity.\n *\n * Segment controls are considered valid, if the segment\n * fits to the window after truncation to the window. Acceptability\n * of data (and SYN, FIN, of course) is checked separately.\n * See tcp_data_queue(), for example.\n *\n * Also, controls (RST is main one) are accepted using RCV.WUP instead\n * of RCV.NXT. Peer still did not advance his SND.UNA when we\n * delayed ACK, so that hisSND.UNA<=ourRCV.WUP.\n * (borrowed from freebsd)\n */\n\nstatic enum skb_drop_reason tcp_sequence(const struct sock *sk,\n\t\t\t\t\t u32 seq, u32 end_seq)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tif (before(end_seq, tp->rcv_wup))\n\t\treturn SKB_DROP_REASON_TCP_OLD_SEQUENCE;\n\n\tif (after(end_seq, tp->rcv_nxt + tcp_receive_window(tp))) {\n\t\tif (after(seq, tp->rcv_nxt + tcp_receive_window(tp)))\n\t\t\treturn SKB_DROP_REASON_TCP_INVALID_SEQUENCE;\n\n\t\t/* Only accept this packet if receive queue is empty. */\n\t\tif (skb_queue_len(&sk->sk_receive_queue))\n\t\t\treturn SKB_DROP_REASON_TCP_INVALID_END_SEQUENCE;\n\t}\n\n\treturn SKB_NOT_DROPPED_YET;\n}\n\n\nvoid tcp_done_with_error(struct sock *sk, int err)\n{\n\t/* This barrier is coupled with smp_rmb() in tcp_poll() */\n\tWRITE_ONCE(sk->sk_err, err);\n\tsmp_wmb();\n\n\ttcp_write_queue_purge(sk);\n\ttcp_done(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk_error_report(sk);\n}\nEXPORT_IPV6_MOD(tcp_done_with_error);\n\n/* When we get a reset we do this. */\nvoid tcp_reset(struct sock *sk, struct sk_buff *skb)\n{\n\tint err;\n\n\ttrace_tcp_receive_reset(sk);\n\n\t/* mptcp can't tell us to ignore reset pkts,\n\t * so just ignore the return value of mptcp_incoming_options().\n\t */\n\tif (sk_is_mptcp(sk))\n\t\tmptcp_incoming_options(sk, skb);\n\n\t/* We want the right error as BSD sees it (and indeed as we do). */\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_SENT:\n\t\terr = ECONNREFUSED;\n\t\tbreak;\n\tcase TCP_CLOSE_WAIT:\n\t\terr = EPIPE;\n\t\tbreak;\n\tcase TCP_CLOSE:\n\t\treturn;\n\tdefault:\n\t\terr = ECONNRESET;\n\t}\n\ttcp_done_with_error(sk, err);\n}\n\n/*\n * \tProcess the FIN bit. This now behaves as it is supposed to work\n *\tand the FIN takes effect when it is validly part of sequence\n *\tspace. Not before when we get holes.\n *\n *\tIf we are ESTABLISHED, a received fin moves us to CLOSE-WAIT\n *\t(and thence onto LAST-ACK and finally, CLOSE, we never enter\n *\tTIME-WAIT)\n *\n *\tIf we are in FINWAIT-1, a received FIN indicates simultaneous\n *\tclose and we go into CLOSING (and later onto TIME-WAIT)\n *\n *\tIf we are in FINWAIT-2, a received FIN moves us to TIME-WAIT.\n */\nvoid tcp_fin(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tinet_csk_schedule_ack(sk);\n\n\tWRITE_ONCE(sk->sk_shutdown, sk->sk_shutdown | RCV_SHUTDOWN);\n\tsock_set_flag(sk, SOCK_DONE);\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_ESTABLISHED:\n\t\t/* Move to CLOSE_WAIT */\n\t\ttcp_set_state(sk, TCP_CLOSE_WAIT);\n\t\tinet_csk_enter_pingpong_mode(sk);\n\t\tbreak;\n\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\t\t/* Received a retransmission of the FIN, do\n\t\t * nothing.\n\t\t */\n\t\tbreak;\n\tcase TCP_LAST_ACK:\n\t\t/* RFC793: Remain in the LAST-ACK state. */\n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1:\n\t\t/* This case occurs when a simultaneous close\n\t\t * happens, we must ack the received FIN and\n\t\t * enter the CLOSING state.\n\t\t */\n\t\ttcp_send_ack(sk);\n\t\ttcp_set_state(sk, TCP_CLOSING);\n\t\tbreak;\n\tcase TCP_FIN_WAIT2:\n\t\t/* Received a FIN -- send ACK and enter TIME_WAIT. */\n\t\ttcp_send_ack(sk);\n\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\tbreak;\n\tdefault:\n\t\t/* Only TCP_LISTEN and TCP_CLOSE are left, in these\n\t\t * cases we should never reach this piece of code.\n\t\t */\n\t\tpr_err(\"%s: Impossible, sk->sk_state=%d\\n\",\n\t\t       __func__, sk->sk_state);\n\t\tbreak;\n\t}\n\n\t/* It _is_ possible, that we have something out-of-order _after_ FIN.\n\t * Probably, we should reset in this case. For now drop them.\n\t */\n\tskb_rbtree_purge(&tp->out_of_order_queue);\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_reset(&tp->rx_opt);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\n\t\t/* Do not send POLL_HUP for half duplex close. */\n\t\tif (sk->sk_shutdown == SHUTDOWN_MASK ||\n\t\t    sk->sk_state == TCP_CLOSE)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_HUP);\n\t\telse\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\t}\n}\n\nstatic inline bool tcp_sack_extend(struct tcp_sack_block *sp, u32 seq,\n\t\t\t\t  u32 end_seq)\n{\n\tif (!after(seq, sp->end_seq) && !after(sp->start_seq, end_seq)) {\n\t\tif (before(seq, sp->start_seq))\n\t\t\tsp->start_seq = seq;\n\t\tif (after(end_seq, sp->end_seq))\n\t\t\tsp->end_seq = end_seq;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void tcp_dsack_set(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_is_sack(tp) && READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_dsack)) {\n\t\tint mib_idx;\n\n\t\tif (before(seq, tp->rcv_nxt))\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOLDSENT;\n\t\telse\n\t\t\tmib_idx = LINUX_MIB_TCPDSACKOFOSENT;\n\n\t\tNET_INC_STATS(sock_net(sk), mib_idx);\n\n\t\ttp->rx_opt.dsack = 1;\n\t\ttp->duplicate_sack[0].start_seq = seq;\n\t\ttp->duplicate_sack[0].end_seq = end_seq;\n\t}\n}\n\nstatic void tcp_dsack_extend(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->rx_opt.dsack)\n\t\ttcp_dsack_set(sk, seq, end_seq);\n\telse\n\t\ttcp_sack_extend(tp->duplicate_sack, seq, end_seq);\n}\n\nstatic void tcp_rcv_spurious_retrans(struct sock *sk, const struct sk_buff *skb)\n{\n\t/* When the ACK path fails or drops most ACKs, the sender would\n\t * timeout and spuriously retransmit the same segment repeatedly.\n\t * If it seems our ACKs are not reaching the other side,\n\t * based on receiving a duplicate data segment with new flowlabel\n\t * (suggesting the sender suffered an RTO), and we are not already\n\t * repathing due to our own RTO, then rehash the socket to repath our\n\t * packets.\n\t */\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (inet_csk(sk)->icsk_ca_state != TCP_CA_Loss &&\n\t    skb->protocol == htons(ETH_P_IPV6) &&\n\t    (tcp_sk(sk)->inet_conn.icsk_ack.lrcv_flowlabel !=\n\t     ntohl(ip6_flowlabel(ipv6_hdr(skb)))) &&\n\t    sk_rethink_txhash(sk))\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPDUPLICATEDATAREHASH);\n\n\t/* Save last flowlabel after a spurious retrans. */\n\ttcp_save_lrcv_flowlabel(sk, skb);\n#endif\n}\n\nstatic void tcp_send_dupack(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t    before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\n\t\tif (tcp_is_sack(tp) && READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_dsack)) {\n\t\t\tu32 end_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t\ttcp_rcv_spurious_retrans(sk, skb);\n\t\t\tif (after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))\n\t\t\t\tend_seq = tp->rcv_nxt;\n\t\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, end_seq);\n\t\t}\n\t}\n\n\ttcp_send_ack(sk);\n}\n\n/* These routines update the SACK block as out-of-order packets arrive or\n * in-order packets close up the sequence space.\n */\nstatic void tcp_sack_maybe_coalesce(struct tcp_sock *tp)\n{\n\tint this_sack;\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tstruct tcp_sack_block *swalk = sp + 1;\n\n\t/* See if the recent change to the first SACK eats into\n\t * or hits the sequence space of other SACK blocks, if so coalesce.\n\t */\n\tfor (this_sack = 1; this_sack < tp->rx_opt.num_sacks;) {\n\t\tif (tcp_sack_extend(sp, swalk->start_seq, swalk->end_seq)) {\n\t\t\tint i;\n\n\t\t\t/* Zap SWALK, by moving every further SACK up by one slot.\n\t\t\t * Decrease num_sacks.\n\t\t\t */\n\t\t\ttp->rx_opt.num_sacks--;\n\t\t\tfor (i = this_sack; i < tp->rx_opt.num_sacks; i++)\n\t\t\t\tsp[i] = sp[i + 1];\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++;\n\t\tswalk++;\n\t}\n}\n\nvoid tcp_sack_compress_send_ack(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (!tp->compressed_ack)\n\t\treturn;\n\n\tif (hrtimer_try_to_cancel(&tp->compressed_ack_timer) == 1)\n\t\t__sock_put(sk);\n\n\t/* Since we have to send one ack finally,\n\t * substract one from tp->compressed_ack to keep\n\t * LINUX_MIB_TCPACKCOMPRESSED accurate.\n\t */\n\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPACKCOMPRESSED,\n\t\t      tp->compressed_ack - 1);\n\n\ttp->compressed_ack = 0;\n\ttcp_send_ack(sk);\n}\n\n/* Reasonable amount of sack blocks included in TCP SACK option\n * The max is 4, but this becomes 3 if TCP timestamps are there.\n * Given that SACK packets might be lost, be conservative and use 2.\n */\n#define TCP_SACK_BLOCKS_EXPECTED 2\n\nstatic void tcp_sack_new_ofo_skb(struct sock *sk, u32 seq, u32 end_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint cur_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\tif (!cur_sacks)\n\t\tgoto new_sack;\n\n\tfor (this_sack = 0; this_sack < cur_sacks; this_sack++, sp++) {\n\t\tif (tcp_sack_extend(sp, seq, end_seq)) {\n\t\t\tif (this_sack >= TCP_SACK_BLOCKS_EXPECTED)\n\t\t\t\ttcp_sack_compress_send_ack(sk);\n\t\t\t/* Rotate this_sack to the first one. */\n\t\t\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t\t\tswap(*sp, *(sp - 1));\n\t\t\tif (cur_sacks > 1)\n\t\t\t\ttcp_sack_maybe_coalesce(tp);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (this_sack >= TCP_SACK_BLOCKS_EXPECTED)\n\t\ttcp_sack_compress_send_ack(sk);\n\n\t/* Could not find an adjacent existing SACK, build a new one,\n\t * put it at the front, and shift everyone else down.  We\n\t * always know there is at least one SACK present already here.\n\t *\n\t * If the sack array is full, forget about the last one.\n\t */\n\tif (this_sack >= TCP_NUM_SACKS) {\n\t\tthis_sack--;\n\t\ttp->rx_opt.num_sacks--;\n\t\tsp--;\n\t}\n\tfor (; this_sack > 0; this_sack--, sp--)\n\t\t*sp = *(sp - 1);\n\nnew_sack:\n\t/* Build the new head SACK, and we're done. */\n\tsp->start_seq = seq;\n\tsp->end_seq = end_seq;\n\ttp->rx_opt.num_sacks++;\n}\n\n/* RCV.NXT advances, some SACKs should be eaten. */\n\nstatic void tcp_sack_remove(struct tcp_sock *tp)\n{\n\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\tint num_sacks = tp->rx_opt.num_sacks;\n\tint this_sack;\n\n\t/* Empty ofo queue, hence, all the SACKs are eaten. Clear. */\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\ttp->rx_opt.num_sacks = 0;\n\t\treturn;\n\t}\n\n\tfor (this_sack = 0; this_sack < num_sacks;) {\n\t\t/* Check if the start of the sack is covered by RCV.NXT. */\n\t\tif (!before(tp->rcv_nxt, sp->start_seq)) {\n\t\t\tint i;\n\n\t\t\t/* RCV.NXT must cover all the block! */\n\t\t\tWARN_ON(before(tp->rcv_nxt, sp->end_seq));\n\n\t\t\t/* Zap this SACK, by moving forward any other SACKS. */\n\t\t\tfor (i = this_sack+1; i < num_sacks; i++)\n\t\t\t\ttp->selective_acks[i-1] = tp->selective_acks[i];\n\t\t\tnum_sacks--;\n\t\t\tcontinue;\n\t\t}\n\t\tthis_sack++;\n\t\tsp++;\n\t}\n\ttp->rx_opt.num_sacks = num_sacks;\n}\n\n/**\n * tcp_try_coalesce - try to merge skb to prior one\n * @sk: socket\n * @to: prior buffer\n * @from: buffer to add in queue\n * @fragstolen: pointer to boolean\n *\n * Before queueing skb @from after @to, try to merge them\n * to reduce overall memory use and queue lengths, if cost is small.\n * Packets in ofo or receive queues can stay a long time.\n * Better try to coalesce them right now to avoid future collapses.\n * Returns true if caller should free @from instead of queueing it\n */\nstatic bool tcp_try_coalesce(struct sock *sk,\n\t\t\t     struct sk_buff *to,\n\t\t\t     struct sk_buff *from,\n\t\t\t     bool *fragstolen)\n{\n\tint delta;\n\n\t*fragstolen = false;\n\n\t/* Its possible this segment overlaps with prior segment in queue */\n\tif (TCP_SKB_CB(from)->seq != TCP_SKB_CB(to)->end_seq)\n\t\treturn false;\n\n\tif (!tcp_skb_can_collapse_rx(to, from))\n\t\treturn false;\n\n\tif (!skb_try_coalesce(to, from, fragstolen, &delta))\n\t\treturn false;\n\n\tatomic_add(delta, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, delta);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOALESCE);\n\tTCP_SKB_CB(to)->end_seq = TCP_SKB_CB(from)->end_seq;\n\tTCP_SKB_CB(to)->ack_seq = TCP_SKB_CB(from)->ack_seq;\n\tTCP_SKB_CB(to)->tcp_flags |= TCP_SKB_CB(from)->tcp_flags;\n\n\tif (TCP_SKB_CB(from)->has_rxtstamp) {\n\t\tTCP_SKB_CB(to)->has_rxtstamp = true;\n\t\tto->tstamp = from->tstamp;\n\t\tskb_hwtstamps(to)->hwtstamp = skb_hwtstamps(from)->hwtstamp;\n\t}\n\n\treturn true;\n}\n\nstatic bool tcp_ooo_try_coalesce(struct sock *sk,\n\t\t\t     struct sk_buff *to,\n\t\t\t     struct sk_buff *from,\n\t\t\t     bool *fragstolen)\n{\n\tbool res = tcp_try_coalesce(sk, to, from, fragstolen);\n\n\t/* In case tcp_drop_reason() is called later, update to->gso_segs */\n\tif (res) {\n\t\tu32 gso_segs = max_t(u16, 1, skb_shinfo(to)->gso_segs) +\n\t\t\t       max_t(u16, 1, skb_shinfo(from)->gso_segs);\n\n\t\tskb_shinfo(to)->gso_segs = min_t(u32, gso_segs, 0xFFFF);\n\t}\n\treturn res;\n}\n\nnoinline_for_tracing static void\ntcp_drop_reason(struct sock *sk, struct sk_buff *skb, enum skb_drop_reason reason)\n{\n\tsk_drops_skbadd(sk, skb);\n\tsk_skb_reason_drop(sk, skb, reason);\n}\n\n/* This one checks to see if we can put data from the\n * out_of_order queue into the receive_queue.\n */\nstatic void tcp_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\t__u32 dsack_high = tp->rcv_nxt;\n\tbool fin, fragstolen, eaten;\n\tstruct sk_buff *skb, *tail;\n\tstruct rb_node *p;\n\n\tp = rb_first(&tp->out_of_order_queue);\n\twhile (p) {\n\t\tskb = rb_to_skb(p);\n\t\tif (after(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))\n\t\t\tbreak;\n\n\t\tif (before(TCP_SKB_CB(skb)->seq, dsack_high)) {\n\t\t\t__u32 dsack = dsack_high;\n\n\t\t\tif (before(TCP_SKB_CB(skb)->end_seq, dsack_high))\n\t\t\t\tdsack = TCP_SKB_CB(skb)->end_seq;\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb)->seq, dsack);\n\t\t}\n\t\tp = rb_next(p);\n\t\trb_erase(&skb->rbnode, &tp->out_of_order_queue);\n\n\t\tif (unlikely(!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt))) {\n\t\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_TCP_OFO_DROP);\n\t\t\tcontinue;\n\t\t}\n\n\t\ttail = skb_peek_tail(&sk->sk_receive_queue);\n\t\teaten = tail && tcp_try_coalesce(sk, tail, skb, &fragstolen);\n\t\ttcp_rcv_nxt_update(tp, TCP_SKB_CB(skb)->end_seq);\n\t\tfin = TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN;\n\t\tif (!eaten)\n\t\t\ttcp_add_receive_queue(sk, skb);\n\t\telse\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\n\t\tif (unlikely(fin)) {\n\t\t\ttcp_fin(sk);\n\t\t\t/* tcp_fin() purges tp->out_of_order_queue,\n\t\t\t * so we must end this loop right now.\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic bool tcp_prune_ofo_queue(struct sock *sk, const struct sk_buff *in_skb);\nstatic int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb);\n\n/* Check if this incoming skb can be added to socket receive queues\n * while satisfying sk->sk_rcvbuf limit.\n *\n * In theory we should use skb->truesize, but this can cause problems\n * when applications use too small SO_RCVBUF values.\n * When LRO / hw gro is used, the socket might have a high tp->scaling_ratio,\n * allowing RWIN to be close to available space.\n * Whenever the receive queue gets full, we can receive a small packet\n * filling RWIN, but with a high skb->truesize, because most NIC use 4K page\n * plus sk_buff metadata even when receiving less than 1500 bytes of payload.\n *\n * Note that we use skb->len to decide to accept or drop this packet,\n * but sk->sk_rmem_alloc is the sum of all skb->truesize.\n */\nstatic bool tcp_can_ingest(const struct sock *sk, const struct sk_buff *skb)\n{\n\tunsigned int rmem = atomic_read(&sk->sk_rmem_alloc);\n\n\treturn rmem + skb->len <= sk->sk_rcvbuf;\n}\n\nstatic int tcp_try_rmem_schedule(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t unsigned int size)\n{\n\tif (!tcp_can_ingest(sk, skb) ||\n\t    !sk_rmem_schedule(sk, skb, size)) {\n\n\t\tif (tcp_prune_queue(sk, skb) < 0)\n\t\t\treturn -1;\n\n\t\twhile (!sk_rmem_schedule(sk, skb, size)) {\n\t\t\tif (!tcp_prune_ofo_queue(sk, skb))\n\t\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node **p, *parent;\n\tstruct sk_buff *skb1;\n\tu32 seq, end_seq;\n\tbool fragstolen;\n\n\ttcp_save_lrcv_flowlabel(sk, skb);\n\ttcp_data_ecn_check(sk, skb);\n\n\tif (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);\n\t\tsk->sk_data_ready(sk);\n\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_PROTO_MEM);\n\t\treturn;\n\t}\n\n\ttcp_measure_rcv_mss(sk, skb);\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n\tinet_csk_schedule_ack(sk);\n\n\ttp->rcv_ooopack += max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);\n\tseq = TCP_SKB_CB(skb)->seq;\n\tend_seq = TCP_SKB_CB(skb)->end_seq;\n\n\tp = &tp->out_of_order_queue.rb_node;\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t/* Initial out of order segment, build 1 SACK. */\n\t\tif (tcp_is_sack(tp)) {\n\t\t\ttp->rx_opt.num_sacks = 1;\n\t\t\ttp->selective_acks[0].start_seq = seq;\n\t\t\ttp->selective_acks[0].end_seq = end_seq;\n\t\t}\n\t\trb_link_node(&skb->rbnode, NULL, p);\n\t\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\t\ttp->ooo_last_skb = skb;\n\t\tgoto end;\n\t}\n\n\t/* In the typical case, we are adding an skb to the end of the list.\n\t * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.\n\t */\n\tif (tcp_ooo_try_coalesce(sk, tp->ooo_last_skb,\n\t\t\t\t skb, &fragstolen)) {\ncoalesce_done:\n\t\t/* For non sack flows, do not grow window to force DUPACK\n\t\t * and trigger fast retransmit.\n\t\t */\n\t\tif (tcp_is_sack(tp))\n\t\t\ttcp_grow_window(sk, skb, true);\n\t\tkfree_skb_partial(skb, fragstolen);\n\t\tskb = NULL;\n\t\tgoto add_sack;\n\t}\n\t/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */\n\tif (!before(seq, TCP_SKB_CB(tp->ooo_last_skb)->end_seq)) {\n\t\tparent = &tp->ooo_last_skb->rbnode;\n\t\tp = &parent->rb_right;\n\t\tgoto insert;\n\t}\n\n\t/* Find place to insert this segment. Handle overlaps on the way. */\n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (before(seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\t\t/* All the bits are present. Drop. */\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop_reason(sk, skb,\n\t\t\t\t\t\tSKB_DROP_REASON_TCP_OFOMERGE);\n\t\t\t\tskb = NULL;\n\t\t\t\ttcp_dsack_set(sk, seq, end_seq);\n\t\t\t\tgoto add_sack;\n\t\t\t}\n\t\t\tif (after(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\t\t/* Partial overlap. */\n\t\t\t\ttcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)->end_seq);\n\t\t\t} else {\n\t\t\t\t/* skb's seq == skb1's seq and skb covers skb1.\n\t\t\t\t * Replace skb1 with skb.\n\t\t\t\t */\n\t\t\t\trb_replace_node(&skb1->rbnode, &skb->rbnode,\n\t\t\t\t\t\t&tp->out_of_order_queue);\n\t\t\t\ttcp_dsack_extend(sk,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop_reason(sk, skb1,\n\t\t\t\t\t\tSKB_DROP_REASON_TCP_OFOMERGE);\n\t\t\t\tgoto merge_right;\n\t\t\t}\n\t\t} else if (tcp_ooo_try_coalesce(sk, skb1,\n\t\t\t\t\t\tskb, &fragstolen)) {\n\t\t\tgoto coalesce_done;\n\t\t}\n\t\tp = &parent->rb_right;\n\t}\ninsert:\n\t/* Insert segment into RB tree. */\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\nmerge_right:\n\t/* Remove other segments covered by skb. */\n\twhile ((skb1 = skb_rb_next(skb)) != NULL) {\n\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tbreak;\n\t\tif (before(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t end_seq);\n\t\t\tbreak;\n\t\t}\n\t\trb_erase(&skb1->rbnode, &tp->out_of_order_queue);\n\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\n\t\ttcp_drop_reason(sk, skb1, SKB_DROP_REASON_TCP_OFOMERGE);\n\t}\n\t/* If there is no skb after us, we are the last_skb ! */\n\tif (!skb1)\n\t\ttp->ooo_last_skb = skb;\n\nadd_sack:\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_new_ofo_skb(sk, seq, end_seq);\nend:\n\tif (skb) {\n\t\t/* For non sack flows, do not grow window to force DUPACK\n\t\t * and trigger fast retransmit.\n\t\t */\n\t\tif (tcp_is_sack(tp))\n\t\t\ttcp_grow_window(sk, skb, false);\n\t\tskb_condense(skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n\t/* do not grow rcvbuf for not-yet-accepted or orphaned sockets. */\n\tif (sk->sk_socket)\n\t\ttcp_rcvbuf_grow(sk);\n}\n\nstatic int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t      bool *fragstolen)\n{\n\tint eaten;\n\tstruct sk_buff *tail = skb_peek_tail(&sk->sk_receive_queue);\n\n\teaten = (tail &&\n\t\t tcp_try_coalesce(sk, tail,\n\t\t\t\t  skb, fragstolen)) ? 1 : 0;\n\ttcp_rcv_nxt_update(tcp_sk(sk), TCP_SKB_CB(skb)->end_seq);\n\tif (!eaten) {\n\t\ttcp_add_receive_queue(sk, skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n\treturn eaten;\n}\n\nint tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct sk_buff *skb;\n\tint err = -ENOMEM;\n\tint data_len = 0;\n\tbool fragstolen;\n\n\tif (size == 0)\n\t\treturn 0;\n\n\tif (size > PAGE_SIZE) {\n\t\tint npages = min_t(size_t, size >> PAGE_SHIFT, MAX_SKB_FRAGS);\n\n\t\tdata_len = npages << PAGE_SHIFT;\n\t\tsize = data_len + (size & ~PAGE_MASK);\n\t}\n\tskb = alloc_skb_with_frags(size - data_len, data_len,\n\t\t\t\t   PAGE_ALLOC_COSTLY_ORDER,\n\t\t\t\t   &err, sk->sk_allocation);\n\tif (!skb)\n\t\tgoto err;\n\n\tskb_put(skb, size - data_len);\n\tskb->data_len = data_len;\n\tskb->len = size;\n\n\tif (tcp_try_rmem_schedule(sk, skb, skb->truesize)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);\n\t\tgoto err_free;\n\t}\n\n\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);\n\tif (err)\n\t\tgoto err_free;\n\n\tTCP_SKB_CB(skb)->seq = tcp_sk(sk)->rcv_nxt;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(skb)->seq + size;\n\tTCP_SKB_CB(skb)->ack_seq = tcp_sk(sk)->snd_una - 1;\n\n\tif (tcp_queue_rcv(sk, skb, &fragstolen)) {\n\t\tWARN_ON_ONCE(fragstolen); /* should not happen */\n\t\t__kfree_skb(skb);\n\t}\n\treturn size;\n\nerr_free:\n\tkfree_skb(skb);\nerr:\n\treturn err;\n\n}\n\nvoid tcp_data_ready(struct sock *sk)\n{\n\tif (tcp_epollin_ready(sk, sk->sk_rcvlowat) || sock_flag(sk, SOCK_DONE))\n\t\tsk->sk_data_ready(sk);\n}\n\nstatic void tcp_data_queue(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tenum skb_drop_reason reason;\n\tbool fragstolen;\n\tint eaten;\n\n\t/* If a subflow has been reset, the packet should not continue\n\t * to be processed, drop the packet.\n\t */\n\tif (sk_is_mptcp(sk) && !mptcp_incoming_options(sk, skb)) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\tif (TCP_SKB_CB(skb)->seq == TCP_SKB_CB(skb)->end_seq) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\ttcp_cleanup_skb(skb);\n\t__skb_pull(skb, tcp_hdr(skb)->doff * 4);\n\n\treason = SKB_DROP_REASON_NOT_SPECIFIED;\n\ttp->rx_opt.dsack = 0;\n\n\t/*  Queue data for delivery to the user.\n\t *  Packets in sequence go to the receive queue.\n\t *  Out of sequence packets to the out_of_order_queue.\n\t */\n\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt) {\n\t\tif (tcp_receive_window(tp) == 0) {\n\t\t\t/* Some stacks are known to send bare FIN packets\n\t\t\t * in a loop even if we send RWIN 0 in our ACK.\n\t\t\t * Accepting this FIN does not hurt memory pressure\n\t\t\t * because the FIN flag will simply be merged to the\n\t\t\t * receive queue tail skb in most cases.\n\t\t\t */\n\t\t\tif (!skb->len &&\n\t\t\t    (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN))\n\t\t\t\tgoto queue_and_out;\n\n\t\t\treason = SKB_DROP_REASON_TCP_ZEROWINDOW;\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\n\t\t\tgoto out_of_window;\n\t\t}\n\n\t\t/* Ok. In sequence. In window. */\nqueue_and_out:\n\t\tif (tcp_try_rmem_schedule(sk, skb, skb->truesize)) {\n\t\t\t/* TODO: maybe ratelimit these WIN 0 ACK ? */\n\t\t\tinet_csk(sk)->icsk_ack.pending |=\n\t\t\t\t\t(ICSK_ACK_NOMEM | ICSK_ACK_NOW);\n\t\t\tinet_csk_schedule_ack(sk);\n\t\t\tsk->sk_data_ready(sk);\n\n\t\t\tif (skb_queue_len(&sk->sk_receive_queue) && skb->len) {\n\t\t\t\treason = SKB_DROP_REASON_PROTO_MEM;\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t}\n\n\t\teaten = tcp_queue_rcv(sk, skb, &fragstolen);\n\t\tif (skb->len)\n\t\t\ttcp_event_data_recv(sk, skb);\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\ttcp_fin(sk);\n\n\t\tif (!RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t\ttcp_ofo_queue(sk);\n\n\t\t\t/* RFC5681. 4.2. SHOULD send immediate ACK, when\n\t\t\t * gap in queue is filled.\n\t\t\t */\n\t\t\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\t\t\tinet_csk(sk)->icsk_ack.pending |= ICSK_ACK_NOW;\n\t\t}\n\n\t\tif (tp->rx_opt.num_sacks)\n\t\t\ttcp_sack_remove(tp);\n\n\t\ttcp_fast_path_check(sk);\n\n\t\tif (eaten > 0)\n\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\ttcp_data_ready(sk);\n\t\treturn;\n\t}\n\n\tif (!after(TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt)) {\n\t\ttcp_rcv_spurious_retrans(sk, skb);\n\t\t/* A retransmit, 2nd most common case.  Force an immediate ack. */\n\t\treason = SKB_DROP_REASON_TCP_OLD_DATA;\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);\n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);\n\nout_of_window:\n\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\t\tinet_csk_schedule_ack(sk);\ndrop:\n\t\ttcp_drop_reason(sk, skb, reason);\n\t\treturn;\n\t}\n\n\t/* Out of window. F.e. zero window probe. */\n\tif (!before(TCP_SKB_CB(skb)->seq,\n\t\t    tp->rcv_nxt + tcp_receive_window(tp))) {\n\t\treason = SKB_DROP_REASON_TCP_OVERWINDOW;\n\t\tgoto out_of_window;\n\t}\n\n\tif (before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\t/* Partial packet, seq < rcv_next < end_seq */\n\t\ttcp_dsack_set(sk, TCP_SKB_CB(skb)->seq, tp->rcv_nxt);\n\n\t\t/* If window is closed, drop tail of packet. But after\n\t\t * remembering D-SACK for its head made in previous line.\n\t\t */\n\t\tif (!tcp_receive_window(tp)) {\n\t\t\treason = SKB_DROP_REASON_TCP_ZEROWINDOW;\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);\n\t\t\tgoto out_of_window;\n\t\t}\n\t\tgoto queue_and_out;\n\t}\n\n\ttcp_data_queue_ofo(sk, skb);\n}\n\nstatic struct sk_buff *tcp_skb_next(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tif (list)\n\t\treturn !skb_queue_is_last(list, skb) ? skb->next : NULL;\n\n\treturn skb_rb_next(skb);\n}\n\nstatic struct sk_buff *tcp_collapse_one(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\t\tstruct rb_root *root)\n{\n\tstruct sk_buff *next = tcp_skb_next(skb, list);\n\n\tif (list)\n\t\t__skb_unlink(skb, list);\n\telse\n\t\trb_erase(&skb->rbnode, root);\n\n\t__kfree_skb(skb);\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOLLAPSED);\n\n\treturn next;\n}\n\n/* Insert skb into rb tree, ordered by TCP_SKB_CB(skb)->seq */\nvoid tcp_rbtree_insert(struct rb_root *root, struct sk_buff *skb)\n{\n\tstruct rb_node **p = &root->rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct sk_buff *skb1;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tp = &parent->rb_left;\n\t\telse\n\t\t\tp = &parent->rb_right;\n\t}\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, root);\n}\n\n/* Collapse contiguous sequence of skbs head..tail with\n * sequence numbers start..end.\n *\n * If tail is NULL, this means until the end of the queue.\n *\n * Segments with FIN/SYN are not collapsed (only because this\n * simplifies code)\n */\nstatic void\ntcp_collapse(struct sock *sk, struct sk_buff_head *list, struct rb_root *root,\n\t     struct sk_buff *head, struct sk_buff *tail, u32 start, u32 end)\n{\n\tstruct sk_buff *skb = head, *n;\n\tstruct sk_buff_head tmp;\n\tbool end_of_skbs;\n\n\t/* First, check that queue is collapsible and find\n\t * the point where collapsing can be useful.\n\t */\nrestart:\n\tfor (end_of_skbs = true; skb != NULL && skb != tail; skb = n) {\n\t\tn = tcp_skb_next(skb, list);\n\n\t\tif (!skb_frags_readable(skb))\n\t\t\tgoto skip_this;\n\n\t\t/* No new bits? It is possible on ofo queue. */\n\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\tskb = tcp_collapse_one(sk, skb, list, root);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\tgoto restart;\n\t\t}\n\n\t\t/* The first skb to collapse is:\n\t\t * - not SYN/FIN and\n\t\t * - bloated or contains data before \"start\" or\n\t\t *   overlaps to the next one and mptcp allow collapsing.\n\t\t */\n\t\tif (!(TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)) &&\n\t\t    (tcp_win_from_space(sk, skb->truesize) > skb->len ||\n\t\t     before(TCP_SKB_CB(skb)->seq, start))) {\n\t\t\tend_of_skbs = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (n && n != tail && skb_frags_readable(n) &&\n\t\t    tcp_skb_can_collapse_rx(skb, n) &&\n\t\t    TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(n)->seq) {\n\t\t\tend_of_skbs = false;\n\t\t\tbreak;\n\t\t}\n\nskip_this:\n\t\t/* Decided to skip this, advance start seq. */\n\t\tstart = TCP_SKB_CB(skb)->end_seq;\n\t}\n\tif (end_of_skbs ||\n\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)) ||\n\t    !skb_frags_readable(skb))\n\t\treturn;\n\n\t__skb_queue_head_init(&tmp);\n\n\twhile (before(start, end)) {\n\t\tint copy = min_t(int, SKB_MAX_ORDER(0, 0), end - start);\n\t\tstruct sk_buff *nskb;\n\n\t\tnskb = alloc_skb(copy, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\tbreak;\n\n\t\tmemcpy(nskb->cb, skb->cb, sizeof(skb->cb));\n\t\tskb_copy_decrypted(nskb, skb);\n\t\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(nskb)->end_seq = start;\n\t\tif (list)\n\t\t\t__skb_queue_before(list, skb, nskb);\n\t\telse\n\t\t\t__skb_queue_tail(&tmp, nskb); /* defer rbtree insertion */\n\t\tskb_set_owner_r(nskb, sk);\n\t\tmptcp_skb_ext_move(nskb, skb);\n\n\t\t/* Copy data, releasing collapsed skbs. */\n\t\twhile (copy > 0) {\n\t\t\tint offset = start - TCP_SKB_CB(skb)->seq;\n\t\t\tint size = TCP_SKB_CB(skb)->end_seq - start;\n\n\t\t\tBUG_ON(offset < 0);\n\t\t\tif (size > 0) {\n\t\t\t\tsize = min(copy, size);\n\t\t\t\tif (skb_copy_bits(skb, offset, skb_put(nskb, size), size))\n\t\t\t\t\tBUG();\n\t\t\t\tTCP_SKB_CB(nskb)->end_seq += size;\n\t\t\t\tcopy -= size;\n\t\t\t\tstart += size;\n\t\t\t}\n\t\t\tif (!before(start, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t\tskb = tcp_collapse_one(sk, skb, list, root);\n\t\t\t\tif (!skb ||\n\t\t\t\t    skb == tail ||\n\t\t\t\t    !tcp_skb_can_collapse_rx(nskb, skb) ||\n\t\t\t\t    (TCP_SKB_CB(skb)->tcp_flags & (TCPHDR_SYN | TCPHDR_FIN)) ||\n\t\t\t\t    !skb_frags_readable(skb))\n\t\t\t\t\tgoto end;\n\t\t\t}\n\t\t}\n\t}\nend:\n\tskb_queue_walk_safe(&tmp, skb, n)\n\t\ttcp_rbtree_insert(root, skb);\n}\n\n/* Collapse ofo queue. Algorithm: select contiguous sequence of skbs\n * and tcp_collapse() them until all the queue is collapsed.\n */\nstatic void tcp_collapse_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 range_truesize, sum_tiny = 0;\n\tstruct sk_buff *skb, *head;\n\tu32 start, end;\n\n\tskb = skb_rb_first(&tp->out_of_order_queue);\nnew_range:\n\tif (!skb) {\n\t\ttp->ooo_last_skb = skb_rb_last(&tp->out_of_order_queue);\n\t\treturn;\n\t}\n\tstart = TCP_SKB_CB(skb)->seq;\n\tend = TCP_SKB_CB(skb)->end_seq;\n\trange_truesize = skb->truesize;\n\n\tfor (head = skb;;) {\n\t\tskb = skb_rb_next(skb);\n\n\t\t/* Range is terminated when we see a gap or when\n\t\t * we are at the queue end.\n\t\t */\n\t\tif (!skb ||\n\t\t    after(TCP_SKB_CB(skb)->seq, end) ||\n\t\t    before(TCP_SKB_CB(skb)->end_seq, start)) {\n\t\t\t/* Do not attempt collapsing tiny skbs */\n\t\t\tif (range_truesize != head->truesize ||\n\t\t\t    end - start >= SKB_WITH_OVERHEAD(PAGE_SIZE)) {\n\t\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,\n\t\t\t\t\t     head, skb, start, end);\n\t\t\t} else {\n\t\t\t\tsum_tiny += range_truesize;\n\t\t\t\tif (sum_tiny > sk->sk_rcvbuf >> 3)\n\t\t\t\t\treturn;\n\t\t\t}\n\t\t\tgoto new_range;\n\t\t}\n\n\t\trange_truesize += skb->truesize;\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->seq, start)))\n\t\t\tstart = TCP_SKB_CB(skb)->seq;\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, end))\n\t\t\tend = TCP_SKB_CB(skb)->end_seq;\n\t}\n}\n\n/*\n * Clean the out-of-order queue to make room.\n * We drop high sequences packets to :\n * 1) Let a chance for holes to be filled.\n *    This means we do not drop packets from ooo queue if their sequence\n *    is before incoming packet sequence.\n * 2) not add too big latencies if thousands of packets sit there.\n *    (But if application shrinks SO_RCVBUF, we could still end up\n *     freeing whole queue here)\n * 3) Drop at least 12.5 % of sk_rcvbuf to avoid malicious attacks.\n *\n * Return true if queue has shrunk.\n */\nstatic bool tcp_prune_ofo_queue(struct sock *sk, const struct sk_buff *in_skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node *node, *prev;\n\tbool pruned = false;\n\tint goal;\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\treturn false;\n\n\tgoal = sk->sk_rcvbuf >> 3;\n\tnode = &tp->ooo_last_skb->rbnode;\n\n\tdo {\n\t\tstruct sk_buff *skb = rb_to_skb(node);\n\n\t\t/* If incoming skb would land last in ofo queue, stop pruning. */\n\t\tif (after(TCP_SKB_CB(in_skb)->seq, TCP_SKB_CB(skb)->seq))\n\t\t\tbreak;\n\t\tpruned = true;\n\t\tprev = rb_prev(node);\n\t\trb_erase(node, &tp->out_of_order_queue);\n\t\tgoal -= skb->truesize;\n\t\ttcp_drop_reason(sk, skb, SKB_DROP_REASON_TCP_OFO_QUEUE_PRUNE);\n\t\ttp->ooo_last_skb = rb_to_skb(prev);\n\t\tif (!prev || goal <= 0) {\n\t\t\tif (tcp_can_ingest(sk, in_skb) &&\n\t\t\t    !tcp_under_memory_pressure(sk))\n\t\t\t\tbreak;\n\t\t\tgoal = sk->sk_rcvbuf >> 3;\n\t\t}\n\t\tnode = prev;\n\t} while (node);\n\n\tif (pruned) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\t\t/* Reset SACK state.  A conforming SACK implementation will\n\t\t * do the same at a timeout based retransmit.  When a connection\n\t\t * is in a sad state like this, we care only about integrity\n\t\t * of the connection not performance.\n\t\t */\n\t\tif (tp->rx_opt.sack_ok)\n\t\t\ttcp_sack_reset(&tp->rx_opt);\n\t}\n\treturn pruned;\n}\n\n/* Reduce allocated memory if we can, trying to get\n * the socket within its memory limits again.\n *\n * Return less than zero if we should start dropping frames\n * until the socket owning process reads some of the data\n * to stabilize the situation.\n */\nstatic int tcp_prune_queue(struct sock *sk, const struct sk_buff *in_skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Do nothing if our queues are empty. */\n\tif (!atomic_read(&sk->sk_rmem_alloc))\n\t\treturn -1;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);\n\n\tif (!tcp_can_ingest(sk, in_skb))\n\t\ttcp_clamp_window(sk);\n\telse if (tcp_under_memory_pressure(sk))\n\t\ttcp_adjust_rcv_ssthresh(sk);\n\n\tif (tcp_can_ingest(sk, in_skb))\n\t\treturn 0;\n\n\ttcp_collapse_ofo_queue(sk);\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\ttcp_collapse(sk, &sk->sk_receive_queue, NULL,\n\t\t\t     skb_peek(&sk->sk_receive_queue),\n\t\t\t     NULL,\n\t\t\t     tp->copied_seq, tp->rcv_nxt);\n\n\tif (tcp_can_ingest(sk, in_skb))\n\t\treturn 0;\n\n\t/* Collapsing did not help, destructive actions follow.\n\t * This must not ever occur. */\n\n\ttcp_prune_ofo_queue(sk, in_skb);\n\n\tif (tcp_can_ingest(sk, in_skb))\n\t\treturn 0;\n\n\t/* If we are really being abused, tell the caller to silently\n\t * drop receive data on the floor.  It will get retransmitted\n\t * and hopefully then we'll have sufficient space.\n\t */\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);\n\n\t/* Massive buffer overcommit. */\n\ttp->pred_flags = 0;\n\treturn -1;\n}\n\nstatic bool tcp_should_expand_sndbuf(struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If the user specified a specific send buffer setting, do\n\t * not modify it.\n\t */\n\tif (sk->sk_userlocks & SOCK_SNDBUF_LOCK)\n\t\treturn false;\n\n\t/* If we are under global TCP memory pressure, do not expand.  */\n\tif (tcp_under_memory_pressure(sk)) {\n\t\tint unused_mem = sk_unused_reserved_mem(sk);\n\n\t\t/* Adjust sndbuf according to reserved mem. But make sure\n\t\t * it never goes below SOCK_MIN_SNDBUF.\n\t\t * See sk_stream_moderate_sndbuf() for more details.\n\t\t */\n\t\tif (unused_mem > SOCK_MIN_SNDBUF)\n\t\t\tWRITE_ONCE(sk->sk_sndbuf, unused_mem);\n\n\t\treturn false;\n\t}\n\n\t/* If we are under soft global TCP memory pressure, do not expand.  */\n\tif (sk_memory_allocated(sk) >= sk_prot_mem_limits(sk, 0))\n\t\treturn false;\n\n\t/* If we filled the congestion window, do not expand.  */\n\tif (tcp_packets_in_flight(tp) >= tcp_snd_cwnd(tp))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void tcp_new_space(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_should_expand_sndbuf(sk)) {\n\t\ttcp_sndbuf_expand(sk);\n\t\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\t}\n\n\tINDIRECT_CALL_1(sk->sk_write_space, sk_stream_write_space, sk);\n}\n\n/* Caller made space either from:\n * 1) Freeing skbs in rtx queues (after tp->snd_una has advanced)\n * 2) Sent skbs from output queue (and thus advancing tp->snd_nxt)\n *\n * We might be able to generate EPOLLOUT to the application if:\n * 1) Space consumed in output/rtx queues is below sk->sk_sndbuf/2\n * 2) notsent amount (tp->write_seq - tp->snd_nxt) became\n *    small enough that tcp_stream_memory_free() decides it\n *    is time to generate EPOLLOUT.\n */\nvoid tcp_check_space(struct sock *sk)\n{\n\t/* pairs with tcp_poll() */\n\tsmp_mb();\n\tif (sk->sk_socket &&\n\t    test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\ttcp_new_space(sk);\n\t\tif (!test_bit(SOCK_NOSPACE, &sk->sk_socket->flags))\n\t\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n}\n\nstatic inline void tcp_data_snd_check(struct sock *sk)\n{\n\ttcp_push_pending_frames(sk);\n\ttcp_check_space(sk);\n}\n\n/*\n * Check if sending an ack is needed.\n */\nstatic void __tcp_ack_snd_check(struct sock *sk, int ofo_possible)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long rtt, delay;\n\n\t    /* More than one full frame received... */\n\tif (((tp->rcv_nxt - tp->rcv_wup) > inet_csk(sk)->icsk_ack.rcv_mss &&\n\t     /* ... and right edge of window advances far enough.\n\t      * (tcp_recvmsg() will send ACK otherwise).\n\t      * If application uses SO_RCVLOWAT, we want send ack now if\n\t      * we have not received enough bytes to satisfy the condition.\n\t      */\n\t    (tp->rcv_nxt - tp->copied_seq < sk->sk_rcvlowat ||\n\t     __tcp_select_window(sk) >= tp->rcv_wnd)) ||\n\t    /* We ACK each frame or... */\n\t    tcp_in_quickack_mode(sk) ||\n\t    /* Protocol state mandates a one-time immediate ACK */\n\t    inet_csk(sk)->icsk_ack.pending & ICSK_ACK_NOW) {\n\t\t/* If we are running from __release_sock() in user context,\n\t\t * Defer the ack until tcp_release_cb().\n\t\t */\n\t\tif (sock_owned_by_user_nocheck(sk) &&\n\t\t    READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_backlog_ack_defer)) {\n\t\t\tset_bit(TCP_ACK_DEFERRED, &sk->sk_tsq_flags);\n\t\t\treturn;\n\t\t}\nsend_now:\n\t\ttcp_send_ack(sk);\n\t\treturn;\n\t}\n\n\tif (!ofo_possible || RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\ttcp_send_delayed_ack(sk);\n\t\treturn;\n\t}\n\n\tif (!tcp_is_sack(tp) ||\n\t    tp->compressed_ack >= READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_comp_sack_nr))\n\t\tgoto send_now;\n\n\tif (tp->compressed_ack_rcv_nxt != tp->rcv_nxt) {\n\t\ttp->compressed_ack_rcv_nxt = tp->rcv_nxt;\n\t\ttp->dup_ack_counter = 0;\n\t}\n\tif (tp->dup_ack_counter < TCP_FASTRETRANS_THRESH) {\n\t\ttp->dup_ack_counter++;\n\t\tgoto send_now;\n\t}\n\ttp->compressed_ack++;\n\tif (hrtimer_is_queued(&tp->compressed_ack_timer))\n\t\treturn;\n\n\t/* compress ack timer : 5 % of rtt, but no more than tcp_comp_sack_delay_ns */\n\n\trtt = tp->rcv_rtt_est.rtt_us;\n\tif (tp->srtt_us && tp->srtt_us < rtt)\n\t\trtt = tp->srtt_us;\n\n\tdelay = min_t(unsigned long,\n\t\t      READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_comp_sack_delay_ns),\n\t\t      rtt * (NSEC_PER_USEC >> 3)/20);\n\tsock_hold(sk);\n\thrtimer_start_range_ns(&tp->compressed_ack_timer, ns_to_ktime(delay),\n\t\t\t       READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_comp_sack_slack_ns),\n\t\t\t       HRTIMER_MODE_REL_PINNED_SOFT);\n}\n\nstatic inline void tcp_ack_snd_check(struct sock *sk)\n{\n\tif (!inet_csk_ack_scheduled(sk)) {\n\t\t/* We sent a data segment already. */\n\t\treturn;\n\t}\n\t__tcp_ack_snd_check(sk, 1);\n}\n\n/*\n *\tThis routine is only called when we have urgent data\n *\tsignaled. Its the 'slow' part of tcp_urg. It could be\n *\tmoved inline now as tcp_urg is only called from one\n *\tplace. We handle URGent data wrong. We have to - as\n *\tBSD still doesn't use the correction from RFC961.\n *\tFor 1003.1g we should support a new option TCP_STDURG to permit\n *\teither form (or just set the sysctl tcp_stdurg).\n */\n\nstatic void tcp_check_urg(struct sock *sk, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 ptr = ntohs(th->urg_ptr);\n\n\tif (ptr && !READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_stdurg))\n\t\tptr--;\n\tptr += ntohl(th->seq);\n\n\t/* Ignore urgent data that we've already seen and read. */\n\tif (after(tp->copied_seq, ptr))\n\t\treturn;\n\n\t/* Do not replay urg ptr.\n\t *\n\t * NOTE: interesting situation not covered by specs.\n\t * Misbehaving sender may send urg ptr, pointing to segment,\n\t * which we already have in ofo queue. We are not able to fetch\n\t * such data and will stay in TCP_URG_NOTYET until will be eaten\n\t * by recvmsg(). Seems, we are not obliged to handle such wicked\n\t * situations. But it is worth to think about possibility of some\n\t * DoSes using some hypothetical application level deadlock.\n\t */\n\tif (before(ptr, tp->rcv_nxt))\n\t\treturn;\n\n\t/* Do we already have a newer (or duplicate) urgent pointer? */\n\tif (tp->urg_data && !after(ptr, tp->urg_seq))\n\t\treturn;\n\n\t/* Tell the world about our new urgent pointer. */\n\tsk_send_sigurg(sk);\n\n\t/* We may be adding urgent data when the last byte read was\n\t * urgent. To do this requires some care. We cannot just ignore\n\t * tp->copied_seq since we would read the last urgent byte again\n\t * as data, nor can we alter copied_seq until this data arrives\n\t * or we break the semantics of SIOCATMARK (and thus sockatmark())\n\t *\n\t * NOTE. Double Dutch. Rendering to plain English: author of comment\n\t * above did something sort of \tsend(\"A\", MSG_OOB); send(\"B\", MSG_OOB);\n\t * and expect that both A and B disappear from stream. This is _wrong_.\n\t * Though this happens in BSD with high probability, this is occasional.\n\t * Any application relying on this is buggy. Note also, that fix \"works\"\n\t * only in this artificial test. Insert some normal data between A and B and we will\n\t * decline of BSD again. Verdict: it is better to remove to trap\n\t * buggy users.\n\t */\n\tif (tp->urg_seq == tp->copied_seq && tp->urg_data &&\n\t    !sock_flag(sk, SOCK_URGINLINE) && tp->copied_seq != tp->rcv_nxt) {\n\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\t\ttp->copied_seq++;\n\t\tif (skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq)) {\n\t\t\t__skb_unlink(skb, &sk->sk_receive_queue);\n\t\t\t__kfree_skb(skb);\n\t\t}\n\t}\n\n\tWRITE_ONCE(tp->urg_data, TCP_URG_NOTYET);\n\tWRITE_ONCE(tp->urg_seq, ptr);\n\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n}\n\n/* This is the 'fast' part of urgent handling. */\nstatic void tcp_urg(struct sock *sk, struct sk_buff *skb, const struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* Check if we get a new urgent pointer - normally not. */\n\tif (unlikely(th->urg))\n\t\ttcp_check_urg(sk, th);\n\n\t/* Do we wait for any urgent data? - normally not... */\n\tif (unlikely(tp->urg_data == TCP_URG_NOTYET)) {\n\t\tu32 ptr = tp->urg_seq - ntohl(th->seq) + (th->doff * 4) -\n\t\t\t  th->syn;\n\n\t\t/* Is the urgent pointer pointing into this packet? */\n\t\tif (ptr < skb->len) {\n\t\t\tu8 tmp;\n\t\t\tif (skb_copy_bits(skb, ptr, &tmp, 1))\n\t\t\t\tBUG();\n\t\t\tWRITE_ONCE(tp->urg_data, TCP_URG_VALID | tmp);\n\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\tsk->sk_data_ready(sk);\n\t\t}\n\t}\n}\n\n/* Accept RST for rcv_nxt - 1 after a FIN.\n * When tcp connections are abruptly terminated from Mac OSX (via ^C), a\n * FIN is sent followed by a RST packet. The RST is sent with the same\n * sequence number as the FIN, and thus according to RFC 5961 a challenge\n * ACK should be sent. However, Mac OSX rate limits replies to challenge\n * ACKs on the closed socket. In addition middleboxes can drop either the\n * challenge ACK or a subsequent RST.\n */\nstatic bool tcp_reset_check(const struct sock *sk, const struct sk_buff *skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\treturn unlikely(TCP_SKB_CB(skb)->seq == (tp->rcv_nxt - 1) &&\n\t\t\t(1 << sk->sk_state) & (TCPF_CLOSE_WAIT | TCPF_LAST_ACK |\n\t\t\t\t\t       TCPF_CLOSING));\n}\n\n/* Does PAWS and seqno based validation of an incoming segment, flags will\n * play significant role here.\n */\nstatic bool tcp_validate_incoming(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  const struct tcphdr *th, int syn_inerr)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool accecn_reflector = false;\n\tSKB_DR(reason);\n\n\t/* RFC1323: H1. Apply PAWS check first. */\n\tif (!tcp_fast_parse_options(sock_net(sk), skb, th, tp) ||\n\t    !tp->rx_opt.saw_tstamp ||\n\t    tcp_paws_check(&tp->rx_opt, TCP_PAWS_WINDOW))\n\t\tgoto step1;\n\n\treason = tcp_disordered_ack_check(sk, skb);\n\tif (!reason)\n\t\tgoto step1;\n\t/* Reset is accepted even if it did not pass PAWS. */\n\tif (th->rst)\n\t\tgoto step1;\n\tif (unlikely(th->syn))\n\t\tgoto syn_challenge;\n\n\t/* Old ACK are common, increment PAWS_OLD_ACK\n\t * and do not send a dupack.\n\t */\n\tif (reason == SKB_DROP_REASON_TCP_RFC7323_PAWS_ACK) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PAWS_OLD_ACK);\n\t\tgoto discard;\n\t}\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PAWSESTABREJECTED);\n\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDPAWS,\n\t\t\t\t  &tp->last_oow_ack_time))\n\t\ttcp_send_dupack(sk, skb);\n\tgoto discard;\n\nstep1:\n\t/* Step 1: check sequence number */\n\treason = tcp_sequence(sk, TCP_SKB_CB(skb)->seq, TCP_SKB_CB(skb)->end_seq);\n\tif (reason) {\n\t\t/* RFC793, page 37: \"In all states except SYN-SENT, all reset\n\t\t * (RST) segments are validated by checking their SEQ-fields.\"\n\t\t * And page 69: \"If an incoming segment is not acceptable,\n\t\t * an acknowledgment should be sent in reply (unless the RST\n\t\t * bit is set, if so drop the segment and return)\".\n\t\t */\n\t\tif (!th->rst) {\n\t\t\tif (th->syn)\n\t\t\t\tgoto syn_challenge;\n\n\t\t\tif (reason == SKB_DROP_REASON_TCP_INVALID_SEQUENCE ||\n\t\t\t    reason == SKB_DROP_REASON_TCP_INVALID_END_SEQUENCE)\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_BEYOND_WINDOW);\n\t\t\tif (!tcp_oow_rate_limited(sock_net(sk), skb,\n\t\t\t\t\t\t  LINUX_MIB_TCPACKSKIPPEDSEQ,\n\t\t\t\t\t\t  &tp->last_oow_ack_time))\n\t\t\t\ttcp_send_dupack(sk, skb);\n\t\t} else if (tcp_reset_check(sk, skb)) {\n\t\t\tgoto reset;\n\t\t}\n\t\tgoto discard;\n\t}\n\n\t/* Step 2: check RST bit */\n\tif (th->rst) {\n\t\t/* RFC 5961 3.2 (extend to match against (RCV.NXT - 1) after a\n\t\t * FIN and SACK too if available):\n\t\t * If seq num matches RCV.NXT or (RCV.NXT - 1) after a FIN, or\n\t\t * the right-most SACK block,\n\t\t * then\n\t\t *     RESET the connection\n\t\t * else\n\t\t *     Send a challenge ACK\n\t\t */\n\t\tif (TCP_SKB_CB(skb)->seq == tp->rcv_nxt ||\n\t\t    tcp_reset_check(sk, skb))\n\t\t\tgoto reset;\n\n\t\tif (tcp_is_sack(tp) && tp->rx_opt.num_sacks > 0) {\n\t\t\tstruct tcp_sack_block *sp = &tp->selective_acks[0];\n\t\t\tint max_sack = sp[0].end_seq;\n\t\t\tint this_sack;\n\n\t\t\tfor (this_sack = 1; this_sack < tp->rx_opt.num_sacks;\n\t\t\t     ++this_sack) {\n\t\t\t\tmax_sack = after(sp[this_sack].end_seq,\n\t\t\t\t\t\t max_sack) ?\n\t\t\t\t\tsp[this_sack].end_seq : max_sack;\n\t\t\t}\n\n\t\t\tif (TCP_SKB_CB(skb)->seq == max_sack)\n\t\t\t\tgoto reset;\n\t\t}\n\n\t\t/* Disable TFO if RST is out-of-order\n\t\t * and no data has been received\n\t\t * for current active TFO socket\n\t\t */\n\t\tif (tp->syn_fastopen && !tp->data_segs_in &&\n\t\t    sk->sk_state == TCP_ESTABLISHED)\n\t\t\ttcp_fastopen_active_disable(sk);\n\t\ttcp_send_challenge_ack(sk, false);\n\t\tSKB_DR_SET(reason, TCP_RESET);\n\t\tgoto discard;\n\t}\n\n\t/* step 3: check security and precedence [ignored] */\n\n\t/* step 4: Check for a SYN\n\t * RFC 5961 4.2 : Send a challenge ack\n\t */\n\tif (th->syn) {\n\t\tif (tcp_ecn_mode_accecn(tp)) {\n\t\t\taccecn_reflector = true;\n\t\t\tif (tp->rx_opt.accecn &&\n\t\t\t    tp->saw_accecn_opt < TCP_ACCECN_OPT_COUNTER_SEEN) {\n\t\t\t\tu8 saw_opt = tcp_accecn_option_init(skb, tp->rx_opt.accecn);\n\n\t\t\t\ttcp_accecn_saw_opt_fail_recv(tp, saw_opt);\n\t\t\t\ttcp_accecn_opt_demand_min(sk, 1);\n\t\t\t}\n\t\t}\n\t\tif (sk->sk_state == TCP_SYN_RECV && sk->sk_socket && th->ack &&\n\t\t    TCP_SKB_CB(skb)->seq + 1 == TCP_SKB_CB(skb)->end_seq &&\n\t\t    TCP_SKB_CB(skb)->seq + 1 == tp->rcv_nxt &&\n\t\t    TCP_SKB_CB(skb)->ack_seq == tp->snd_nxt)\n\t\t\tgoto pass;\nsyn_challenge:\n\t\tif (syn_inerr)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNCHALLENGE);\n\t\ttcp_send_challenge_ack(sk, accecn_reflector);\n\t\tSKB_DR_SET(reason, TCP_INVALID_SYN);\n\t\tgoto discard;\n\t}\n\npass:\n\tbpf_skops_parse_hdr(sk, skb);\n\n\treturn true;\n\ndiscard:\n\ttcp_drop_reason(sk, skb, reason);\n\treturn false;\n\nreset:\n\ttcp_reset(sk, skb);\n\t__kfree_skb(skb);\n\treturn false;\n}\n\n/*\n *\tTCP receive function for the ESTABLISHED state.\n *\n *\tIt is split into a fast path and a slow path. The fast path is\n * \tdisabled when:\n *\t- A zero window was announced from us - zero window probing\n *        is only handled properly in the slow path.\n *\t- Out of order segments arrived.\n *\t- Urgent data is expected.\n *\t- There is no buffer space left\n *\t- Unexpected TCP flags/window values/header lengths are received\n *\t  (detected by checking the TCP header against pred_flags)\n *\t- Data is sent in both directions. Fast path only supports pure senders\n *\t  or pure receivers (this means either the sequence number or the ack\n *\t  value must stay constant)\n *\t- Unexpected TCP option.\n *\n *\tWhen these conditions are not satisfied it drops into a standard\n *\treceive procedure patterned after RFC793 to handle all cases.\n *\tThe first three cases are guaranteed by proper pred_flags setting,\n *\tthe rest is checked inline. Fast processing is turned on in\n *\ttcp_data_queue when everything is OK.\n */\nvoid tcp_rcv_established(struct sock *sk, struct sk_buff *skb)\n{\n\tenum skb_drop_reason reason = SKB_DROP_REASON_NOT_SPECIFIED;\n\tconst struct tcphdr *th = (const struct tcphdr *)skb->data;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int len = skb->len;\n\n\t/* TCP congestion window tracking */\n\ttrace_tcp_probe(sk, skb);\n\n\ttcp_mstamp_refresh(tp);\n\tif (unlikely(!rcu_access_pointer(sk->sk_rx_dst)))\n\t\tinet_csk(sk)->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t/*\n\t *\tHeader prediction.\n\t *\tThe code loosely follows the one in the famous\n\t *\t\"30 instruction TCP receive\" Van Jacobson mail.\n\t *\n\t *\tVan's trick is to deposit buffers into socket queue\n\t *\ton a device interrupt, to call tcp_recv function\n\t *\ton the receive process context and checksum and copy\n\t *\tthe buffer to user space. smart...\n\t *\n\t *\tOur current scheme is not silly either but we take the\n\t *\textra cost of the net_bh soft interrupt processing...\n\t *\tWe do checksum and copy also but from device to kernel.\n\t */\n\n\ttp->rx_opt.saw_tstamp = 0;\n\ttp->rx_opt.accecn = 0;\n\n\t/*\tpred_flags is 0xS?10 << 16 + snd_wnd\n\t *\tif header_prediction is to be made\n\t *\t'S' will always be tp->tcp_header_len >> 2\n\t *\t'?' will be 0 for the fast path, otherwise pred_flags is 0 to\n\t *  turn it off\t(when there are holes in the receive\n\t *\t space for instance)\n\t *\tPSH flag is ignored.\n\t */\n\n\tif ((tcp_flag_word(th) & TCP_HP_BITS) == tp->pred_flags &&\n\t    TCP_SKB_CB(skb)->seq == tp->rcv_nxt &&\n\t    !after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {\n\t\tint tcp_header_len = tp->tcp_header_len;\n\t\ts32 delta = 0;\n\t\tint flag = 0;\n\n\t\t/* Timestamp header prediction: tcp_header_len\n\t\t * is automatically equal to th->doff*4 due to pred_flags\n\t\t * match.\n\t\t */\n\n\t\t/* Check timestamp */\n\t\tif (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) {\n\t\t\t/* No? Slow path! */\n\t\t\tif (!tcp_parse_aligned_timestamp(tp, th))\n\t\t\t\tgoto slow_path;\n\n\t\t\tdelta = tp->rx_opt.rcv_tsval -\n\t\t\t\ttp->rx_opt.ts_recent;\n\t\t\t/* If PAWS failed, check it more carefully in slow path */\n\t\t\tif (delta < 0)\n\t\t\t\tgoto slow_path;\n\n\t\t\t/* DO NOT update ts_recent here, if checksum fails\n\t\t\t * and timestamp was corrupted part, it will result\n\t\t\t * in a hung connection since we will drop all\n\t\t\t * future packets due to the PAWS test.\n\t\t\t */\n\t\t}\n\n\t\tif (len <= tcp_header_len) {\n\t\t\t/* Bulk data transfer: sender */\n\t\t\tif (len == tcp_header_len) {\n\t\t\t\t/* Predicted packet is in window by definition.\n\t\t\t\t * seq == rcv_nxt and rcv_wup <= rcv_nxt.\n\t\t\t\t * Hence, check seq<=rcv_wup reduces to:\n\t\t\t\t */\n\t\t\t\tif (tcp_header_len ==\n\t\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\t\tflag |= __tcp_replace_ts_recent(tp,\n\t\t\t\t\t\t\t\t\tdelta);\n\n\t\t\t\ttcp_ecn_received_counters(sk, skb, 0);\n\n\t\t\t\t/* We know that such packets are checksummed\n\t\t\t\t * on entry.\n\t\t\t\t */\n\t\t\t\ttcp_ack(sk, skb, flag);\n\t\t\t\t__kfree_skb(skb);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\t/* When receiving pure ack in fast path, update\n\t\t\t\t * last ts ecr directly instead of calling\n\t\t\t\t * tcp_rcv_rtt_measure_ts()\n\t\t\t\t */\n\t\t\t\ttp->rcv_rtt_last_tsecr = tp->rx_opt.rcv_tsecr;\n\t\t\t\treturn;\n\t\t\t} else { /* Header too small */\n\t\t\t\treason = SKB_DROP_REASON_PKT_TOO_SMALL;\n\t\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t} else {\n\t\t\tint eaten = 0;\n\t\t\tbool fragstolen = false;\n\n\t\t\tif (tcp_checksum_complete(skb))\n\t\t\t\tgoto csum_error;\n\n\t\t\tif (after(TCP_SKB_CB(skb)->end_seq,\n\t\t\t\t  tp->rcv_nxt + tcp_receive_window(tp)))\n\t\t\t\tgoto validate;\n\n\t\t\tif ((int)skb->truesize > sk->sk_forward_alloc)\n\t\t\t\tgoto step5;\n\n\t\t\t/* Predicted packet is in window by definition.\n\t\t\t * seq == rcv_nxt and rcv_wup <= rcv_nxt.\n\t\t\t * Hence, check seq<=rcv_wup reduces to:\n\t\t\t */\n\t\t\tif (tcp_header_len ==\n\t\t\t    (sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &&\n\t\t\t    tp->rcv_nxt == tp->rcv_wup)\n\t\t\t\tflag |= __tcp_replace_ts_recent(tp,\n\t\t\t\t\t\t\t\tdelta);\n\n\t\t\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPHITS);\n\n\t\t\t/* Bulk data transfer: receiver */\n\t\t\ttcp_cleanup_skb(skb);\n\t\t\t__skb_pull(skb, tcp_header_len);\n\t\t\ttcp_ecn_received_counters(sk, skb,\n\t\t\t\t\t\t  len - tcp_header_len);\n\t\t\teaten = tcp_queue_rcv(sk, skb, &fragstolen);\n\n\t\t\ttcp_event_data_recv(sk, skb);\n\n\t\t\tif (TCP_SKB_CB(skb)->ack_seq != tp->snd_una) {\n\t\t\t\t/* Well, only one small jumplet in fast path... */\n\t\t\t\ttcp_ack(sk, skb, flag | FLAG_DATA);\n\t\t\t\ttcp_data_snd_check(sk);\n\t\t\t\tif (!inet_csk_ack_scheduled(sk))\n\t\t\t\t\tgoto no_ack;\n\t\t\t} else {\n\t\t\t\ttcp_update_wl(tp, TCP_SKB_CB(skb)->seq);\n\t\t\t}\n\n\t\t\t__tcp_ack_snd_check(sk, 0);\nno_ack:\n\t\t\tif (eaten)\n\t\t\t\tkfree_skb_partial(skb, fragstolen);\n\t\t\ttcp_data_ready(sk);\n\t\t\treturn;\n\t\t}\n\t}\n\nslow_path:\n\tif (len < (th->doff << 2) || tcp_checksum_complete(skb))\n\t\tgoto csum_error;\n\n\tif (!th->ack && !th->rst && !th->syn) {\n\t\treason = SKB_DROP_REASON_TCP_FLAGS;\n\t\tgoto discard;\n\t}\n\n\t/*\n\t *\tStandard slow path.\n\t */\nvalidate:\n\tif (!tcp_validate_incoming(sk, skb, th, 1))\n\t\treturn;\n\nstep5:\n\ttcp_ecn_received_counters_payload(sk, skb);\n\n\treason = tcp_ack(sk, skb, FLAG_SLOWPATH | FLAG_UPDATE_TS_RECENT);\n\tif ((int)reason < 0) {\n\t\treason = -reason;\n\t\tgoto discard;\n\t}\n\ttcp_rcv_rtt_measure_ts(sk, skb);\n\n\t/* Process urgent data. */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\ttcp_data_queue(sk, skb);\n\n\ttcp_data_snd_check(sk);\n\ttcp_ack_snd_check(sk);\n\treturn;\n\ncsum_error:\n\treason = SKB_DROP_REASON_TCP_CSUM;\n\ttrace_tcp_bad_csum(skb);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);\n\ndiscard:\n\ttcp_drop_reason(sk, skb, reason);\n}\nEXPORT_IPV6_MOD(tcp_rcv_established);\n\nvoid tcp_init_transfer(struct sock *sk, int bpf_op, struct sk_buff *skb)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttcp_mtup_init(sk);\n\ticsk->icsk_af_ops->rebuild_header(sk);\n\ttcp_init_metrics(sk);\n\n\t/* Initialize the congestion window to start the transfer.\n\t * Cut cwnd down to 1 per RFC5681 if SYN or SYN-ACK has been\n\t * retransmitted. In light of RFC6298 more aggressive 1sec\n\t * initRTO, we only reset cwnd when more than 1 SYN/SYN-ACK\n\t * retransmission has occurred.\n\t */\n\tif (tp->total_retrans > 1 && tp->undo_marker)\n\t\ttcp_snd_cwnd_set(tp, 1);\n\telse\n\t\ttcp_snd_cwnd_set(tp, tcp_init_cwnd(tp, __sk_dst_get(sk)));\n\ttp->snd_cwnd_stamp = tcp_jiffies32;\n\n\tbpf_skops_established(sk, bpf_op, skb);\n\t/* Initialize congestion control unless BPF initialized it already: */\n\tif (!icsk->icsk_ca_initialized)\n\t\ttcp_init_congestion_control(sk);\n\ttcp_init_buffer_space(sk);\n}\n\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\ttcp_ao_finish_connect(sk, skb);\n\ttcp_set_state(sk, TCP_ESTABLISHED);\n\ticsk->icsk_ack.lrcvtime = tcp_jiffies32;\n\n\tif (skb) {\n\t\ticsk->icsk_af_ops->sk_rx_dst_set(sk, skb);\n\t\tsecurity_inet_conn_established(sk, skb);\n\t\tsk_mark_napi_id(sk, skb);\n\t}\n\n\ttcp_init_transfer(sk, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB, skb);\n\n\t/* Prevent spurious tcp_cwnd_restart() on first data\n\t * packet.\n\t */\n\ttp->lsndtime = tcp_jiffies32;\n\n\tif (sock_flag(sk, SOCK_KEEPOPEN))\n\t\ttcp_reset_keepalive_timer(sk, keepalive_time_when(tp));\n\n\tif (!tp->rx_opt.snd_wscale)\n\t\t__tcp_fast_path_on(tp, tp->snd_wnd);\n\telse\n\t\ttp->pred_flags = 0;\n}\n\nstatic bool tcp_rcv_fastopen_synack(struct sock *sk, struct sk_buff *synack,\n\t\t\t\t    struct tcp_fastopen_cookie *cookie)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *data = tp->syn_data ? tcp_rtx_queue_head(sk) : NULL;\n\tu16 mss = tp->rx_opt.mss_clamp, try_exp = 0;\n\tbool syn_drop = false;\n\n\tif (mss == READ_ONCE(tp->rx_opt.user_mss)) {\n\t\tstruct tcp_options_received opt;\n\n\t\t/* Get original SYNACK MSS value if user MSS sets mss_clamp */\n\t\ttcp_clear_options(&opt);\n\t\topt.user_mss = opt.mss_clamp = 0;\n\t\ttcp_parse_options(sock_net(sk), synack, &opt, 0, NULL);\n\t\tmss = opt.mss_clamp;\n\t}\n\n\tif (!tp->syn_fastopen) {\n\t\t/* Ignore an unsolicited cookie */\n\t\tcookie->len = -1;\n\t} else if (tp->total_retrans) {\n\t\t/* SYN timed out and the SYN-ACK neither has a cookie nor\n\t\t * acknowledges data. Presumably the remote received only\n\t\t * the retransmitted (regular) SYNs: either the original\n\t\t * SYN-data or the corresponding SYN-ACK was dropped.\n\t\t */\n\t\tsyn_drop = (cookie->len < 0 && data);\n\t} else if (cookie->len < 0 && !tp->syn_data) {\n\t\t/* We requested a cookie but didn't get it. If we did not use\n\t\t * the (old) exp opt format then try so next time (try_exp=1).\n\t\t * Otherwise we go back to use the RFC7413 opt (try_exp=2).\n\t\t */\n\t\ttry_exp = tp->syn_fastopen_exp ? 2 : 1;\n\t}\n\n\ttcp_fastopen_cache_set(sk, mss, cookie, syn_drop, try_exp);\n\n\tif (data) { /* Retransmit unacked data in SYN */\n\t\tif (tp->total_retrans)\n\t\t\ttp->fastopen_client_fail = TFO_SYN_RETRANSMITTED;\n\t\telse\n\t\t\ttp->fastopen_client_fail = TFO_DATA_NOT_ACKED;\n\t\tskb_rbtree_walk_from(data)\n\t\t\t tcp_mark_skb_lost(sk, data);\n\t\ttcp_non_congestion_loss_retransmit(sk);\n\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\tLINUX_MIB_TCPFASTOPENACTIVEFAIL);\n\t\treturn true;\n\t}\n\ttp->syn_data_acked = tp->syn_data;\n\tif (tp->syn_data_acked) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFASTOPENACTIVE);\n\t\t/* SYN-data is counted as two separate packets in tcp_ack() */\n\t\tif (tp->delivered > 1)\n\t\t\t--tp->delivered;\n\t}\n\n\ttcp_fastopen_add_skb(sk, synack);\n\n\treturn false;\n}\n\nstatic void smc_check_reset_syn(struct tcp_sock *tp)\n{\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc && !tp->rx_opt.smc_ok)\n\t\t\ttp->syn_smc = 0;\n\t}\n#endif\n}\n\nstatic void tcp_try_undo_spurious_syn(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 syn_stamp;\n\n\t/* undo_marker is set when SYN or SYNACK times out. The timeout is\n\t * spurious if the ACK's timestamp option echo value matches the\n\t * original SYN timestamp.\n\t */\n\tsyn_stamp = tp->retrans_stamp;\n\tif (tp->undo_marker && syn_stamp && tp->rx_opt.saw_tstamp &&\n\t    syn_stamp == tp->rx_opt.rcv_tsecr)\n\t\ttp->undo_marker = 0;\n}\n\nstatic int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t const struct tcphdr *th)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\tint saved_clamp = tp->rx_opt.mss_clamp;\n\tbool fastopen_fail;\n\tSKB_DR(reason);\n\n\ttcp_parse_options(sock_net(sk), skb, &tp->rx_opt, 0, &foc);\n\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr)\n\t\ttp->rx_opt.rcv_tsecr -= tp->tsoffset;\n\n\tif (th->ack) {\n\t\t/* rfc793:\n\t\t * \"If the state is SYN-SENT then\n\t\t *    first check the ACK bit\n\t\t *      If the ACK bit is set\n\t\t *\t  If SEG.ACK =< ISS, or SEG.ACK > SND.NXT, send\n\t\t *        a reset (unless the RST bit is set, if so drop\n\t\t *        the segment and return)\"\n\t\t */\n\t\tif (!after(TCP_SKB_CB(skb)->ack_seq, tp->snd_una) ||\n\t\t    after(TCP_SKB_CB(skb)->ack_seq, tp->snd_nxt)) {\n\t\t\t/* Previous FIN/ACK or RST/ACK might be ignored. */\n\t\t\tif (icsk->icsk_retransmits == 0)\n\t\t\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t\t\t     TCP_TIMEOUT_MIN, false);\n\t\t\tSKB_DR_SET(reason, TCP_INVALID_ACK_SEQUENCE);\n\t\t\tgoto reset_and_undo;\n\t\t}\n\n\t\tif (tp->rx_opt.saw_tstamp && tp->rx_opt.rcv_tsecr &&\n\t\t    !between(tp->rx_opt.rcv_tsecr, tp->retrans_stamp,\n\t\t\t     tcp_time_stamp_ts(tp))) {\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_PAWSACTIVEREJECTED);\n\t\t\tSKB_DR_SET(reason, TCP_RFC7323_PAWS);\n\t\t\tgoto reset_and_undo;\n\t\t}\n\n\t\t/* Now ACK is acceptable.\n\t\t *\n\t\t * \"If the RST bit is set\n\t\t *    If the ACK was acceptable then signal the user \"error:\n\t\t *    connection reset\", drop the segment, enter CLOSED state,\n\t\t *    delete TCB, and return.\"\n\t\t */\n\n\t\tif (th->rst) {\n\t\t\ttcp_reset(sk, skb);\nconsume:\n\t\t\t__kfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\t/* rfc793:\n\t\t *   \"fifth, if neither of the SYN or RST bits is set then\n\t\t *    drop the segment and return.\"\n\t\t *\n\t\t *    See note below!\n\t\t *                                        --ANK(990513)\n\t\t */\n\t\tif (!th->syn) {\n\t\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\t\tgoto discard_and_undo;\n\t\t}\n\t\t/* rfc793:\n\t\t *   \"If the SYN bit is on ...\n\t\t *    are acceptable then ...\n\t\t *    (our SYN has been ACKed), change the connection\n\t\t *    state to ESTABLISHED...\"\n\t\t */\n\n\t\tif (tcp_ecn_mode_any(tp))\n\t\t\ttcp_ecn_rcv_synack(sk, skb, th,\n\t\t\t\t\t   TCP_SKB_CB(skb)->ip_dsfield);\n\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\t\ttcp_try_undo_spurious_syn(sk);\n\t\ttcp_ack(sk, skb, FLAG_SLOWPATH);\n\n\t\t/* Ok.. it's good. Set up sequence numbers and\n\t\t * move to established.\n\t\t */\n\t\tWRITE_ONCE(tp->rcv_nxt, TCP_SKB_CB(skb)->seq + 1);\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments is\n\t\t * never scaled.\n\t\t */\n\t\ttp->snd_wnd = ntohs(th->window);\n\n\t\tif (!tp->rx_opt.wscale_ok) {\n\t\t\ttp->rx_opt.snd_wscale = tp->rx_opt.rcv_wscale = 0;\n\t\t\tWRITE_ONCE(tp->window_clamp,\n\t\t\t\t   min(tp->window_clamp, 65535U));\n\t\t}\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok\t   = 1;\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttp->advmss\t    -= TCPOLEN_TSTAMP_ALIGNED;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\t/* Remember, tcp_poll() does not lock socket!\n\t\t * Change state from SYN-SENT only after copied_seq\n\t\t * is initialized. */\n\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\n\t\tsmc_check_reset_syn(tp);\n\n\t\tsmp_mb();\n\n\t\ttcp_finish_connect(sk, skb);\n\n\t\tfastopen_fail = (tp->syn_fastopen || tp->syn_data) &&\n\t\t\t\ttcp_rcv_fastopen_synack(sk, skb, &foc);\n\n\t\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\t\tsk->sk_state_change(sk);\n\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\t\t}\n\t\tif (fastopen_fail)\n\t\t\treturn -1;\n\t\tif (sk->sk_write_pending ||\n\t\t    READ_ONCE(icsk->icsk_accept_queue.rskq_defer_accept) ||\n\t\t    inet_csk_in_pingpong_mode(sk)) {\n\t\t\t/* Save one ACK. Data will be ready after\n\t\t\t * several ticks, if write_pending is set.\n\t\t\t *\n\t\t\t * It may be deleted, but with this feature tcpdumps\n\t\t\t * look so _wonderfully_ clever, that I was not able\n\t\t\t * to stand against the temptation 8)     --ANK\n\t\t\t */\n\t\t\tinet_csk_schedule_ack(sk);\n\t\t\ttcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);\n\t\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_DACK,\n\t\t\t\t\t     TCP_DELACK_MAX, false);\n\t\t\tgoto consume;\n\t\t}\n\t\ttcp_send_ack_reflect_ect(sk, tcp_ecn_mode_accecn(tp));\n\t\treturn -1;\n\t}\n\n\t/* No ACK in the segment */\n\n\tif (th->rst) {\n\t\t/* rfc793:\n\t\t * \"If the RST bit is set\n\t\t *\n\t\t *      Otherwise (no ACK) drop the segment and return.\"\n\t\t */\n\t\tSKB_DR_SET(reason, TCP_RESET);\n\t\tgoto discard_and_undo;\n\t}\n\n\t/* PAWS check. */\n\tif (tp->rx_opt.ts_recent_stamp && tp->rx_opt.saw_tstamp &&\n\t    tcp_paws_reject(&tp->rx_opt, 0)) {\n\t\tSKB_DR_SET(reason, TCP_RFC7323_PAWS);\n\t\tgoto discard_and_undo;\n\t}\n\tif (th->syn) {\n\t\t/* We see SYN without ACK. It is attempt of\n\t\t * simultaneous connect with crossed SYNs.\n\t\t * Particularly, it can be connect to self.\n\t\t */\n#ifdef CONFIG_TCP_AO\n\t\tstruct tcp_ao_info *ao;\n\n\t\tao = rcu_dereference_protected(tp->ao_info,\n\t\t\t\t\t       lockdep_sock_is_held(sk));\n\t\tif (ao) {\n\t\t\tWRITE_ONCE(ao->risn, th->seq);\n\t\t\tao->rcv_sne = 0;\n\t\t}\n#endif\n\t\ttcp_set_state(sk, TCP_SYN_RECV);\n\n\t\tif (tp->rx_opt.saw_tstamp) {\n\t\t\ttp->rx_opt.tstamp_ok = 1;\n\t\t\ttcp_store_ts_recent(tp);\n\t\t\ttp->tcp_header_len =\n\t\t\t\tsizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;\n\t\t} else {\n\t\t\ttp->tcp_header_len = sizeof(struct tcphdr);\n\t\t}\n\n\t\tWRITE_ONCE(tp->rcv_nxt, TCP_SKB_CB(skb)->seq + 1);\n\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\t\ttp->rcv_wup = TCP_SKB_CB(skb)->seq + 1;\n\n\t\t/* RFC1323: The window in SYN & SYN/ACK segments is\n\t\t * never scaled.\n\t\t */\n\t\ttp->snd_wnd    = ntohs(th->window);\n\t\ttp->snd_wl1    = TCP_SKB_CB(skb)->seq;\n\t\ttp->max_window = tp->snd_wnd;\n\n\t\ttcp_ecn_rcv_syn(tp, th, skb);\n\n\t\ttcp_mtup_init(sk);\n\t\ttcp_sync_mss(sk, icsk->icsk_pmtu_cookie);\n\t\ttcp_initialize_rcv_mss(sk);\n\n\t\ttcp_send_synack(sk);\n#if 0\n\t\t/* Note, we could accept data and URG from this segment.\n\t\t * There are no obstacles to make this (except that we must\n\t\t * either change tcp_recvmsg() to prevent it from returning data\n\t\t * before 3WHS completes per RFC793, or employ TCP Fast Open).\n\t\t *\n\t\t * However, if we ignore data in ACKless segments sometimes,\n\t\t * we have no reasons to accept it sometimes.\n\t\t * Also, seems the code doing it in step6 of tcp_rcv_state_process\n\t\t * is not flawless. So, discard packet for sanity.\n\t\t * Uncomment this return to process the data.\n\t\t */\n\t\treturn -1;\n#else\n\t\tgoto consume;\n#endif\n\t}\n\t/* \"fifth, if neither of the SYN or RST bits is set then\n\t * drop the segment and return.\"\n\t */\n\ndiscard_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\ttcp_drop_reason(sk, skb, reason);\n\treturn 0;\n\nreset_and_undo:\n\ttcp_clear_options(&tp->rx_opt);\n\ttp->rx_opt.mss_clamp = saved_clamp;\n\t/* we can reuse/return @reason to its caller to handle the exception */\n\treturn reason;\n}\n\nstatic void tcp_rcv_synrecv_state_fastopen(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct request_sock *req;\n\n\t/* If we are still handling the SYNACK RTO, see if timestamp ECR allows\n\t * undo. If peer SACKs triggered fast recovery, we can't undo here.\n\t */\n\tif (inet_csk(sk)->icsk_ca_state == TCP_CA_Loss && !tp->packets_out)\n\t\ttcp_try_undo_recovery(sk);\n\n\ttcp_update_rto_time(tp);\n\tWRITE_ONCE(inet_csk(sk)->icsk_retransmits, 0);\n\t/* In tcp_fastopen_synack_timer() on the first SYNACK RTO we set\n\t * retrans_stamp but don't enter CA_Loss, so in case that happened we\n\t * need to zero retrans_stamp here to prevent spurious\n\t * retransmits_timed_out(). However, if the ACK of our SYNACK caused us\n\t * to enter CA_Recovery then we need to leave retrans_stamp as it was\n\t * set entering CA_Recovery, for correct retransmits_timed_out() and\n\t * undo behavior.\n\t */\n\ttcp_retrans_stamp_cleanup(sk);\n\n\t/* Once we leave TCP_SYN_RECV or TCP_FIN_WAIT_1,\n\t * we no longer need req so release it.\n\t */\n\treq = rcu_dereference_protected(tp->fastopen_rsk,\n\t\t\t\t\tlockdep_sock_is_held(sk));\n\treqsk_fastopen_remove(sk, req, false);\n\n\t/* Re-arm the timer because data may have been sent out.\n\t * This is similar to the regular data transmission case\n\t * when new data has just been ack'ed.\n\t *\n\t * (TFO) - we could try to be more aggressive and\n\t * retransmitting any data sooner based on when they\n\t * are sent out.\n\t */\n\ttcp_rearm_rto(sk);\n}\n\n/*\n *\tThis function implements the receiving procedure of RFC 793 for\n *\tall states except ESTABLISHED and TIME_WAIT.\n *\tIt's called from both tcp_v4_rcv and tcp_v6_rcv and should be\n *\taddress independent.\n */\n\nenum skb_drop_reason\ntcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tstruct request_sock *req;\n\tint queued = 0;\n\tSKB_DR(reason);\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\tSKB_DR_SET(reason, TCP_CLOSE);\n\t\tgoto discard;\n\n\tcase TCP_LISTEN:\n\t\tif (th->ack)\n\t\t\treturn SKB_DROP_REASON_TCP_FLAGS;\n\n\t\tif (th->rst) {\n\t\t\tSKB_DR_SET(reason, TCP_RESET);\n\t\t\tgoto discard;\n\t\t}\n\t\tif (th->syn) {\n\t\t\tif (th->fin) {\n\t\t\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t\t/* It is possible that we process SYN packets from backlog,\n\t\t\t * so we need to make sure to disable BH and RCU right there.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tlocal_bh_disable();\n\t\t\ticsk->icsk_af_ops->conn_request(sk, skb);\n\t\t\tlocal_bh_enable();\n\t\t\trcu_read_unlock();\n\n\t\t\tconsume_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\tgoto discard;\n\n\tcase TCP_SYN_SENT:\n\t\ttp->rx_opt.saw_tstamp = 0;\n\t\ttcp_mstamp_refresh(tp);\n\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th);\n\t\tif (queued >= 0)\n\t\t\treturn queued;\n\n\t\t/* Do step6 onward by hand. */\n\t\ttcp_urg(sk, skb, th);\n\t\t__kfree_skb(skb);\n\t\ttcp_data_snd_check(sk);\n\t\treturn 0;\n\t}\n\n\ttcp_mstamp_refresh(tp);\n\ttp->rx_opt.saw_tstamp = 0;\n\treq = rcu_dereference_protected(tp->fastopen_rsk,\n\t\t\t\t\tlockdep_sock_is_held(sk));\n\tif (req) {\n\t\tbool req_stolen;\n\n\t\tWARN_ON_ONCE(sk->sk_state != TCP_SYN_RECV &&\n\t\t    sk->sk_state != TCP_FIN_WAIT1);\n\n\t\tSKB_DR_SET(reason, TCP_FASTOPEN);\n\t\tif (!tcp_check_req(sk, skb, req, true, &req_stolen, &reason))\n\t\t\tgoto discard;\n\t}\n\n\tif (!th->ack && !th->rst && !th->syn) {\n\t\tSKB_DR_SET(reason, TCP_FLAGS);\n\t\tgoto discard;\n\t}\n\tif (!tcp_validate_incoming(sk, skb, th, 0))\n\t\treturn 0;\n\n\t/* step 5: check the ACK field */\n\treason = tcp_ack(sk, skb, FLAG_SLOWPATH |\n\t\t\t\t  FLAG_UPDATE_TS_RECENT |\n\t\t\t\t  FLAG_NO_CHALLENGE_ACK);\n\n\tif ((int)reason <= 0) {\n\t\tif (sk->sk_state == TCP_SYN_RECV) {\n\t\t\t/* send one RST */\n\t\t\tif (!reason)\n\t\t\t\treturn SKB_DROP_REASON_TCP_OLD_ACK;\n\t\t\treturn -reason;\n\t\t}\n\t\t/* accept old ack during closing */\n\t\tif ((int)reason < 0) {\n\t\t\ttcp_send_challenge_ack(sk, false);\n\t\t\treason = -reason;\n\t\t\tgoto discard;\n\t\t}\n\t}\n\tSKB_DR_SET(reason, NOT_SPECIFIED);\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\t\ttp->delivered++; /* SYN-ACK delivery isn't tracked in tcp_ack */\n\t\tif (!tp->srtt_us)\n\t\t\ttcp_synack_rtt_meas(sk, req);\n\n\t\tif (tp->rx_opt.tstamp_ok)\n\t\t\ttp->advmss -= TCPOLEN_TSTAMP_ALIGNED;\n\n\t\tif (req) {\n\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\n\t\t} else {\n\t\t\ttcp_try_undo_spurious_syn(sk);\n\t\t\ttp->retrans_stamp = 0;\n\t\t\ttcp_init_transfer(sk, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB,\n\t\t\t\t\t  skb);\n\t\t\tWRITE_ONCE(tp->copied_seq, tp->rcv_nxt);\n\t\t}\n\t\ttcp_ao_established(sk);\n\t\tsmp_mb();\n\t\ttcp_set_state(sk, TCP_ESTABLISHED);\n\t\tsk->sk_state_change(sk);\n\n\t\t/* Note, that this wakeup is only for marginal crossed SYN case.\n\t\t * Passively open sockets are not waked up, because\n\t\t * sk->sk_sleep == NULL and sk->sk_socket == NULL.\n\t\t */\n\t\tif (sk->sk_socket)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_OUT);\n\n\t\ttp->snd_una = TCP_SKB_CB(skb)->ack_seq;\n\t\ttp->snd_wnd = ntohs(th->window) << tp->rx_opt.snd_wscale;\n\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\n\t\tif (!inet_csk(sk)->icsk_ca_ops->cong_control)\n\t\t\ttcp_update_pacing_rate(sk);\n\n\t\t/* Prevent spurious tcp_cwnd_restart() on first data packet */\n\t\ttp->lsndtime = tcp_jiffies32;\n\n\t\ttcp_initialize_rcv_mss(sk);\n\t\tif (tcp_ecn_mode_accecn(tp))\n\t\t\ttcp_accecn_third_ack(sk, skb, tp->syn_ect_snt);\n\t\ttcp_fast_path_on(tp);\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\ttcp_shutdown(sk, SEND_SHUTDOWN);\n\n\t\tbreak;\n\n\tcase TCP_FIN_WAIT1: {\n\t\tint tmo;\n\n\t\tif (req)\n\t\t\ttcp_rcv_synrecv_state_fastopen(sk);\n\n\t\tif (tp->snd_una != tp->write_seq)\n\t\t\tbreak;\n\n\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\n\t\tWRITE_ONCE(sk->sk_shutdown, sk->sk_shutdown | SEND_SHUTDOWN);\n\n\t\tsk_dst_confirm(sk);\n\n\t\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\t\t/* Wake up lingering close() */\n\t\t\tsk->sk_state_change(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (READ_ONCE(tp->linger2) < 0) {\n\t\t\ttcp_done(sk);\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\treturn SKB_DROP_REASON_TCP_ABORT_ON_DATA;\n\t\t}\n\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t/* Receive out of order FIN after close() */\n\t\t\tif (tp->syn_fastopen && th->fin)\n\t\t\t\ttcp_fastopen_active_disable(sk);\n\t\t\ttcp_done(sk);\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\treturn SKB_DROP_REASON_TCP_ABORT_ON_DATA;\n\t\t}\n\n\t\ttmo = tcp_fin_time(sk);\n\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\ttcp_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\n\t\t} else if (th->fin || sock_owned_by_user(sk)) {\n\t\t\t/* Bad case. We could lose such FIN otherwise.\n\t\t\t * It is not a big problem, but it looks confusing\n\t\t\t * and not so rare event. We still can lose it now,\n\t\t\t * if it spins in bh_lock_sock(), but it is really\n\t\t\t * marginal case.\n\t\t\t */\n\t\t\ttcp_reset_keepalive_timer(sk, tmo);\n\t\t} else {\n\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\tgoto consume;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase TCP_CLOSING:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\t\tgoto consume;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_LAST_ACK:\n\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\ttcp_update_metrics(sk);\n\t\t\ttcp_done(sk);\n\t\t\tgoto consume;\n\t\t}\n\t\tbreak;\n\t}\n\n\t/* step 6: check the URG bit */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\tcase TCP_LAST_ACK:\n\t\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt)) {\n\t\t\t/* If a subflow has been reset, the packet should not\n\t\t\t * continue to be processed, drop the packet.\n\t\t\t */\n\t\t\tif (sk_is_mptcp(sk) && !mptcp_incoming_options(sk, skb))\n\t\t\t\tgoto discard;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_FIN_WAIT2:\n\t\t/* RFC 793 says to queue data in these states,\n\t\t * RFC 1122 says we MUST send a reset.\n\t\t * BSD 4.4 also does reset.\n\t\t */\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\ttcp_reset(sk, skb);\n\t\t\t\treturn SKB_DROP_REASON_TCP_ABORT_ON_DATA;\n\t\t\t}\n\t\t}\n\t\tfallthrough;\n\tcase TCP_ESTABLISHED:\n\t\ttcp_data_queue(sk, skb);\n\t\tqueued = 1;\n\t\tbreak;\n\t}\n\n\t/* tcp_data could move socket to TIME-WAIT */\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\ttcp_data_snd_check(sk);\n\t\ttcp_ack_snd_check(sk);\n\t}\n\n\tif (!queued) {\ndiscard:\n\t\ttcp_drop_reason(sk, skb, reason);\n\t}\n\treturn 0;\n\nconsume:\n\t__kfree_skb(skb);\n\treturn 0;\n}\nEXPORT_IPV6_MOD(tcp_rcv_state_process);\n\nstatic inline void pr_drop_req(struct request_sock *req, __u16 port, int family)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\tif (family == AF_INET)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI4/%u\\n\",\n\t\t\t\t    &ireq->ir_rmt_addr, port);\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (family == AF_INET6)\n\t\tnet_dbg_ratelimited(\"drop open request from %pI6/%u\\n\",\n\t\t\t\t    &ireq->ir_v6_rmt_addr, port);\n#endif\n}\n\n/* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set\n *\n * If we receive a SYN packet with these bits set, it means a\n * network is playing bad games with TOS bits. In order to\n * avoid possible false congestion notifications, we disable\n * TCP ECN negotiation.\n *\n * Exception: tcp_ca wants ECN. This is required for DCTCP\n * congestion control: Linux DCTCP asserts ECT on all packets,\n * including SYN, which is most optimal solution; however,\n * others, such as FreeBSD do not.\n *\n * Exception: At least one of the reserved bits of the TCP header (th->res1) is\n * set, indicating the use of a future TCP extension (such as AccECN). See\n * RFC8311 \u00a74.3 which updates RFC3168 to allow the development of such\n * extensions.\n */\nstatic void tcp_ecn_create_request(struct request_sock *req,\n\t\t\t\t   const struct sk_buff *skb,\n\t\t\t\t   const struct sock *listen_sk,\n\t\t\t\t   const struct dst_entry *dst)\n{\n\tconst struct tcphdr *th = tcp_hdr(skb);\n\tconst struct net *net = sock_net(listen_sk);\n\tbool th_ecn = th->ece && th->cwr;\n\tbool ect, ecn_ok;\n\tu32 ecn_ok_dst;\n\n\tif (tcp_accecn_syn_requested(th) &&\n\t    READ_ONCE(net->ipv4.sysctl_tcp_ecn) >= 3) {\n\t\tinet_rsk(req)->ecn_ok = 1;\n\t\ttcp_rsk(req)->accecn_ok = 1;\n\t\ttcp_rsk(req)->syn_ect_rcv = TCP_SKB_CB(skb)->ip_dsfield &\n\t\t\t\t\t    INET_ECN_MASK;\n\t\treturn;\n\t}\n\n\tif (!th_ecn)\n\t\treturn;\n\n\tect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);\n\tecn_ok_dst = dst_feature(dst, DST_FEATURE_ECN_MASK);\n\tecn_ok = READ_ONCE(net->ipv4.sysctl_tcp_ecn) || ecn_ok_dst;\n\n\tif (((!ect || th->res1 || th->ae) && ecn_ok) ||\n\t    tcp_ca_needs_ecn(listen_sk) ||\n\t    (ecn_ok_dst & DST_FEATURE_ECN_CA) ||\n\t    tcp_bpf_ca_needs_ecn((struct sock *)req))\n\t\tinet_rsk(req)->ecn_ok = 1;\n}\n\nstatic void tcp_openreq_init(struct request_sock *req,\n\t\t\t     const struct tcp_options_received *rx_opt,\n\t\t\t     struct sk_buff *skb, const struct sock *sk)\n{\n\tstruct inet_request_sock *ireq = inet_rsk(req);\n\n\treq->rsk_rcv_wnd = 0;\t\t/* So that tcp_send_synack() knows! */\n\ttcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;\n\ttcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;\n\ttcp_rsk(req)->snt_synack = 0;\n\ttcp_rsk(req)->snt_tsval_first = 0;\n\ttcp_rsk(req)->last_oow_ack_time = 0;\n\ttcp_rsk(req)->accecn_ok = 0;\n\ttcp_rsk(req)->saw_accecn_opt = TCP_ACCECN_OPT_NOT_SEEN;\n\ttcp_rsk(req)->accecn_fail_mode = 0;\n\ttcp_rsk(req)->syn_ect_rcv = 0;\n\ttcp_rsk(req)->syn_ect_snt = 0;\n\treq->mss = rx_opt->mss_clamp;\n\treq->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;\n\tireq->tstamp_ok = rx_opt->tstamp_ok;\n\tireq->sack_ok = rx_opt->sack_ok;\n\tireq->snd_wscale = rx_opt->snd_wscale;\n\tireq->wscale_ok = rx_opt->wscale_ok;\n\tireq->acked = 0;\n\tireq->ecn_ok = 0;\n\tireq->ir_rmt_port = tcp_hdr(skb)->source;\n\tireq->ir_num = ntohs(tcp_hdr(skb)->dest);\n\tireq->ir_mark = inet_request_mark(sk, skb);\n#if IS_ENABLED(CONFIG_SMC)\n\tireq->smc_ok = rx_opt->smc_ok && !(tcp_sk(sk)->smc_hs_congested &&\n\t\t\ttcp_sk(sk)->smc_hs_congested(sk));\n#endif\n}\n\n/*\n * Return true if a syncookie should be sent\n */\nstatic bool tcp_syn_flood_action(struct sock *sk, const char *proto)\n{\n\tstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\n\tconst char *msg = \"Dropping request\";\n\tstruct net *net = sock_net(sk);\n\tbool want_cookie = false;\n\tu8 syncookies;\n\n\tsyncookies = READ_ONCE(net->ipv4.sysctl_tcp_syncookies);\n\n#ifdef CONFIG_SYN_COOKIES\n\tif (syncookies) {\n\t\tmsg = \"Sending cookies\";\n\t\twant_cookie = true;\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDOCOOKIES);\n\t} else\n#endif\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPREQQFULLDROP);\n\n\tif (syncookies != 2 && !READ_ONCE(queue->synflood_warned)) {\n\t\tWRITE_ONCE(queue->synflood_warned, 1);\n\t\tif (IS_ENABLED(CONFIG_IPV6) && sk->sk_family == AF_INET6) {\n\t\t\tnet_info_ratelimited(\"%s: Possible SYN flooding on port [%pI6c]:%u. %s.\\n\",\n\t\t\t\t\tproto, inet6_rcv_saddr(sk),\n\t\t\t\t\tsk->sk_num, msg);\n\t\t} else {\n\t\t\tnet_info_ratelimited(\"%s: Possible SYN flooding on port %pI4:%u. %s.\\n\",\n\t\t\t\t\tproto, &sk->sk_rcv_saddr,\n\t\t\t\t\tsk->sk_num, msg);\n\t\t}\n\t}\n\n\treturn want_cookie;\n}\n\nstatic void tcp_reqsk_record_syn(const struct sock *sk,\n\t\t\t\t struct request_sock *req,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tif (tcp_sk(sk)->save_syn) {\n\t\tu32 len = skb_network_header_len(skb) + tcp_hdrlen(skb);\n\t\tstruct saved_syn *saved_syn;\n\t\tu32 mac_hdrlen;\n\t\tvoid *base;\n\n\t\tif (tcp_sk(sk)->save_syn == 2) {  /* Save full header. */\n\t\t\tbase = skb_mac_header(skb);\n\t\t\tmac_hdrlen = skb_mac_header_len(skb);\n\t\t\tlen += mac_hdrlen;\n\t\t} else {\n\t\t\tbase = skb_network_header(skb);\n\t\t\tmac_hdrlen = 0;\n\t\t}\n\n\t\tsaved_syn = kmalloc(struct_size(saved_syn, data, len),\n\t\t\t\t    GFP_ATOMIC);\n\t\tif (saved_syn) {\n\t\t\tsaved_syn->mac_hdrlen = mac_hdrlen;\n\t\t\tsaved_syn->network_hdrlen = skb_network_header_len(skb);\n\t\t\tsaved_syn->tcp_hdrlen = tcp_hdrlen(skb);\n\t\t\tmemcpy(saved_syn->data, base, len);\n\t\t\treq->saved_syn = saved_syn;\n\t\t}\n\t}\n}\n\n/* If a SYN cookie is required and supported, returns a clamped MSS value to be\n * used for SYN cookie generation.\n */\nu16 tcp_get_syncookie_mss(struct request_sock_ops *rsk_ops,\n\t\t\t  const struct tcp_request_sock_ops *af_ops,\n\t\t\t  struct sock *sk, struct tcphdr *th)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu16 mss;\n\n\tif (READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_syncookies) != 2 &&\n\t    !inet_csk_reqsk_queue_is_full(sk))\n\t\treturn 0;\n\n\tif (!tcp_syn_flood_action(sk, rsk_ops->slab_name))\n\t\treturn 0;\n\n\tif (sk_acceptq_is_full(sk)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\n\t\treturn 0;\n\t}\n\n\tmss = tcp_parse_mss_option(th, READ_ONCE(tp->rx_opt.user_mss));\n\tif (!mss)\n\t\tmss = af_ops->mss_clamp;\n\n\treturn mss;\n}\nEXPORT_IPV6_MOD_GPL(tcp_get_syncookie_mss);\n\nint tcp_conn_request(struct request_sock_ops *rsk_ops,\n\t\t     const struct tcp_request_sock_ops *af_ops,\n\t\t     struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_fastopen_cookie foc = { .len = -1 };\n\tstruct tcp_options_received tmp_opt;\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct sock *fastopen_sk = NULL;\n\tstruct request_sock *req;\n\tbool want_cookie = false;\n\tstruct dst_entry *dst;\n\tstruct flowi fl;\n\tu8 syncookies;\n\tu32 isn;\n\n#ifdef CONFIG_TCP_AO\n\tconst struct tcp_ao_hdr *aoh;\n#endif\n\n\tisn = __this_cpu_read(tcp_tw_isn);\n\tif (isn) {\n\t\t/* TW buckets are converted to open requests without\n\t\t * limitations, they conserve resources and peer is\n\t\t * evidently real one.\n\t\t */\n\t\t__this_cpu_write(tcp_tw_isn, 0);\n\t} else {\n\t\tsyncookies = READ_ONCE(net->ipv4.sysctl_tcp_syncookies);\n\n\t\tif (syncookies == 2 || inet_csk_reqsk_queue_is_full(sk)) {\n\t\t\twant_cookie = tcp_syn_flood_action(sk,\n\t\t\t\t\t\t\t   rsk_ops->slab_name);\n\t\t\tif (!want_cookie)\n\t\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (sk_acceptq_is_full(sk)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);\n\t\tgoto drop;\n\t}\n\n\treq = inet_reqsk_alloc(rsk_ops, sk, !want_cookie);\n\tif (!req)\n\t\tgoto drop;\n\n\treq->syncookie = want_cookie;\n\ttcp_rsk(req)->af_specific = af_ops;\n\ttcp_rsk(req)->ts_off = 0;\n\ttcp_rsk(req)->req_usec_ts = false;\n#if IS_ENABLED(CONFIG_MPTCP)\n\ttcp_rsk(req)->is_mptcp = 0;\n#endif\n\n\ttcp_clear_options(&tmp_opt);\n\ttmp_opt.mss_clamp = af_ops->mss_clamp;\n\ttmp_opt.user_mss  = READ_ONCE(tp->rx_opt.user_mss);\n\ttcp_parse_options(sock_net(sk), skb, &tmp_opt, 0,\n\t\t\t  want_cookie ? NULL : &foc);\n\n\tif (want_cookie && !tmp_opt.saw_tstamp)\n\t\ttcp_clear_options(&tmp_opt);\n\n\tif (IS_ENABLED(CONFIG_SMC) && want_cookie)\n\t\ttmp_opt.smc_ok = 0;\n\n\ttmp_opt.tstamp_ok = tmp_opt.saw_tstamp;\n\ttcp_openreq_init(req, &tmp_opt, skb, sk);\n\tinet_rsk(req)->no_srccheck = inet_test_bit(TRANSPARENT, sk);\n\n\t/* Note: tcp_v6_init_req() might override ir_iif for link locals */\n\tinet_rsk(req)->ir_iif = inet_request_bound_dev_if(sk, skb);\n\n\tdst = af_ops->route_req(sk, skb, &fl, req, isn);\n\tif (!dst)\n\t\tgoto drop_and_free;\n\n\tif (tmp_opt.tstamp_ok) {\n\t\ttcp_rsk(req)->req_usec_ts = dst_tcp_usec_ts(dst);\n\t\ttcp_rsk(req)->ts_off = af_ops->init_ts_off(net, skb);\n\t}\n\tif (!want_cookie && !isn) {\n\t\tint max_syn_backlog = READ_ONCE(net->ipv4.sysctl_max_syn_backlog);\n\n\t\t/* Kill the following clause, if you dislike this way. */\n\t\tif (!syncookies &&\n\t\t    (max_syn_backlog - inet_csk_reqsk_queue_len(sk) <\n\t\t     (max_syn_backlog >> 2)) &&\n\t\t    !tcp_peer_is_proven(req, dst)) {\n\t\t\t/* Without syncookies last quarter of\n\t\t\t * backlog is filled with destinations,\n\t\t\t * proven to be alive.\n\t\t\t * It means that we continue to communicate\n\t\t\t * to destinations, already remembered\n\t\t\t * to the moment of synflood.\n\t\t\t */\n\t\t\tpr_drop_req(req, ntohs(tcp_hdr(skb)->source),\n\t\t\t\t    rsk_ops->family);\n\t\t\tgoto drop_and_release;\n\t\t}\n\n\t\tisn = af_ops->init_seq(skb);\n\t}\n\n\ttcp_ecn_create_request(req, skb, sk, dst);\n\n\tif (want_cookie) {\n\t\tisn = cookie_init_sequence(af_ops, sk, skb, &req->mss);\n\t\tif (!tmp_opt.tstamp_ok)\n\t\t\tinet_rsk(req)->ecn_ok = 0;\n\t}\n\n#ifdef CONFIG_TCP_AO\n\tif (tcp_parse_auth_options(tcp_hdr(skb), NULL, &aoh))\n\t\tgoto drop_and_release; /* Invalid TCP options */\n\tif (aoh) {\n\t\ttcp_rsk(req)->used_tcp_ao = true;\n\t\ttcp_rsk(req)->ao_rcv_next = aoh->keyid;\n\t\ttcp_rsk(req)->ao_keyid = aoh->rnext_keyid;\n\n\t} else {\n\t\ttcp_rsk(req)->used_tcp_ao = false;\n\t}\n#endif\n\ttcp_rsk(req)->snt_isn = isn;\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\ttcp_rsk(req)->syn_tos = TCP_SKB_CB(skb)->ip_dsfield;\n\ttcp_openreq_init_rwin(req, sk, dst);\n\tsk_rx_queue_set(req_to_sk(req), skb);\n\tif (!want_cookie) {\n\t\ttcp_reqsk_record_syn(sk, req, skb);\n\t\tfastopen_sk = tcp_try_fastopen(sk, skb, req, &foc, dst);\n\t}\n\tif (fastopen_sk) {\n\t\taf_ops->send_synack(fastopen_sk, dst, &fl, req,\n\t\t\t\t    &foc, TCP_SYNACK_FASTOPEN, skb);\n\t\t/* Add the child socket directly into the accept queue */\n\t\tif (!inet_csk_reqsk_queue_add(sk, req, fastopen_sk)) {\n\t\t\treqsk_fastopen_remove(fastopen_sk, req, false);\n\t\t\tbh_unlock_sock(fastopen_sk);\n\t\t\tsock_put(fastopen_sk);\n\t\t\tgoto drop_and_free;\n\t\t}\n\t\tsk->sk_data_ready(sk);\n\t\tbh_unlock_sock(fastopen_sk);\n\t\tsock_put(fastopen_sk);\n\t} else {\n\t\ttcp_rsk(req)->tfo_listener = false;\n\t\tif (!want_cookie) {\n\t\t\treq->timeout = tcp_timeout_init((struct sock *)req);\n\t\t\tif (unlikely(!inet_csk_reqsk_queue_hash_add(sk, req,\n\t\t\t\t\t\t\t\t    req->timeout))) {\n\t\t\t\treqsk_free(req);\n\t\t\t\tdst_release(dst);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t}\n\t\taf_ops->send_synack(sk, dst, &fl, req, &foc,\n\t\t\t\t    !want_cookie ? TCP_SYNACK_NORMAL :\n\t\t\t\t\t\t   TCP_SYNACK_COOKIE,\n\t\t\t\t    skb);\n\t\tif (want_cookie) {\n\t\t\treqsk_free(req);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treqsk_put(req);\n\treturn 0;\n\ndrop_and_release:\n\tdst_release(dst);\ndrop_and_free:\n\t__reqsk_free(req);\ndrop:\n\ttcp_listendrop(sk);\n\treturn 0;\n}\nEXPORT_IPV6_MOD(tcp_conn_request);\n", "patch": "@@ -87,7 +87,7 @@ int sysctl_tcp_adv_win_scale __read_mostly = 1;\n EXPORT_SYMBOL(sysctl_tcp_adv_win_scale);\n \n /* rfc5961 challenge ack rate limiting */\n-int sysctl_tcp_challenge_ack_limit = 100;\n+int sysctl_tcp_challenge_ack_limit = 1000;\n \n int sysctl_tcp_stdurg __read_mostly;\n int sysctl_tcp_rfc1337 __read_mostly;\n@@ -3458,21 +3458,26 @@ static void tcp_send_challenge_ack(struct sock *sk, const struct sk_buff *skb)\n \tstatic u32 challenge_timestamp;\n \tstatic unsigned int challenge_count;\n \tstruct tcp_sock *tp = tcp_sk(sk);\n-\tu32 now;\n+\tu32 count, now;\n \n \t/* First check our per-socket dupack rate limit. */\n \tif (tcp_oow_rate_limited(sock_net(sk), skb,\n \t\t\t\t LINUX_MIB_TCPACKSKIPPEDCHALLENGE,\n \t\t\t\t &tp->last_oow_ack_time))\n \t\treturn;\n \n-\t/* Then check the check host-wide RFC 5961 rate limit. */\n+\t/* Then check host-wide RFC 5961 rate limit. */\n \tnow = jiffies / HZ;\n \tif (now != challenge_timestamp) {\n+\t\tu32 half = (sysctl_tcp_challenge_ack_limit + 1) >> 1;\n+\n \t\tchallenge_timestamp = now;\n-\t\tchallenge_count = 0;\n+\t\tWRITE_ONCE(challenge_count, half +\n+\t\t\t   prandom_u32_max(sysctl_tcp_challenge_ack_limit));\n \t}\n-\tif (++challenge_count <= sysctl_tcp_challenge_ack_limit) {\n+\tcount = READ_ONCE(challenge_count);\n+\tif (count > 0) {\n+\t\tWRITE_ONCE(challenge_count, count - 1);\n \t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPCHALLENGEACK);\n \t\ttcp_send_ack(sk);\n \t}", "file_path": "files/2016_8\\102", "file_language": "c", "file_name": "net/ipv4/tcp_input.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 41, "cve_id": "CVE-2016-5412", "cwe_id": ["CWE-399"], "cve_language": "C", "cve_description": "arch/powerpc/kvm/book3s_hv_rmhandlers.S in the Linux kernel through 4.7 on PowerPC platforms, when CONFIG_KVM_BOOK3S_64_HV is enabled, allows guest OS users to cause a denial of service (host OS infinite loop) by making a H_CEDE hypercall during the existence of a suspended transaction.", "cvss": "6.5", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "CHANGED", "C": "NONE", "I": "NONE", "A": "HIGH", "commit_id": "f024ee098476a3e620232e4a78cfac505f121245", "commit_message": "KVM: PPC: Book3S HV: Pull out TM state save/restore into separate procedures\n\nThis moves the transactional memory state save and restore sequences\nout of the guest entry/exit paths into separate procedures.  This is\nso that these sequences can be used in going into and out of nap\nin a subsequent patch.\n\nThe only code changes here are (a) saving and restore LR on the\nstack, since these new procedures get called with a bl instruction,\n(b) explicitly saving r1 into the PACA instead of assuming that\nHSTATE_HOST_R1(r13) is already set, and (c) removing an unnecessary\nand redundant setting of MSR[TM] that should have been removed by\ncommit 9d4d0bdd9e0a (\"KVM: PPC: Book3S HV: Add transactional memory\nsupport\", 2013-09-24) but wasn't.\n\nCc: stable@vger.kernel.org # v3.15+\nSigned-off-by: Paul Mackerras <paulus@ozlabs.org>", "commit_date": "2016-07-28T06:09:34Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/f024ee098476a3e620232e4a78cfac505f121245", "html_url": "https://github.com/torvalds/linux/commit/f024ee098476a3e620232e4a78cfac505f121245", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "912902ce78b0d48f717f9128e61fb9bffbd65f86", "url_before": "https://api.github.com/repos/torvalds/linux/commits/912902ce78b0d48f717f9128e61fb9bffbd65f86", "html_url_before": "https://github.com/torvalds/linux/commit/912902ce78b0d48f717f9128e61fb9bffbd65f86"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/f024ee098476a3e620232e4a78cfac505f121245/arch/powerpc/kvm/book3s_hv_rmhandlers.S", "code": "/*\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * Copyright 2011 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * Derived from book3s_rmhandlers.S and other files, which are:\n *\n * Copyright SUSE Linux Products GmbH 2009\n *\n * Authors: Alexander Graf <agraf@suse.de>\n */\n\n#include <asm/ppc_asm.h>\n#include <asm/kvm_asm.h>\n#include <asm/reg.h>\n#include <asm/mmu.h>\n#include <asm/page.h>\n#include <asm/ptrace.h>\n#include <asm/hvcall.h>\n#include <asm/asm-offsets.h>\n#include <asm/exception-64s.h>\n#include <asm/kvm_book3s_asm.h>\n#include <asm/book3s/64/mmu-hash.h>\n#include <asm/tm.h>\n#include <asm/opal.h>\n\n#define VCPU_GPRS_TM(reg) (((reg) * ULONG_SIZE) + VCPU_GPR_TM)\n\n/* Values in HSTATE_NAPPING(r13) */\n#define NAPPING_CEDE\t1\n#define NAPPING_NOVCPU\t2\n\n/*\n * Call kvmppc_hv_entry in real mode.\n * Must be called with interrupts hard-disabled.\n *\n * Input Registers:\n *\n * LR = return address to continue at after eventually re-enabling MMU\n */\n_GLOBAL_TOC(kvmppc_hv_entry_trampoline)\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -112(r1)\n\tmfmsr\tr10\n\tLOAD_REG_ADDR(r5, kvmppc_call_hv_entry)\n\tli\tr0,MSR_RI\n\tandc\tr0,r10,r0\n\tli\tr6,MSR_IR | MSR_DR\n\tandc\tr6,r10,r6\n\tmtmsrd\tr0,1\t\t/* clear RI in MSR */\n\tmtsrr0\tr5\n\tmtsrr1\tr6\n\tRFI\n\nkvmppc_call_hv_entry:\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from guest - restore host state and return to caller */\n\nBEGIN_FTR_SECTION\n\t/* Restore host DABR and DABRX */\n\tld\tr5,HSTATE_DABR(r13)\n\tli\tr6,7\n\tmtspr\tSPRN_DABR,r5\n\tmtspr\tSPRN_DABRX,r6\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n\t/* Restore SPRG3 */\n\tld\tr3,PACA_SPRG_VDSO(r13)\n\tmtspr\tSPRN_SPRG_VDSO_WRITE,r3\n\n\t/* Reload the host's PMU registers */\n\tld\tr3, PACALPPACAPTR(r13)\t/* is the host using the PMU? */\n\tlbz\tr4, LPPACA_PMCINUSE(r3)\n\tcmpwi\tr4, 0\n\tbeq\t23f\t\t\t/* skip if not */\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_MMCR0(r13)\n\tandi.\tr4, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr4, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, HSTATE_PMC1(r13)\n\tlwz\tr4, HSTATE_PMC2(r13)\n\tlwz\tr5, HSTATE_PMC3(r13)\n\tlwz\tr6, HSTATE_PMC4(r13)\n\tlwz\tr8, HSTATE_PMC5(r13)\n\tlwz\tr9, HSTATE_PMC6(r13)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r4\n\tmtspr\tSPRN_PMC3, r5\n\tmtspr\tSPRN_PMC4, r6\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, HSTATE_MMCR0(r13)\n\tld\tr4, HSTATE_MMCR1(r13)\n\tld\tr5, HSTATE_MMCRA(r13)\n\tld\tr6, HSTATE_SIAR(r13)\n\tld\tr7, HSTATE_SDAR(r13)\n\tmtspr\tSPRN_MMCR1, r4\n\tmtspr\tSPRN_MMCRA, r5\n\tmtspr\tSPRN_SIAR, r6\n\tmtspr\tSPRN_SDAR, r7\nBEGIN_FTR_SECTION\n\tld\tr8, HSTATE_MMCR2(r13)\n\tld\tr9, HSTATE_SIER(r13)\n\tmtspr\tSPRN_MMCR2, r8\n\tmtspr\tSPRN_SIER, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n23:\n\n\t/*\n\t * Reload DEC.  HDEC interrupts were disabled when\n\t * we reloaded the host's LPCR value.\n\t */\n\tld\tr3, HSTATE_DECEXP(r13)\n\tmftb\tr4\n\tsubf\tr4, r4, r3\n\tmtspr\tSPRN_DEC, r4\n\n\t/* hwthread_req may have got set by cede or no vcpu, so clear it */\n\tli\tr0, 0\n\tstb\tr0, HSTATE_HWTHREAD_REQ(r13)\n\n\t/*\n\t * For external and machine check interrupts, we need\n\t * to call the Linux handler to process the interrupt.\n\t * We do that by jumping to absolute address 0x500 for\n\t * external interrupts, or the machine_check_fwnmi label\n\t * for machine checks (since firmware might have patched\n\t * the vector area at 0x200).  The [h]rfid at the end of the\n\t * handler will return to the book3s_hv_interrupts.S code.\n\t * For other interrupts we do the rfid to get back\n\t * to the book3s_hv_interrupts.S code here.\n\t */\n\tld\tr8, 112+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, 112\n\tld\tr7, HSTATE_HOST_MSR(r13)\n\n\tcmpwi\tcr1, r12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tcmpwi\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbeq\t11f\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\tbeq \t15f\t/* Invoke the H_DOORBELL handler */\n\tcmpwi\tcr2, r12, BOOK3S_INTERRUPT_HMI\n\tbeq\tcr2, 14f\t\t\t/* HMI check */\n\n\t/* RFI into the highmem handler, or branch to interrupt handler */\n\tmfmsr\tr6\n\tli\tr0, MSR_RI\n\tandc\tr6, r6, r0\n\tmtmsrd\tr6, 1\t\t\t/* Clear RI in MSR */\n\tmtsrr0\tr8\n\tmtsrr1\tr7\n\tbeq\tcr1, 13f\t\t/* machine check */\n\tRFI\n\n\t/* On POWER7, we have external interrupts set to use HSRR0/1 */\n11:\tmtspr\tSPRN_HSRR0, r8\n\tmtspr\tSPRN_HSRR1, r7\n\tba\t0x500\n\n13:\tb\tmachine_check_fwnmi\n\n14:\tmtspr\tSPRN_HSRR0, r8\n\tmtspr\tSPRN_HSRR1, r7\n\tb\thmi_exception_after_realmode\n\n15:\tmtspr SPRN_HSRR0, r8\n\tmtspr SPRN_HSRR1, r7\n\tba    0xe80\n\nkvmppc_primary_no_guest:\n\t/* We handle this much like a ceded vcpu */\n\t/* put the HDEC into the DEC, since HDEC interrupts don't wake us */\n\tmfspr\tr3, SPRN_HDEC\n\tmtspr\tSPRN_DEC, r3\n\t/*\n\t * Make sure the primary has finished the MMU switch.\n\t * We should never get here on a secondary thread, but\n\t * check it for robustness' sake.\n\t */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n65:\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t65b\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\t/* set our bit in napping_threads */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n1:\tlwarx\tr3, 0, r6\n\tor\tr3, r3, r0\n\tstwcx.\tr3, 0, r6\n\tbne\t1b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr12, 0\n\tlwz\tr7, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7, 0x100\n\tbge\tkvm_novcpu_exit\t/* another thread already exiting */\n\tli\tr3, NAPPING_NOVCPU\n\tstb\tr3, HSTATE_NAPPING(r13)\n\n\tli\tr3, 0\t\t/* Don't wake on privileged (OS) doorbell */\n\tb\tkvm_do_nap\n\nkvm_novcpu_wakeup:\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tli\tr0, 0\n\tstb\tr0, HSTATE_NAPPING(r13)\n\n\t/* check the wake reason */\n\tbl\tkvmppc_check_wake_reason\n\n\t/* see if any other thread is already exiting */\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tbge\tkvm_novcpu_exit\n\n\t/* clear our bit in napping_threads */\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n4:\tlwarx\tr7, 0, r6\n\tandc\tr7, r7, r0\n\tstwcx.\tr7, 0, r6\n\tbne\t4b\n\n\t/* See if the wake reason means we need to exit */\n\tcmpdi\tr3, 0\n\tbge\tkvm_novcpu_exit\n\n\t/* See if our timeslice has expired (HDEC is negative) */\n\tmfspr\tr0, SPRN_HDEC\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n\tcmpwi\tr0, 0\n\tblt\tkvm_novcpu_exit\n\n\t/* Got an IPI but other vcpus aren't yet exiting, must be a latecomer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n#endif\n\tb\tkvmppc_got_guest\n\nkvm_novcpu_exit:\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\t13f\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n13:\tmr\tr3, r12\n\tstw\tr12, 112-4(r1)\n\tbl\tkvmhv_commence_exit\n\tnop\n\tlwz\tr12, 112-4(r1)\n\tb\tkvmhv_switch_to_host\n\n/*\n * We come in here when wakened from nap mode.\n * Relocation is off and most register values are lost.\n * r13 points to the PACA.\n */\n\t.globl\tkvm_start_guest\nkvm_start_guest:\n\n\t/* Set runlatch bit the minute you wake up from nap */\n\tmfspr\tr0, SPRN_CTRLF\n\tori \tr0, r0, 1\n\tmtspr\tSPRN_CTRLT, r0\n\n\tld\tr2,PACATOC(r13)\n\n\tli\tr0,KVM_HWTHREAD_IN_KVM\n\tstb\tr0,HSTATE_HWTHREAD_STATE(r13)\n\n\t/* NV GPR values from power7_idle() will no longer be valid */\n\tli\tr0,1\n\tstb\tr0,PACA_NAPSTATELOST(r13)\n\n\t/* were we napping due to cede? */\n\tlbz\tr0,HSTATE_NAPPING(r13)\n\tcmpwi\tr0,NAPPING_CEDE\n\tbeq\tkvm_end_cede\n\tcmpwi\tr0,NAPPING_NOVCPU\n\tbeq\tkvm_novcpu_wakeup\n\n\tld\tr1,PACAEMERGSP(r13)\n\tsubi\tr1,r1,STACK_FRAME_OVERHEAD\n\n\t/*\n\t * We weren't napping due to cede, so this must be a secondary\n\t * thread being woken up to run a guest, or being woken up due\n\t * to a stray IPI.  (Or due to some machine check or hypervisor\n\t * maintenance interrupt while the core is in KVM.)\n\t */\n\n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\tcmpdi\tr3, 0\n\tbge\tkvm_no_guest\n\n\t/* get vcore pointer, NULL if we have nothing to run */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5,0\n\t/* if we have no vcore to run, go back to sleep */\n\tbeq\tkvm_no_guest\n\nkvm_secondary_got_guest:\n\n\t/* Set HSTATE_DSCR(r13) to something sensible */\n\tld\tr6, PACA_DSCR_DEFAULT(r13)\n\tstd\tr6, HSTATE_DSCR(r13)\n\n\t/* On thread 0 of a subcore, set HDEC to max */\n\tlbz\tr4, HSTATE_PTID(r13)\n\tcmpwi\tr4, 0\n\tbne\t63f\n\tlis\tr6, 0x7fff\n\tori\tr6, r6, 0xffff\n\tmtspr\tSPRN_HDEC, r6\n\t/* and set per-LPAR registers, if doing dynamic micro-threading */\n\tld\tr6, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr6, 0\n\tbeq\t63f\n\tld\tr0, KVM_SPLIT_RPR(r6)\n\tmtspr\tSPRN_RPR, r0\n\tld\tr0, KVM_SPLIT_PMMAR(r6)\n\tmtspr\tSPRN_PMMAR, r0\n\tld\tr0, KVM_SPLIT_LDBAR(r6)\n\tmtspr\tSPRN_LDBAR, r0\n\tisync\n63:\n\t/* Order load of vcpu after load of vcore */\n\tlwsync\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from the guest, go back to nap */\n\t/* Clear our vcpu and vcore pointers so we don't come back in early */\n\tli\tr0, 0\n\tstd\tr0, HSTATE_KVM_VCPU(r13)\n\t/*\n\t * Once we clear HSTATE_KVM_VCORE(r13), the code in\n\t * kvmppc_run_core() is going to assume that all our vcpu\n\t * state is visible in memory.  This lwsync makes sure\n\t * that that is true.\n\t */\n\tlwsync\n\tstd\tr0, HSTATE_KVM_VCORE(r13)\n\n\t/*\n\t * All secondaries exiting guest will fall through this path.\n\t * Before proceeding, just check for HMI interrupt and\n\t * invoke opal hmi handler. By now we are sure that the\n\t * primary thread on this core/subcore has already made partition\n\t * switch/TB resync and we are good to call opal hmi handler.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\tkvm_no_guest\n\n\tli\tr3,0\t\t\t/* NULL argument */\n\tbl\thmi_exception_realmode\n/*\n * At this point we have finished executing in the guest.\n * We need to wait for hwthread_req to become zero, since\n * we may not turn on the MMU while hwthread_req is non-zero.\n * While waiting we also need to check if we get given a vcpu to run.\n */\nkvm_no_guest:\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t53f\n\tHMT_MEDIUM\n\tli\tr0, KVM_HWTHREAD_IN_KERNEL\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\t/* need to recheck hwthread_req after a barrier, to avoid race */\n\tsync\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t54f\n/*\n * We jump to power7_wakeup_loss, which will return to the caller\n * of power7_nap in the powernv cpu offline loop.  The value we\n * put in r3 becomes the return value for power7_nap.\n */\n\tli\tr3, LPCR_PECE0\n\tmfspr\tr4, SPRN_LPCR\n\trlwimi\tr4, r3, 0, LPCR_PECE0 | LPCR_PECE1\n\tmtspr\tSPRN_LPCR, r4\n\tli\tr3, 0\n\tb\tpower7_wakeup_loss\n\n53:\tHMT_LOW\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5, 0\n\tbne\t60f\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr3, 0\n\tbeq\tkvm_no_guest\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\tkvm_no_guest\n\tHMT_MEDIUM\n\tb\tkvm_unsplit_nap\n60:\tHMT_MEDIUM\n\tb\tkvm_secondary_got_guest\n\n54:\tli\tr0, KVM_HWTHREAD_IN_KVM\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\tb\tkvm_no_guest\n\n/*\n * Here the primary thread is trying to return the core to\n * whole-core mode, so we need to nap.\n */\nkvm_unsplit_nap:\n\t/*\n\t * When secondaries are napping in kvm_unsplit_nap() with\n\t * hwthread_req = 1, HMI goes ignored even though subcores are\n\t * already exited the guest. Hence HMI keeps waking up secondaries\n\t * from nap in a loop and secondaries always go back to nap since\n\t * no vcore is assigned to them. This makes impossible for primary\n\t * thread to get hold of secondary threads resulting into a soft\n\t * lockup in KVM path.\n\t *\n\t * Let us check if HMI is pending and handle it before we go to nap.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\t55f\n\tli\tr3, 0\t\t\t/* NULL argument */\n\tbl\thmi_exception_realmode\n55:\n\t/*\n\t * Ensure that secondary doesn't nap when it has\n\t * its vcore pointer set.\n\t */\n\tsync\t\t/* matches smp_mb() before setting split_info.do_nap */\n\tld\tr0, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr0, 0\n\tbne\tkvm_no_guest\n\t/* clear any pending message */\nBEGIN_FTR_SECTION\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\t/* Set kvm_split_mode.napped[tid] = 1 */\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tli\tr0, 1\n\tlhz\tr4, PACAPACAINDEX(r13)\n\tclrldi\tr4, r4, 61\t/* micro-threading => P8 => 8 threads/core */\n\taddi\tr4, r4, KVM_SPLIT_NAPPED\n\tstbx\tr0, r3, r4\n\t/* Check the do_nap flag again after setting napped[] */\n\tsync\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\t57f\n\tli\tr3, (LPCR_PECEDH | LPCR_PECE0) >> 4\n\tmfspr\tr4, SPRN_LPCR\n\trlwimi\tr4, r3, 4, (LPCR_PECEDP | LPCR_PECEDH | LPCR_PECE0 | LPCR_PECE1)\n\tmtspr\tSPRN_LPCR, r4\n\tisync\n\tstd\tr0, HSTATE_SCRATCH0(r13)\n\tptesync\n\tld\tr0, HSTATE_SCRATCH0(r13)\n1:\tcmpd\tr0, r0\n\tbne\t1b\n\tnap\n\tb\t.\n\n57:\tli\tr0, 0\n\tstbx\tr0, r3, r4\n\tb\tkvm_no_guest\n\n/******************************************************************************\n *                                                                            *\n *                               Entry code                                   *\n *                                                                            *\n *****************************************************************************/\n\n.global kvmppc_hv_entry\nkvmppc_hv_entry:\n\n\t/* Required state:\n\t *\n\t * R4 = vcpu pointer (or NULL)\n\t * MSR = ~IR|DR\n\t * R13 = PACA\n\t * R1 = host R1\n\t * R2 = TOC\n\t * all other volatile GPRS = free\n\t */\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -112(r1)\n\n\t/* Save R1 in the PACA */\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\tli\tr6, KVM_GUEST_MODE_HOST_HV\n\tstb\tr6, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\t/* Store initial timestamp */\n\tcmpdi\tr4, 0\n\tbeq\t1f\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n1:\n#endif\n\t/* Clear out SLB */\n\tli\tr6,0\n\tslbmte\tr6,r6\n\tslbia\n\tptesync\n\n\t/*\n\t * POWER7/POWER8 host -> guest partition switch code.\n\t * We don't have to lock against concurrent tlbies,\n\t * but we do have to coordinate across hardware threads.\n\t */\n\t/* Set bit in entry map iff exit map is zero. */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tli\tr7, 1\n\tlbz\tr6, HSTATE_PTID(r13)\n\tsld\tr7, r7, r6\n\taddi\tr9, r5, VCORE_ENTRY_EXIT\n21:\tlwarx\tr3, 0, r9\n\tcmpwi\tr3, 0x100\t\t/* any threads starting to exit? */\n\tbge\tsecondary_too_late\t/* if so we're too late to the party */\n\tor\tr3, r3, r7\n\tstwcx.\tr3, 0, r9\n\tbne\t21b\n\n\t/* Primary thread switches to guest partition. */\n\tld\tr9,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\tcmpwi\tr6,0\n\tbne\t10f\n\tld\tr6,KVM_SDR1(r9)\n\tlwz\tr7,KVM_LPID(r9)\n\tli\tr0,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r0\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to partition page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\n\t/* See if we need to flush the TLB */\n\tlhz\tr6,PACAPACAINDEX(r13)\t/* test_bit(cpu, need_tlb_flush) */\n\tclrldi\tr7,r6,64-6\t\t/* extract bit number (6 bits) */\n\tsrdi\tr6,r6,6\t\t\t/* doubleword number */\n\tsldi\tr6,r6,3\t\t\t/* address offset */\n\tadd\tr6,r6,r9\n\taddi\tr6,r6,KVM_NEED_FLUSH\t/* dword in kvm->arch.need_tlb_flush */\n\tli\tr0,1\n\tsld\tr0,r0,r7\n\tld\tr7,0(r6)\n\tand.\tr7,r7,r0\n\tbeq\t22f\n23:\tldarx\tr7,0,r6\t\t\t/* if set, clear the bit */\n\tandc\tr7,r7,r0\n\tstdcx.\tr7,0,r6\n\tbne\t23b\n\t/* Flush the TLB of any entries for this LPID */\n\t/* use arch 2.07S as a proxy for POWER8 */\nBEGIN_FTR_SECTION\n\tli\tr6,512\t\t\t/* POWER8 has 512 sets */\nFTR_SECTION_ELSE\n\tli\tr6,128\t\t\t/* POWER7 has 128 sets */\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)\n\tmtctr\tr6\n\tli\tr7,0x800\t\t/* IS field = 0b10 */\n\tptesync\n28:\ttlbiel\tr7\n\taddi\tr7,r7,0x1000\n\tbdnz\t28b\n\tptesync\n\n\t/* Add timebase offset onto timebase */\n22:\tld\tr8,VCORE_TB_OFFSET(r5)\n\tcmpdi\tr8,0\n\tbeq\t37f\n\tmftb\tr6\t\t/* current host timebase */\n\tadd\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t/* update upper 40 bits */\n\tmftb\tr7\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t37f\n\taddis\tr8,r8,0x100\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n\t/* Load guest PCR value to select appropriate compat mode */\n37:\tld\tr7, VCORE_PCR(r5)\n\tcmpdi\tr7, 0\n\tbeq\t38f\n\tmtspr\tSPRN_PCR, r7\n38:\n\nBEGIN_FTR_SECTION\n\t/* DPDES is shared between threads */\n\tld\tr8, VCORE_DPDES(r5)\n\tmtspr\tSPRN_DPDES, r8\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* Mark the subcore state as inside guest */\n\tbl\tkvmppc_subcore_enter_guest\n\tnop\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tli\tr0,1\n\tstb\tr0,VCORE_IN_GUEST(r5)\t/* signal secondaries to continue */\n\n\t/* Do we have a guest vcpu to run? */\n10:\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\nkvmppc_got_guest:\n\n\t/* Load up guest SLB entries */\n\tlwz\tr5,VCPU_SLB_MAX(r4)\n\tcmpwi\tr5,0\n\tbeq\t9f\n\tmtctr\tr5\n\taddi\tr6,r4,VCPU_SLB\n1:\tld\tr8,VCPU_SLB_E(r6)\n\tld\tr9,VCPU_SLB_V(r6)\n\tslbmte\tr9,r8\n\taddi\tr6,r6,VCPU_SLB_SIZE\n\tbdnz\t1b\n9:\n\t/* Increment yield count if they have a VPA */\n\tld\tr3, VCPU_VPA(r4)\n\tcmpdi\tr3, 0\n\tbeq\t25f\n\tli\tr6, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr5, r3, r6\n\taddi\tr5, r5, 1\n\tSTWX_BE\tr5, r3, r6\n\tli\tr6, 1\n\tstb\tr6, VCPU_VPA_DIRTY(r4)\n25:\n\n\t/* Save purr/spurr */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tstd\tr5,HSTATE_PURR(r13)\n\tstd\tr6,HSTATE_SPURR(r13)\n\tld\tr7,VCPU_PURR(r4)\n\tld\tr8,VCPU_SPURR(r4)\n\tmtspr\tSPRN_PURR,r7\n\tmtspr\tSPRN_SPURR,r8\n\nBEGIN_FTR_SECTION\n\t/* Set partition DABR */\n\t/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */\n\tlwz\tr5,VCPU_DABRX(r4)\n\tld\tr6,VCPU_DABR(r4)\n\tmtspr\tSPRN_DABRX,r5\n\tmtspr\tSPRN_DABR,r6\n\tisync\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tbl\tkvmppc_restore_tm\nEND_FTR_SECTION_IFSET(CPU_FTR_TM)\n#endif\n\n\t/* Load guest PMU registers */\n\t/* R4 is live here (vcpu pointer) */\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tisync\nBEGIN_FTR_SECTION\n\tld\tr3, VCPU_MMCR(r4)\n\tandi.\tr5, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr5, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, VCPU_PMC(r4)\t/* always load up guest PMU registers */\n\tlwz\tr5, VCPU_PMC + 4(r4)\t/* to prevent information leak */\n\tlwz\tr6, VCPU_PMC + 8(r4)\n\tlwz\tr7, VCPU_PMC + 12(r4)\n\tlwz\tr8, VCPU_PMC + 16(r4)\n\tlwz\tr9, VCPU_PMC + 20(r4)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r5\n\tmtspr\tSPRN_PMC3, r6\n\tmtspr\tSPRN_PMC4, r7\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, VCPU_MMCR(r4)\n\tld\tr5, VCPU_MMCR + 8(r4)\n\tld\tr6, VCPU_MMCR + 16(r4)\n\tld\tr7, VCPU_SIAR(r4)\n\tld\tr8, VCPU_SDAR(r4)\n\tmtspr\tSPRN_MMCR1, r5\n\tmtspr\tSPRN_MMCRA, r6\n\tmtspr\tSPRN_SIAR, r7\n\tmtspr\tSPRN_SDAR, r8\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_MMCR + 24(r4)\n\tld\tr6, VCPU_SIER(r4)\n\tlwz\tr7, VCPU_PMC + 24(r4)\n\tlwz\tr8, VCPU_PMC + 28(r4)\n\tld\tr9, VCPU_MMCR + 32(r4)\n\tmtspr\tSPRN_MMCR2, r5\n\tmtspr\tSPRN_SIER, r6\n\tmtspr\tSPRN_SPMC1, r7\n\tmtspr\tSPRN_SPMC2, r8\n\tmtspr\tSPRN_MMCRS, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n\n\t/* Load up FP, VMX and VSX registers */\n\tbl\tkvmppc_load_fp\n\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n\n\t/* Switch DSCR to guest value */\n\tld\tr5, VCPU_DSCR(r4)\n\tmtspr\tSPRN_DSCR, r5\n\nBEGIN_FTR_SECTION\n\t/* Skip next section on POWER7 */\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Load up POWER8-specific registers */\n\tld\tr5, VCPU_IAMR(r4)\n\tlwz\tr6, VCPU_PSPB(r4)\n\tld\tr7, VCPU_FSCR(r4)\n\tmtspr\tSPRN_IAMR, r5\n\tmtspr\tSPRN_PSPB, r6\n\tmtspr\tSPRN_FSCR, r7\n\tld\tr5, VCPU_DAWR(r4)\n\tld\tr6, VCPU_DAWRX(r4)\n\tld\tr7, VCPU_CIABR(r4)\n\tld\tr8, VCPU_TAR(r4)\n\tmtspr\tSPRN_DAWR, r5\n\tmtspr\tSPRN_DAWRX, r6\n\tmtspr\tSPRN_CIABR, r7\n\tmtspr\tSPRN_TAR, r8\n\tld\tr5, VCPU_IC(r4)\n\tld\tr6, VCPU_VTB(r4)\n\tmtspr\tSPRN_IC, r5\n\tmtspr\tSPRN_VTB, r6\n\tld\tr8, VCPU_EBBHR(r4)\n\tmtspr\tSPRN_EBBHR, r8\n\tld\tr5, VCPU_EBBRR(r4)\n\tld\tr6, VCPU_BESCR(r4)\n\tld\tr7, VCPU_CSIGR(r4)\n\tld\tr8, VCPU_TACR(r4)\n\tmtspr\tSPRN_EBBRR, r5\n\tmtspr\tSPRN_BESCR, r6\n\tmtspr\tSPRN_CSIGR, r7\n\tmtspr\tSPRN_TACR, r8\n\tld\tr5, VCPU_TCSCR(r4)\n\tld\tr6, VCPU_ACOP(r4)\n\tlwz\tr7, VCPU_GUEST_PID(r4)\n\tld\tr8, VCPU_WORT(r4)\n\tmtspr\tSPRN_TCSCR, r5\n\tmtspr\tSPRN_ACOP, r6\n\tmtspr\tSPRN_PID, r7\n\tmtspr\tSPRN_WORT, r8\n8:\n\n\t/*\n\t * Set the decrementer to the guest decrementer.\n\t */\n\tld\tr8,VCPU_DEC_EXPIRES(r4)\n\t/* r8 is a host timebase value here, convert to guest TB */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr6,VCORE_TB_OFFSET(r5)\n\tadd\tr8,r8,r6\n\tmftb\tr7\n\tsubf\tr3,r7,r8\n\tmtspr\tSPRN_DEC,r3\n\tstw\tr3,VCPU_DEC(r4)\n\n\tld\tr5, VCPU_SPRG0(r4)\n\tld\tr6, VCPU_SPRG1(r4)\n\tld\tr7, VCPU_SPRG2(r4)\n\tld\tr8, VCPU_SPRG3(r4)\n\tmtspr\tSPRN_SPRG0, r5\n\tmtspr\tSPRN_SPRG1, r6\n\tmtspr\tSPRN_SPRG2, r7\n\tmtspr\tSPRN_SPRG3, r8\n\n\t/* Load up DAR and DSISR */\n\tld\tr5, VCPU_DAR(r4)\n\tlwz\tr6, VCPU_DSISR(r4)\n\tmtspr\tSPRN_DAR, r5\n\tmtspr\tSPRN_DSISR, r6\n\n\t/* Restore AMR and UAMOR, set AMOR to all 1s */\n\tld\tr5,VCPU_AMR(r4)\n\tld\tr6,VCPU_UAMOR(r4)\n\tli\tr7,-1\n\tmtspr\tSPRN_AMR,r5\n\tmtspr\tSPRN_UAMOR,r6\n\tmtspr\tSPRN_AMOR,r7\n\n\t/* Restore state of CTRL run bit; assume 1 on entry */\n\tlwz\tr5,VCPU_CTRL(r4)\n\tandi.\tr5,r5,1\n\tbne\t4f\n\tmfspr\tr6,SPRN_CTRLF\n\tclrrdi\tr6,r6,1\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/* Secondary threads wait for primary to have done partition switch */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr6, HSTATE_PTID(r13)\n\tcmpwi\tr6, 0\n\tbeq\t21f\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbne\t21f\n\tHMT_LOW\n20:\tlwz\tr3, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr3, 0x100\n\tbge\tno_switch_exit\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t20b\n\tHMT_MEDIUM\n21:\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n\t/* Check if HDEC expires soon */\n\tmfspr\tr3, SPRN_HDEC\n\tcmpwi\tr3, 512\t\t/* 1 microsecond */\n\tblt\thdec_soon\n\n\tld\tr6, VCPU_CTR(r4)\n\tld\tr7, VCPU_XER(r4)\n\n\tmtctr\tr6\n\tmtxer\tr7\n\nkvmppc_cede_reentry:\t\t/* r4 = vcpu, r13 = paca */\n\tld\tr10, VCPU_PC(r4)\n\tld\tr11, VCPU_MSR(r4)\n\tld\tr6, VCPU_SRR0(r4)\n\tld\tr7, VCPU_SRR1(r4)\n\tmtspr\tSPRN_SRR0, r6\n\tmtspr\tSPRN_SRR1, r7\n\ndeliver_guest_interrupt:\n\t/* r11 = vcpu->arch.msr & ~MSR_HV */\n\trldicl\tr11, r11, 63 - MSR_HV_LG, 1\n\trotldi\tr11, r11, 1 + MSR_HV_LG\n\tori\tr11, r11, MSR_ME\n\n\t/* Check if we can deliver an external or decrementer interrupt now */\n\tld\tr0, VCPU_PENDING_EXC(r4)\n\trldicl\tr0, r0, 64 - BOOK3S_IRQPRIO_EXTERNAL_LEVEL, 63\n\tcmpdi\tcr1, r0, 0\n\tandi.\tr8, r11, MSR_EE\n\tmfspr\tr8, SPRN_LPCR\n\t/* Insert EXTERNAL_LEVEL bit into LPCR at the MER bit position */\n\trldimi\tr8, r0, LPCR_MER_SH, 63 - LPCR_MER_SH\n\tmtspr\tSPRN_LPCR, r8\n\tisync\n\tbeq\t5f\n\tli\tr0, BOOK3S_INTERRUPT_EXTERNAL\n\tbne\tcr1, 12f\n\tmfspr\tr0, SPRN_DEC\n\tcmpwi\tr0, 0\n\tli\tr0, BOOK3S_INTERRUPT_DECREMENTER\n\tbge\t5f\n\n12:\tmtspr\tSPRN_SRR0, r10\n\tmr\tr10,r0\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr9, r4\n\tbl\tkvmppc_msr_interrupt\n5:\n\n/*\n * Required state:\n * R4 = vcpu\n * R10: value for HSRR0\n * R11: value for HSRR1\n * R13 = PACA\n */\nfast_guest_return:\n\tli\tr0,0\n\tstb\tr0,VCPU_CEDED(r4)\t/* cancel cede */\n\tmtspr\tSPRN_HSRR0,r10\n\tmtspr\tSPRN_HSRR1,r11\n\n\t/* Activate guest mode, so faults get handled by KVM */\n\tli\tr9, KVM_GUEST_MODE_GUEST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\t/* Accumulate timing */\n\taddi\tr3, r4, VCPU_TB_GUEST\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\t/* Enter guest */\n\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_CFAR(r4)\n\tmtspr\tSPRN_CFAR, r5\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr0, VCPU_PPR(r4)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\tld\tr5, VCPU_LR(r4)\n\tlwz\tr6, VCPU_CR(r4)\n\tmtlr\tr5\n\tmtcr\tr6\n\n\tld\tr1, VCPU_GPR(R1)(r4)\n\tld\tr2, VCPU_GPR(R2)(r4)\n\tld\tr3, VCPU_GPR(R3)(r4)\n\tld\tr5, VCPU_GPR(R5)(r4)\n\tld\tr6, VCPU_GPR(R6)(r4)\n\tld\tr7, VCPU_GPR(R7)(r4)\n\tld\tr8, VCPU_GPR(R8)(r4)\n\tld\tr9, VCPU_GPR(R9)(r4)\n\tld\tr10, VCPU_GPR(R10)(r4)\n\tld\tr11, VCPU_GPR(R11)(r4)\n\tld\tr12, VCPU_GPR(R12)(r4)\n\tld\tr13, VCPU_GPR(R13)(r4)\n\nBEGIN_FTR_SECTION\n\tmtspr\tSPRN_PPR, r0\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\tld\tr0, VCPU_GPR(R0)(r4)\n\tld\tr4, VCPU_GPR(R4)(r4)\n\n\thrfid\n\tb\t.\n\nsecondary_too_late:\n\tli\tr12, 0\n\tcmpdi\tr4, 0\n\tbeq\t11f\n\tstw\tr12, VCPU_TRAP(r4)\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n11:\tb\tkvmhv_switch_to_host\n\nno_switch_exit:\n\tHMT_MEDIUM\n\tli\tr12, 0\n\tb\t12f\nhdec_soon:\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n12:\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n\tb\tguest_exit_cont\n\n/******************************************************************************\n *                                                                            *\n *                               Exit code                                    *\n *                                                                            *\n *****************************************************************************/\n\n/*\n * We come here from the first-level interrupt handlers.\n */\n\t.globl\tkvmppc_interrupt_hv\nkvmppc_interrupt_hv:\n\t/*\n\t * Register contents:\n\t * R12\t\t= interrupt vector\n\t * R13\t\t= PACA\n\t * guest CR, R12 saved in shadow VCPU SCRATCH1/0\n\t * guest R13 saved in SPRN_SCRATCH0\n\t */\n\tstd\tr9, HSTATE_SCRATCH2(r13)\n\n\tlbz\tr9, HSTATE_IN_GUEST(r13)\n\tcmpwi\tr9, KVM_GUEST_MODE_HOST_HV\n\tbeq\tkvmppc_bad_host_intr\n#ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE\n\tcmpwi\tr9, KVM_GUEST_MODE_GUEST\n\tld\tr9, HSTATE_SCRATCH2(r13)\n\tbeq\tkvmppc_interrupt_pr\n#endif\n\t/* We're now back in the host but in guest MMU context */\n\tli\tr9, KVM_GUEST_MODE_HOST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Save registers */\n\n\tstd\tr0, VCPU_GPR(R0)(r9)\n\tstd\tr1, VCPU_GPR(R1)(r9)\n\tstd\tr2, VCPU_GPR(R2)(r9)\n\tstd\tr3, VCPU_GPR(R3)(r9)\n\tstd\tr4, VCPU_GPR(R4)(r9)\n\tstd\tr5, VCPU_GPR(R5)(r9)\n\tstd\tr6, VCPU_GPR(R6)(r9)\n\tstd\tr7, VCPU_GPR(R7)(r9)\n\tstd\tr8, VCPU_GPR(R8)(r9)\n\tld\tr0, HSTATE_SCRATCH2(r13)\n\tstd\tr0, VCPU_GPR(R9)(r9)\n\tstd\tr10, VCPU_GPR(R10)(r9)\n\tstd\tr11, VCPU_GPR(R11)(r9)\n\tld\tr3, HSTATE_SCRATCH0(r13)\n\tlwz\tr4, HSTATE_SCRATCH1(r13)\n\tstd\tr3, VCPU_GPR(R12)(r9)\n\tstw\tr4, VCPU_CR(r9)\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_CFAR(r13)\n\tstd\tr3, VCPU_CFAR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr4, HSTATE_PPR(r13)\n\tstd\tr4, VCPU_PPR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\t/* Restore R1/R2 so we can handle faults */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr2, PACATOC(r13)\n\n\tmfspr\tr10, SPRN_SRR0\n\tmfspr\tr11, SPRN_SRR1\n\tstd\tr10, VCPU_SRR0(r9)\n\tstd\tr11, VCPU_SRR1(r9)\n\tandi.\tr0, r12, 2\t\t/* need to read HSRR0/1? */\n\tbeq\t1f\n\tmfspr\tr10, SPRN_HSRR0\n\tmfspr\tr11, SPRN_HSRR1\n\tclrrdi\tr12, r12, 2\n1:\tstd\tr10, VCPU_PC(r9)\n\tstd\tr11, VCPU_MSR(r9)\n\n\tGET_SCRATCH0(r3)\n\tmflr\tr4\n\tstd\tr3, VCPU_GPR(R13)(r9)\n\tstd\tr4, VCPU_LR(r9)\n\n\tstw\tr12,VCPU_TRAP(r9)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r9, VCPU_TB_RMINTR\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n\tld\tr5, VCPU_GPR(R5)(r9)\n\tld\tr6, VCPU_GPR(R6)(r9)\n\tld\tr7, VCPU_GPR(R7)(r9)\n\tld\tr8, VCPU_GPR(R8)(r9)\n#endif\n\n\t/* Save HEIR (HV emulation assist reg) in emul_inst\n\t   if this is an HEI (HV emulation interrupt, e40) */\n\tli\tr3,KVM_INST_FETCH_FAILED\n\tstw\tr3,VCPU_LAST_INST(r9)\n\tcmpwi\tr12,BOOK3S_INTERRUPT_H_EMUL_ASSIST\n\tbne\t11f\n\tmfspr\tr3,SPRN_HEIR\n11:\tstw\tr3,VCPU_HEIR(r9)\n\n\t/* these are volatile across C function calls */\n\tmfctr\tr3\n\tmfxer\tr4\n\tstd\tr3, VCPU_CTR(r9)\n\tstd\tr4, VCPU_XER(r9)\n\n\t/* If this is a page table miss then see if it's theirs or ours */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tbeq\tkvmppc_hdsi\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tbeq\tkvmppc_hisi\n\n\t/* See if this is a leftover HDEC interrupt */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_HV_DECREMENTER\n\tbne\t2f\n\tmfspr\tr3,SPRN_HDEC\n\tcmpwi\tr3,0\n\tmr\tr4,r9\n\tbge\tfast_guest_return\n2:\n\t/* See if this is an hcall we can handle in real mode */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tbeq\thcall_try_real_mode\n\n\t/* Hypervisor doorbell - exit only if host IPI flag set */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\tbne\t3f\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbeq\t4f\n\tb\tguest_exit_cont\n3:\n\t/* External interrupt ? */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbne+\tguest_exit_cont\n\n\t/* External interrupt, first check for host_ipi. If this is\n\t * set, we know the host wants us out so let's do it now\n\t */\n\tbl\tkvmppc_read_intr\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\n\t/* Check if any CPU is heading out to the host, if so head out too */\n4:\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tmr\tr4, r9\n\tblt\tdeliver_guest_interrupt\n\nguest_exit_cont:\t\t/* r9 = vcpu, r12 = trap, r13 = paca */\n\t/* Save more register state  */\n\tmfdar\tr6\n\tmfdsisr\tr7\n\tstd\tr6, VCPU_DAR(r9)\n\tstw\tr7, VCPU_DSISR(r9)\n\t/* don't overwrite fault_dar/fault_dsisr if HDSI */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tbeq\tmc_cont\n\tstd\tr6, VCPU_FAULT_DAR(r9)\n\tstw\tr7, VCPU_FAULT_DSISR(r9)\n\n\t/* See if it is a machine check */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tbeq\tmachine_check_realmode\nmc_cont:\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r9, VCPU_TB_RMEXIT\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\tmr \tr3, r12\n\t/* Increment exit count, poke other threads to exit */\n\tbl\tkvmhv_commence_exit\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tlwz\tr12, VCPU_TRAP(r9)\n\n\t/* Stop others sending VCPU interrupts to this physical CPU */\n\tli\tr0, -1\n\tstw\tr0, VCPU_CPU(r9)\n\tstw\tr0, VCPU_THREAD_CPU(r9)\n\n\t/* Save guest CTRL register, set runlatch to 1 */\n\tmfspr\tr6,SPRN_CTRLF\n\tstw\tr6,VCPU_CTRL(r9)\n\tandi.\tr0,r6,1\n\tbne\t4f\n\tori\tr6,r6,1\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/* Read the guest SLB and save it away */\n\tlwz\tr0,VCPU_SLB_NR(r9)\t/* number of entries in SLB */\n\tmtctr\tr0\n\tli\tr6,0\n\taddi\tr7,r9,VCPU_SLB\n\tli\tr5,0\n1:\tslbmfee\tr8,r6\n\tandis.\tr0,r8,SLB_ESID_V@h\n\tbeq\t2f\n\tadd\tr8,r8,r6\t\t/* put index in */\n\tslbmfev\tr3,r6\n\tstd\tr8,VCPU_SLB_E(r7)\n\tstd\tr3,VCPU_SLB_V(r7)\n\taddi\tr7,r7,VCPU_SLB_SIZE\n\taddi\tr5,r5,1\n2:\taddi\tr6,r6,1\n\tbdnz\t1b\n\tstw\tr5,VCPU_SLB_MAX(r9)\n\n\t/*\n\t * Save the guest PURR/SPURR\n\t */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tld\tr7,VCPU_PURR(r9)\n\tld\tr8,VCPU_SPURR(r9)\n\tstd\tr5,VCPU_PURR(r9)\n\tstd\tr6,VCPU_SPURR(r9)\n\tsubf\tr5,r7,r5\n\tsubf\tr6,r8,r6\n\n\t/*\n\t * Restore host PURR/SPURR and add guest times\n\t * so that the time in the guest gets accounted.\n\t */\n\tld\tr3,HSTATE_PURR(r13)\n\tld\tr4,HSTATE_SPURR(r13)\n\tadd\tr3,r3,r5\n\tadd\tr4,r4,r6\n\tmtspr\tSPRN_PURR,r3\n\tmtspr\tSPRN_SPURR,r4\n\n\t/* Save DEC */\n\tmfspr\tr5,SPRN_DEC\n\tmftb\tr6\n\textsw\tr5,r5\n\tadd\tr5,r5,r6\n\t/* r5 is a guest timebase value here, convert to host TB */\n\tld\tr3,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_TB_OFFSET(r3)\n\tsubf\tr5,r4,r5\n\tstd\tr5,VCPU_DEC_EXPIRES(r9)\n\nBEGIN_FTR_SECTION\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Save POWER8-specific registers */\n\tmfspr\tr5, SPRN_IAMR\n\tmfspr\tr6, SPRN_PSPB\n\tmfspr\tr7, SPRN_FSCR\n\tstd\tr5, VCPU_IAMR(r9)\n\tstw\tr6, VCPU_PSPB(r9)\n\tstd\tr7, VCPU_FSCR(r9)\n\tmfspr\tr5, SPRN_IC\n\tmfspr\tr6, SPRN_VTB\n\tmfspr\tr7, SPRN_TAR\n\tstd\tr5, VCPU_IC(r9)\n\tstd\tr6, VCPU_VTB(r9)\n\tstd\tr7, VCPU_TAR(r9)\n\tmfspr\tr8, SPRN_EBBHR\n\tstd\tr8, VCPU_EBBHR(r9)\n\tmfspr\tr5, SPRN_EBBRR\n\tmfspr\tr6, SPRN_BESCR\n\tmfspr\tr7, SPRN_CSIGR\n\tmfspr\tr8, SPRN_TACR\n\tstd\tr5, VCPU_EBBRR(r9)\n\tstd\tr6, VCPU_BESCR(r9)\n\tstd\tr7, VCPU_CSIGR(r9)\n\tstd\tr8, VCPU_TACR(r9)\n\tmfspr\tr5, SPRN_TCSCR\n\tmfspr\tr6, SPRN_ACOP\n\tmfspr\tr7, SPRN_PID\n\tmfspr\tr8, SPRN_WORT\n\tstd\tr5, VCPU_TCSCR(r9)\n\tstd\tr6, VCPU_ACOP(r9)\n\tstw\tr7, VCPU_GUEST_PID(r9)\n\tstd\tr8, VCPU_WORT(r9)\n\t/*\n\t * Restore various registers to 0, where non-zero values\n\t * set by the guest could disrupt the host.\n\t */\n\tli\tr0, 0\n\tmtspr\tSPRN_IAMR, r0\n\tmtspr\tSPRN_CIABR, r0\n\tmtspr\tSPRN_DAWRX, r0\n\tmtspr\tSPRN_TCSCR, r0\n\tmtspr\tSPRN_WORT, r0\n\t/* Set MMCRS to 1<<31 to freeze and disable the SPMC counters */\n\tli\tr0, 1\n\tsldi\tr0, r0, 31\n\tmtspr\tSPRN_MMCRS, r0\n8:\n\n\t/* Save and reset AMR and UAMOR before turning on the MMU */\n\tmfspr\tr5,SPRN_AMR\n\tmfspr\tr6,SPRN_UAMOR\n\tstd\tr5,VCPU_AMR(r9)\n\tstd\tr6,VCPU_UAMOR(r9)\n\tli\tr6,0\n\tmtspr\tSPRN_AMR,r6\n\n\t/* Switch DSCR back to host value */\n\tmfspr\tr8, SPRN_DSCR\n\tld\tr7, HSTATE_DSCR(r13)\n\tstd\tr8, VCPU_DSCR(r9)\n\tmtspr\tSPRN_DSCR, r7\n\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r9)\n\tstd\tr15, VCPU_GPR(R15)(r9)\n\tstd\tr16, VCPU_GPR(R16)(r9)\n\tstd\tr17, VCPU_GPR(R17)(r9)\n\tstd\tr18, VCPU_GPR(R18)(r9)\n\tstd\tr19, VCPU_GPR(R19)(r9)\n\tstd\tr20, VCPU_GPR(R20)(r9)\n\tstd\tr21, VCPU_GPR(R21)(r9)\n\tstd\tr22, VCPU_GPR(R22)(r9)\n\tstd\tr23, VCPU_GPR(R23)(r9)\n\tstd\tr24, VCPU_GPR(R24)(r9)\n\tstd\tr25, VCPU_GPR(R25)(r9)\n\tstd\tr26, VCPU_GPR(R26)(r9)\n\tstd\tr27, VCPU_GPR(R27)(r9)\n\tstd\tr28, VCPU_GPR(R28)(r9)\n\tstd\tr29, VCPU_GPR(R29)(r9)\n\tstd\tr30, VCPU_GPR(R30)(r9)\n\tstd\tr31, VCPU_GPR(R31)(r9)\n\n\t/* Save SPRGs */\n\tmfspr\tr3, SPRN_SPRG0\n\tmfspr\tr4, SPRN_SPRG1\n\tmfspr\tr5, SPRN_SPRG2\n\tmfspr\tr6, SPRN_SPRG3\n\tstd\tr3, VCPU_SPRG0(r9)\n\tstd\tr4, VCPU_SPRG1(r9)\n\tstd\tr5, VCPU_SPRG2(r9)\n\tstd\tr6, VCPU_SPRG3(r9)\n\n\t/* save FP state */\n\tmr\tr3, r9\n\tbl\tkvmppc_save_fp\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tbl\tkvmppc_save_tm\nEND_FTR_SECTION_IFSET(CPU_FTR_TM)\n#endif\n\n\t/* Increment yield count if they have a VPA */\n\tld\tr8, VCPU_VPA(r9)\t/* do they have a VPA? */\n\tcmpdi\tr8, 0\n\tbeq\t25f\n\tli\tr4, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr3, r8, r4\n\taddi\tr3, r3, 1\n\tSTWX_BE\tr3, r8, r4\n\tli\tr3, 1\n\tstb\tr3, VCPU_VPA_DIRTY(r9)\n25:\n\t/* Save PMU registers if requested */\n\t/* r8 and cr0.eq are live here */\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER8 seems to have a hardware bug where setting\n\t * MMCR0[PMAE] along with MMCR0[PMC1CE] and/or MMCR0[PMCjCE]\n\t * when some counters are already negative doesn't seem\n\t * to cause a performance monitor alert (and hence interrupt).\n\t * The effect of this is that when saving the PMU state,\n\t * if there is no PMU alert pending when we read MMCR0\n\t * before freezing the counters, but one becomes pending\n\t * before we read the counters, we lose it.\n\t * To work around this, we need a way to freeze the counters\n\t * before reading MMCR0.  Normally, freezing the counters\n\t * is done by writing MMCR0 (to set MMCR0[FC]) which\n\t * unavoidably writes MMCR0[PMA0] as well.  On POWER8,\n\t * we can also freeze the counters using MMCR2, by writing\n\t * 1s to all the counter freeze condition bits (there are\n\t * 9 bits each for 6 counters).\n\t */\n\tli\tr3, -1\t\t\t/* set all freeze bits */\n\tclrrdi\tr3, r3, 10\n\tmfspr\tr10, SPRN_MMCR2\n\tmtspr\tSPRN_MMCR2, r3\n\tisync\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmfspr\tr4, SPRN_MMCR0\t\t/* save MMCR0 */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tmfspr\tr6, SPRN_MMCRA\n\t/* Clear MMCRA in order to disable SDAR updates */\n\tli\tr7, 0\n\tmtspr\tSPRN_MMCRA, r7\n\tisync\n\tbeq\t21f\t\t\t/* if no VPA, save PMU stuff anyway */\n\tlbz\tr7, LPPACA_PMCINUSE(r8)\n\tcmpwi\tr7, 0\t\t\t/* did they ask for PMU stuff to be saved? */\n\tbne\t21f\n\tstd\tr3, VCPU_MMCR(r9)\t/* if not, set saved MMCR0 to FC */\n\tb\t22f\n21:\tmfspr\tr5, SPRN_MMCR1\n\tmfspr\tr7, SPRN_SIAR\n\tmfspr\tr8, SPRN_SDAR\n\tstd\tr4, VCPU_MMCR(r9)\n\tstd\tr5, VCPU_MMCR + 8(r9)\n\tstd\tr6, VCPU_MMCR + 16(r9)\nBEGIN_FTR_SECTION\n\tstd\tr10, VCPU_MMCR + 24(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr7, VCPU_SIAR(r9)\n\tstd\tr8, VCPU_SDAR(r9)\n\tmfspr\tr3, SPRN_PMC1\n\tmfspr\tr4, SPRN_PMC2\n\tmfspr\tr5, SPRN_PMC3\n\tmfspr\tr6, SPRN_PMC4\n\tmfspr\tr7, SPRN_PMC5\n\tmfspr\tr8, SPRN_PMC6\n\tstw\tr3, VCPU_PMC(r9)\n\tstw\tr4, VCPU_PMC + 4(r9)\n\tstw\tr5, VCPU_PMC + 8(r9)\n\tstw\tr6, VCPU_PMC + 12(r9)\n\tstw\tr7, VCPU_PMC + 16(r9)\n\tstw\tr8, VCPU_PMC + 20(r9)\nBEGIN_FTR_SECTION\n\tmfspr\tr5, SPRN_SIER\n\tmfspr\tr6, SPRN_SPMC1\n\tmfspr\tr7, SPRN_SPMC2\n\tmfspr\tr8, SPRN_MMCRS\n\tstd\tr5, VCPU_SIER(r9)\n\tstw\tr6, VCPU_PMC + 24(r9)\n\tstw\tr7, VCPU_PMC + 28(r9)\n\tstd\tr8, VCPU_MMCR + 32(r9)\n\tlis\tr4, 0x8000\n\tmtspr\tSPRN_MMCRS, r4\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n22:\n\t/* Clear out SLB */\n\tli\tr5,0\n\tslbmte\tr5,r5\n\tslbia\n\tptesync\n\n\t/*\n\t * POWER7/POWER8 guest -> host partition switch code.\n\t * We don't have to lock against tlbies but we do\n\t * have to coordinate the hardware threads.\n\t */\nkvmhv_switch_to_host:\n\t/* Secondary threads wait for primary to do partition switch */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\tlbz\tr3,HSTATE_PTID(r13)\n\tcmpwi\tr3,0\n\tbeq\t15f\n\tHMT_LOW\n13:\tlbz\tr3,VCORE_IN_GUEST(r5)\n\tcmpwi\tr3,0\n\tbne\t13b\n\tHMT_MEDIUM\n\tb\t16f\n\n\t/* Primary thread waits for all the secondaries to exit guest */\n15:\tlwz\tr3,VCORE_ENTRY_EXIT(r5)\n\trlwinm\tr0,r3,32-8,0xff\n\tclrldi\tr3,r3,56\n\tcmpw\tr3,r0\n\tbne\t15b\n\tisync\n\n\t/* Did we actually switch to the guest at all? */\n\tlbz\tr6, VCORE_IN_GUEST(r5)\n\tcmpwi\tr6, 0\n\tbeq\t19f\n\n\t/* Primary thread switches back to host partition */\n\tld\tr6,KVM_HOST_SDR1(r4)\n\tlwz\tr7,KVM_HOST_LPID(r4)\n\tli\tr8,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r8\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to partition page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\nBEGIN_FTR_SECTION\n\t/* DPDES is shared between threads */\n\tmfspr\tr7, SPRN_DPDES\n\tstd\tr7, VCORE_DPDES(r5)\n\t/* clear DPDES so we don't get guest doorbells in the host */\n\tli\tr8, 0\n\tmtspr\tSPRN_DPDES, r8\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* If HMI, call kvmppc_realmode_hmi_handler() */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\t27f\n\tbl\tkvmppc_realmode_hmi_handler\n\tnop\n\tli\tr12, BOOK3S_INTERRUPT_HMI\n\t/*\n\t * At this point kvmppc_realmode_hmi_handler would have resync-ed\n\t * the TB. Hence it is not required to subtract guest timebase\n\t * offset from timebase. So, skip it.\n\t *\n\t * Also, do not call kvmppc_subcore_exit_guest() because it has\n\t * been invoked as part of kvmppc_realmode_hmi_handler().\n\t */\n\tb\t30f\n\n27:\n\t/* Subtract timebase offset from timebase */\n\tld\tr8,VCORE_TB_OFFSET(r5)\n\tcmpdi\tr8,0\n\tbeq\t17f\n\tmftb\tr6\t\t\t/* current guest timebase */\n\tsubf\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t\t/* update upper 40 bits */\n\tmftb\tr7\t\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t17f\n\taddis\tr8,r8,0x100\t\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n17:\tbl\tkvmppc_subcore_exit_guest\n\tnop\n30:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\n\t/* Reset PCR */\n\tld\tr0, VCORE_PCR(r5)\n\tcmpdi\tr0, 0\n\tbeq\t18f\n\tli\tr0, 0\n\tmtspr\tSPRN_PCR, r0\n18:\n\t/* Signal secondary CPUs to continue */\n\tstb\tr0,VCORE_IN_GUEST(r5)\n19:\tlis\tr8,0x7fff\t\t/* MAX_INT@h */\n\tmtspr\tSPRN_HDEC,r8\n\n16:\tld\tr8,KVM_HOST_LPCR(r4)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n\t/* load host SLB entries */\n\tld\tr8,PACA_SLBSHADOWPTR(r13)\n\n\t.rept\tSLB_NUM_BOLTED\n\tli\tr3, SLBSHADOW_SAVEAREA\n\tLDX_BE\tr5, r8, r3\n\taddi\tr3, r3, 8\n\tLDX_BE\tr6, r8, r3\n\tandis.\tr7,r5,SLB_ESID_V@h\n\tbeq\t1f\n\tslbmte\tr6,r5\n1:\taddi\tr8,r8,16\n\t.endr\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\t/* Finish timing, if we have a vcpu */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tli\tr3, 0\n\tbeq\t2f\n\tbl\tkvmhv_accumulate_time\n2:\n#endif\n\t/* Unset guest mode */\n\tli\tr0, KVM_GUEST_MODE_NONE\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\tld\tr0, 112+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, 112\n\tmtlr\tr0\n\tblr\n\n/*\n * Check whether an HDSI is an HPTE not found fault or something else.\n * If it is an HPTE not found fault that is due to the guest accessing\n * a page that they have mapped but which we have paged out, then\n * we continue on with the guest exit path.  In all other cases,\n * reflect the HDSI to the guest as a DSI.\n */\nkvmppc_hdsi:\n\tmfspr\tr4, SPRN_HDAR\n\tmfspr\tr6, SPRN_HDSISR\n\t/* HPTE not found fault or protection fault? */\n\tandis.\tr0, r6, (DSISR_NOHPTE | DSISR_PROTFAULT)@h\n\tbeq\t1f\t\t\t/* if not, send it to the guest */\n\tandi.\tr0, r11, MSR_DR\t\t/* data relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r4, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_DATA_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\tstd\tr4, VCPU_FAULT_DAR(r9)\n\tstw\tr6, VCPU_FAULT_DSISR(r9)\n\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tli\tr7, 1\t\t\t/* data fault */\n\tbl\tkvmppc_hpte_hv_fault\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\t6f\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\tcmpdi\tr3, -2\t\t\t/* MMIO emulation; need instr word */\n\tbeq\t2f\n\n\t/* Synthesize a DSI (or DSegI) for the guest */\n\tld\tr4, VCPU_FAULT_DAR(r9)\n\tmr\tr6, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_DATA_STORAGE\n\tmtspr\tSPRN_DSISR, r6\n7:\tmtspr\tSPRN_DAR, r4\n\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\nfast_interrupt_c_return:\n6:\tld\tr7, VCPU_CTR(r9)\n\tld\tr8, VCPU_XER(r9)\n\tmtctr\tr7\n\tmtxer\tr8\n\tmr\tr4, r9\n\tb\tfast_guest_return\n\n3:\tld\tr5, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r5)\n\tb\t4b\n\n\t/* If this is for emulated MMIO, load the instruction word */\n2:\tli\tr8, KVM_INST_FETCH_FAILED\t/* In case lwz faults */\n\n\t/* Set guest mode to 'jump over instruction' so if lwz faults\n\t * we'll just continue at the next IP. */\n\tli\tr0, KVM_GUEST_MODE_SKIP\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\t/* Do the access with MSR:DR enabled */\n\tmfmsr\tr3\n\tori\tr4, r3, MSR_DR\t\t/* Enable paging for data */\n\tmtmsrd\tr4\n\tlwz\tr8, 0(r10)\n\tmtmsrd\tr3\n\n\t/* Store the result */\n\tstw\tr8, VCPU_LAST_INST(r9)\n\n\t/* Unset guest mode. */\n\tli\tr0, KVM_GUEST_MODE_HOST_HV\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\tb\tguest_exit_cont\n\n/*\n * Similarly for an HISI, reflect it to the guest as an ISI unless\n * it is an HPTE not found fault for a page that we have paged out.\n */\nkvmppc_hisi:\n\tandis.\tr0, r11, SRR1_ISI_NOPT@h\n\tbeq\t1f\n\tandi.\tr0, r11, MSR_IR\t\t/* instruction relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r10, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_INST_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tmr\tr4, r10\n\tmr\tr6, r11\n\tli\tr7, 0\t\t\t/* instruction fault */\n\tbl\tkvmppc_hpte_hv_fault\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\tfast_interrupt_c_return\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\n\t/* Synthesize an ISI (or ISegI) for the guest */\n\tmr\tr11, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_INST_STORAGE\n7:\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\n\tb\tfast_interrupt_c_return\n\n3:\tld\tr6, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r6)\n\tb\t4b\n\n/*\n * Try to handle an hcall in real mode.\n * Returns to the guest if we handle it, or continues on up to\n * the kernel if we can't (i.e. if we don't have a handler for\n * it, or if the handler returns H_TOO_HARD).\n *\n * r5 - r8 contain hcall args,\n * r9 = vcpu, r10 = pc, r11 = msr, r12 = trap, r13 = paca\n */\nhcall_try_real_mode:\n\tld\tr3,VCPU_GPR(R3)(r9)\n\tandi.\tr0,r11,MSR_PR\n\t/* sc 1 from userspace - reflect to guest syscall */\n\tbne\tsc_1_fast_return\n\tclrrdi\tr3,r3,2\n\tcmpldi\tr3,hcall_real_table_end - hcall_real_table\n\tbge\tguest_exit_cont\n\t/* See if this hcall is enabled for in-kernel handling */\n\tld\tr4, VCPU_KVM(r9)\n\tsrdi\tr0, r3, 8\t/* r0 = (r3 / 4) >> 6 */\n\tsldi\tr0, r0, 3\t/* index into kvm->arch.enabled_hcalls[] */\n\tadd\tr4, r4, r0\n\tld\tr0, KVM_ENABLED_HCALLS(r4)\n\trlwinm\tr4, r3, 32-2, 0x3f\t/* r4 = (r3 / 4) & 0x3f */\n\tsrd\tr0, r0, r4\n\tandi.\tr0, r0, 1\n\tbeq\tguest_exit_cont\n\t/* Get pointer to handler, if any, and call it */\n\tLOAD_REG_ADDR(r4, hcall_real_table)\n\tlwax\tr3,r3,r4\n\tcmpwi\tr3,0\n\tbeq\tguest_exit_cont\n\tadd\tr12,r3,r4\n\tmtctr\tr12\n\tmr\tr3,r9\t\t/* get vcpu pointer */\n\tld\tr4,VCPU_GPR(R4)(r9)\n\tbctrl\n\tcmpdi\tr3,H_TOO_HARD\n\tbeq\thcall_real_fallback\n\tld\tr4,HSTATE_KVM_VCPU(r13)\n\tstd\tr3,VCPU_GPR(R3)(r4)\n\tld\tr10,VCPU_PC(r4)\n\tld\tr11,VCPU_MSR(r4)\n\tb\tfast_guest_return\n\nsc_1_fast_return:\n\tmtspr\tSPRN_SRR0,r10\n\tmtspr\tSPRN_SRR1,r11\n\tli\tr10, BOOK3S_INTERRUPT_SYSCALL\n\tbl\tkvmppc_msr_interrupt\n\tmr\tr4,r9\n\tb\tfast_guest_return\n\n\t/* We've attempted a real mode hcall, but it's punted it back\n\t * to userspace.  We need to restore some clobbered volatiles\n\t * before resuming the pass-it-to-qemu path */\nhcall_real_fallback:\n\tli\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\tb\tguest_exit_cont\n\n\t.globl\thcall_real_table\nhcall_real_table:\n\t.long\t0\t\t/* 0 - unused */\n\t.long\tDOTSYM(kvmppc_h_remove) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_enter) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_read) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_mod) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_ref) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_protect) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_get_tce) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_put_tce) - hcall_real_table\n\t.long\t0\t\t/* 0x24 - H_SET_SPRG0 */\n\t.long\tDOTSYM(kvmppc_h_set_dabr) - hcall_real_table\n\t.long\t0\t\t/* 0x2c */\n\t.long\t0\t\t/* 0x30 */\n\t.long\t0\t\t/* 0x34 */\n\t.long\t0\t\t/* 0x38 */\n\t.long\t0\t\t/* 0x3c */\n\t.long\t0\t\t/* 0x40 */\n\t.long\t0\t\t/* 0x44 */\n\t.long\t0\t\t/* 0x48 */\n\t.long\t0\t\t/* 0x4c */\n\t.long\t0\t\t/* 0x50 */\n\t.long\t0\t\t/* 0x54 */\n\t.long\t0\t\t/* 0x58 */\n\t.long\t0\t\t/* 0x5c */\n\t.long\t0\t\t/* 0x60 */\n#ifdef CONFIG_KVM_XICS\n\t.long\tDOTSYM(kvmppc_rm_h_eoi) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_cppr) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_ipi) - hcall_real_table\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\tDOTSYM(kvmppc_rm_h_xirr) - hcall_real_table\n#else\n\t.long\t0\t\t/* 0x64 - H_EOI */\n\t.long\t0\t\t/* 0x68 - H_CPPR */\n\t.long\t0\t\t/* 0x6c - H_IPI */\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\t0\t\t/* 0x74 - H_XIRR */\n#endif\n\t.long\t0\t\t/* 0x78 */\n\t.long\t0\t\t/* 0x7c */\n\t.long\t0\t\t/* 0x80 */\n\t.long\t0\t\t/* 0x84 */\n\t.long\t0\t\t/* 0x88 */\n\t.long\t0\t\t/* 0x8c */\n\t.long\t0\t\t/* 0x90 */\n\t.long\t0\t\t/* 0x94 */\n\t.long\t0\t\t/* 0x98 */\n\t.long\t0\t\t/* 0x9c */\n\t.long\t0\t\t/* 0xa0 */\n\t.long\t0\t\t/* 0xa4 */\n\t.long\t0\t\t/* 0xa8 */\n\t.long\t0\t\t/* 0xac */\n\t.long\t0\t\t/* 0xb0 */\n\t.long\t0\t\t/* 0xb4 */\n\t.long\t0\t\t/* 0xb8 */\n\t.long\t0\t\t/* 0xbc */\n\t.long\t0\t\t/* 0xc0 */\n\t.long\t0\t\t/* 0xc4 */\n\t.long\t0\t\t/* 0xc8 */\n\t.long\t0\t\t/* 0xcc */\n\t.long\t0\t\t/* 0xd0 */\n\t.long\t0\t\t/* 0xd4 */\n\t.long\t0\t\t/* 0xd8 */\n\t.long\t0\t\t/* 0xdc */\n\t.long\tDOTSYM(kvmppc_h_cede) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_confer) - hcall_real_table\n\t.long\t0\t\t/* 0xe8 */\n\t.long\t0\t\t/* 0xec */\n\t.long\t0\t\t/* 0xf0 */\n\t.long\t0\t\t/* 0xf4 */\n\t.long\t0\t\t/* 0xf8 */\n\t.long\t0\t\t/* 0xfc */\n\t.long\t0\t\t/* 0x100 */\n\t.long\t0\t\t/* 0x104 */\n\t.long\t0\t\t/* 0x108 */\n\t.long\t0\t\t/* 0x10c */\n\t.long\t0\t\t/* 0x110 */\n\t.long\t0\t\t/* 0x114 */\n\t.long\t0\t\t/* 0x118 */\n\t.long\t0\t\t/* 0x11c */\n\t.long\t0\t\t/* 0x120 */\n\t.long\tDOTSYM(kvmppc_h_bulk_remove) - hcall_real_table\n\t.long\t0\t\t/* 0x128 */\n\t.long\t0\t\t/* 0x12c */\n\t.long\t0\t\t/* 0x130 */\n\t.long\tDOTSYM(kvmppc_h_set_xdabr) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_stuff_tce) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_put_tce_indirect) - hcall_real_table\n\t.long\t0\t\t/* 0x140 */\n\t.long\t0\t\t/* 0x144 */\n\t.long\t0\t\t/* 0x148 */\n\t.long\t0\t\t/* 0x14c */\n\t.long\t0\t\t/* 0x150 */\n\t.long\t0\t\t/* 0x154 */\n\t.long\t0\t\t/* 0x158 */\n\t.long\t0\t\t/* 0x15c */\n\t.long\t0\t\t/* 0x160 */\n\t.long\t0\t\t/* 0x164 */\n\t.long\t0\t\t/* 0x168 */\n\t.long\t0\t\t/* 0x16c */\n\t.long\t0\t\t/* 0x170 */\n\t.long\t0\t\t/* 0x174 */\n\t.long\t0\t\t/* 0x178 */\n\t.long\t0\t\t/* 0x17c */\n\t.long\t0\t\t/* 0x180 */\n\t.long\t0\t\t/* 0x184 */\n\t.long\t0\t\t/* 0x188 */\n\t.long\t0\t\t/* 0x18c */\n\t.long\t0\t\t/* 0x190 */\n\t.long\t0\t\t/* 0x194 */\n\t.long\t0\t\t/* 0x198 */\n\t.long\t0\t\t/* 0x19c */\n\t.long\t0\t\t/* 0x1a0 */\n\t.long\t0\t\t/* 0x1a4 */\n\t.long\t0\t\t/* 0x1a8 */\n\t.long\t0\t\t/* 0x1ac */\n\t.long\t0\t\t/* 0x1b0 */\n\t.long\t0\t\t/* 0x1b4 */\n\t.long\t0\t\t/* 0x1b8 */\n\t.long\t0\t\t/* 0x1bc */\n\t.long\t0\t\t/* 0x1c0 */\n\t.long\t0\t\t/* 0x1c4 */\n\t.long\t0\t\t/* 0x1c8 */\n\t.long\t0\t\t/* 0x1cc */\n\t.long\t0\t\t/* 0x1d0 */\n\t.long\t0\t\t/* 0x1d4 */\n\t.long\t0\t\t/* 0x1d8 */\n\t.long\t0\t\t/* 0x1dc */\n\t.long\t0\t\t/* 0x1e0 */\n\t.long\t0\t\t/* 0x1e4 */\n\t.long\t0\t\t/* 0x1e8 */\n\t.long\t0\t\t/* 0x1ec */\n\t.long\t0\t\t/* 0x1f0 */\n\t.long\t0\t\t/* 0x1f4 */\n\t.long\t0\t\t/* 0x1f8 */\n\t.long\t0\t\t/* 0x1fc */\n\t.long\t0\t\t/* 0x200 */\n\t.long\t0\t\t/* 0x204 */\n\t.long\t0\t\t/* 0x208 */\n\t.long\t0\t\t/* 0x20c */\n\t.long\t0\t\t/* 0x210 */\n\t.long\t0\t\t/* 0x214 */\n\t.long\t0\t\t/* 0x218 */\n\t.long\t0\t\t/* 0x21c */\n\t.long\t0\t\t/* 0x220 */\n\t.long\t0\t\t/* 0x224 */\n\t.long\t0\t\t/* 0x228 */\n\t.long\t0\t\t/* 0x22c */\n\t.long\t0\t\t/* 0x230 */\n\t.long\t0\t\t/* 0x234 */\n\t.long\t0\t\t/* 0x238 */\n\t.long\t0\t\t/* 0x23c */\n\t.long\t0\t\t/* 0x240 */\n\t.long\t0\t\t/* 0x244 */\n\t.long\t0\t\t/* 0x248 */\n\t.long\t0\t\t/* 0x24c */\n\t.long\t0\t\t/* 0x250 */\n\t.long\t0\t\t/* 0x254 */\n\t.long\t0\t\t/* 0x258 */\n\t.long\t0\t\t/* 0x25c */\n\t.long\t0\t\t/* 0x260 */\n\t.long\t0\t\t/* 0x264 */\n\t.long\t0\t\t/* 0x268 */\n\t.long\t0\t\t/* 0x26c */\n\t.long\t0\t\t/* 0x270 */\n\t.long\t0\t\t/* 0x274 */\n\t.long\t0\t\t/* 0x278 */\n\t.long\t0\t\t/* 0x27c */\n\t.long\t0\t\t/* 0x280 */\n\t.long\t0\t\t/* 0x284 */\n\t.long\t0\t\t/* 0x288 */\n\t.long\t0\t\t/* 0x28c */\n\t.long\t0\t\t/* 0x290 */\n\t.long\t0\t\t/* 0x294 */\n\t.long\t0\t\t/* 0x298 */\n\t.long\t0\t\t/* 0x29c */\n\t.long\t0\t\t/* 0x2a0 */\n\t.long\t0\t\t/* 0x2a4 */\n\t.long\t0\t\t/* 0x2a8 */\n\t.long\t0\t\t/* 0x2ac */\n\t.long\t0\t\t/* 0x2b0 */\n\t.long\t0\t\t/* 0x2b4 */\n\t.long\t0\t\t/* 0x2b8 */\n\t.long\t0\t\t/* 0x2bc */\n\t.long\t0\t\t/* 0x2c0 */\n\t.long\t0\t\t/* 0x2c4 */\n\t.long\t0\t\t/* 0x2c8 */\n\t.long\t0\t\t/* 0x2cc */\n\t.long\t0\t\t/* 0x2d0 */\n\t.long\t0\t\t/* 0x2d4 */\n\t.long\t0\t\t/* 0x2d8 */\n\t.long\t0\t\t/* 0x2dc */\n\t.long\t0\t\t/* 0x2e0 */\n\t.long\t0\t\t/* 0x2e4 */\n\t.long\t0\t\t/* 0x2e8 */\n\t.long\t0\t\t/* 0x2ec */\n\t.long\t0\t\t/* 0x2f0 */\n\t.long\t0\t\t/* 0x2f4 */\n\t.long\t0\t\t/* 0x2f8 */\n\t.long\t0\t\t/* 0x2fc */\n\t.long\tDOTSYM(kvmppc_h_random) - hcall_real_table\n\t.globl\thcall_real_table_end\nhcall_real_table_end:\n\n_GLOBAL(kvmppc_h_set_xdabr)\n\tandi.\tr0, r5, DABRX_USER | DABRX_KERNEL\n\tbeq\t6f\n\tli\tr0, DABRX_USER | DABRX_KERNEL | DABRX_BTI\n\tandc.\tr0, r5, r0\n\tbeq\t3f\n6:\tli\tr3, H_PARAMETER\n\tblr\n\n_GLOBAL(kvmppc_h_set_dabr)\n\tli\tr5, DABRX_USER | DABRX_KERNEL\n3:\nBEGIN_FTR_SECTION\n\tb\t2f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr4,VCPU_DABR(r3)\n\tstw\tr5, VCPU_DABRX(r3)\n\tmtspr\tSPRN_DABRX, r5\n\t/* Work around P7 bug where DABR can get corrupted on mtspr */\n1:\tmtspr\tSPRN_DABR,r4\n\tmfspr\tr5, SPRN_DABR\n\tcmpd\tr4, r5\n\tbne\t1b\n\tisync\n\tli\tr3,0\n\tblr\n\n\t/* Emulate H_SET_DABR/X on P8 for the sake of compat mode guests */\n2:\trlwimi\tr5, r4, 5, DAWRX_DR | DAWRX_DW\n\trlwimi\tr5, r4, 2, DAWRX_WT\n\tclrrdi\tr4, r4, 3\n\tstd\tr4, VCPU_DAWR(r3)\n\tstd\tr5, VCPU_DAWRX(r3)\n\tmtspr\tSPRN_DAWR, r4\n\tmtspr\tSPRN_DAWRX, r5\n\tli\tr3, 0\n\tblr\n\n_GLOBAL(kvmppc_h_cede)\t\t/* r3 = vcpu pointer, r11 = msr, r13 = paca */\n\tori\tr11,r11,MSR_EE\n\tstd\tr11,VCPU_MSR(r3)\n\tli\tr0,1\n\tstb\tr0,VCPU_CEDED(r3)\n\tsync\t\t\t/* order setting ceded vs. testing prodded */\n\tlbz\tr5,VCPU_PRODDED(r3)\n\tcmpwi\tr5,0\n\tbne\tkvm_cede_prodded\n\tli\tr12,0\t\t/* set trap to 0 to say hcall is handled */\n\tstw\tr12,VCPU_TRAP(r3)\n\tli\tr0,H_SUCCESS\n\tstd\tr0,VCPU_GPR(R3)(r3)\n\n\t/*\n\t * Set our bit in the bitmask of napping threads unless all the\n\t * other threads are already napping, in which case we send this\n\t * up to the host.\n\t */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr6,HSTATE_PTID(r13)\n\tlwz\tr8,VCORE_ENTRY_EXIT(r5)\n\tclrldi\tr8,r8,56\n\tli\tr0,1\n\tsld\tr0,r0,r6\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n31:\tlwarx\tr4,0,r6\n\tor\tr4,r4,r0\n\tcmpw\tr4,r8\n\tbeq\tkvm_cede_exit\n\tstwcx.\tr4,0,r6\n\tbne\t31b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr0,NAPPING_CEDE\n\tstb\tr0,HSTATE_NAPPING(r13)\n\tlwz\tr7,VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7,0x100\n\tbge\t33f\t\t/* another thread already exiting */\n\n/*\n * Although not specifically required by the architecture, POWER7\n * preserves the following registers in nap mode, even if an SMT mode\n * switch occurs: SLB entries, PURR, SPURR, AMOR, UAMOR, AMR, SPRG0-3,\n * DAR, DSISR, DABR, DABRX, DSCR, PMCx, MMCRx, SIAR, SDAR.\n */\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r3)\n\tstd\tr15, VCPU_GPR(R15)(r3)\n\tstd\tr16, VCPU_GPR(R16)(r3)\n\tstd\tr17, VCPU_GPR(R17)(r3)\n\tstd\tr18, VCPU_GPR(R18)(r3)\n\tstd\tr19, VCPU_GPR(R19)(r3)\n\tstd\tr20, VCPU_GPR(R20)(r3)\n\tstd\tr21, VCPU_GPR(R21)(r3)\n\tstd\tr22, VCPU_GPR(R22)(r3)\n\tstd\tr23, VCPU_GPR(R23)(r3)\n\tstd\tr24, VCPU_GPR(R24)(r3)\n\tstd\tr25, VCPU_GPR(R25)(r3)\n\tstd\tr26, VCPU_GPR(R26)(r3)\n\tstd\tr27, VCPU_GPR(R27)(r3)\n\tstd\tr28, VCPU_GPR(R28)(r3)\n\tstd\tr29, VCPU_GPR(R29)(r3)\n\tstd\tr30, VCPU_GPR(R30)(r3)\n\tstd\tr31, VCPU_GPR(R31)(r3)\n\n\t/* save FP state */\n\tbl\tkvmppc_save_fp\n\n\t/*\n\t * Set DEC to the smaller of DEC and HDEC, so that we wake\n\t * no later than the end of our timeslice (HDEC interrupts\n\t * don't wake us from nap).\n\t */\n\tmfspr\tr3, SPRN_DEC\n\tmfspr\tr4, SPRN_HDEC\n\tmftb\tr5\n\tcmpw\tr3, r4\n\tble\t67f\n\tmtspr\tSPRN_DEC, r4\n67:\n\t/* save expiry time of guest decrementer */\n\textsw\tr3, r3\n\tadd\tr3, r3, r5\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr6, VCORE_TB_OFFSET(r5)\n\tsubf\tr3, r6, r3\t/* convert to host TB value */\n\tstd\tr3, VCPU_DEC_EXPIRES(r4)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\taddi\tr3, r4, VCPU_TB_CEDE\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\tlis\tr3, LPCR_PECEDP@h\t/* Do wake on privileged doorbell */\n\n\t/*\n\t * Take a nap until a decrementer or external or doobell interrupt\n\t * occurs, with PECE1 and PECE0 set in LPCR.\n\t * On POWER8, set PECEDH, and if we are ceding, also set PECEDP.\n\t * Also clear the runlatch bit before napping.\n\t */\nkvm_do_nap:\n\tmfspr\tr0, SPRN_CTRLF\n\tclrrdi\tr0, r0, 1\n\tmtspr\tSPRN_CTRLT, r0\n\n\tli\tr0,1\n\tstb\tr0,HSTATE_HWTHREAD_REQ(r13)\n\tmfspr\tr5,SPRN_LPCR\n\tori\tr5,r5,LPCR_PECE0 | LPCR_PECE1\nBEGIN_FTR_SECTION\n\tori\tr5, r5, LPCR_PECEDH\n\trlwimi\tr5, r3, 0, LPCR_PECEDP\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_LPCR,r5\n\tisync\n\tli\tr0, 0\n\tstd\tr0, HSTATE_SCRATCH0(r13)\n\tptesync\n\tld\tr0, HSTATE_SCRATCH0(r13)\n1:\tcmpd\tr0, r0\n\tbne\t1b\n\tnap\n\tb\t.\n\n33:\tmr\tr4, r3\n\tli\tr3, 0\n\tli\tr12, 0\n\tb\t34f\n\nkvm_end_cede:\n\t/* get vcpu pointer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\n\t/* Woken by external or decrementer interrupt */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMINTR\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\t/* load up FP state */\n\tbl\tkvmppc_load_fp\n\n\t/* Restore guest decrementer */\n\tld\tr3, VCPU_DEC_EXPIRES(r4)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr6, VCORE_TB_OFFSET(r5)\n\tadd\tr3, r3, r6\t/* convert host TB to guest TB value */\n\tmftb\tr7\n\tsubf\tr3, r7, r3\n\tmtspr\tSPRN_DEC, r3\n\n\t/* Load NV GPRS */\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n \n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\n\t/* clear our bit in vcore->napping_threads */\n34:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr7,HSTATE_PTID(r13)\n\tli\tr0,1\n\tsld\tr0,r0,r7\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n32:\tlwarx\tr7,0,r6\n\tandc\tr7,r7,r0\n\tstwcx.\tr7,0,r6\n\tbne\t32b\n\tli\tr0,0\n\tstb\tr0,HSTATE_NAPPING(r13)\n\n\t/* See if the wake reason means we need to exit */\n\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\n\t/* see if any other thread is already exiting */\n\tlwz\tr0,VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0,0x100\n\tbge\tguest_exit_cont\n\n\tb\tkvmppc_cede_reentry\t/* if not go back to guest */\n\n\t/* cede when already previously prodded case */\nkvm_cede_prodded:\n\tli\tr0,0\n\tstb\tr0,VCPU_PRODDED(r3)\n\tsync\t\t\t/* order testing prodded vs. clearing ceded */\n\tstb\tr0,VCPU_CEDED(r3)\n\tli\tr3,H_SUCCESS\n\tblr\n\n\t/* we've ceded but we want to give control to the host */\nkvm_cede_exit:\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tb\tguest_exit_cont\n\n\t/* Try to handle a machine check in real mode */\nmachine_check_realmode:\n\tmr\tr3, r9\t\t/* get vcpu pointer */\n\tbl\tkvmppc_realmode_machine_check\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\t/*\n\t * Deliver unhandled/fatal (e.g. UE) MCE errors to guest through\n\t * machine check interrupt (set HSRR0 to 0x200). And for handled\n\t * errors (no-fatal), just go back to guest execution with current\n\t * HSRR0 instead of exiting guest. This new approach will inject\n\t * machine check to guest for fatal error causing guest to crash.\n\t *\n\t * The old code used to return to host for unhandled errors which\n\t * was causing guest to hang with soft lockups inside guest and\n\t * makes it difficult to recover guest instance.\n\t *\n\t * if we receive machine check with MSR(RI=0) then deliver it to\n\t * guest as machine check causing guest to crash.\n\t */\n\tld\tr11, VCPU_MSR(r9)\n\trldicl.\tr0, r11, 64-MSR_HV_LG, 63 /* check if it happened in HV mode */\n\tbne\tmc_cont\t\t\t/* if so, exit to host */\n\tandi.\tr10, r11, MSR_RI\t/* check for unrecoverable exception */\n\tbeq\t1f\t\t\t/* Deliver a machine check to guest */\n\tld\tr10, VCPU_PC(r9)\n\tcmpdi\tr3, 0\t\t/* Did we handle MCE ? */\n\tbne\t2f\t/* Continue guest execution. */\n\t/* If not, deliver a machine check.  SRR0/1 are already set */\n1:\tli\tr10, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tbl\tkvmppc_msr_interrupt\n2:\tb\tfast_interrupt_c_return\n\n/*\n * Check the reason we woke from nap, and take appropriate action.\n * Returns (in r3):\n *\t0 if nothing needs to be done\n *\t1 if something happened that needs to be handled by the host\n *\t-1 if there was a guest wakeup (IPI or msgsnd)\n *\n * Also sets r12 to the interrupt vector for any interrupt that needs\n * to be handled now by the host (0x500 for external interrupt), or zero.\n * Modifies r0, r6, r7, r8.\n */\nkvmppc_check_wake_reason:\n\tmfspr\tr6, SPRN_SRR1\nBEGIN_FTR_SECTION\n\trlwinm\tr6, r6, 45-31, 0xf\t/* extract wake reason field (P8) */\nFTR_SECTION_ELSE\n\trlwinm\tr6, r6, 45-31, 0xe\t/* P7 wake reason field is 3 bits */\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 8\t\t\t/* was it an external interrupt? */\n\tli\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbeq\tkvmppc_read_intr\t/* if so, see what it was */\n\tli\tr3, 0\n\tli\tr12, 0\n\tcmpwi\tr6, 6\t\t\t/* was it the decrementer? */\n\tbeq\t0f\nBEGIN_FTR_SECTION\n\tcmpwi\tr6, 5\t\t\t/* privileged doorbell? */\n\tbeq\t0f\n\tcmpwi\tr6, 3\t\t\t/* hypervisor doorbell? */\n\tbeq\t3f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 0xa\t\t\t/* Hypervisor maintenance ? */\n\tbeq\t4f\n\tli\tr3, 1\t\t\t/* anything else, return 1 */\n0:\tblr\n\n\t/* hypervisor doorbell */\n3:\tli\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\n\t/*\n\t * Clear the doorbell as we will invoke the handler\n\t * explicitly in the guest exit path.\n\t */\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\n\t/* see if it's a host IPI */\n\tli\tr3, 1\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbnelr\n\t/* if not, return -1 */\n\tli\tr3, -1\n\tblr\n\n\t/* Woken up due to Hypervisor maintenance interrupt */\n4:\tli\tr12, BOOK3S_INTERRUPT_HMI\n\tli\tr3, 1\n\tblr\n\n/*\n * Determine what sort of external interrupt is pending (if any).\n * Returns:\n *\t0 if no interrupt is pending\n *\t1 if an interrupt is pending that needs to be handled by the host\n *\t-1 if there was a guest wakeup IPI (which has now been cleared)\n * Modifies r0, r6, r7, r8, returns value in r3.\n */\nkvmppc_read_intr:\n\t/* see if a host IPI is pending */\n\tli\tr3, 1\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbne\t1f\n\n\t/* Now read the interrupt from the ICP */\n\tld\tr6, HSTATE_XICS_PHYS(r13)\n\tli\tr7, XICS_XIRR\n\tcmpdi\tr6, 0\n\tbeq-\t1f\n\tlwzcix\tr0, r6, r7\n\t/*\n\t * Save XIRR for later. Since we get in in reverse endian on LE\n\t * systems, save it byte reversed and fetch it back in host endian.\n\t */\n\tli\tr3, HSTATE_SAVED_XIRR\n\tSTWX_BE\tr0, r3, r13\n#ifdef __LITTLE_ENDIAN__\n\tlwz\tr3, HSTATE_SAVED_XIRR(r13)\n#else\n\tmr\tr3, r0\n#endif\n\trlwinm.\tr3, r3, 0, 0xffffff\n\tsync\n\tbeq\t1f\t\t\t/* if nothing pending in the ICP */\n\n\t/* We found something in the ICP...\n\t *\n\t * If it's not an IPI, stash it in the PACA and return to\n\t * the host, we don't (yet) handle directing real external\n\t * interrupts directly to the guest\n\t */\n\tcmpwi\tr3, XICS_IPI\t\t/* if there is, is it an IPI? */\n\tbne\t42f\n\n\t/* It's an IPI, clear the MFRR and EOI it */\n\tli\tr3, 0xff\n\tli\tr8, XICS_MFRR\n\tstbcix\tr3, r6, r8\t\t/* clear the IPI */\n\tstwcix\tr0, r6, r7\t\t/* EOI it */\n\tsync\n\n\t/* We need to re-check host IPI now in case it got set in the\n\t * meantime. If it's clear, we bounce the interrupt to the\n\t * guest\n\t */\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbne-\t43f\n\n\t/* OK, it's an IPI for us */\n\tli\tr12, 0\n\tli\tr3, -1\n1:\tblr\n\n42:\t/* It's not an IPI and it's for the host. We saved a copy of XIRR in\n\t * the PACA earlier, it will be picked up by the host ICP driver\n\t */\n\tli\tr3, 1\n\tb\t1b\n\n43:\t/* We raced with the host, we need to resend that IPI, bummer */\n\tli\tr0, IPI_PRIORITY\n\tstbcix\tr0, r6, r8\t\t/* set the IPI */\n\tsync\n\tli\tr3, 1\n\tb\t1b\n\n/*\n * Save away FP, VMX and VSX registers.\n * r3 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nkvmppc_save_fp:\n\tmflr\tr30\n\tmr\tr31,r3\n\tmfmsr\tr5\n\tori\tr8,r5,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r3,VCPU_FPRS\n\tbl\tstore_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tstore_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tmfspr\tr6,SPRN_VRSAVE\n\tstw\tr6,VCPU_VRSAVE(r31)\n\tmtlr\tr30\n\tblr\n\n/*\n * Load up FP, VMX and VSX registers\n * r4 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nkvmppc_load_fp:\n\tmflr\tr30\n\tmr\tr31,r4\n\tmfmsr\tr9\n\tori\tr8,r9,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r4,VCPU_FPRS\n\tbl\tload_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tload_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tlwz\tr7,VCPU_VRSAVE(r31)\n\tmtspr\tSPRN_VRSAVE,r7\n\tmtlr\tr30\n\tmr\tr4,r31\n\tblr\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n/*\n * Save transactional state and TM-related registers.\n * Called with r9 pointing to the vcpu struct.\n * This can modify all checkpointed registers, but\n * restores r1, r2 and r9 (vcpu pointer) before exit.\n */\nkvmppc_save_tm:\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\n\t/* Turn on TM. */\n\tmfmsr\tr8\n\tli\tr0, 1\n\trldimi\tr8, r0, MSR_TM_LG, 63-MSR_TM_LG\n\tmtmsrd\tr8\n\n\tld\tr5, VCPU_MSR(r9)\n\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n\tbeq\t1f\t/* TM not active in guest. */\n\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\tli\tr3, TM_CAUSE_KVM_RESCHED\n\n\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n\tli\tr5, 0\n\tmtmsrd\tr5, 1\n\n\t/* All GPRs are volatile at this point. */\n\tTRECLAIM(R3)\n\n\t/* Temporarily store r13 and r9 so we have some regs to play with */\n\tSET_SCRATCH0(r13)\n\tGET_PACA(r13)\n\tstd\tr9, PACATMSCRATCH(r13)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Get a few more GPRs free. */\n\tstd\tr29, VCPU_GPRS_TM(29)(r9)\n\tstd\tr30, VCPU_GPRS_TM(30)(r9)\n\tstd\tr31, VCPU_GPRS_TM(31)(r9)\n\n\t/* Save away PPR and DSCR soon so don't run with user values. */\n\tmfspr\tr31, SPRN_PPR\n\tHMT_MEDIUM\n\tmfspr\tr30, SPRN_DSCR\n\tld\tr29, HSTATE_DSCR(r13)\n\tmtspr\tSPRN_DSCR, r29\n\n\t/* Save all but r9, r13 & r29-r31 */\n\treg = 0\n\t.rept\t29\n\t.if (reg != 9) && (reg != 13)\n\tstd\treg, VCPU_GPRS_TM(reg)(r9)\n\t.endif\n\treg = reg + 1\n\t.endr\n\t/* ... now save r13 */\n\tGET_SCRATCH0(r4)\n\tstd\tr4, VCPU_GPRS_TM(13)(r9)\n\t/* ... and save r9 */\n\tld\tr4, PACATMSCRATCH(r13)\n\tstd\tr4, VCPU_GPRS_TM(9)(r9)\n\n\t/* Reload stack pointer and TOC. */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr2, PACATOC(r13)\n\n\t/* Set MSR RI now we have r1 and r13 back. */\n\tli\tr5, MSR_RI\n\tmtmsrd\tr5, 1\n\n\t/* Save away checkpinted SPRs. */\n\tstd\tr31, VCPU_PPR_TM(r9)\n\tstd\tr30, VCPU_DSCR_TM(r9)\n\tmflr\tr5\n\tmfcr\tr6\n\tmfctr\tr7\n\tmfspr\tr8, SPRN_AMR\n\tmfspr\tr10, SPRN_TAR\n\tstd\tr5, VCPU_LR_TM(r9)\n\tstw\tr6, VCPU_CR_TM(r9)\n\tstd\tr7, VCPU_CTR_TM(r9)\n\tstd\tr8, VCPU_AMR_TM(r9)\n\tstd\tr10, VCPU_TAR_TM(r9)\n\n\t/* Restore r12 as trap number. */\n\tlwz\tr12, VCPU_TRAP(r9)\n\n\t/* Save FP/VSX. */\n\taddi\tr3, r9, VCPU_FPRS_TM\n\tbl\tstore_fp_state\n\taddi\tr3, r9, VCPU_VRS_TM\n\tbl\tstore_vr_state\n\tmfspr\tr6, SPRN_VRSAVE\n\tstw\tr6, VCPU_VRSAVE_TM(r9)\n1:\n\t/*\n\t * We need to save these SPRs after the treclaim so that the software\n\t * error code is recorded correctly in the TEXASR.  Also the user may\n\t * change these outside of a transaction, so they must always be\n\t * context switched.\n\t */\n\tmfspr\tr5, SPRN_TFHAR\n\tmfspr\tr6, SPRN_TFIAR\n\tmfspr\tr7, SPRN_TEXASR\n\tstd\tr5, VCPU_TFHAR(r9)\n\tstd\tr6, VCPU_TFIAR(r9)\n\tstd\tr7, VCPU_TEXASR(r9)\n\n\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n\n/*\n * Restore transactional state and TM-related registers.\n * Called with r4 pointing to the vcpu struct.\n * This potentially modifies all checkpointed registers.\n * It restores r1, r2, r4 from the PACA.\n */\nkvmppc_restore_tm:\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\n\t/* Turn on TM/FP/VSX/VMX so we can restore them. */\n\tmfmsr\tr5\n\tli\tr6, MSR_TM >> 32\n\tsldi\tr6, r6, 32\n\tor\tr5, r5, r6\n\tori\tr5, r5, MSR_FP\n\toris\tr5, r5, (MSR_VEC | MSR_VSX)@h\n\tmtmsrd\tr5\n\n\t/*\n\t * The user may change these outside of a transaction, so they must\n\t * always be context switched.\n\t */\n\tld\tr5, VCPU_TFHAR(r4)\n\tld\tr6, VCPU_TFIAR(r4)\n\tld\tr7, VCPU_TEXASR(r4)\n\tmtspr\tSPRN_TFHAR, r5\n\tmtspr\tSPRN_TFIAR, r6\n\tmtspr\tSPRN_TEXASR, r7\n\n\tld\tr5, VCPU_MSR(r4)\n\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n\tbeqlr\t\t/* TM not active in guest */\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\t/* Make sure the failure summary is set, otherwise we'll program check\n\t * when we trechkpt.  It's possible that this might have been not set\n\t * on a kvmppc_set_one_reg() call but we shouldn't let this crash the\n\t * host.\n\t */\n\toris\tr7, r7, (TEXASR_FS)@h\n\tmtspr\tSPRN_TEXASR, r7\n\n\t/*\n\t * We need to load up the checkpointed state for the guest.\n\t * We need to do this early as it will blow away any GPRs, VSRs and\n\t * some SPRs.\n\t */\n\n\tmr\tr31, r4\n\taddi\tr3, r31, VCPU_FPRS_TM\n\tbl\tload_fp_state\n\taddi\tr3, r31, VCPU_VRS_TM\n\tbl\tload_vr_state\n\tmr\tr4, r31\n\tlwz\tr7, VCPU_VRSAVE_TM(r4)\n\tmtspr\tSPRN_VRSAVE, r7\n\n\tld\tr5, VCPU_LR_TM(r4)\n\tlwz\tr6, VCPU_CR_TM(r4)\n\tld\tr7, VCPU_CTR_TM(r4)\n\tld\tr8, VCPU_AMR_TM(r4)\n\tld\tr9, VCPU_TAR_TM(r4)\n\tmtlr\tr5\n\tmtcr\tr6\n\tmtctr\tr7\n\tmtspr\tSPRN_AMR, r8\n\tmtspr\tSPRN_TAR, r9\n\n\t/*\n\t * Load up PPR and DSCR values but don't put them in the actual SPRs\n\t * till the last moment to avoid running with userspace PPR and DSCR for\n\t * too long.\n\t */\n\tld\tr29, VCPU_DSCR_TM(r4)\n\tld\tr30, VCPU_PPR_TM(r4)\n\n\tstd\tr2, PACATMSCRATCH(r13) /* Save TOC */\n\n\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n\tli\tr5, 0\n\tmtmsrd\tr5, 1\n\n\t/* Load GPRs r0-r28 */\n\treg = 0\n\t.rept\t29\n\tld\treg, VCPU_GPRS_TM(reg)(r31)\n\treg = reg + 1\n\t.endr\n\n\tmtspr\tSPRN_DSCR, r29\n\tmtspr\tSPRN_PPR, r30\n\n\t/* Load final GPRs */\n\tld\t29, VCPU_GPRS_TM(29)(r31)\n\tld\t30, VCPU_GPRS_TM(30)(r31)\n\tld\t31, VCPU_GPRS_TM(31)(r31)\n\n\t/* TM checkpointed state is now setup.  All GPRs are now volatile. */\n\tTRECHKPT\n\n\t/* Now let's get back the state we need. */\n\tHMT_MEDIUM\n\tGET_PACA(r13)\n\tld\tr29, HSTATE_DSCR(r13)\n\tmtspr\tSPRN_DSCR, r29\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr2, PACATMSCRATCH(r13)\n\n\t/* Set the MSR RI since we have our registers back. */\n\tli\tr5, MSR_RI\n\tmtmsrd\tr5, 1\n\n\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n#endif\n\n/*\n * We come here if we get any exception or interrupt while we are\n * executing host real mode code while in guest MMU context.\n * For now just spin, but we should do something better.\n */\nkvmppc_bad_host_intr:\n\tb\t.\n\n/*\n * This mimics the MSR transition on IRQ delivery.  The new guest MSR is taken\n * from VCPU_INTR_MSR and is modified based on the required TM state changes.\n *   r11 has the guest MSR value (in/out)\n *   r9 has a vcpu pointer (in)\n *   r0 is used as a scratch register\n */\nkvmppc_msr_interrupt:\n\trldicl\tr0, r11, 64 - MSR_TS_S_LG, 62\n\tcmpwi\tr0, 2 /* Check if we are in transactional state..  */\n\tld\tr11, VCPU_INTR_MSR(r9)\n\tbne\t1f\n\t/* ... if transactional, change to suspended */\n\tli\tr0, 1\n1:\trldimi\tr11, r0, MSR_TS_S_LG, 63 - MSR_TS_T_LG\n\tblr\n\n/*\n * This works around a hardware bug on POWER8E processors, where\n * writing a 1 to the MMCR0[PMAO] bit doesn't generate a\n * performance monitor interrupt.  Instead, when we need to have\n * an interrupt pending, we have to arrange for a counter to overflow.\n */\nkvmppc_fix_pmao:\n\tli\tr3, 0\n\tmtspr\tSPRN_MMCR2, r3\n\tlis\tr3, (MMCR0_PMXE | MMCR0_FCECE)@h\n\tori\tr3, r3, MMCR0_PMCjCE | MMCR0_C56RUN\n\tmtspr\tSPRN_MMCR0, r3\n\tlis\tr3, 0x7fff\n\tori\tr3, r3, 0xffff\n\tmtspr\tSPRN_PMC6, r3\n\tisync\n\tblr\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n/*\n * Start timing an activity\n * r3 = pointer to time accumulation struct, r4 = vcpu\n */\nkvmhv_start_timing:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr6, VCORE_IN_GUEST(r5)\n\tcmpwi\tr6, 0\n\tbeq\t5f\t\t\t\t/* if in guest, need to */\n\tld\tr6, VCORE_TB_OFFSET(r5)\t\t/* subtract timebase offset */\n5:\tmftb\tr5\n\tsubf\tr5, r6, r5\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tstd\tr5, VCPU_ACTIVITY_START(r4)\n\tblr\n\n/*\n * Accumulate time to one activity and start another.\n * r3 = pointer to new time accumulation struct, r4 = vcpu\n */\nkvmhv_accumulate_time:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr8, VCORE_IN_GUEST(r5)\n\tcmpwi\tr8, 0\n\tbeq\t4f\t\t\t\t/* if in guest, need to */\n\tld\tr8, VCORE_TB_OFFSET(r5)\t\t/* subtract timebase offset */\n4:\tld\tr5, VCPU_CUR_ACTIVITY(r4)\n\tld\tr6, VCPU_ACTIVITY_START(r4)\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tmftb\tr7\n\tsubf\tr7, r8, r7\n\tstd\tr7, VCPU_ACTIVITY_START(r4)\n\tcmpdi\tr5, 0\n\tbeqlr\n\tsubf\tr3, r6, r7\n\tld\tr8, TAS_SEQCOUNT(r5)\n\tcmpdi\tr8, 0\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tlwsync\n\tld\tr7, TAS_TOTAL(r5)\n\tadd\tr7, r7, r3\n\tstd\tr7, TAS_TOTAL(r5)\n\tld\tr6, TAS_MIN(r5)\n\tld\tr7, TAS_MAX(r5)\n\tbeq\t3f\n\tcmpd\tr3, r6\n\tbge\t1f\n3:\tstd\tr3, TAS_MIN(r5)\n1:\tcmpd\tr3, r7\n\tble\t2f\n\tstd\tr3, TAS_MAX(r5)\n2:\tlwsync\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tblr\n#endif\n", "code_before": "/* SPDX-License-Identifier: GPL-2.0-only */\n/*\n *\n * Copyright 2011 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * Derived from book3s_rmhandlers.S and other files, which are:\n *\n * Copyright SUSE Linux Products GmbH 2009\n *\n * Authors: Alexander Graf <agraf@suse.de>\n */\n\n#include <linux/export.h>\n#include <linux/linkage.h>\n#include <linux/objtool.h>\n#include <asm/ppc_asm.h>\n#include <asm/code-patching-asm.h>\n#include <asm/kvm_asm.h>\n#include <asm/reg.h>\n#include <asm/mmu.h>\n#include <asm/page.h>\n#include <asm/ptrace.h>\n#include <asm/hvcall.h>\n#include <asm/asm-offsets.h>\n#include <asm/exception-64s.h>\n#include <asm/kvm_book3s_asm.h>\n#include <asm/book3s/64/mmu-hash.h>\n#include <asm/tm.h>\n#include <asm/opal.h>\n#include <asm/thread_info.h>\n#include <asm/asm-compat.h>\n#include <asm/feature-fixups.h>\n#include <asm/cpuidle.h>\n\n/* Values in HSTATE_NAPPING(r13) */\n#define NAPPING_CEDE\t1\n#define NAPPING_NOVCPU\t2\n#define NAPPING_UNSPLIT\t3\n\n/* Stack frame offsets for kvmppc_hv_entry */\n#define SFS\t\t\t160\n#define STACK_SLOT_TRAP\t\t(SFS-4)\n#define STACK_SLOT_TID\t\t(SFS-16)\n#define STACK_SLOT_PSSCR\t(SFS-24)\n#define STACK_SLOT_PID\t\t(SFS-32)\n#define STACK_SLOT_IAMR\t\t(SFS-40)\n#define STACK_SLOT_CIABR\t(SFS-48)\n#define STACK_SLOT_DAWR0\t(SFS-56)\n#define STACK_SLOT_DAWRX0\t(SFS-64)\n#define STACK_SLOT_HFSCR\t(SFS-72)\n#define STACK_SLOT_AMR\t\t(SFS-80)\n#define STACK_SLOT_UAMOR\t(SFS-88)\n#define STACK_SLOT_FSCR\t\t(SFS-96)\n\n/*\n * Use the last LPID (all implemented LPID bits = 1) for partition switching.\n * This is reserved in the LPID allocator. POWER7 only implements 0x3ff, but\n * we write 0xfff into the LPID SPR anyway, which seems to work and just\n * ignores the top bits.\n */\n#define   LPID_RSVD\t\t0xfff\n\n/*\n * Call kvmppc_hv_entry in real mode.\n * Must be called with interrupts hard-disabled.\n *\n * Input Registers:\n *\n * LR = return address to continue at after eventually re-enabling MMU\n */\n_GLOBAL_TOC(kvmppc_hv_entry_trampoline)\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -112(r1)\n\tmfmsr\tr10\n\tstd\tr10, HSTATE_HOST_MSR(r13)\n\tLOAD_REG_ADDR(r5, kvmppc_call_hv_entry)\n\tli\tr0,MSR_RI\n\tandc\tr0,r10,r0\n\tli\tr6,MSR_IR | MSR_DR\n\tandc\tr6,r10,r6\n\tmtmsrd\tr0,1\t\t/* clear RI in MSR */\n\tmtsrr0\tr5\n\tmtsrr1\tr6\n\tRFI_TO_KERNEL\n\nkvmppc_call_hv_entry:\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from guest - restore host state and return to caller */\n\nBEGIN_FTR_SECTION\n\t/* Restore host DABR and DABRX */\n\tld\tr5,HSTATE_DABR(r13)\n\tli\tr6,7\n\tmtspr\tSPRN_DABR,r5\n\tmtspr\tSPRN_DABRX,r6\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n\t/* Restore SPRG3 */\n\tld\tr3,PACA_SPRG_VDSO(r13)\n\tmtspr\tSPRN_SPRG_VDSO_WRITE,r3\n\n\t/* Reload the host's PMU registers */\n\tbl\tkvmhv_load_host_pmu\n\n\t/*\n\t * Reload DEC.  HDEC interrupts were disabled when\n\t * we reloaded the host's LPCR value.\n\t */\n\tld\tr3, HSTATE_DECEXP(r13)\n\tmftb\tr4\n\tsubf\tr4, r4, r3\n\tmtspr\tSPRN_DEC, r4\n\n\t/* hwthread_req may have got set by cede or no vcpu, so clear it */\n\tli\tr0, 0\n\tstb\tr0, HSTATE_HWTHREAD_REQ(r13)\n\n\t/*\n\t * For external interrupts we need to call the Linux\n\t * handler to process the interrupt. We do that by jumping\n\t * to absolute address 0x500 for external interrupts.\n\t * The [h]rfid at the end of the handler will return to\n\t * the book3s_hv_interrupts.S code. For other interrupts\n\t * we do the rfid to get back to the book3s_hv_interrupts.S\n\t * code here.\n\t */\n\tld\tr8, 112+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, 112\n\tld\tr7, HSTATE_HOST_MSR(r13)\n\n\t/* Return the trap number on this thread as the return value */\n\tmr\tr3, r12\n\n\t/* RFI into the highmem handler */\n\tmfmsr\tr6\n\tli\tr0, MSR_RI\n\tandc\tr6, r6, r0\n\tmtmsrd\tr6, 1\t\t\t/* Clear RI in MSR */\n\tmtsrr0\tr8\n\tmtsrr1\tr7\n\tRFI_TO_KERNEL\n\nkvmppc_primary_no_guest:\n\t/* We handle this much like a ceded vcpu */\n\t/* put the HDEC into the DEC, since HDEC interrupts don't wake us */\n\t/* HDEC may be larger than DEC for arch >= v3.00, but since the */\n\t/* HDEC value came from DEC in the first place, it will fit */\n\tmfspr\tr3, SPRN_HDEC\n\tmtspr\tSPRN_DEC, r3\n\t/*\n\t * Make sure the primary has finished the MMU switch.\n\t * We should never get here on a secondary thread, but\n\t * check it for robustness' sake.\n\t */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n65:\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t65b\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\t/* set our bit in napping_threads */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n1:\tlwarx\tr3, 0, r6\n\tor\tr3, r3, r0\n\tstwcx.\tr3, 0, r6\n\tbne\t1b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr12, 0\n\tlwz\tr7, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7, 0x100\n\tbge\tkvm_novcpu_exit\t/* another thread already exiting */\n\tli\tr3, NAPPING_NOVCPU\n\tstb\tr3, HSTATE_NAPPING(r13)\n\n\tli\tr3, 0\t\t/* Don't wake on privileged (OS) doorbell */\n\tb\tkvm_do_nap\n\n/*\n * kvm_novcpu_wakeup\n *\tEntered from kvm_start_guest if kvm_hstate.napping is set\n *\tto NAPPING_NOVCPU\n *\t\tr2 = kernel TOC\n *\t\tr13 = paca\n */\nkvm_novcpu_wakeup:\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tli\tr0, 0\n\tstb\tr0, HSTATE_NAPPING(r13)\n\n\t/* check the wake reason */\n\tbl\tkvmppc_check_wake_reason\n\n\t/*\n\t * Restore volatile registers since we could have called\n\t * a C routine in kvmppc_check_wake_reason.\n\t *\tr5 = VCORE\n\t */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\n\t/* see if any other thread is already exiting */\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tbge\tkvm_novcpu_exit\n\n\t/* clear our bit in napping_threads */\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n4:\tlwarx\tr7, 0, r6\n\tandc\tr7, r7, r0\n\tstwcx.\tr7, 0, r6\n\tbne\t4b\n\n\t/* See if the wake reason means we need to exit */\n\tcmpdi\tr3, 0\n\tbge\tkvm_novcpu_exit\n\n\t/* See if our timeslice has expired (HDEC is negative) */\n\tmfspr\tr0, SPRN_HDEC\n\textsw\tr0, r0\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n\tcmpdi\tr0, 0\n\tblt\tkvm_novcpu_exit\n\n\t/* Got an IPI but other vcpus aren't yet exiting, must be a latecomer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n#endif\n\tb\tkvmppc_got_guest\n\nkvm_novcpu_exit:\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\t13f\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n13:\tmr\tr3, r12\n\tstw\tr12, STACK_SLOT_TRAP(r1)\n\tbl\tkvmhv_commence_exit\n\tnop\n\tb\tkvmhv_switch_to_host\n\n/*\n * We come in here when wakened from Linux offline idle code.\n * Relocation is off\n * r3 contains the SRR1 wakeup value, SRR1 is trashed.\n */\n_GLOBAL(idle_kvm_start_guest)\n\tmfcr\tr5\n\tmflr\tr0\n\tstd\tr5, 8(r1)\t// Save CR in caller's frame\n\tstd\tr0, 16(r1)\t// Save LR in caller's frame\n\t// Create frame on emergency stack\n\tld\tr4, PACAEMERGSP(r13)\n\tstdu\tr1, -SWITCH_FRAME_SIZE(r4)\n\t// Switch to new frame on emergency stack\n\tmr\tr1, r4\n\tstd\tr3, 32(r1)\t// Save SRR1 wakeup value\n\tSAVE_NVGPRS(r1)\n\n\t/*\n\t * Could avoid this and pass it through in r3. For now,\n\t * code expects it to be in SRR1.\n\t */\n\tmtspr\tSPRN_SRR1,r3\n\n\tli\tr0,0\n\tstb\tr0,PACA_FTRACE_ENABLED(r13)\n\n\tli\tr0,KVM_HWTHREAD_IN_KVM\n\tstb\tr0,HSTATE_HWTHREAD_STATE(r13)\n\n\t/* kvm cede / napping does not come through here */\n\tlbz\tr0,HSTATE_NAPPING(r13)\n\ttwnei\tr0,0\n\n\tb\t1f\n\nkvm_unsplit_wakeup:\n\tli\tr0, 0\n\tstb\tr0, HSTATE_NAPPING(r13)\n\n1:\n\n\t/*\n\t * We weren't napping due to cede, so this must be a secondary\n\t * thread being woken up to run a guest, or being woken up due\n\t * to a stray IPI.  (Or due to some machine check or hypervisor\n\t * maintenance interrupt while the core is in KVM.)\n\t */\n\n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\t/*\n\t * kvmppc_check_wake_reason could invoke a C routine, but we\n\t * have no volatile registers to restore when we return.\n\t */\n\n\tcmpdi\tr3, 0\n\tbge\tkvm_no_guest\n\n\t/* get vcore pointer, NULL if we have nothing to run */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5,0\n\t/* if we have no vcore to run, go back to sleep */\n\tbeq\tkvm_no_guest\n\nkvm_secondary_got_guest:\n\n\t// About to go to guest, clear saved SRR1\n\tli\tr0, 0\n\tstd\tr0, 32(r1)\n\n\t/* Set HSTATE_DSCR(r13) to something sensible */\n\tld\tr6, PACA_DSCR_DEFAULT(r13)\n\tstd\tr6, HSTATE_DSCR(r13)\n\n\t/* On thread 0 of a subcore, set HDEC to max */\n\tlbz\tr4, HSTATE_PTID(r13)\n\tcmpwi\tr4, 0\n\tbne\t63f\n\tlis\tr6,0x7fff\t\t/* MAX_INT@h */\n\tmtspr\tSPRN_HDEC, r6\n\t/* and set per-LPAR registers, if doing dynamic micro-threading */\n\tld\tr6, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr6, 0\n\tbeq\t63f\n\tld\tr0, KVM_SPLIT_RPR(r6)\n\tmtspr\tSPRN_RPR, r0\n\tld\tr0, KVM_SPLIT_PMMAR(r6)\n\tmtspr\tSPRN_PMMAR, r0\n\tld\tr0, KVM_SPLIT_LDBAR(r6)\n\tmtspr\tSPRN_LDBAR, r0\n\tisync\n63:\n\t/* Order load of vcpu after load of vcore */\n\tlwsync\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from the guest, go back to nap */\n\t/* Clear our vcpu and vcore pointers so we don't come back in early */\n\tli\tr0, 0\n\tstd\tr0, HSTATE_KVM_VCPU(r13)\n\t/*\n\t * Once we clear HSTATE_KVM_VCORE(r13), the code in\n\t * kvmppc_run_core() is going to assume that all our vcpu\n\t * state is visible in memory.  This lwsync makes sure\n\t * that that is true.\n\t */\n\tlwsync\n\tstd\tr0, HSTATE_KVM_VCORE(r13)\n\n\t/*\n\t * All secondaries exiting guest will fall through this path.\n\t * Before proceeding, just check for HMI interrupt and\n\t * invoke opal hmi handler. By now we are sure that the\n\t * primary thread on this core/subcore has already made partition\n\t * switch/TB resync and we are good to call opal hmi handler.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\tkvm_no_guest\n\n\tli\tr3,0\t\t\t/* NULL argument */\n\tbl\tCFUNC(hmi_exception_realmode)\n/*\n * At this point we have finished executing in the guest.\n * We need to wait for hwthread_req to become zero, since\n * we may not turn on the MMU while hwthread_req is non-zero.\n * While waiting we also need to check if we get given a vcpu to run.\n */\nkvm_no_guest:\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t53f\n\tHMT_MEDIUM\n\tli\tr0, KVM_HWTHREAD_IN_KERNEL\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\t/* need to recheck hwthread_req after a barrier, to avoid race */\n\tsync\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t54f\n\n\t/*\n\t * Jump to idle_return_gpr_loss, which returns to the\n\t * idle_kvm_start_guest caller.\n\t */\n\tli\tr3, LPCR_PECE0\n\tmfspr\tr4, SPRN_LPCR\n\trlwimi\tr4, r3, 0, LPCR_PECE0 | LPCR_PECE1\n\tmtspr\tSPRN_LPCR, r4\n\t// Return SRR1 wakeup value, or 0 if we went into the guest\n\tld\tr3, 32(r1)\n\tREST_NVGPRS(r1)\n\tld\tr1, 0(r1)\t// Switch back to caller stack\n\tld\tr0, 16(r1)\t// Reload LR\n\tld\tr5, 8(r1)\t// Reload CR\n\tmtlr\tr0\n\tmtcr\tr5\n\tblr\n\n53:\n\tHMT_LOW\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5, 0\n\tbne\t60f\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr3, 0\n\tbeq\tkvm_no_guest\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\tkvm_no_guest\n\tHMT_MEDIUM\n\tb\tkvm_unsplit_nap\n60:\tHMT_MEDIUM\n\tb\tkvm_secondary_got_guest\n\n54:\tli\tr0, KVM_HWTHREAD_IN_KVM\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\tb\tkvm_no_guest\n\n/*\n * Here the primary thread is trying to return the core to\n * whole-core mode, so we need to nap.\n */\nkvm_unsplit_nap:\n\t/*\n\t * When secondaries are napping in kvm_unsplit_nap() with\n\t * hwthread_req = 1, HMI goes ignored even though subcores are\n\t * already exited the guest. Hence HMI keeps waking up secondaries\n\t * from nap in a loop and secondaries always go back to nap since\n\t * no vcore is assigned to them. This makes impossible for primary\n\t * thread to get hold of secondary threads resulting into a soft\n\t * lockup in KVM path.\n\t *\n\t * Let us check if HMI is pending and handle it before we go to nap.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\t55f\n\tli\tr3, 0\t\t\t/* NULL argument */\n\tbl\tCFUNC(hmi_exception_realmode)\n55:\n\t/*\n\t * Ensure that secondary doesn't nap when it has\n\t * its vcore pointer set.\n\t */\n\tsync\t\t/* matches smp_mb() before setting split_info.do_nap */\n\tld\tr0, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr0, 0\n\tbne\tkvm_no_guest\n\t/* clear any pending message */\nBEGIN_FTR_SECTION\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\t/* Set kvm_split_mode.napped[tid] = 1 */\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tli\tr0, 1\n\tlhz\tr4, PACAPACAINDEX(r13)\n\tclrldi\tr4, r4, 61\t/* micro-threading => P8 => 8 threads/core */\n\taddi\tr4, r4, KVM_SPLIT_NAPPED\n\tstbx\tr0, r3, r4\n\t/* Check the do_nap flag again after setting napped[] */\n\tsync\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\t57f\n\tli\tr3, NAPPING_UNSPLIT\n\tstb\tr3, HSTATE_NAPPING(r13)\n\tli\tr3, (LPCR_PECEDH | LPCR_PECE0) >> 4\n\tmfspr\tr5, SPRN_LPCR\n\trlwimi\tr5, r3, 4, (LPCR_PECEDP | LPCR_PECEDH | LPCR_PECE0 | LPCR_PECE1)\n\tb\tkvm_nap_sequence\n\n57:\tli\tr0, 0\n\tstbx\tr0, r3, r4\n\tb\tkvm_no_guest\n\n/******************************************************************************\n *                                                                            *\n *                               Entry code                                   *\n *                                                                            *\n *****************************************************************************/\n\nSYM_CODE_START_LOCAL(kvmppc_hv_entry)\n\n\t/* Required state:\n\t *\n\t * R4 = vcpu pointer (or NULL)\n\t * MSR = ~IR|DR\n\t * R13 = PACA\n\t * R1 = host R1\n\t * R2 = TOC\n\t * all other volatile GPRS = free\n\t * Does not preserve non-volatile GPRs or CR fields\n\t */\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -SFS(r1)\n\n\t/* Save R1 in the PACA */\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\tli\tr6, KVM_GUEST_MODE_HOST_HV\n\tstb\tr6, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\t/* Store initial timestamp */\n\tcmpdi\tr4, 0\n\tbeq\t1f\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n1:\n#endif\n\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr9, VCORE_KVM(r5)\t/* pointer to struct kvm */\n\n\t/*\n\t * POWER7/POWER8 host -> guest partition switch code.\n\t * We don't have to lock against concurrent tlbies,\n\t * but we do have to coordinate across hardware threads.\n\t */\n\t/* Set bit in entry map iff exit map is zero. */\n\tli\tr7, 1\n\tlbz\tr6, HSTATE_PTID(r13)\n\tsld\tr7, r7, r6\n\taddi\tr8, r5, VCORE_ENTRY_EXIT\n21:\tlwarx\tr3, 0, r8\n\tcmpwi\tr3, 0x100\t\t/* any threads starting to exit? */\n\tbge\tsecondary_too_late\t/* if so we're too late to the party */\n\tor\tr3, r3, r7\n\tstwcx.\tr3, 0, r8\n\tbne\t21b\n\n\t/* Primary thread switches to guest partition. */\n\tcmpwi\tr6,0\n\tbne\t10f\n\n\tlwz\tr7,KVM_LPID(r9)\n\tld\tr6,KVM_SDR1(r9)\n\tli\tr0,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r0\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to partition page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\n\t/* See if we need to flush the TLB. */\n\tmr\tr3, r9\t\t\t/* kvm pointer */\n\tlhz\tr4, PACAPACAINDEX(r13)\t/* physical cpu number */\n\tli\tr5, 0\t\t\t/* nested vcpu pointer */\n\tbl\tkvmppc_check_need_tlb_flush\n\tnop\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\n\t/* Add timebase offset onto timebase */\n22:\tld\tr8,VCORE_TB_OFFSET(r5)\n\tcmpdi\tr8,0\n\tbeq\t37f\n\tstd\tr8, VCORE_TB_OFFSET_APPL(r5)\n\tmftb\tr6\t\t/* current host timebase */\n\tadd\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t/* update upper 40 bits */\n\tmftb\tr7\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t37f\n\taddis\tr8,r8,0x100\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n\t/* Load guest PCR value to select appropriate compat mode */\n37:\tld\tr7, VCORE_PCR(r5)\n\tLOAD_REG_IMMEDIATE(r6, PCR_MASK)\n\tcmpld\tr7, r6\n\tbeq\t38f\n\tor\tr7, r7, r6\n\tmtspr\tSPRN_PCR, r7\n38:\n\nBEGIN_FTR_SECTION\n\t/* DPDES and VTB are shared between threads */\n\tld\tr8, VCORE_DPDES(r5)\n\tld\tr7, VCORE_VTB(r5)\n\tmtspr\tSPRN_DPDES, r8\n\tmtspr\tSPRN_VTB, r7\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* Mark the subcore state as inside guest */\n\tbl\tkvmppc_subcore_enter_guest\n\tnop\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tli\tr0,1\n\tstb\tr0,VCORE_IN_GUEST(r5)\t/* signal secondaries to continue */\n\n\t/* Do we have a guest vcpu to run? */\n10:\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\nkvmppc_got_guest:\n\t/* Increment yield count if they have a VPA */\n\tld\tr3, VCPU_VPA(r4)\n\tcmpdi\tr3, 0\n\tbeq\t25f\n\tli\tr6, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr5, r3, r6\n\taddi\tr5, r5, 1\n\tSTWX_BE\tr5, r3, r6\n\tli\tr6, 1\n\tstb\tr6, VCPU_VPA_DIRTY(r4)\n25:\n\n\t/* Save purr/spurr */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tstd\tr5,HSTATE_PURR(r13)\n\tstd\tr6,HSTATE_SPURR(r13)\n\tld\tr7,VCPU_PURR(r4)\n\tld\tr8,VCPU_SPURR(r4)\n\tmtspr\tSPRN_PURR,r7\n\tmtspr\tSPRN_SPURR,r8\n\n\t/* Save host values of some registers */\nBEGIN_FTR_SECTION\n\tmfspr\tr5, SPRN_CIABR\n\tmfspr\tr6, SPRN_DAWR0\n\tmfspr\tr7, SPRN_DAWRX0\n\tmfspr\tr8, SPRN_IAMR\n\tstd\tr5, STACK_SLOT_CIABR(r1)\n\tstd\tr6, STACK_SLOT_DAWR0(r1)\n\tstd\tr7, STACK_SLOT_DAWRX0(r1)\n\tstd\tr8, STACK_SLOT_IAMR(r1)\n\tmfspr\tr5, SPRN_FSCR\n\tstd\tr5, STACK_SLOT_FSCR(r1)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\tmfspr\tr5, SPRN_AMR\n\tstd\tr5, STACK_SLOT_AMR(r1)\n\tmfspr\tr6, SPRN_UAMOR\n\tstd\tr6, STACK_SLOT_UAMOR(r1)\n\nBEGIN_FTR_SECTION\n\t/* Set partition DABR */\n\t/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */\n\tlwz\tr5,VCPU_DABRX(r4)\n\tld\tr6,VCPU_DABR(r4)\n\tmtspr\tSPRN_DABRX,r5\n\tmtspr\tSPRN_DABR,r6\n\tisync\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tmr      r3, r4\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_restore_tm_hv\n\tnop\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n91:\n#endif\n\n\t/* Load guest PMU registers; r4 = vcpu pointer here */\n\tmr\tr3, r4\n\tbl\tkvmhv_load_guest_pmu\n\n\t/* Load up FP, VMX and VSX registers */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_load_fp\n\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n\n\t/* Switch DSCR to guest value */\n\tld\tr5, VCPU_DSCR(r4)\n\tmtspr\tSPRN_DSCR, r5\n\nBEGIN_FTR_SECTION\n\t/* Skip next section on POWER7 */\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Load up POWER8-specific registers */\n\tld\tr5, VCPU_IAMR(r4)\n\tlwz\tr6, VCPU_PSPB(r4)\n\tld\tr7, VCPU_FSCR(r4)\n\tmtspr\tSPRN_IAMR, r5\n\tmtspr\tSPRN_PSPB, r6\n\tmtspr\tSPRN_FSCR, r7\n\t/*\n\t * Handle broken DAWR case by not writing it. This means we\n\t * can still store the DAWR register for migration.\n\t */\n\tLOAD_REG_ADDR(r5, dawr_force_enable)\n\tlbz\tr5, 0(r5)\n\tcmpdi\tr5, 0\n\tbeq\t1f\n\tld\tr5, VCPU_DAWR0(r4)\n\tld\tr6, VCPU_DAWRX0(r4)\n\tmtspr\tSPRN_DAWR0, r5\n\tmtspr\tSPRN_DAWRX0, r6\n1:\n\tld\tr7, VCPU_CIABR(r4)\n\tld\tr8, VCPU_TAR(r4)\n\tmtspr\tSPRN_CIABR, r7\n\tmtspr\tSPRN_TAR, r8\n\tld\tr5, VCPU_IC(r4)\n\tld\tr8, VCPU_EBBHR(r4)\n\tmtspr\tSPRN_IC, r5\n\tmtspr\tSPRN_EBBHR, r8\n\tld\tr5, VCPU_EBBRR(r4)\n\tld\tr6, VCPU_BESCR(r4)\n\tlwz\tr7, VCPU_GUEST_PID(r4)\n\tld\tr8, VCPU_WORT(r4)\n\tmtspr\tSPRN_EBBRR, r5\n\tmtspr\tSPRN_BESCR, r6\n\tmtspr\tSPRN_PID, r7\n\tmtspr\tSPRN_WORT, r8\n\t/* POWER8-only registers */\n\tld\tr5, VCPU_TCSCR(r4)\n\tld\tr6, VCPU_ACOP(r4)\n\tld\tr7, VCPU_CSIGR(r4)\n\tld\tr8, VCPU_TACR(r4)\n\tmtspr\tSPRN_TCSCR, r5\n\tmtspr\tSPRN_ACOP, r6\n\tmtspr\tSPRN_CSIGR, r7\n\tmtspr\tSPRN_TACR, r8\n\tnop\n8:\n\n\tld\tr5, VCPU_SPRG0(r4)\n\tld\tr6, VCPU_SPRG1(r4)\n\tld\tr7, VCPU_SPRG2(r4)\n\tld\tr8, VCPU_SPRG3(r4)\n\tmtspr\tSPRN_SPRG0, r5\n\tmtspr\tSPRN_SPRG1, r6\n\tmtspr\tSPRN_SPRG2, r7\n\tmtspr\tSPRN_SPRG3, r8\n\n\t/* Load up DAR and DSISR */\n\tld\tr5, VCPU_DAR(r4)\n\tlwz\tr6, VCPU_DSISR(r4)\n\tmtspr\tSPRN_DAR, r5\n\tmtspr\tSPRN_DSISR, r6\n\n\t/* Restore AMR and UAMOR, set AMOR to all 1s */\n\tld\tr5,VCPU_AMR(r4)\n\tld\tr6,VCPU_UAMOR(r4)\n\tmtspr\tSPRN_AMR,r5\n\tmtspr\tSPRN_UAMOR,r6\n\n\t/* Restore state of CTRL run bit; the host currently has it set to 1 */\n\tlwz\tr5,VCPU_CTRL(r4)\n\tandi.\tr5,r5,1\n\tbne\t4f\n\tli\tr6,0\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/* Secondary threads wait for primary to have done partition switch */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr6, HSTATE_PTID(r13)\n\tcmpwi\tr6, 0\n\tbeq\t21f\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbne\t21f\n\tHMT_LOW\n20:\tlwz\tr3, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr3, 0x100\n\tbge\tno_switch_exit\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t20b\n\tHMT_MEDIUM\n21:\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n\t/*\n\t * Set the decrementer to the guest decrementer.\n\t */\n\tld\tr8,VCPU_DEC_EXPIRES(r4)\n\tmftb\tr7\n\tsubf\tr3,r7,r8\n\tmtspr\tSPRN_DEC,r3\n\n\t/* Check if HDEC expires soon */\n\tmfspr\tr3, SPRN_HDEC\n\textsw\tr3, r3\n\tcmpdi\tr3, 512\t\t/* 1 microsecond */\n\tblt\thdec_soon\n\n\t/* Clear out and reload the SLB */\n\tli\tr6, 0\n\tslbmte\tr6, r6\n\tPPC_SLBIA(6)\n\tptesync\n\n\t/* Load up guest SLB entries (N.B. slb_max will be 0 for radix) */\n\tlwz\tr5,VCPU_SLB_MAX(r4)\n\tcmpwi\tr5,0\n\tbeq\t9f\n\tmtctr\tr5\n\taddi\tr6,r4,VCPU_SLB\n1:\tld\tr8,VCPU_SLB_E(r6)\n\tld\tr9,VCPU_SLB_V(r6)\n\tslbmte\tr9,r8\n\taddi\tr6,r6,VCPU_SLB_SIZE\n\tbdnz\t1b\n9:\n\ndeliver_guest_interrupt:\t/* r4 = vcpu, r13 = paca */\n\t/* Check if we can deliver an external or decrementer interrupt now */\n\tld\tr0, VCPU_PENDING_EXC(r4)\n\tcmpdi\tr0, 0\n\tbeq\t71f\n\tmr\tr3, r4\n\tbl\tCFUNC(kvmppc_guest_entry_inject_int)\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n71:\n\tld\tr6, VCPU_SRR0(r4)\n\tld\tr7, VCPU_SRR1(r4)\n\tmtspr\tSPRN_SRR0, r6\n\tmtspr\tSPRN_SRR1, r7\n\n\tld\tr10, VCPU_PC(r4)\n\tld\tr11, VCPU_MSR(r4)\n\t/* r11 = vcpu->arch.msr & ~MSR_HV */\n\trldicl\tr11, r11, 63 - MSR_HV_LG, 1\n\trotldi\tr11, r11, 1 + MSR_HV_LG\n\tori\tr11, r11, MSR_ME\n\n\tld\tr6, VCPU_CTR(r4)\n\tld\tr7, VCPU_XER(r4)\n\tmtctr\tr6\n\tmtxer\tr7\n\n/*\n * Required state:\n * R4 = vcpu\n * R10: value for HSRR0\n * R11: value for HSRR1\n * R13 = PACA\n */\nfast_guest_return:\n\tli\tr0,0\n\tstb\tr0,VCPU_CEDED(r4)\t/* cancel cede */\n\tmtspr\tSPRN_HSRR0,r10\n\tmtspr\tSPRN_HSRR1,r11\n\n\t/* Activate guest mode, so faults get handled by KVM */\n\tli\tr9, KVM_GUEST_MODE_GUEST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\t/* Accumulate timing */\n\taddi\tr3, r4, VCPU_TB_GUEST\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\t/* Enter guest */\n\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_CFAR(r4)\n\tmtspr\tSPRN_CFAR, r5\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr0, VCPU_PPR(r4)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\tld\tr5, VCPU_LR(r4)\n\tmtlr\tr5\n\n\tld\tr1, VCPU_GPR(R1)(r4)\n\tld\tr5, VCPU_GPR(R5)(r4)\n\tld\tr8, VCPU_GPR(R8)(r4)\n\tld\tr9, VCPU_GPR(R9)(r4)\n\tld\tr10, VCPU_GPR(R10)(r4)\n\tld\tr11, VCPU_GPR(R11)(r4)\n\tld\tr12, VCPU_GPR(R12)(r4)\n\tld\tr13, VCPU_GPR(R13)(r4)\n\nBEGIN_FTR_SECTION\n\tmtspr\tSPRN_PPR, r0\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\tld\tr6, VCPU_GPR(R6)(r4)\n\tld\tr7, VCPU_GPR(R7)(r4)\n\n\tld\tr0, VCPU_CR(r4)\n\tmtcr\tr0\n\n\tld\tr0, VCPU_GPR(R0)(r4)\n\tld\tr2, VCPU_GPR(R2)(r4)\n\tld\tr3, VCPU_GPR(R3)(r4)\n\tld\tr4, VCPU_GPR(R4)(r4)\n\tHRFI_TO_GUEST\n\tb\t.\nSYM_CODE_END(kvmppc_hv_entry)\n\nsecondary_too_late:\n\tli\tr12, 0\n\tstw\tr12, STACK_SLOT_TRAP(r1)\n\tcmpdi\tr4, 0\n\tbeq\t11f\n\tstw\tr12, VCPU_TRAP(r4)\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n11:\tb\tkvmhv_switch_to_host\n\nno_switch_exit:\n\tHMT_MEDIUM\n\tli\tr12, 0\n\tb\t12f\nhdec_soon:\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n12:\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n\tb\tguest_bypass\n\n/******************************************************************************\n *                                                                            *\n *                               Exit code                                    *\n *                                                                            *\n *****************************************************************************/\n\n/*\n * We come here from the first-level interrupt handlers.\n */\n\t.globl\tkvmppc_interrupt_hv\nkvmppc_interrupt_hv:\n\t/*\n\t * Register contents:\n\t * R9\t\t= HSTATE_IN_GUEST\n\t * R12\t\t= (guest CR << 32) | interrupt vector\n\t * R13\t\t= PACA\n\t * guest R12 saved in shadow VCPU SCRATCH0\n\t * guest R13 saved in SPRN_SCRATCH0\n\t * guest R9 saved in HSTATE_SCRATCH2\n\t */\n\t/* We're now back in the host but in guest MMU context */\n\tcmpwi\tr9,KVM_GUEST_MODE_HOST_HV\n\tbeq\tkvmppc_bad_host_intr\n\tli\tr9, KVM_GUEST_MODE_HOST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Save registers */\n\n\tstd\tr0, VCPU_GPR(R0)(r9)\n\tstd\tr1, VCPU_GPR(R1)(r9)\n\tstd\tr2, VCPU_GPR(R2)(r9)\n\tstd\tr3, VCPU_GPR(R3)(r9)\n\tstd\tr4, VCPU_GPR(R4)(r9)\n\tstd\tr5, VCPU_GPR(R5)(r9)\n\tstd\tr6, VCPU_GPR(R6)(r9)\n\tstd\tr7, VCPU_GPR(R7)(r9)\n\tstd\tr8, VCPU_GPR(R8)(r9)\n\tld\tr0, HSTATE_SCRATCH2(r13)\n\tstd\tr0, VCPU_GPR(R9)(r9)\n\tstd\tr10, VCPU_GPR(R10)(r9)\n\tstd\tr11, VCPU_GPR(R11)(r9)\n\tld\tr3, HSTATE_SCRATCH0(r13)\n\tstd\tr3, VCPU_GPR(R12)(r9)\n\t/* CR is in the high half of r12 */\n\tsrdi\tr4, r12, 32\n\tstd\tr4, VCPU_CR(r9)\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_CFAR(r13)\n\tstd\tr3, VCPU_CFAR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr4, HSTATE_PPR(r13)\n\tstd\tr4, VCPU_PPR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\t/* Restore R1/R2 so we can handle faults */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tLOAD_PACA_TOC()\n\n\tmfspr\tr10, SPRN_SRR0\n\tmfspr\tr11, SPRN_SRR1\n\tstd\tr10, VCPU_SRR0(r9)\n\tstd\tr11, VCPU_SRR1(r9)\n\t/* trap is in the low half of r12, clear CR from the high half */\n\tclrldi\tr12, r12, 32\n\tandi.\tr0, r12, 2\t\t/* need to read HSRR0/1? */\n\tbeq\t1f\n\tmfspr\tr10, SPRN_HSRR0\n\tmfspr\tr11, SPRN_HSRR1\n\tclrrdi\tr12, r12, 2\n1:\tstd\tr10, VCPU_PC(r9)\n\tstd\tr11, VCPU_MSR(r9)\n\n\tGET_SCRATCH0(r3)\n\tmflr\tr4\n\tstd\tr3, VCPU_GPR(R13)(r9)\n\tstd\tr4, VCPU_LR(r9)\n\n\tstw\tr12,VCPU_TRAP(r9)\n\n\t/*\n\t * Now that we have saved away SRR0/1 and HSRR0/1,\n\t * interrupts are recoverable in principle, so set MSR_RI.\n\t * This becomes important for relocation-on interrupts from\n\t * the guest, which we can get in radix mode on POWER9.\n\t */\n\tli\tr0, MSR_RI\n\tmtmsrd\tr0, 1\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r9, VCPU_TB_RMINTR\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n\tld\tr5, VCPU_GPR(R5)(r9)\n\tld\tr6, VCPU_GPR(R6)(r9)\n\tld\tr7, VCPU_GPR(R7)(r9)\n\tld\tr8, VCPU_GPR(R8)(r9)\n#endif\n\n\t/* Save HEIR (HV emulation assist reg) in emul_inst\n\t   if this is an HEI (HV emulation interrupt, e40) */\n\tli\tr3,KVM_INST_FETCH_FAILED\n\tstd\tr3,VCPU_LAST_INST(r9)\n\tcmpwi\tr12,BOOK3S_INTERRUPT_H_EMUL_ASSIST\n\tbne\t11f\n\tmfspr\tr3,SPRN_HEIR\n11:\tstd\tr3,VCPU_HEIR(r9)\n\n\t/* these are volatile across C function calls */\n\tmfctr\tr3\n\tmfxer\tr4\n\tstd\tr3, VCPU_CTR(r9)\n\tstd\tr4, VCPU_XER(r9)\n\n\t/* Save more register state  */\n\tmfdar\tr3\n\tmfdsisr\tr4\n\tstd\tr3, VCPU_DAR(r9)\n\tstw\tr4, VCPU_DSISR(r9)\n\n\t/* If this is a page table miss then see if it's theirs or ours */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tbeq\tkvmppc_hdsi\n\tstd\tr3, VCPU_FAULT_DAR(r9)\n\tstw\tr4, VCPU_FAULT_DSISR(r9)\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tbeq\tkvmppc_hisi\n\n\t/* See if this is a leftover HDEC interrupt */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_HV_DECREMENTER\n\tbne\t2f\n\tmfspr\tr3,SPRN_HDEC\n\textsw\tr3, r3\n\tcmpdi\tr3,0\n\tmr\tr4,r9\n\tbge\tfast_guest_return\n2:\n\t/* See if this is an hcall we can handle in real mode */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tbeq\thcall_try_real_mode\n\n\t/* Hypervisor doorbell - exit only if host IPI flag set */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\tbne\t3f\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbeq\tmaybe_reenter_guest\n\tb\tguest_exit_cont\n3:\n\t/* If it's a hypervisor facility unavailable interrupt, save HFSCR */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_FAC_UNAVAIL\n\tbne\t14f\n\tmfspr\tr3, SPRN_HFSCR\n\tstd\tr3, VCPU_HFSCR(r9)\n\tb\tguest_exit_cont\n14:\n\t/* External interrupt ? */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbeq\tkvmppc_guest_external\n\t/* See if it is a machine check */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tbeq\tmachine_check_realmode\n\t/* Or a hypervisor maintenance interrupt */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbeq\thmi_realmode\n\nguest_exit_cont:\t\t/* r9 = vcpu, r12 = trap, r13 = paca */\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r9, VCPU_TB_RMEXIT\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\t/*\n\t * Possibly flush the link stack here, before we do a blr in\n\t * kvmhv_switch_to_host.\n\t */\n1:\tnop\n\tpatch_site 1b patch__call_kvm_flush_link_stack\n\n\t/* For hash guest, read the guest SLB and save it away */\n\tli\tr5, 0\n\tlwz\tr0,VCPU_SLB_NR(r9)\t/* number of entries in SLB */\n\tmtctr\tr0\n\tli\tr6,0\n\taddi\tr7,r9,VCPU_SLB\n1:\tslbmfee\tr8,r6\n\tandis.\tr0,r8,SLB_ESID_V@h\n\tbeq\t2f\n\tadd\tr8,r8,r6\t\t/* put index in */\n\tslbmfev\tr3,r6\n\tstd\tr8,VCPU_SLB_E(r7)\n\tstd\tr3,VCPU_SLB_V(r7)\n\taddi\tr7,r7,VCPU_SLB_SIZE\n\taddi\tr5,r5,1\n2:\taddi\tr6,r6,1\n\tbdnz\t1b\n\t/* Finally clear out the SLB */\n\tli\tr0,0\n\tslbmte\tr0,r0\n\tPPC_SLBIA(6)\n\tptesync\n\tstw\tr5,VCPU_SLB_MAX(r9)\n\n\t/* load host SLB entries */\n\tld\tr8,PACA_SLBSHADOWPTR(r13)\n\n\t.rept\tSLB_NUM_BOLTED\n\tli\tr3, SLBSHADOW_SAVEAREA\n\tLDX_BE\tr5, r8, r3\n\taddi\tr3, r3, 8\n\tLDX_BE\tr6, r8, r3\n\tandis.\tr7,r5,SLB_ESID_V@h\n\tbeq\t1f\n\tslbmte\tr6,r5\n1:\taddi\tr8,r8,16\n\t.endr\n\nguest_bypass:\n\tstw\tr12, STACK_SLOT_TRAP(r1)\n\n\t/* Save DEC */\n\t/* Do this before kvmhv_commence_exit so we know TB is guest TB */\n\tld\tr3, HSTATE_KVM_VCORE(r13)\n\tmfspr\tr5,SPRN_DEC\n\tmftb\tr6\n\textsw\tr5,r5\n16:\tadd\tr5,r5,r6\n\tstd\tr5,VCPU_DEC_EXPIRES(r9)\n\n\t/* Increment exit count, poke other threads to exit */\n\tmr \tr3, r12\n\tbl\tkvmhv_commence_exit\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Stop others sending VCPU interrupts to this physical CPU */\n\tli\tr0, -1\n\tstw\tr0, VCPU_CPU(r9)\n\tstw\tr0, VCPU_THREAD_CPU(r9)\n\n\t/* Save guest CTRL register, set runlatch to 1 if it was clear */\n\tmfspr\tr6,SPRN_CTRLF\n\tstw\tr6,VCPU_CTRL(r9)\n\tandi.\tr0,r6,1\n\tbne\t4f\n\tli\tr6,1\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/*\n\t * Save the guest PURR/SPURR\n\t */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tld\tr7,VCPU_PURR(r9)\n\tld\tr8,VCPU_SPURR(r9)\n\tstd\tr5,VCPU_PURR(r9)\n\tstd\tr6,VCPU_SPURR(r9)\n\tsubf\tr5,r7,r5\n\tsubf\tr6,r8,r6\n\n\t/*\n\t * Restore host PURR/SPURR and add guest times\n\t * so that the time in the guest gets accounted.\n\t */\n\tld\tr3,HSTATE_PURR(r13)\n\tld\tr4,HSTATE_SPURR(r13)\n\tadd\tr3,r3,r5\n\tadd\tr4,r4,r6\n\tmtspr\tSPRN_PURR,r3\n\tmtspr\tSPRN_SPURR,r4\n\nBEGIN_FTR_SECTION\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Save POWER8-specific registers */\n\tmfspr\tr5, SPRN_IAMR\n\tmfspr\tr6, SPRN_PSPB\n\tmfspr\tr7, SPRN_FSCR\n\tstd\tr5, VCPU_IAMR(r9)\n\tstw\tr6, VCPU_PSPB(r9)\n\tstd\tr7, VCPU_FSCR(r9)\n\tmfspr\tr5, SPRN_IC\n\tmfspr\tr7, SPRN_TAR\n\tstd\tr5, VCPU_IC(r9)\n\tstd\tr7, VCPU_TAR(r9)\n\tmfspr\tr8, SPRN_EBBHR\n\tstd\tr8, VCPU_EBBHR(r9)\n\tmfspr\tr5, SPRN_EBBRR\n\tmfspr\tr6, SPRN_BESCR\n\tmfspr\tr7, SPRN_PID\n\tmfspr\tr8, SPRN_WORT\n\tstd\tr5, VCPU_EBBRR(r9)\n\tstd\tr6, VCPU_BESCR(r9)\n\tstw\tr7, VCPU_GUEST_PID(r9)\n\tstd\tr8, VCPU_WORT(r9)\n\tmfspr\tr5, SPRN_TCSCR\n\tmfspr\tr6, SPRN_ACOP\n\tmfspr\tr7, SPRN_CSIGR\n\tmfspr\tr8, SPRN_TACR\n\tstd\tr5, VCPU_TCSCR(r9)\n\tstd\tr6, VCPU_ACOP(r9)\n\tstd\tr7, VCPU_CSIGR(r9)\n\tstd\tr8, VCPU_TACR(r9)\nBEGIN_FTR_SECTION\n\tld\tr5, STACK_SLOT_FSCR(r1)\n\tmtspr\tSPRN_FSCR, r5\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\t/*\n\t * Restore various registers to 0, where non-zero values\n\t * set by the guest could disrupt the host.\n\t */\n\tli\tr0, 0\n\tmtspr\tSPRN_PSPB, r0\n\tmtspr\tSPRN_WORT, r0\n\tmtspr\tSPRN_TCSCR, r0\n\t/* Set MMCRS to 1<<31 to freeze and disable the SPMC counters */\n\tli\tr0, 1\n\tsldi\tr0, r0, 31\n\tmtspr\tSPRN_MMCRS, r0\n\n\t/* Save and restore AMR, IAMR and UAMOR before turning on the MMU */\n\tld\tr8, STACK_SLOT_IAMR(r1)\n\tmtspr\tSPRN_IAMR, r8\n\n8:\t/* Power7 jumps back in here */\n\tmfspr\tr5,SPRN_AMR\n\tmfspr\tr6,SPRN_UAMOR\n\tstd\tr5,VCPU_AMR(r9)\n\tstd\tr6,VCPU_UAMOR(r9)\n\tld\tr5,STACK_SLOT_AMR(r1)\n\tld\tr6,STACK_SLOT_UAMOR(r1)\n\tmtspr\tSPRN_AMR, r5\n\tmtspr\tSPRN_UAMOR, r6\n\n\t/* Switch DSCR back to host value */\n\tmfspr\tr8, SPRN_DSCR\n\tld\tr7, HSTATE_DSCR(r13)\n\tstd\tr8, VCPU_DSCR(r9)\n\tmtspr\tSPRN_DSCR, r7\n\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r9)\n\tstd\tr15, VCPU_GPR(R15)(r9)\n\tstd\tr16, VCPU_GPR(R16)(r9)\n\tstd\tr17, VCPU_GPR(R17)(r9)\n\tstd\tr18, VCPU_GPR(R18)(r9)\n\tstd\tr19, VCPU_GPR(R19)(r9)\n\tstd\tr20, VCPU_GPR(R20)(r9)\n\tstd\tr21, VCPU_GPR(R21)(r9)\n\tstd\tr22, VCPU_GPR(R22)(r9)\n\tstd\tr23, VCPU_GPR(R23)(r9)\n\tstd\tr24, VCPU_GPR(R24)(r9)\n\tstd\tr25, VCPU_GPR(R25)(r9)\n\tstd\tr26, VCPU_GPR(R26)(r9)\n\tstd\tr27, VCPU_GPR(R27)(r9)\n\tstd\tr28, VCPU_GPR(R28)(r9)\n\tstd\tr29, VCPU_GPR(R29)(r9)\n\tstd\tr30, VCPU_GPR(R30)(r9)\n\tstd\tr31, VCPU_GPR(R31)(r9)\n\n\t/* Save SPRGs */\n\tmfspr\tr3, SPRN_SPRG0\n\tmfspr\tr4, SPRN_SPRG1\n\tmfspr\tr5, SPRN_SPRG2\n\tmfspr\tr6, SPRN_SPRG3\n\tstd\tr3, VCPU_SPRG0(r9)\n\tstd\tr4, VCPU_SPRG1(r9)\n\tstd\tr5, VCPU_SPRG2(r9)\n\tstd\tr6, VCPU_SPRG3(r9)\n\n\t/* save FP state */\n\tmr\tr3, r9\n\tbl\tkvmppc_save_fp\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tmr      r3, r9\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_save_tm_hv\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n91:\n#endif\n\n\t/* Increment yield count if they have a VPA */\n\tld\tr8, VCPU_VPA(r9)\t/* do they have a VPA? */\n\tcmpdi\tr8, 0\n\tbeq\t25f\n\tli\tr4, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr3, r8, r4\n\taddi\tr3, r3, 1\n\tSTWX_BE\tr3, r8, r4\n\tli\tr3, 1\n\tstb\tr3, VCPU_VPA_DIRTY(r9)\n25:\n\t/* Save PMU registers if requested */\n\t/* r8 and cr0.eq are live here */\n\tmr\tr3, r9\n\tli\tr4, 1\n\tbeq\t21f\t\t\t/* if no VPA, save PMU stuff anyway */\n\tlbz\tr4, LPPACA_PMCINUSE(r8)\n21:\tbl\tkvmhv_save_guest_pmu\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Restore host values of some registers */\nBEGIN_FTR_SECTION\n\tld\tr5, STACK_SLOT_CIABR(r1)\n\tld\tr6, STACK_SLOT_DAWR0(r1)\n\tld\tr7, STACK_SLOT_DAWRX0(r1)\n\tmtspr\tSPRN_CIABR, r5\n\t/*\n\t * If the DAWR doesn't work, it's ok to write these here as\n\t * this value should always be zero\n\t*/\n\tmtspr\tSPRN_DAWR0, r6\n\tmtspr\tSPRN_DAWRX0, r7\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/*\n\t * POWER7/POWER8 guest -> host partition switch code.\n\t * We don't have to lock against tlbies but we do\n\t * have to coordinate the hardware threads.\n\t * Here STACK_SLOT_TRAP(r1) contains the trap number.\n\t */\nkvmhv_switch_to_host:\n\t/* Secondary threads wait for primary to do partition switch */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\tlbz\tr3,HSTATE_PTID(r13)\n\tcmpwi\tr3,0\n\tbeq\t15f\n\tHMT_LOW\n13:\tlbz\tr3,VCORE_IN_GUEST(r5)\n\tcmpwi\tr3,0\n\tbne\t13b\n\tHMT_MEDIUM\n\tb\t16f\n\n\t/* Primary thread waits for all the secondaries to exit guest */\n15:\tlwz\tr3,VCORE_ENTRY_EXIT(r5)\n\trlwinm\tr0,r3,32-8,0xff\n\tclrldi\tr3,r3,56\n\tcmpw\tr3,r0\n\tbne\t15b\n\tisync\n\n\t/* Did we actually switch to the guest at all? */\n\tlbz\tr6, VCORE_IN_GUEST(r5)\n\tcmpwi\tr6, 0\n\tbeq\t19f\n\n\t/* Primary thread switches back to host partition */\n\tlwz\tr7,KVM_HOST_LPID(r4)\n\tld\tr6,KVM_HOST_SDR1(r4)\n\tli\tr8,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r8\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to host page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\nBEGIN_FTR_SECTION\n\t/* DPDES and VTB are shared between threads */\n\tmfspr\tr7, SPRN_DPDES\n\tmfspr\tr8, SPRN_VTB\n\tstd\tr7, VCORE_DPDES(r5)\n\tstd\tr8, VCORE_VTB(r5)\n\t/* clear DPDES so we don't get guest doorbells in the host */\n\tli\tr8, 0\n\tmtspr\tSPRN_DPDES, r8\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* Subtract timebase offset from timebase */\n\tld\tr8, VCORE_TB_OFFSET_APPL(r5)\n\tcmpdi\tr8,0\n\tbeq\t17f\n\tli\tr0, 0\n\tstd\tr0, VCORE_TB_OFFSET_APPL(r5)\n\tmftb\tr6\t\t\t/* current guest timebase */\n\tsubf\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t\t/* update upper 40 bits */\n\tmftb\tr7\t\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t17f\n\taddis\tr8,r8,0x100\t\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n17:\n\t/*\n\t * If this is an HMI, we called kvmppc_realmode_hmi_handler\n\t * above, which may or may not have already called\n\t * kvmppc_subcore_exit_guest.  Fortunately, all that\n\t * kvmppc_subcore_exit_guest does is clear a flag, so calling\n\t * it again here is benign even if kvmppc_realmode_hmi_handler\n\t * has already called it.\n\t */\n\tbl\tkvmppc_subcore_exit_guest\n\tnop\n30:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\n\t/* Reset PCR */\n\tld\tr0, VCORE_PCR(r5)\n\tLOAD_REG_IMMEDIATE(r6, PCR_MASK)\n\tcmpld\tr0, r6\n\tbeq\t18f\n\tmtspr\tSPRN_PCR, r6\n18:\n\t/* Signal secondary CPUs to continue */\n\tli\tr0, 0\n\tstb\tr0,VCORE_IN_GUEST(r5)\n19:\tlis\tr8,0x7fff\t\t/* MAX_INT@h */\n\tmtspr\tSPRN_HDEC,r8\n\n16:\tld\tr8,KVM_HOST_LPCR(r4)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\t/* Finish timing, if we have a vcpu */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tli\tr3, 0\n\tbeq\t2f\n\tbl\tkvmhv_accumulate_time\n2:\n#endif\n\t/* Unset guest mode */\n\tli\tr0, KVM_GUEST_MODE_NONE\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\tlwz\tr12, STACK_SLOT_TRAP(r1)\t/* return trap # in r12 */\n\tld\tr0, SFS+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, SFS\n\tmtlr\tr0\n\tblr\n\n.balign 32\n.global kvm_flush_link_stack\nkvm_flush_link_stack:\n\t/* Save LR into r0 */\n\tmflr\tr0\n\n\t/* Flush the link stack. On Power8 it's up to 32 entries in size. */\n\t.rept 32\n\tANNOTATE_INTRA_FUNCTION_CALL\n\tbl\t.+4\n\t.endr\n\n\t/* And on Power9 it's up to 64. */\nBEGIN_FTR_SECTION\n\t.rept 32\n\tANNOTATE_INTRA_FUNCTION_CALL\n\tbl\t.+4\n\t.endr\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\n\n\t/* Restore LR */\n\tmtlr\tr0\n\tblr\n\nkvmppc_guest_external:\n\t/* External interrupt, first check for host_ipi. If this is\n\t * set, we know the host wants us out so let's do it now\n\t */\n\tbl\tCFUNC(kvmppc_read_intr)\n\n\t/*\n\t * Restore the active volatile registers after returning from\n\t * a C function.\n\t */\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\n\t/*\n\t * kvmppc_read_intr return codes:\n\t *\n\t * Exit to host (r3 > 0)\n\t *   1 An interrupt is pending that needs to be handled by the host\n\t *     Exit guest and return to host by branching to guest_exit_cont\n\t *\n\t *   2 Passthrough that needs completion in the host\n\t *     Exit guest and return to host by branching to guest_exit_cont\n\t *     However, we also set r12 to BOOK3S_INTERRUPT_HV_RM_HARD\n\t *     to indicate to the host to complete handling the interrupt\n\t *\n\t * Before returning to guest, we check if any CPU is heading out\n\t * to the host and if so, we head out also. If no CPUs are heading\n\t * check return values <= 0.\n\t *\n\t * Return to guest (r3 <= 0)\n\t *  0 No external interrupt is pending\n\t * -1 A guest wakeup IPI (which has now been cleared)\n\t *    In either case, we return to guest to deliver any pending\n\t *    guest interrupts.\n\t *\n\t * -2 A PCI passthrough external interrupt was handled\n\t *    (interrupt was delivered directly to guest)\n\t *    Return to guest to deliver any pending guest interrupts.\n\t */\n\n\tcmpdi\tr3, 1\n\tble\t1f\n\n\t/* Return code = 2 */\n\tli\tr12, BOOK3S_INTERRUPT_HV_RM_HARD\n\tstw\tr12, VCPU_TRAP(r9)\n\tb\tguest_exit_cont\n\n1:\t/* Return code <= 1 */\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\n\t/* Return code <= 0 */\nmaybe_reenter_guest:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tmr\tr4, r9\n\tblt\tdeliver_guest_interrupt\n\tb\tguest_exit_cont\n\n/*\n * Check whether an HDSI is an HPTE not found fault or something else.\n * If it is an HPTE not found fault that is due to the guest accessing\n * a page that they have mapped but which we have paged out, then\n * we continue on with the guest exit path.  In all other cases,\n * reflect the HDSI to the guest as a DSI.\n */\nkvmppc_hdsi:\n\tmfspr\tr4, SPRN_HDAR\n\tmfspr\tr6, SPRN_HDSISR\n\t/* HPTE not found fault or protection fault? */\n\tandis.\tr0, r6, (DSISR_NOHPTE | DSISR_PROTFAULT)@h\n\tbeq\t1f\t\t\t/* if not, send it to the guest */\n\tandi.\tr0, r11, MSR_DR\t\t/* data relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r4, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_DATA_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\tstd\tr4, VCPU_FAULT_DAR(r9)\n\tstw\tr6, VCPU_FAULT_DSISR(r9)\n\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tli\tr7, 1\t\t\t/* data fault */\n\tbl\tCFUNC(kvmppc_hpte_hv_fault)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\t6f\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\tcmpdi\tr3, -2\t\t\t/* MMIO emulation; need instr word */\n\tbeq\t2f\n\n\t/* Synthesize a DSI (or DSegI) for the guest */\n\tld\tr4, VCPU_FAULT_DAR(r9)\n\tmr\tr6, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_DATA_STORAGE\n\tmtspr\tSPRN_DSISR, r6\n7:\tmtspr\tSPRN_DAR, r4\n\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\nfast_interrupt_c_return:\n6:\tld\tr7, VCPU_CTR(r9)\n\tld\tr8, VCPU_XER(r9)\n\tmtctr\tr7\n\tmtxer\tr8\n\tmr\tr4, r9\n\tb\tfast_guest_return\n\n3:\tld\tr5, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r5)\n\tb\t4b\n\n\t/* If this is for emulated MMIO, load the instruction word */\n2:\tli\tr8, KVM_INST_FETCH_FAILED\t/* In case lwz faults */\n\n\t/* Set guest mode to 'jump over instruction' so if lwz faults\n\t * we'll just continue at the next IP. */\n\tli\tr0, KVM_GUEST_MODE_SKIP\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\t/* Do the access with MSR:DR enabled */\n\tmfmsr\tr3\n\tori\tr4, r3, MSR_DR\t\t/* Enable paging for data */\n\tmtmsrd\tr4\n\tlwz\tr8, 0(r10)\n\tmtmsrd\tr3\n\n\t/* Store the result */\n\tstd\tr8, VCPU_LAST_INST(r9)\n\n\t/* Unset guest mode. */\n\tli\tr0, KVM_GUEST_MODE_HOST_HV\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\tb\tguest_exit_cont\n\n/*\n * Similarly for an HISI, reflect it to the guest as an ISI unless\n * it is an HPTE not found fault for a page that we have paged out.\n */\nkvmppc_hisi:\n\tandis.\tr0, r11, SRR1_ISI_NOPT@h\n\tbeq\t1f\n\tandi.\tr0, r11, MSR_IR\t\t/* instruction relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r10, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_INST_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tmr\tr4, r10\n\tmr\tr6, r11\n\tli\tr7, 0\t\t\t/* instruction fault */\n\tbl\tCFUNC(kvmppc_hpte_hv_fault)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\tfast_interrupt_c_return\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\n\t/* Synthesize an ISI (or ISegI) for the guest */\n\tmr\tr11, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_INST_STORAGE\n7:\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\n\tb\tfast_interrupt_c_return\n\n3:\tld\tr6, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r6)\n\tb\t4b\n\n/*\n * Try to handle an hcall in real mode.\n * Returns to the guest if we handle it, or continues on up to\n * the kernel if we can't (i.e. if we don't have a handler for\n * it, or if the handler returns H_TOO_HARD).\n *\n * r5 - r8 contain hcall args,\n * r9 = vcpu, r10 = pc, r11 = msr, r12 = trap, r13 = paca\n */\nhcall_try_real_mode:\n\tld\tr3,VCPU_GPR(R3)(r9)\n\tandi.\tr0,r11,MSR_PR\n\t/* sc 1 from userspace - reflect to guest syscall */\n\tbne\tsc_1_fast_return\n\tclrrdi\tr3,r3,2\n\tcmpldi\tr3,hcall_real_table_end - hcall_real_table\n\tbge\tguest_exit_cont\n\t/* See if this hcall is enabled for in-kernel handling */\n\tld\tr4, VCPU_KVM(r9)\n\tsrdi\tr0, r3, 8\t/* r0 = (r3 / 4) >> 6 */\n\tsldi\tr0, r0, 3\t/* index into kvm->arch.enabled_hcalls[] */\n\tadd\tr4, r4, r0\n\tld\tr0, KVM_ENABLED_HCALLS(r4)\n\trlwinm\tr4, r3, 32-2, 0x3f\t/* r4 = (r3 / 4) & 0x3f */\n\tsrd\tr0, r0, r4\n\tandi.\tr0, r0, 1\n\tbeq\tguest_exit_cont\n\t/* Get pointer to handler, if any, and call it */\n\tLOAD_REG_ADDR(r4, hcall_real_table)\n\tlwax\tr3,r3,r4\n\tcmpwi\tr3,0\n\tbeq\tguest_exit_cont\n\tadd\tr12,r3,r4\n\tmtctr\tr12\n\tmr\tr3,r9\t\t/* get vcpu pointer */\n\tld\tr4,VCPU_GPR(R4)(r9)\n\tbctrl\n\tcmpdi\tr3,H_TOO_HARD\n\tbeq\thcall_real_fallback\n\tld\tr4,HSTATE_KVM_VCPU(r13)\n\tstd\tr3,VCPU_GPR(R3)(r4)\n\tld\tr10,VCPU_PC(r4)\n\tld\tr11,VCPU_MSR(r4)\n\tb\tfast_guest_return\n\nsc_1_fast_return:\n\tmtspr\tSPRN_SRR0,r10\n\tmtspr\tSPRN_SRR1,r11\n\tli\tr10, BOOK3S_INTERRUPT_SYSCALL\n\tbl\tkvmppc_msr_interrupt\n\tmr\tr4,r9\n\tb\tfast_guest_return\n\n\t/* We've attempted a real mode hcall, but it's punted it back\n\t * to userspace.  We need to restore some clobbered volatiles\n\t * before resuming the pass-it-to-qemu path */\nhcall_real_fallback:\n\tli\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\tb\tguest_exit_cont\n\n\t.globl\thcall_real_table\nhcall_real_table:\n\t.long\t0\t\t/* 0 - unused */\n\t.long\tDOTSYM(kvmppc_h_remove) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_enter) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_read) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_mod) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_ref) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_protect) - hcall_real_table\n\t.long\t0\t\t/* 0x1c */\n\t.long\t0\t\t/* 0x20 */\n\t.long\t0\t\t/* 0x24 - H_SET_SPRG0 */\n\t.long\tDOTSYM(kvmppc_h_set_dabr) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_page_init) - hcall_real_table\n\t.long\t0\t\t/* 0x30 */\n\t.long\t0\t\t/* 0x34 */\n\t.long\t0\t\t/* 0x38 */\n\t.long\t0\t\t/* 0x3c */\n\t.long\t0\t\t/* 0x40 */\n\t.long\t0\t\t/* 0x44 */\n\t.long\t0\t\t/* 0x48 */\n\t.long\t0\t\t/* 0x4c */\n\t.long\t0\t\t/* 0x50 */\n\t.long\t0\t\t/* 0x54 */\n\t.long\t0\t\t/* 0x58 */\n\t.long\t0\t\t/* 0x5c */\n\t.long\t0\t\t/* 0x60 */\n#ifdef CONFIG_KVM_XICS\n\t.long\tDOTSYM(xics_rm_h_eoi) - hcall_real_table\n\t.long\tDOTSYM(xics_rm_h_cppr) - hcall_real_table\n\t.long\tDOTSYM(xics_rm_h_ipi) - hcall_real_table\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\tDOTSYM(xics_rm_h_xirr) - hcall_real_table\n#else\n\t.long\t0\t\t/* 0x64 - H_EOI */\n\t.long\t0\t\t/* 0x68 - H_CPPR */\n\t.long\t0\t\t/* 0x6c - H_IPI */\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\t0\t\t/* 0x74 - H_XIRR */\n#endif\n\t.long\t0\t\t/* 0x78 */\n\t.long\t0\t\t/* 0x7c */\n\t.long\t0\t\t/* 0x80 */\n\t.long\t0\t\t/* 0x84 */\n\t.long\t0\t\t/* 0x88 */\n\t.long\t0\t\t/* 0x8c */\n\t.long\t0\t\t/* 0x90 */\n\t.long\t0\t\t/* 0x94 */\n\t.long\t0\t\t/* 0x98 */\n\t.long\t0\t\t/* 0x9c */\n\t.long\t0\t\t/* 0xa0 */\n\t.long\t0\t\t/* 0xa4 */\n\t.long\t0\t\t/* 0xa8 */\n\t.long\t0\t\t/* 0xac */\n\t.long\t0\t\t/* 0xb0 */\n\t.long\t0\t\t/* 0xb4 */\n\t.long\t0\t\t/* 0xb8 */\n\t.long\t0\t\t/* 0xbc */\n\t.long\t0\t\t/* 0xc0 */\n\t.long\t0\t\t/* 0xc4 */\n\t.long\t0\t\t/* 0xc8 */\n\t.long\t0\t\t/* 0xcc */\n\t.long\t0\t\t/* 0xd0 */\n\t.long\t0\t\t/* 0xd4 */\n\t.long\t0\t\t/* 0xd8 */\n\t.long\t0\t\t/* 0xdc */\n\t.long\tDOTSYM(kvmppc_h_cede) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_confer) - hcall_real_table\n\t.long\t0\t\t/* 0xe8 */\n\t.long\t0\t\t/* 0xec */\n\t.long\t0\t\t/* 0xf0 */\n\t.long\t0\t\t/* 0xf4 */\n\t.long\t0\t\t/* 0xf8 */\n\t.long\t0\t\t/* 0xfc */\n\t.long\t0\t\t/* 0x100 */\n\t.long\t0\t\t/* 0x104 */\n\t.long\t0\t\t/* 0x108 */\n\t.long\t0\t\t/* 0x10c */\n\t.long\t0\t\t/* 0x110 */\n\t.long\t0\t\t/* 0x114 */\n\t.long\t0\t\t/* 0x118 */\n\t.long\t0\t\t/* 0x11c */\n\t.long\t0\t\t/* 0x120 */\n\t.long\tDOTSYM(kvmppc_h_bulk_remove) - hcall_real_table\n\t.long\t0\t\t/* 0x128 */\n\t.long\t0\t\t/* 0x12c */\n\t.long\t0\t\t/* 0x130 */\n\t.long\tDOTSYM(kvmppc_h_set_xdabr) - hcall_real_table\n\t.long\t0\t\t/* 0x138 */\n\t.long\t0\t\t/* 0x13c */\n\t.long\t0\t\t/* 0x140 */\n\t.long\t0\t\t/* 0x144 */\n\t.long\t0\t\t/* 0x148 */\n\t.long\t0\t\t/* 0x14c */\n\t.long\t0\t\t/* 0x150 */\n\t.long\t0\t\t/* 0x154 */\n\t.long\t0\t\t/* 0x158 */\n\t.long\t0\t\t/* 0x15c */\n\t.long\t0\t\t/* 0x160 */\n\t.long\t0\t\t/* 0x164 */\n\t.long\t0\t\t/* 0x168 */\n\t.long\t0\t\t/* 0x16c */\n\t.long\t0\t\t/* 0x170 */\n\t.long\t0\t\t/* 0x174 */\n\t.long\t0\t\t/* 0x178 */\n\t.long\t0\t\t/* 0x17c */\n\t.long\t0\t\t/* 0x180 */\n\t.long\t0\t\t/* 0x184 */\n\t.long\t0\t\t/* 0x188 */\n\t.long\t0\t\t/* 0x18c */\n\t.long\t0\t\t/* 0x190 */\n\t.long\t0\t\t/* 0x194 */\n\t.long\t0\t\t/* 0x198 */\n\t.long\t0\t\t/* 0x19c */\n\t.long\t0\t\t/* 0x1a0 */\n\t.long\t0\t\t/* 0x1a4 */\n\t.long\t0\t\t/* 0x1a8 */\n\t.long\t0\t\t/* 0x1ac */\n\t.long\t0\t\t/* 0x1b0 */\n\t.long\t0\t\t/* 0x1b4 */\n\t.long\t0\t\t/* 0x1b8 */\n\t.long\t0\t\t/* 0x1bc */\n\t.long\t0\t\t/* 0x1c0 */\n\t.long\t0\t\t/* 0x1c4 */\n\t.long\t0\t\t/* 0x1c8 */\n\t.long\t0\t\t/* 0x1cc */\n\t.long\t0\t\t/* 0x1d0 */\n\t.long\t0\t\t/* 0x1d4 */\n\t.long\t0\t\t/* 0x1d8 */\n\t.long\t0\t\t/* 0x1dc */\n\t.long\t0\t\t/* 0x1e0 */\n\t.long\t0\t\t/* 0x1e4 */\n\t.long\t0\t\t/* 0x1e8 */\n\t.long\t0\t\t/* 0x1ec */\n\t.long\t0\t\t/* 0x1f0 */\n\t.long\t0\t\t/* 0x1f4 */\n\t.long\t0\t\t/* 0x1f8 */\n\t.long\t0\t\t/* 0x1fc */\n\t.long\t0\t\t/* 0x200 */\n\t.long\t0\t\t/* 0x204 */\n\t.long\t0\t\t/* 0x208 */\n\t.long\t0\t\t/* 0x20c */\n\t.long\t0\t\t/* 0x210 */\n\t.long\t0\t\t/* 0x214 */\n\t.long\t0\t\t/* 0x218 */\n\t.long\t0\t\t/* 0x21c */\n\t.long\t0\t\t/* 0x220 */\n\t.long\t0\t\t/* 0x224 */\n\t.long\t0\t\t/* 0x228 */\n\t.long\t0\t\t/* 0x22c */\n\t.long\t0\t\t/* 0x230 */\n\t.long\t0\t\t/* 0x234 */\n\t.long\t0\t\t/* 0x238 */\n\t.long\t0\t\t/* 0x23c */\n\t.long\t0\t\t/* 0x240 */\n\t.long\t0\t\t/* 0x244 */\n\t.long\t0\t\t/* 0x248 */\n\t.long\t0\t\t/* 0x24c */\n\t.long\t0\t\t/* 0x250 */\n\t.long\t0\t\t/* 0x254 */\n\t.long\t0\t\t/* 0x258 */\n\t.long\t0\t\t/* 0x25c */\n\t.long\t0\t\t/* 0x260 */\n\t.long\t0\t\t/* 0x264 */\n\t.long\t0\t\t/* 0x268 */\n\t.long\t0\t\t/* 0x26c */\n\t.long\t0\t\t/* 0x270 */\n\t.long\t0\t\t/* 0x274 */\n\t.long\t0\t\t/* 0x278 */\n\t.long\t0\t\t/* 0x27c */\n\t.long\t0\t\t/* 0x280 */\n\t.long\t0\t\t/* 0x284 */\n\t.long\t0\t\t/* 0x288 */\n\t.long\t0\t\t/* 0x28c */\n\t.long\t0\t\t/* 0x290 */\n\t.long\t0\t\t/* 0x294 */\n\t.long\t0\t\t/* 0x298 */\n\t.long\t0\t\t/* 0x29c */\n\t.long\t0\t\t/* 0x2a0 */\n\t.long\t0\t\t/* 0x2a4 */\n\t.long\t0\t\t/* 0x2a8 */\n\t.long\t0\t\t/* 0x2ac */\n\t.long\t0\t\t/* 0x2b0 */\n\t.long\t0\t\t/* 0x2b4 */\n\t.long\t0\t\t/* 0x2b8 */\n\t.long\t0\t\t/* 0x2bc */\n\t.long\t0\t\t/* 0x2c0 */\n\t.long\t0\t\t/* 0x2c4 */\n\t.long\t0\t\t/* 0x2c8 */\n\t.long\t0\t\t/* 0x2cc */\n\t.long\t0\t\t/* 0x2d0 */\n\t.long\t0\t\t/* 0x2d4 */\n\t.long\t0\t\t/* 0x2d8 */\n\t.long\t0\t\t/* 0x2dc */\n\t.long\t0\t\t/* 0x2e0 */\n\t.long\t0\t\t/* 0x2e4 */\n\t.long\t0\t\t/* 0x2e8 */\n\t.long\t0\t\t/* 0x2ec */\n\t.long\t0\t\t/* 0x2f0 */\n\t.long\t0\t\t/* 0x2f4 */\n\t.long\t0\t\t/* 0x2f8 */\n#ifdef CONFIG_KVM_XICS\n\t.long\tDOTSYM(xics_rm_h_xirr_x) - hcall_real_table\n#else\n\t.long\t0\t\t/* 0x2fc - H_XIRR_X*/\n#endif\n\t.long\tDOTSYM(kvmppc_rm_h_random) - hcall_real_table\n\t.globl\thcall_real_table_end\nhcall_real_table_end:\n\n_GLOBAL_TOC(kvmppc_h_set_xdabr)\nEXPORT_SYMBOL_GPL(kvmppc_h_set_xdabr)\n\tandi.\tr0, r5, DABRX_USER | DABRX_KERNEL\n\tbeq\t6f\n\tli\tr0, DABRX_USER | DABRX_KERNEL | DABRX_BTI\n\tandc.\tr0, r5, r0\n\tbeq\t3f\n6:\tli\tr3, H_PARAMETER\n\tblr\n\n_GLOBAL_TOC(kvmppc_h_set_dabr)\nEXPORT_SYMBOL_GPL(kvmppc_h_set_dabr)\n\tli\tr5, DABRX_USER | DABRX_KERNEL\n3:\nBEGIN_FTR_SECTION\n\tb\t2f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr4,VCPU_DABR(r3)\n\tstw\tr5, VCPU_DABRX(r3)\n\tmtspr\tSPRN_DABRX, r5\n\t/* Work around P7 bug where DABR can get corrupted on mtspr */\n1:\tmtspr\tSPRN_DABR,r4\n\tmfspr\tr5, SPRN_DABR\n\tcmpd\tr4, r5\n\tbne\t1b\n\tisync\n\tli\tr3,0\n\tblr\n\n2:\n\tLOAD_REG_ADDR(r11, dawr_force_enable)\n\tlbz\tr11, 0(r11)\n\tcmpdi\tr11, 0\n\tbne\t3f\n\tli\tr3, H_HARDWARE\n\tblr\n3:\n\t/* Emulate H_SET_DABR/X on P8 for the sake of compat mode guests */\n\trlwimi\tr5, r4, 5, DAWRX_DR | DAWRX_DW\n\trlwimi\tr5, r4, 2, DAWRX_WT\n\tclrrdi\tr4, r4, 3\n\tstd\tr4, VCPU_DAWR0(r3)\n\tstd\tr5, VCPU_DAWRX0(r3)\n\t/*\n\t * If came in through the real mode hcall handler then it is necessary\n\t * to write the registers since the return path won't. Otherwise it is\n\t * sufficient to store then in the vcpu struct as they will be loaded\n\t * next time the vcpu is run.\n\t */\n\tmfmsr\tr6\n\tandi.\tr6, r6, MSR_DR\t\t/* in real mode? */\n\tbne\t4f\n\tmtspr\tSPRN_DAWR0, r4\n\tmtspr\tSPRN_DAWRX0, r5\n4:\tli\tr3, 0\n\tblr\n\n_GLOBAL(kvmppc_h_cede)\t\t/* r3 = vcpu pointer, r11 = msr, r13 = paca */\n\tori\tr11,r11,MSR_EE\n\tstd\tr11,VCPU_MSR(r3)\n\tli\tr0,1\n\tstb\tr0,VCPU_CEDED(r3)\n\tsync\t\t\t/* order setting ceded vs. testing prodded */\n\tlbz\tr5,VCPU_PRODDED(r3)\n\tcmpwi\tr5,0\n\tbne\tkvm_cede_prodded\n\tli\tr12,0\t\t/* set trap to 0 to say hcall is handled */\n\tstw\tr12,VCPU_TRAP(r3)\n\tli\tr0,H_SUCCESS\n\tstd\tr0,VCPU_GPR(R3)(r3)\n\n\t/*\n\t * Set our bit in the bitmask of napping threads unless all the\n\t * other threads are already napping, in which case we send this\n\t * up to the host.\n\t */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr6,HSTATE_PTID(r13)\n\tlwz\tr8,VCORE_ENTRY_EXIT(r5)\n\tclrldi\tr8,r8,56\n\tli\tr0,1\n\tsld\tr0,r0,r6\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n31:\tlwarx\tr4,0,r6\n\tor\tr4,r4,r0\n\tcmpw\tr4,r8\n\tbeq\tkvm_cede_exit\n\tstwcx.\tr4,0,r6\n\tbne\t31b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr0,NAPPING_CEDE\n\tstb\tr0,HSTATE_NAPPING(r13)\n\tlwz\tr7,VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7,0x100\n\tbge\t33f\t\t/* another thread already exiting */\n\n/*\n * Although not specifically required by the architecture, POWER7\n * preserves the following registers in nap mode, even if an SMT mode\n * switch occurs: SLB entries, PURR, SPURR, AMOR, UAMOR, AMR, SPRG0-3,\n * DAR, DSISR, DABR, DABRX, DSCR, PMCx, MMCRx, SIAR, SDAR.\n */\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r3)\n\tstd\tr15, VCPU_GPR(R15)(r3)\n\tstd\tr16, VCPU_GPR(R16)(r3)\n\tstd\tr17, VCPU_GPR(R17)(r3)\n\tstd\tr18, VCPU_GPR(R18)(r3)\n\tstd\tr19, VCPU_GPR(R19)(r3)\n\tstd\tr20, VCPU_GPR(R20)(r3)\n\tstd\tr21, VCPU_GPR(R21)(r3)\n\tstd\tr22, VCPU_GPR(R22)(r3)\n\tstd\tr23, VCPU_GPR(R23)(r3)\n\tstd\tr24, VCPU_GPR(R24)(r3)\n\tstd\tr25, VCPU_GPR(R25)(r3)\n\tstd\tr26, VCPU_GPR(R26)(r3)\n\tstd\tr27, VCPU_GPR(R27)(r3)\n\tstd\tr28, VCPU_GPR(R28)(r3)\n\tstd\tr29, VCPU_GPR(R29)(r3)\n\tstd\tr30, VCPU_GPR(R30)(r3)\n\tstd\tr31, VCPU_GPR(R31)(r3)\n\n\t/* save FP state */\n\tbl\tkvmppc_save_fp\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tld\tr3, HSTATE_KVM_VCPU(r13)\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_save_tm_hv\n\tnop\n91:\n#endif\n\n\t/*\n\t * Set DEC to the smaller of DEC and HDEC, so that we wake\n\t * no later than the end of our timeslice (HDEC interrupts\n\t * don't wake us from nap).\n\t */\n\tmfspr\tr3, SPRN_DEC\n\tmfspr\tr4, SPRN_HDEC\n\tmftb\tr5\n\textsw\tr3, r3\n\textsw\tr4, r4\n\tcmpd\tr3, r4\n\tble\t67f\n\tmtspr\tSPRN_DEC, r4\n67:\n\t/* save expiry time of guest decrementer */\n\tadd\tr3, r3, r5\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tstd\tr3, VCPU_DEC_EXPIRES(r4)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\taddi\tr3, r4, VCPU_TB_CEDE\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\tlis\tr3, LPCR_PECEDP@h\t/* Do wake on privileged doorbell */\n\n\t/* Go back to host stack */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\n\t/*\n\t * Take a nap until a decrementer or external or doobell interrupt\n\t * occurs, with PECE1 and PECE0 set in LPCR.\n\t * On POWER8, set PECEDH, and if we are ceding, also set PECEDP.\n\t * Also clear the runlatch bit before napping.\n\t */\nkvm_do_nap:\n\tli\tr0,0\n\tmtspr\tSPRN_CTRLT, r0\n\n\tli\tr0,1\n\tstb\tr0,HSTATE_HWTHREAD_REQ(r13)\n\tmfspr\tr5,SPRN_LPCR\n\tori\tr5,r5,LPCR_PECE0 | LPCR_PECE1\nBEGIN_FTR_SECTION\n\tori\tr5, r5, LPCR_PECEDH\n\trlwimi\tr5, r3, 0, LPCR_PECEDP\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\nkvm_nap_sequence:\t\t/* desired LPCR value in r5 */\n\tli\tr3, PNV_THREAD_NAP\n\tmtspr\tSPRN_LPCR,r5\n\tisync\n\n\tbl\tisa206_idle_insn_mayloss\n\n\tli\tr0,1\n\tmtspr\tSPRN_CTRLT, r0\n\n\tmtspr\tSPRN_SRR1, r3\n\n\tli\tr0, 0\n\tstb\tr0, PACA_FTRACE_ENABLED(r13)\n\n\tli\tr0, KVM_HWTHREAD_IN_KVM\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\n\tlbz\tr0, HSTATE_NAPPING(r13)\n\tcmpwi\tr0, NAPPING_CEDE\n\tbeq\tkvm_end_cede\n\tcmpwi\tr0, NAPPING_NOVCPU\n\tbeq\tkvm_novcpu_wakeup\n\tcmpwi\tr0, NAPPING_UNSPLIT\n\tbeq\tkvm_unsplit_wakeup\n\ttwi\t31,0,0 /* Nap state must not be zero */\n\n33:\tmr\tr4, r3\n\tli\tr3, 0\n\tli\tr12, 0\n\tb\t34f\n\nkvm_end_cede:\n\t/* Woken by external or decrementer interrupt */\n\n\t/* get vcpu pointer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMINTR\n\tbl\tkvmhv_accumulate_time\n#endif\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tmr      r3, r4\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_restore_tm_hv\n\tnop\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n91:\n#endif\n\n\t/* load up FP state */\n\tbl\tkvmppc_load_fp\n\n\t/* Restore guest decrementer */\n\tld\tr3, VCPU_DEC_EXPIRES(r4)\n\tmftb\tr7\n\tsubf\tr3, r7, r3\n\tmtspr\tSPRN_DEC, r3\n\n\t/* Load NV GPRS */\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n\n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\n\t/*\n\t * Restore volatile registers since we could have called a\n\t * C routine in kvmppc_check_wake_reason\n\t *\tr4 = VCPU\n\t * r3 tells us whether we need to return to host or not\n\t * WARNING: it gets checked further down:\n\t * should not modify r3 until this check is done.\n\t */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\n\t/* clear our bit in vcore->napping_threads */\n34:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr7,HSTATE_PTID(r13)\n\tli\tr0,1\n\tsld\tr0,r0,r7\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n32:\tlwarx\tr7,0,r6\n\tandc\tr7,r7,r0\n\tstwcx.\tr7,0,r6\n\tbne\t32b\n\tli\tr0,0\n\tstb\tr0,HSTATE_NAPPING(r13)\n\n\t/* See if the wake reason saved in r3 means we need to exit */\n\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\tb\tmaybe_reenter_guest\n\n\t/* cede when already previously prodded case */\nkvm_cede_prodded:\n\tli\tr0,0\n\tstb\tr0,VCPU_PRODDED(r3)\n\tsync\t\t\t/* order testing prodded vs. clearing ceded */\n\tstb\tr0,VCPU_CEDED(r3)\n\tli\tr3,H_SUCCESS\n\tblr\n\n\t/* we've ceded but we want to give control to the host */\nkvm_cede_exit:\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tb\tguest_exit_cont\n\n\t/* Try to do machine check recovery in real mode */\nmachine_check_realmode:\n\tmr\tr3, r9\t\t/* get vcpu pointer */\n\tbl\tkvmppc_realmode_machine_check\n\tnop\n\t/* all machine checks go to virtual mode for further handling */\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tb\tguest_exit_cont\n\n/*\n * Call C code to handle a HMI in real mode.\n * Only the primary thread does the call, secondary threads are handled\n * by calling hmi_exception_realmode() after kvmppc_hv_entry returns.\n * r9 points to the vcpu on entry\n */\nhmi_realmode:\n\tlbz\tr0, HSTATE_PTID(r13)\n\tcmpwi\tr0, 0\n\tbne\tguest_exit_cont\n\tbl\tCFUNC(kvmppc_realmode_hmi_handler)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_HMI\n\tb\tguest_exit_cont\n\n/*\n * Check the reason we woke from nap, and take appropriate action.\n * Returns (in r3):\n *\t0 if nothing needs to be done\n *\t1 if something happened that needs to be handled by the host\n *\t-1 if there was a guest wakeup (IPI or msgsnd)\n *\t-2 if we handled a PCI passthrough interrupt (returned by\n *\t\tkvmppc_read_intr only)\n *\n * Also sets r12 to the interrupt vector for any interrupt that needs\n * to be handled now by the host (0x500 for external interrupt), or zero.\n * Modifies all volatile registers (since it may call a C function).\n * This routine calls kvmppc_read_intr, a C function, if an external\n * interrupt is pending.\n */\nSYM_FUNC_START_LOCAL(kvmppc_check_wake_reason)\n\tmfspr\tr6, SPRN_SRR1\nBEGIN_FTR_SECTION\n\trlwinm\tr6, r6, 45-31, 0xf\t/* extract wake reason field (P8) */\nFTR_SECTION_ELSE\n\trlwinm\tr6, r6, 45-31, 0xe\t/* P7 wake reason field is 3 bits */\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 8\t\t\t/* was it an external interrupt? */\n\tbeq\t7f\t\t\t/* if so, see what it was */\n\tli\tr3, 0\n\tli\tr12, 0\n\tcmpwi\tr6, 6\t\t\t/* was it the decrementer? */\n\tbeq\t0f\nBEGIN_FTR_SECTION\n\tcmpwi\tr6, 5\t\t\t/* privileged doorbell? */\n\tbeq\t0f\n\tcmpwi\tr6, 3\t\t\t/* hypervisor doorbell? */\n\tbeq\t3f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 0xa\t\t\t/* Hypervisor maintenance ? */\n\tbeq\t4f\n\tli\tr3, 1\t\t\t/* anything else, return 1 */\n0:\tblr\n\n\t/* hypervisor doorbell */\n3:\tli\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\n\t/*\n\t * Clear the doorbell as we will invoke the handler\n\t * explicitly in the guest exit path.\n\t */\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\n\t/* see if it's a host IPI */\n\tli\tr3, 1\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbnelr\n\t/* if not, return -1 */\n\tli\tr3, -1\n\tblr\n\n\t/* Woken up due to Hypervisor maintenance interrupt */\n4:\tli\tr12, BOOK3S_INTERRUPT_HMI\n\tli\tr3, 1\n\tblr\n\n\t/* external interrupt - create a stack frame so we can call C */\n7:\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -PPC_MIN_STKFRM(r1)\n\tbl\tCFUNC(kvmppc_read_intr)\n\tnop\n\tli\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tcmpdi\tr3, 1\n\tble\t1f\n\n\t/*\n\t * Return code of 2 means PCI passthrough interrupt, but\n\t * we need to return back to host to complete handling the\n\t * interrupt. Trap reason is expected in r12 by guest\n\t * exit code.\n\t */\n\tli\tr12, BOOK3S_INTERRUPT_HV_RM_HARD\n1:\n\tld\tr0, PPC_MIN_STKFRM+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, PPC_MIN_STKFRM\n\tmtlr\tr0\n\tblr\nSYM_FUNC_END(kvmppc_check_wake_reason)\n\n/*\n * Save away FP, VMX and VSX registers.\n * r3 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nSYM_FUNC_START_LOCAL(kvmppc_save_fp)\n\tmflr\tr30\n\tmr\tr31,r3\n\tmfmsr\tr5\n\tori\tr8,r5,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r3,VCPU_FPRS\n\tbl\tstore_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tstore_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tmfspr\tr6,SPRN_VRSAVE\n\tstw\tr6,VCPU_VRSAVE(r31)\n\tmtlr\tr30\n\tblr\nSYM_FUNC_END(kvmppc_save_fp)\n\n/*\n * Load up FP, VMX and VSX registers\n * r4 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nSYM_FUNC_START_LOCAL(kvmppc_load_fp)\n\tmflr\tr30\n\tmr\tr31,r4\n\tmfmsr\tr9\n\tori\tr8,r9,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r4,VCPU_FPRS\n\tbl\tload_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tload_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tlwz\tr7,VCPU_VRSAVE(r31)\n\tmtspr\tSPRN_VRSAVE,r7\n\tmtlr\tr30\n\tmr\tr4,r31\n\tblr\nSYM_FUNC_END(kvmppc_load_fp)\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n/*\n * Save transactional state and TM-related registers.\n * Called with r3 pointing to the vcpu struct and r4 containing\n * the guest MSR value.\n * r5 is non-zero iff non-volatile register state needs to be maintained.\n * If r5 == 0, this can modify all checkpointed registers, but\n * restores r1 and r2 before exit.\n */\n_GLOBAL_TOC(kvmppc_save_tm_hv)\nEXPORT_SYMBOL_GPL(kvmppc_save_tm_hv)\n\t/* See if we need to handle fake suspend mode */\nBEGIN_FTR_SECTION\n\tb\t__kvmppc_save_tm\nEND_FTR_SECTION_IFCLR(CPU_FTR_P9_TM_HV_ASSIST)\n\n\tlbz\tr0, HSTATE_FAKE_SUSPEND(r13) /* Were we fake suspended? */\n\tcmpwi\tr0, 0\n\tbeq\t__kvmppc_save_tm\n\n\t/* The following code handles the fake_suspend = 1 case */\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -TM_FRAME_SIZE(r1)\n\n\t/* Turn on TM. */\n\tmfmsr\tr8\n\tli\tr0, 1\n\trldimi\tr8, r0, MSR_TM_LG, 63-MSR_TM_LG\n\tmtmsrd\tr8\n\n\trldicl. r8, r8, 64 - MSR_TS_S_LG, 62 /* Did we actually hrfid? */\n\tbeq\t4f\nBEGIN_FTR_SECTION\n\tbl\tpnv_power9_force_smt4_catch\nEND_FTR_SECTION_IFSET(CPU_FTR_P9_TM_XER_SO_BUG)\n\tnop\n\n\t/*\n\t * It's possible that treclaim. may modify registers, if we have lost\n\t * track of fake-suspend state in the guest due to it using rfscv.\n\t * Save and restore registers in case this occurs.\n\t */\n\tmfspr\tr3, SPRN_DSCR\n\tmfspr\tr4, SPRN_XER\n\tmfspr\tr5, SPRN_AMR\n\t/* SPRN_TAR would need to be saved here if the kernel ever used it */\n\tmfcr\tr12\n\tSAVE_NVGPRS(r1)\n\tSAVE_GPR(2, r1)\n\tSAVE_GPR(3, r1)\n\tSAVE_GPR(4, r1)\n\tSAVE_GPR(5, r1)\n\tstw\tr12, 8(r1)\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\t/* We have to treclaim here because that's the only way to do S->N */\n\tli\tr3, TM_CAUSE_KVM_RESCHED\n\tTRECLAIM(R3)\n\n\tGET_PACA(r13)\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tREST_GPR(2, r1)\n\tREST_GPR(3, r1)\n\tREST_GPR(4, r1)\n\tREST_GPR(5, r1)\n\tlwz\tr12, 8(r1)\n\tREST_NVGPRS(r1)\n\tmtspr\tSPRN_DSCR, r3\n\tmtspr\tSPRN_XER, r4\n\tmtspr\tSPRN_AMR, r5\n\tmtcr\tr12\n\tHMT_MEDIUM\n\n\t/*\n\t * We were in fake suspend, so we are not going to save the\n\t * register state as the guest checkpointed state (since\n\t * we already have it), therefore we can now use any volatile GPR.\n\t * In fact treclaim in fake suspend state doesn't modify\n\t * any registers.\n\t */\n\nBEGIN_FTR_SECTION\n\tbl\tpnv_power9_force_smt4_release\nEND_FTR_SECTION_IFSET(CPU_FTR_P9_TM_XER_SO_BUG)\n\tnop\n\n4:\n\tmfspr\tr3, SPRN_PSSCR\n\t/* PSSCR_FAKE_SUSPEND is a write-only bit, but clear it anyway */\n\tli\tr0, PSSCR_FAKE_SUSPEND\n\tandc\tr3, r3, r0\n\tmtspr\tSPRN_PSSCR, r3\n\n\t/* Don't save TEXASR, use value from last exit in real suspend state */\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tmfspr\tr5, SPRN_TFHAR\n\tmfspr\tr6, SPRN_TFIAR\n\tstd\tr5, VCPU_TFHAR(r9)\n\tstd\tr6, VCPU_TFIAR(r9)\n\n\taddi\tr1, r1, TM_FRAME_SIZE\n\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n\n/*\n * Restore transactional state and TM-related registers.\n * Called with r3 pointing to the vcpu struct\n * and r4 containing the guest MSR value.\n * r5 is non-zero iff non-volatile register state needs to be maintained.\n * This potentially modifies all checkpointed registers.\n * It restores r1 and r2 from the PACA.\n */\n_GLOBAL_TOC(kvmppc_restore_tm_hv)\nEXPORT_SYMBOL_GPL(kvmppc_restore_tm_hv)\n\t/*\n\t * If we are doing TM emulation for the guest on a POWER9 DD2,\n\t * then we don't actually do a trechkpt -- we either set up\n\t * fake-suspend mode, or emulate a TM rollback.\n\t */\nBEGIN_FTR_SECTION\n\tb\t__kvmppc_restore_tm\nEND_FTR_SECTION_IFCLR(CPU_FTR_P9_TM_HV_ASSIST)\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\n\tli\tr0, 0\n\tstb\tr0, HSTATE_FAKE_SUSPEND(r13)\n\n\t/* Turn on TM so we can restore TM SPRs */\n\tmfmsr\tr5\n\tli\tr0, 1\n\trldimi\tr5, r0, MSR_TM_LG, 63-MSR_TM_LG\n\tmtmsrd\tr5\n\n\t/*\n\t * The user may change these outside of a transaction, so they must\n\t * always be context switched.\n\t */\n\tld\tr5, VCPU_TFHAR(r3)\n\tld\tr6, VCPU_TFIAR(r3)\n\tld\tr7, VCPU_TEXASR(r3)\n\tmtspr\tSPRN_TFHAR, r5\n\tmtspr\tSPRN_TFIAR, r6\n\tmtspr\tSPRN_TEXASR, r7\n\n\trldicl. r5, r4, 64 - MSR_TS_S_LG, 62\n\tbeqlr\t\t/* TM not active in guest */\n\n\t/* Make sure the failure summary is set */\n\toris\tr7, r7, (TEXASR_FS)@h\n\tmtspr\tSPRN_TEXASR, r7\n\n\tcmpwi\tr5, 1\t\t/* check for suspended state */\n\tbgt\t10f\n\tstb\tr5, HSTATE_FAKE_SUSPEND(r13)\n\tb\t9f\t\t/* and return */\n10:\tstdu\tr1, -PPC_MIN_STKFRM(r1)\n\t/* guest is in transactional state, so simulate rollback */\n\tbl\tkvmhv_emulate_tm_rollback\n\tnop\n\taddi\tr1, r1, PPC_MIN_STKFRM\n9:\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n\n/*\n * We come here if we get any exception or interrupt while we are\n * executing host real mode code while in guest MMU context.\n * r12 is (CR << 32) | vector\n * r13 points to our PACA\n * r12 is saved in HSTATE_SCRATCH0(r13)\n * r9 is saved in HSTATE_SCRATCH2(r13)\n * r13 is saved in HSPRG1\n * cfar is saved in HSTATE_CFAR(r13)\n * ppr is saved in HSTATE_PPR(r13)\n */\nkvmppc_bad_host_intr:\n\t/*\n\t * Switch to the emergency stack, but start half-way down in\n\t * case we were already on it.\n\t */\n\tmr\tr9, r1\n\tstd\tr1, PACAR1(r13)\n\tld\tr1, PACAEMERGSP(r13)\n\tsubi\tr1, r1, THREAD_SIZE/2 + INT_FRAME_SIZE\n\tstd\tr9, 0(r1)\n\tstd\tr0, GPR0(r1)\n\tstd\tr9, GPR1(r1)\n\tstd\tr2, GPR2(r1)\n\tSAVE_GPRS(3, 8, r1)\n\tsrdi\tr0, r12, 32\n\tclrldi\tr12, r12, 32\n\tstd\tr0, _CCR(r1)\n\tstd\tr12, _TRAP(r1)\n\tandi.\tr0, r12, 2\n\tbeq\t1f\n\tmfspr\tr3, SPRN_HSRR0\n\tmfspr\tr4, SPRN_HSRR1\n\tmfspr\tr5, SPRN_HDAR\n\tmfspr\tr6, SPRN_HDSISR\n\tb\t2f\n1:\tmfspr\tr3, SPRN_SRR0\n\tmfspr\tr4, SPRN_SRR1\n\tmfspr\tr5, SPRN_DAR\n\tmfspr\tr6, SPRN_DSISR\n2:\tstd\tr3, _NIP(r1)\n\tstd\tr4, _MSR(r1)\n\tstd\tr5, _DAR(r1)\n\tstd\tr6, _DSISR(r1)\n\tld\tr9, HSTATE_SCRATCH2(r13)\n\tld\tr12, HSTATE_SCRATCH0(r13)\n\tGET_SCRATCH0(r0)\n\tSAVE_GPRS(9, 12, r1)\n\tstd\tr0, GPR13(r1)\n\tSAVE_NVGPRS(r1)\n\tld\tr5, HSTATE_CFAR(r13)\n\tstd\tr5, ORIG_GPR3(r1)\n\tmflr\tr3\n\tmfctr\tr4\n\tmfxer\tr5\n\tlbz\tr6, PACAIRQSOFTMASK(r13)\n\tstd\tr3, _LINK(r1)\n\tstd\tr4, _CTR(r1)\n\tstd\tr5, _XER(r1)\n\tstd\tr6, SOFTE(r1)\n\tLOAD_PACA_TOC()\n\tLOAD_REG_IMMEDIATE(3, STACK_FRAME_REGS_MARKER)\n\tstd\tr3, STACK_INT_FRAME_MARKER(r1)\n\n\t/*\n\t * XXX On POWER7 and POWER8, we just spin here since we don't\n\t * know what the other threads are doing (and we don't want to\n\t * coordinate with them) - but at least we now have register state\n\t * in memory that we might be able to look at from another CPU.\n\t */\n\tb\t.\n\n/*\n * This mimics the MSR transition on IRQ delivery.  The new guest MSR is taken\n * from VCPU_INTR_MSR and is modified based on the required TM state changes.\n *   r11 has the guest MSR value (in/out)\n *   r9 has a vcpu pointer (in)\n *   r0 is used as a scratch register\n */\nSYM_FUNC_START_LOCAL(kvmppc_msr_interrupt)\n\trldicl\tr0, r11, 64 - MSR_TS_S_LG, 62\n\tcmpwi\tr0, 2 /* Check if we are in transactional state..  */\n\tld\tr11, VCPU_INTR_MSR(r9)\n\tbne\t1f\n\t/* ... if transactional, change to suspended */\n\tli\tr0, 1\n1:\trldimi\tr11, r0, MSR_TS_S_LG, 63 - MSR_TS_T_LG\n\tblr\nSYM_FUNC_END(kvmppc_msr_interrupt)\n\n/*\n * void kvmhv_load_guest_pmu(struct kvm_vcpu *vcpu)\n *\n * Load up guest PMU state.  R3 points to the vcpu struct.\n */\nSYM_FUNC_START_LOCAL(kvmhv_load_guest_pmu)\n\tmr\tr4, r3\n\tmflr\tr0\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tisync\nBEGIN_FTR_SECTION\n\tld\tr3, VCPU_MMCR(r4)\n\tandi.\tr5, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr5, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, VCPU_PMC(r4)\t/* always load up guest PMU registers */\n\tlwz\tr5, VCPU_PMC + 4(r4)\t/* to prevent information leak */\n\tlwz\tr6, VCPU_PMC + 8(r4)\n\tlwz\tr7, VCPU_PMC + 12(r4)\n\tlwz\tr8, VCPU_PMC + 16(r4)\n\tlwz\tr9, VCPU_PMC + 20(r4)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r5\n\tmtspr\tSPRN_PMC3, r6\n\tmtspr\tSPRN_PMC4, r7\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, VCPU_MMCR(r4)\n\tld\tr5, VCPU_MMCR + 8(r4)\n\tld\tr6, VCPU_MMCRA(r4)\n\tld\tr7, VCPU_SIAR(r4)\n\tld\tr8, VCPU_SDAR(r4)\n\tmtspr\tSPRN_MMCR1, r5\n\tmtspr\tSPRN_MMCRA, r6\n\tmtspr\tSPRN_SIAR, r7\n\tmtspr\tSPRN_SDAR, r8\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_MMCR + 16(r4)\n\tld\tr6, VCPU_SIER(r4)\n\tmtspr\tSPRN_MMCR2, r5\n\tmtspr\tSPRN_SIER, r6\n\tlwz\tr7, VCPU_PMC + 24(r4)\n\tlwz\tr8, VCPU_PMC + 28(r4)\n\tld\tr9, VCPU_MMCRS(r4)\n\tmtspr\tSPRN_SPMC1, r7\n\tmtspr\tSPRN_SPMC2, r8\n\tmtspr\tSPRN_MMCRS, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n\tmtlr\tr0\n\tblr\nSYM_FUNC_END(kvmhv_load_guest_pmu)\n\n/*\n * void kvmhv_load_host_pmu(void)\n *\n * Reload host PMU state saved in the PACA by kvmhv_save_host_pmu.\n */\nSYM_FUNC_START_LOCAL(kvmhv_load_host_pmu)\n\tmflr\tr0\n\tlbz\tr4, PACA_PMCINUSE(r13) /* is the host using the PMU? */\n\tcmpwi\tr4, 0\n\tbeq\t23f\t\t\t/* skip if not */\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_MMCR0(r13)\n\tandi.\tr4, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr4, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, HSTATE_PMC1(r13)\n\tlwz\tr4, HSTATE_PMC2(r13)\n\tlwz\tr5, HSTATE_PMC3(r13)\n\tlwz\tr6, HSTATE_PMC4(r13)\n\tlwz\tr8, HSTATE_PMC5(r13)\n\tlwz\tr9, HSTATE_PMC6(r13)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r4\n\tmtspr\tSPRN_PMC3, r5\n\tmtspr\tSPRN_PMC4, r6\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, HSTATE_MMCR0(r13)\n\tld\tr4, HSTATE_MMCR1(r13)\n\tld\tr5, HSTATE_MMCRA(r13)\n\tld\tr6, HSTATE_SIAR(r13)\n\tld\tr7, HSTATE_SDAR(r13)\n\tmtspr\tSPRN_MMCR1, r4\n\tmtspr\tSPRN_MMCRA, r5\n\tmtspr\tSPRN_SIAR, r6\n\tmtspr\tSPRN_SDAR, r7\nBEGIN_FTR_SECTION\n\tld\tr8, HSTATE_MMCR2(r13)\n\tld\tr9, HSTATE_SIER(r13)\n\tmtspr\tSPRN_MMCR2, r8\n\tmtspr\tSPRN_SIER, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n\tmtlr\tr0\n23:\tblr\nSYM_FUNC_END(kvmhv_load_host_pmu)\n\n/*\n * void kvmhv_save_guest_pmu(struct kvm_vcpu *vcpu, bool pmu_in_use)\n *\n * Save guest PMU state into the vcpu struct.\n * r3 = vcpu, r4 = full save flag (PMU in use flag set in VPA)\n */\nSYM_FUNC_START_LOCAL(kvmhv_save_guest_pmu)\n\tmr\tr9, r3\n\tmr\tr8, r4\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER8 seems to have a hardware bug where setting\n\t * MMCR0[PMAE] along with MMCR0[PMC1CE] and/or MMCR0[PMCjCE]\n\t * when some counters are already negative doesn't seem\n\t * to cause a performance monitor alert (and hence interrupt).\n\t * The effect of this is that when saving the PMU state,\n\t * if there is no PMU alert pending when we read MMCR0\n\t * before freezing the counters, but one becomes pending\n\t * before we read the counters, we lose it.\n\t * To work around this, we need a way to freeze the counters\n\t * before reading MMCR0.  Normally, freezing the counters\n\t * is done by writing MMCR0 (to set MMCR0[FC]) which\n\t * unavoidably writes MMCR0[PMA0] as well.  On POWER8,\n\t * we can also freeze the counters using MMCR2, by writing\n\t * 1s to all the counter freeze condition bits (there are\n\t * 9 bits each for 6 counters).\n\t */\n\tli\tr3, -1\t\t\t/* set all freeze bits */\n\tclrrdi\tr3, r3, 10\n\tmfspr\tr10, SPRN_MMCR2\n\tmtspr\tSPRN_MMCR2, r3\n\tisync\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmfspr\tr4, SPRN_MMCR0\t\t/* save MMCR0 */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tmfspr\tr6, SPRN_MMCRA\n\t/* Clear MMCRA in order to disable SDAR updates */\n\tli\tr7, 0\n\tmtspr\tSPRN_MMCRA, r7\n\tisync\n\tcmpwi\tr8, 0\t\t\t/* did they ask for PMU stuff to be saved? */\n\tbne\t21f\n\tstd\tr3, VCPU_MMCR(r9)\t/* if not, set saved MMCR0 to FC */\n\tb\t22f\n21:\tmfspr\tr5, SPRN_MMCR1\n\tmfspr\tr7, SPRN_SIAR\n\tmfspr\tr8, SPRN_SDAR\n\tstd\tr4, VCPU_MMCR(r9)\n\tstd\tr5, VCPU_MMCR + 8(r9)\n\tstd\tr6, VCPU_MMCRA(r9)\nBEGIN_FTR_SECTION\n\tstd\tr10, VCPU_MMCR + 16(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr7, VCPU_SIAR(r9)\n\tstd\tr8, VCPU_SDAR(r9)\n\tmfspr\tr3, SPRN_PMC1\n\tmfspr\tr4, SPRN_PMC2\n\tmfspr\tr5, SPRN_PMC3\n\tmfspr\tr6, SPRN_PMC4\n\tmfspr\tr7, SPRN_PMC5\n\tmfspr\tr8, SPRN_PMC6\n\tstw\tr3, VCPU_PMC(r9)\n\tstw\tr4, VCPU_PMC + 4(r9)\n\tstw\tr5, VCPU_PMC + 8(r9)\n\tstw\tr6, VCPU_PMC + 12(r9)\n\tstw\tr7, VCPU_PMC + 16(r9)\n\tstw\tr8, VCPU_PMC + 20(r9)\nBEGIN_FTR_SECTION\n\tmfspr\tr5, SPRN_SIER\n\tstd\tr5, VCPU_SIER(r9)\n\tmfspr\tr6, SPRN_SPMC1\n\tmfspr\tr7, SPRN_SPMC2\n\tmfspr\tr8, SPRN_MMCRS\n\tstw\tr6, VCPU_PMC + 24(r9)\n\tstw\tr7, VCPU_PMC + 28(r9)\n\tstd\tr8, VCPU_MMCRS(r9)\n\tlis\tr4, 0x8000\n\tmtspr\tSPRN_MMCRS, r4\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n22:\tblr\nSYM_FUNC_END(kvmhv_save_guest_pmu)\n\n/*\n * This works around a hardware bug on POWER8E processors, where\n * writing a 1 to the MMCR0[PMAO] bit doesn't generate a\n * performance monitor interrupt.  Instead, when we need to have\n * an interrupt pending, we have to arrange for a counter to overflow.\n */\nkvmppc_fix_pmao:\n\tli\tr3, 0\n\tmtspr\tSPRN_MMCR2, r3\n\tlis\tr3, (MMCR0_PMXE | MMCR0_FCECE)@h\n\tori\tr3, r3, MMCR0_PMCjCE | MMCR0_C56RUN\n\tmtspr\tSPRN_MMCR0, r3\n\tlis\tr3, 0x7fff\n\tori\tr3, r3, 0xffff\n\tmtspr\tSPRN_PMC6, r3\n\tisync\n\tblr\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n/*\n * Start timing an activity\n * r3 = pointer to time accumulation struct, r4 = vcpu\n */\nkvmhv_start_timing:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr6, VCORE_TB_OFFSET_APPL(r5)\n\tmftb\tr5\n\tsubf\tr5, r6, r5\t/* subtract current timebase offset */\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tstd\tr5, VCPU_ACTIVITY_START(r4)\n\tblr\n\n/*\n * Accumulate time to one activity and start another.\n * r3 = pointer to new time accumulation struct, r4 = vcpu\n */\nkvmhv_accumulate_time:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr8, VCORE_TB_OFFSET_APPL(r5)\n\tld\tr5, VCPU_CUR_ACTIVITY(r4)\n\tld\tr6, VCPU_ACTIVITY_START(r4)\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tmftb\tr7\n\tsubf\tr7, r8, r7\t/* subtract current timebase offset */\n\tstd\tr7, VCPU_ACTIVITY_START(r4)\n\tcmpdi\tr5, 0\n\tbeqlr\n\tsubf\tr3, r6, r7\n\tld\tr8, TAS_SEQCOUNT(r5)\n\tcmpdi\tr8, 0\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tlwsync\n\tld\tr7, TAS_TOTAL(r5)\n\tadd\tr7, r7, r3\n\tstd\tr7, TAS_TOTAL(r5)\n\tld\tr6, TAS_MIN(r5)\n\tld\tr7, TAS_MAX(r5)\n\tbeq\t3f\n\tcmpd\tr3, r6\n\tbge\t1f\n3:\tstd\tr3, TAS_MIN(r5)\n1:\tcmpd\tr3, r7\n\tble\t2f\n\tstd\tr3, TAS_MAX(r5)\n2:\tlwsync\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tblr\n#endif\n", "patch": "@@ -689,112 +689,8 @@ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n \n #ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n BEGIN_FTR_SECTION\n-\tb\tskip_tm\n-END_FTR_SECTION_IFCLR(CPU_FTR_TM)\n-\n-\t/* Turn on TM/FP/VSX/VMX so we can restore them. */\n-\tmfmsr\tr5\n-\tli\tr6, MSR_TM >> 32\n-\tsldi\tr6, r6, 32\n-\tor\tr5, r5, r6\n-\tori\tr5, r5, MSR_FP\n-\toris\tr5, r5, (MSR_VEC | MSR_VSX)@h\n-\tmtmsrd\tr5\n-\n-\t/*\n-\t * The user may change these outside of a transaction, so they must\n-\t * always be context switched.\n-\t */\n-\tld\tr5, VCPU_TFHAR(r4)\n-\tld\tr6, VCPU_TFIAR(r4)\n-\tld\tr7, VCPU_TEXASR(r4)\n-\tmtspr\tSPRN_TFHAR, r5\n-\tmtspr\tSPRN_TFIAR, r6\n-\tmtspr\tSPRN_TEXASR, r7\n-\n-\tld\tr5, VCPU_MSR(r4)\n-\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n-\tbeq\tskip_tm\t/* TM not active in guest */\n-\n-\t/* Make sure the failure summary is set, otherwise we'll program check\n-\t * when we trechkpt.  It's possible that this might have been not set\n-\t * on a kvmppc_set_one_reg() call but we shouldn't let this crash the\n-\t * host.\n-\t */\n-\toris\tr7, r7, (TEXASR_FS)@h\n-\tmtspr\tSPRN_TEXASR, r7\n-\n-\t/*\n-\t * We need to load up the checkpointed state for the guest.\n-\t * We need to do this early as it will blow away any GPRs, VSRs and\n-\t * some SPRs.\n-\t */\n-\n-\tmr\tr31, r4\n-\taddi\tr3, r31, VCPU_FPRS_TM\n-\tbl\tload_fp_state\n-\taddi\tr3, r31, VCPU_VRS_TM\n-\tbl\tload_vr_state\n-\tmr\tr4, r31\n-\tlwz\tr7, VCPU_VRSAVE_TM(r4)\n-\tmtspr\tSPRN_VRSAVE, r7\n-\n-\tld\tr5, VCPU_LR_TM(r4)\n-\tlwz\tr6, VCPU_CR_TM(r4)\n-\tld\tr7, VCPU_CTR_TM(r4)\n-\tld\tr8, VCPU_AMR_TM(r4)\n-\tld\tr9, VCPU_TAR_TM(r4)\n-\tmtlr\tr5\n-\tmtcr\tr6\n-\tmtctr\tr7\n-\tmtspr\tSPRN_AMR, r8\n-\tmtspr\tSPRN_TAR, r9\n-\n-\t/*\n-\t * Load up PPR and DSCR values but don't put them in the actual SPRs\n-\t * till the last moment to avoid running with userspace PPR and DSCR for\n-\t * too long.\n-\t */\n-\tld\tr29, VCPU_DSCR_TM(r4)\n-\tld\tr30, VCPU_PPR_TM(r4)\n-\n-\tstd\tr2, PACATMSCRATCH(r13) /* Save TOC */\n-\n-\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n-\tli\tr5, 0\n-\tmtmsrd\tr5, 1\n-\n-\t/* Load GPRs r0-r28 */\n-\treg = 0\n-\t.rept\t29\n-\tld\treg, VCPU_GPRS_TM(reg)(r31)\n-\treg = reg + 1\n-\t.endr\n-\n-\tmtspr\tSPRN_DSCR, r29\n-\tmtspr\tSPRN_PPR, r30\n-\n-\t/* Load final GPRs */\n-\tld\t29, VCPU_GPRS_TM(29)(r31)\n-\tld\t30, VCPU_GPRS_TM(30)(r31)\n-\tld\t31, VCPU_GPRS_TM(31)(r31)\n-\n-\t/* TM checkpointed state is now setup.  All GPRs are now volatile. */\n-\tTRECHKPT\n-\n-\t/* Now let's get back the state we need. */\n-\tHMT_MEDIUM\n-\tGET_PACA(r13)\n-\tld\tr29, HSTATE_DSCR(r13)\n-\tmtspr\tSPRN_DSCR, r29\n-\tld\tr4, HSTATE_KVM_VCPU(r13)\n-\tld\tr1, HSTATE_HOST_R1(r13)\n-\tld\tr2, PACATMSCRATCH(r13)\n-\n-\t/* Set the MSR RI since we have our registers back. */\n-\tli\tr5, MSR_RI\n-\tmtmsrd\tr5, 1\n-skip_tm:\n+\tbl\tkvmppc_restore_tm\n+END_FTR_SECTION_IFSET(CPU_FTR_TM)\n #endif\n \n \t/* Load guest PMU registers */\n@@ -875,12 +771,6 @@ BEGIN_FTR_SECTION\n \t/* Skip next section on POWER7 */\n \tb\t8f\n END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n-\t/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */\n-\tmfmsr\tr8\n-\tli\tr0, 1\n-\trldimi\tr8, r0, MSR_TM_LG, 63-MSR_TM_LG\n-\tmtmsrd\tr8\n-\n \t/* Load up POWER8-specific registers */\n \tld\tr5, VCPU_IAMR(r4)\n \tlwz\tr6, VCPU_PSPB(r4)\n@@ -1470,106 +1360,8 @@ END_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n \n #ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n BEGIN_FTR_SECTION\n-\tb\t2f\n-END_FTR_SECTION_IFCLR(CPU_FTR_TM)\n-\t/* Turn on TM. */\n-\tmfmsr\tr8\n-\tli\tr0, 1\n-\trldimi\tr8, r0, MSR_TM_LG, 63-MSR_TM_LG\n-\tmtmsrd\tr8\n-\n-\tld\tr5, VCPU_MSR(r9)\n-\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n-\tbeq\t1f\t/* TM not active in guest. */\n-\n-\tli\tr3, TM_CAUSE_KVM_RESCHED\n-\n-\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n-\tli\tr5, 0\n-\tmtmsrd\tr5, 1\n-\n-\t/* All GPRs are volatile at this point. */\n-\tTRECLAIM(R3)\n-\n-\t/* Temporarily store r13 and r9 so we have some regs to play with */\n-\tSET_SCRATCH0(r13)\n-\tGET_PACA(r13)\n-\tstd\tr9, PACATMSCRATCH(r13)\n-\tld\tr9, HSTATE_KVM_VCPU(r13)\n-\n-\t/* Get a few more GPRs free. */\n-\tstd\tr29, VCPU_GPRS_TM(29)(r9)\n-\tstd\tr30, VCPU_GPRS_TM(30)(r9)\n-\tstd\tr31, VCPU_GPRS_TM(31)(r9)\n-\n-\t/* Save away PPR and DSCR soon so don't run with user values. */\n-\tmfspr\tr31, SPRN_PPR\n-\tHMT_MEDIUM\n-\tmfspr\tr30, SPRN_DSCR\n-\tld\tr29, HSTATE_DSCR(r13)\n-\tmtspr\tSPRN_DSCR, r29\n-\n-\t/* Save all but r9, r13 & r29-r31 */\n-\treg = 0\n-\t.rept\t29\n-\t.if (reg != 9) && (reg != 13)\n-\tstd\treg, VCPU_GPRS_TM(reg)(r9)\n-\t.endif\n-\treg = reg + 1\n-\t.endr\n-\t/* ... now save r13 */\n-\tGET_SCRATCH0(r4)\n-\tstd\tr4, VCPU_GPRS_TM(13)(r9)\n-\t/* ... and save r9 */\n-\tld\tr4, PACATMSCRATCH(r13)\n-\tstd\tr4, VCPU_GPRS_TM(9)(r9)\n-\n-\t/* Reload stack pointer and TOC. */\n-\tld\tr1, HSTATE_HOST_R1(r13)\n-\tld\tr2, PACATOC(r13)\n-\n-\t/* Set MSR RI now we have r1 and r13 back. */\n-\tli\tr5, MSR_RI\n-\tmtmsrd\tr5, 1\n-\n-\t/* Save away checkpinted SPRs. */\n-\tstd\tr31, VCPU_PPR_TM(r9)\n-\tstd\tr30, VCPU_DSCR_TM(r9)\n-\tmflr\tr5\n-\tmfcr\tr6\n-\tmfctr\tr7\n-\tmfspr\tr8, SPRN_AMR\n-\tmfspr\tr10, SPRN_TAR\n-\tstd\tr5, VCPU_LR_TM(r9)\n-\tstw\tr6, VCPU_CR_TM(r9)\n-\tstd\tr7, VCPU_CTR_TM(r9)\n-\tstd\tr8, VCPU_AMR_TM(r9)\n-\tstd\tr10, VCPU_TAR_TM(r9)\n-\n-\t/* Restore r12 as trap number. */\n-\tlwz\tr12, VCPU_TRAP(r9)\n-\n-\t/* Save FP/VSX. */\n-\taddi\tr3, r9, VCPU_FPRS_TM\n-\tbl\tstore_fp_state\n-\taddi\tr3, r9, VCPU_VRS_TM\n-\tbl\tstore_vr_state\n-\tmfspr\tr6, SPRN_VRSAVE\n-\tstw\tr6, VCPU_VRSAVE_TM(r9)\n-1:\n-\t/*\n-\t * We need to save these SPRs after the treclaim so that the software\n-\t * error code is recorded correctly in the TEXASR.  Also the user may\n-\t * change these outside of a transaction, so they must always be\n-\t * context switched.\n-\t */\n-\tmfspr\tr5, SPRN_TFHAR\n-\tmfspr\tr6, SPRN_TFIAR\n-\tmfspr\tr7, SPRN_TEXASR\n-\tstd\tr5, VCPU_TFHAR(r9)\n-\tstd\tr6, VCPU_TFIAR(r9)\n-\tstd\tr7, VCPU_TEXASR(r9)\n-2:\n+\tbl\tkvmppc_save_tm\n+END_FTR_SECTION_IFSET(CPU_FTR_TM)\n #endif\n \n \t/* Increment yield count if they have a VPA */\n@@ -2694,6 +2486,239 @@ END_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n \tmr\tr4,r31\n \tblr\n \n+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n+/*\n+ * Save transactional state and TM-related registers.\n+ * Called with r9 pointing to the vcpu struct.\n+ * This can modify all checkpointed registers, but\n+ * restores r1, r2 and r9 (vcpu pointer) before exit.\n+ */\n+kvmppc_save_tm:\n+\tmflr\tr0\n+\tstd\tr0, PPC_LR_STKOFF(r1)\n+\n+\t/* Turn on TM. */\n+\tmfmsr\tr8\n+\tli\tr0, 1\n+\trldimi\tr8, r0, MSR_TM_LG, 63-MSR_TM_LG\n+\tmtmsrd\tr8\n+\n+\tld\tr5, VCPU_MSR(r9)\n+\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n+\tbeq\t1f\t/* TM not active in guest. */\n+\n+\tstd\tr1, HSTATE_HOST_R1(r13)\n+\tli\tr3, TM_CAUSE_KVM_RESCHED\n+\n+\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n+\tli\tr5, 0\n+\tmtmsrd\tr5, 1\n+\n+\t/* All GPRs are volatile at this point. */\n+\tTRECLAIM(R3)\n+\n+\t/* Temporarily store r13 and r9 so we have some regs to play with */\n+\tSET_SCRATCH0(r13)\n+\tGET_PACA(r13)\n+\tstd\tr9, PACATMSCRATCH(r13)\n+\tld\tr9, HSTATE_KVM_VCPU(r13)\n+\n+\t/* Get a few more GPRs free. */\n+\tstd\tr29, VCPU_GPRS_TM(29)(r9)\n+\tstd\tr30, VCPU_GPRS_TM(30)(r9)\n+\tstd\tr31, VCPU_GPRS_TM(31)(r9)\n+\n+\t/* Save away PPR and DSCR soon so don't run with user values. */\n+\tmfspr\tr31, SPRN_PPR\n+\tHMT_MEDIUM\n+\tmfspr\tr30, SPRN_DSCR\n+\tld\tr29, HSTATE_DSCR(r13)\n+\tmtspr\tSPRN_DSCR, r29\n+\n+\t/* Save all but r9, r13 & r29-r31 */\n+\treg = 0\n+\t.rept\t29\n+\t.if (reg != 9) && (reg != 13)\n+\tstd\treg, VCPU_GPRS_TM(reg)(r9)\n+\t.endif\n+\treg = reg + 1\n+\t.endr\n+\t/* ... now save r13 */\n+\tGET_SCRATCH0(r4)\n+\tstd\tr4, VCPU_GPRS_TM(13)(r9)\n+\t/* ... and save r9 */\n+\tld\tr4, PACATMSCRATCH(r13)\n+\tstd\tr4, VCPU_GPRS_TM(9)(r9)\n+\n+\t/* Reload stack pointer and TOC. */\n+\tld\tr1, HSTATE_HOST_R1(r13)\n+\tld\tr2, PACATOC(r13)\n+\n+\t/* Set MSR RI now we have r1 and r13 back. */\n+\tli\tr5, MSR_RI\n+\tmtmsrd\tr5, 1\n+\n+\t/* Save away checkpinted SPRs. */\n+\tstd\tr31, VCPU_PPR_TM(r9)\n+\tstd\tr30, VCPU_DSCR_TM(r9)\n+\tmflr\tr5\n+\tmfcr\tr6\n+\tmfctr\tr7\n+\tmfspr\tr8, SPRN_AMR\n+\tmfspr\tr10, SPRN_TAR\n+\tstd\tr5, VCPU_LR_TM(r9)\n+\tstw\tr6, VCPU_CR_TM(r9)\n+\tstd\tr7, VCPU_CTR_TM(r9)\n+\tstd\tr8, VCPU_AMR_TM(r9)\n+\tstd\tr10, VCPU_TAR_TM(r9)\n+\n+\t/* Restore r12 as trap number. */\n+\tlwz\tr12, VCPU_TRAP(r9)\n+\n+\t/* Save FP/VSX. */\n+\taddi\tr3, r9, VCPU_FPRS_TM\n+\tbl\tstore_fp_state\n+\taddi\tr3, r9, VCPU_VRS_TM\n+\tbl\tstore_vr_state\n+\tmfspr\tr6, SPRN_VRSAVE\n+\tstw\tr6, VCPU_VRSAVE_TM(r9)\n+1:\n+\t/*\n+\t * We need to save these SPRs after the treclaim so that the software\n+\t * error code is recorded correctly in the TEXASR.  Also the user may\n+\t * change these outside of a transaction, so they must always be\n+\t * context switched.\n+\t */\n+\tmfspr\tr5, SPRN_TFHAR\n+\tmfspr\tr6, SPRN_TFIAR\n+\tmfspr\tr7, SPRN_TEXASR\n+\tstd\tr5, VCPU_TFHAR(r9)\n+\tstd\tr6, VCPU_TFIAR(r9)\n+\tstd\tr7, VCPU_TEXASR(r9)\n+\n+\tld\tr0, PPC_LR_STKOFF(r1)\n+\tmtlr\tr0\n+\tblr\n+\n+/*\n+ * Restore transactional state and TM-related registers.\n+ * Called with r4 pointing to the vcpu struct.\n+ * This potentially modifies all checkpointed registers.\n+ * It restores r1, r2, r4 from the PACA.\n+ */\n+kvmppc_restore_tm:\n+\tmflr\tr0\n+\tstd\tr0, PPC_LR_STKOFF(r1)\n+\n+\t/* Turn on TM/FP/VSX/VMX so we can restore them. */\n+\tmfmsr\tr5\n+\tli\tr6, MSR_TM >> 32\n+\tsldi\tr6, r6, 32\n+\tor\tr5, r5, r6\n+\tori\tr5, r5, MSR_FP\n+\toris\tr5, r5, (MSR_VEC | MSR_VSX)@h\n+\tmtmsrd\tr5\n+\n+\t/*\n+\t * The user may change these outside of a transaction, so they must\n+\t * always be context switched.\n+\t */\n+\tld\tr5, VCPU_TFHAR(r4)\n+\tld\tr6, VCPU_TFIAR(r4)\n+\tld\tr7, VCPU_TEXASR(r4)\n+\tmtspr\tSPRN_TFHAR, r5\n+\tmtspr\tSPRN_TFIAR, r6\n+\tmtspr\tSPRN_TEXASR, r7\n+\n+\tld\tr5, VCPU_MSR(r4)\n+\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n+\tbeqlr\t\t/* TM not active in guest */\n+\tstd\tr1, HSTATE_HOST_R1(r13)\n+\n+\t/* Make sure the failure summary is set, otherwise we'll program check\n+\t * when we trechkpt.  It's possible that this might have been not set\n+\t * on a kvmppc_set_one_reg() call but we shouldn't let this crash the\n+\t * host.\n+\t */\n+\toris\tr7, r7, (TEXASR_FS)@h\n+\tmtspr\tSPRN_TEXASR, r7\n+\n+\t/*\n+\t * We need to load up the checkpointed state for the guest.\n+\t * We need to do this early as it will blow away any GPRs, VSRs and\n+\t * some SPRs.\n+\t */\n+\n+\tmr\tr31, r4\n+\taddi\tr3, r31, VCPU_FPRS_TM\n+\tbl\tload_fp_state\n+\taddi\tr3, r31, VCPU_VRS_TM\n+\tbl\tload_vr_state\n+\tmr\tr4, r31\n+\tlwz\tr7, VCPU_VRSAVE_TM(r4)\n+\tmtspr\tSPRN_VRSAVE, r7\n+\n+\tld\tr5, VCPU_LR_TM(r4)\n+\tlwz\tr6, VCPU_CR_TM(r4)\n+\tld\tr7, VCPU_CTR_TM(r4)\n+\tld\tr8, VCPU_AMR_TM(r4)\n+\tld\tr9, VCPU_TAR_TM(r4)\n+\tmtlr\tr5\n+\tmtcr\tr6\n+\tmtctr\tr7\n+\tmtspr\tSPRN_AMR, r8\n+\tmtspr\tSPRN_TAR, r9\n+\n+\t/*\n+\t * Load up PPR and DSCR values but don't put them in the actual SPRs\n+\t * till the last moment to avoid running with userspace PPR and DSCR for\n+\t * too long.\n+\t */\n+\tld\tr29, VCPU_DSCR_TM(r4)\n+\tld\tr30, VCPU_PPR_TM(r4)\n+\n+\tstd\tr2, PACATMSCRATCH(r13) /* Save TOC */\n+\n+\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n+\tli\tr5, 0\n+\tmtmsrd\tr5, 1\n+\n+\t/* Load GPRs r0-r28 */\n+\treg = 0\n+\t.rept\t29\n+\tld\treg, VCPU_GPRS_TM(reg)(r31)\n+\treg = reg + 1\n+\t.endr\n+\n+\tmtspr\tSPRN_DSCR, r29\n+\tmtspr\tSPRN_PPR, r30\n+\n+\t/* Load final GPRs */\n+\tld\t29, VCPU_GPRS_TM(29)(r31)\n+\tld\t30, VCPU_GPRS_TM(30)(r31)\n+\tld\t31, VCPU_GPRS_TM(31)(r31)\n+\n+\t/* TM checkpointed state is now setup.  All GPRs are now volatile. */\n+\tTRECHKPT\n+\n+\t/* Now let's get back the state we need. */\n+\tHMT_MEDIUM\n+\tGET_PACA(r13)\n+\tld\tr29, HSTATE_DSCR(r13)\n+\tmtspr\tSPRN_DSCR, r29\n+\tld\tr4, HSTATE_KVM_VCPU(r13)\n+\tld\tr1, HSTATE_HOST_R1(r13)\n+\tld\tr2, PACATMSCRATCH(r13)\n+\n+\t/* Set the MSR RI since we have our registers back. */\n+\tli\tr5, MSR_RI\n+\tmtmsrd\tr5, 1\n+\n+\tld\tr0, PPC_LR_STKOFF(r1)\n+\tmtlr\tr0\n+\tblr\n+#endif\n+\n /*\n  * We come here if we get any exception or interrupt while we are\n  * executing host real mode code while in guest MMU context.", "file_path": "files/2016_8\\103", "file_language": "S", "file_name": "arch/powerpc/kvm/book3s_hv_rmhandlers.S", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 42, "cve_id": "CVE-2016-5412", "cwe_id": ["CWE-399"], "cve_language": "C", "cve_description": "arch/powerpc/kvm/book3s_hv_rmhandlers.S in the Linux kernel through 4.7 on PowerPC platforms, when CONFIG_KVM_BOOK3S_64_HV is enabled, allows guest OS users to cause a denial of service (host OS infinite loop) by making a H_CEDE hypercall during the existence of a suspended transaction.", "cvss": "6.5", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "CHANGED", "C": "NONE", "I": "NONE", "A": "HIGH", "commit_id": "93d17397e4e2182fdaad503e2f9da46202c0f1c3", "commit_message": "KVM: PPC: Book3S HV: Save/restore TM state in H_CEDE\n\nIt turns out that if the guest does a H_CEDE while the CPU is in\na transactional state, and the H_CEDE does a nap, and the nap\nloses the architected state of the CPU (which is is allowed to do),\nthen we lose the checkpointed state of the virtual CPU.  In addition,\nthe transactional-memory state recorded in the MSR gets reset back\nto non-transactional, and when we try to return to the guest, we take\na TM bad thing type of program interrupt because we are trying to\ntransition from non-transactional to transactional with a hrfid\ninstruction, which is not permitted.\n\nThe result of the program interrupt occurring at that point is that\nthe host CPU will hang in an infinite loop with interrupts disabled.\nThus this is a denial of service vulnerability in the host which can\nbe triggered by any guest (and depending on the guest kernel, it can\npotentially triggered by unprivileged userspace in the guest).\n\nThis vulnerability has been assigned the ID CVE-2016-5412.\n\nTo fix this, we save the TM state before napping and restore it\non exit from the nap, when handling a H_CEDE in real mode.  The\ncase where H_CEDE exits to host virtual mode is already OK (as are\nother hcalls which exit to host virtual mode) because the exit\npath saves the TM state.\n\nCc: stable@vger.kernel.org # v3.15+\nSigned-off-by: Paul Mackerras <paulus@ozlabs.org>", "commit_date": "2016-07-28T06:10:07Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/93d17397e4e2182fdaad503e2f9da46202c0f1c3", "html_url": "https://github.com/torvalds/linux/commit/93d17397e4e2182fdaad503e2f9da46202c0f1c3", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "f024ee098476a3e620232e4a78cfac505f121245", "url_before": "https://api.github.com/repos/torvalds/linux/commits/f024ee098476a3e620232e4a78cfac505f121245", "html_url_before": "https://github.com/torvalds/linux/commit/f024ee098476a3e620232e4a78cfac505f121245"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/93d17397e4e2182fdaad503e2f9da46202c0f1c3/arch/powerpc/kvm/book3s_hv_rmhandlers.S", "code": "/*\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * Copyright 2011 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * Derived from book3s_rmhandlers.S and other files, which are:\n *\n * Copyright SUSE Linux Products GmbH 2009\n *\n * Authors: Alexander Graf <agraf@suse.de>\n */\n\n#include <asm/ppc_asm.h>\n#include <asm/kvm_asm.h>\n#include <asm/reg.h>\n#include <asm/mmu.h>\n#include <asm/page.h>\n#include <asm/ptrace.h>\n#include <asm/hvcall.h>\n#include <asm/asm-offsets.h>\n#include <asm/exception-64s.h>\n#include <asm/kvm_book3s_asm.h>\n#include <asm/book3s/64/mmu-hash.h>\n#include <asm/tm.h>\n#include <asm/opal.h>\n\n#define VCPU_GPRS_TM(reg) (((reg) * ULONG_SIZE) + VCPU_GPR_TM)\n\n/* Values in HSTATE_NAPPING(r13) */\n#define NAPPING_CEDE\t1\n#define NAPPING_NOVCPU\t2\n\n/*\n * Call kvmppc_hv_entry in real mode.\n * Must be called with interrupts hard-disabled.\n *\n * Input Registers:\n *\n * LR = return address to continue at after eventually re-enabling MMU\n */\n_GLOBAL_TOC(kvmppc_hv_entry_trampoline)\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -112(r1)\n\tmfmsr\tr10\n\tLOAD_REG_ADDR(r5, kvmppc_call_hv_entry)\n\tli\tr0,MSR_RI\n\tandc\tr0,r10,r0\n\tli\tr6,MSR_IR | MSR_DR\n\tandc\tr6,r10,r6\n\tmtmsrd\tr0,1\t\t/* clear RI in MSR */\n\tmtsrr0\tr5\n\tmtsrr1\tr6\n\tRFI\n\nkvmppc_call_hv_entry:\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from guest - restore host state and return to caller */\n\nBEGIN_FTR_SECTION\n\t/* Restore host DABR and DABRX */\n\tld\tr5,HSTATE_DABR(r13)\n\tli\tr6,7\n\tmtspr\tSPRN_DABR,r5\n\tmtspr\tSPRN_DABRX,r6\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n\t/* Restore SPRG3 */\n\tld\tr3,PACA_SPRG_VDSO(r13)\n\tmtspr\tSPRN_SPRG_VDSO_WRITE,r3\n\n\t/* Reload the host's PMU registers */\n\tld\tr3, PACALPPACAPTR(r13)\t/* is the host using the PMU? */\n\tlbz\tr4, LPPACA_PMCINUSE(r3)\n\tcmpwi\tr4, 0\n\tbeq\t23f\t\t\t/* skip if not */\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_MMCR0(r13)\n\tandi.\tr4, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr4, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, HSTATE_PMC1(r13)\n\tlwz\tr4, HSTATE_PMC2(r13)\n\tlwz\tr5, HSTATE_PMC3(r13)\n\tlwz\tr6, HSTATE_PMC4(r13)\n\tlwz\tr8, HSTATE_PMC5(r13)\n\tlwz\tr9, HSTATE_PMC6(r13)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r4\n\tmtspr\tSPRN_PMC3, r5\n\tmtspr\tSPRN_PMC4, r6\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, HSTATE_MMCR0(r13)\n\tld\tr4, HSTATE_MMCR1(r13)\n\tld\tr5, HSTATE_MMCRA(r13)\n\tld\tr6, HSTATE_SIAR(r13)\n\tld\tr7, HSTATE_SDAR(r13)\n\tmtspr\tSPRN_MMCR1, r4\n\tmtspr\tSPRN_MMCRA, r5\n\tmtspr\tSPRN_SIAR, r6\n\tmtspr\tSPRN_SDAR, r7\nBEGIN_FTR_SECTION\n\tld\tr8, HSTATE_MMCR2(r13)\n\tld\tr9, HSTATE_SIER(r13)\n\tmtspr\tSPRN_MMCR2, r8\n\tmtspr\tSPRN_SIER, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n23:\n\n\t/*\n\t * Reload DEC.  HDEC interrupts were disabled when\n\t * we reloaded the host's LPCR value.\n\t */\n\tld\tr3, HSTATE_DECEXP(r13)\n\tmftb\tr4\n\tsubf\tr4, r4, r3\n\tmtspr\tSPRN_DEC, r4\n\n\t/* hwthread_req may have got set by cede or no vcpu, so clear it */\n\tli\tr0, 0\n\tstb\tr0, HSTATE_HWTHREAD_REQ(r13)\n\n\t/*\n\t * For external and machine check interrupts, we need\n\t * to call the Linux handler to process the interrupt.\n\t * We do that by jumping to absolute address 0x500 for\n\t * external interrupts, or the machine_check_fwnmi label\n\t * for machine checks (since firmware might have patched\n\t * the vector area at 0x200).  The [h]rfid at the end of the\n\t * handler will return to the book3s_hv_interrupts.S code.\n\t * For other interrupts we do the rfid to get back\n\t * to the book3s_hv_interrupts.S code here.\n\t */\n\tld\tr8, 112+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, 112\n\tld\tr7, HSTATE_HOST_MSR(r13)\n\n\tcmpwi\tcr1, r12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tcmpwi\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbeq\t11f\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\tbeq \t15f\t/* Invoke the H_DOORBELL handler */\n\tcmpwi\tcr2, r12, BOOK3S_INTERRUPT_HMI\n\tbeq\tcr2, 14f\t\t\t/* HMI check */\n\n\t/* RFI into the highmem handler, or branch to interrupt handler */\n\tmfmsr\tr6\n\tli\tr0, MSR_RI\n\tandc\tr6, r6, r0\n\tmtmsrd\tr6, 1\t\t\t/* Clear RI in MSR */\n\tmtsrr0\tr8\n\tmtsrr1\tr7\n\tbeq\tcr1, 13f\t\t/* machine check */\n\tRFI\n\n\t/* On POWER7, we have external interrupts set to use HSRR0/1 */\n11:\tmtspr\tSPRN_HSRR0, r8\n\tmtspr\tSPRN_HSRR1, r7\n\tba\t0x500\n\n13:\tb\tmachine_check_fwnmi\n\n14:\tmtspr\tSPRN_HSRR0, r8\n\tmtspr\tSPRN_HSRR1, r7\n\tb\thmi_exception_after_realmode\n\n15:\tmtspr SPRN_HSRR0, r8\n\tmtspr SPRN_HSRR1, r7\n\tba    0xe80\n\nkvmppc_primary_no_guest:\n\t/* We handle this much like a ceded vcpu */\n\t/* put the HDEC into the DEC, since HDEC interrupts don't wake us */\n\tmfspr\tr3, SPRN_HDEC\n\tmtspr\tSPRN_DEC, r3\n\t/*\n\t * Make sure the primary has finished the MMU switch.\n\t * We should never get here on a secondary thread, but\n\t * check it for robustness' sake.\n\t */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n65:\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t65b\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\t/* set our bit in napping_threads */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n1:\tlwarx\tr3, 0, r6\n\tor\tr3, r3, r0\n\tstwcx.\tr3, 0, r6\n\tbne\t1b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr12, 0\n\tlwz\tr7, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7, 0x100\n\tbge\tkvm_novcpu_exit\t/* another thread already exiting */\n\tli\tr3, NAPPING_NOVCPU\n\tstb\tr3, HSTATE_NAPPING(r13)\n\n\tli\tr3, 0\t\t/* Don't wake on privileged (OS) doorbell */\n\tb\tkvm_do_nap\n\nkvm_novcpu_wakeup:\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tli\tr0, 0\n\tstb\tr0, HSTATE_NAPPING(r13)\n\n\t/* check the wake reason */\n\tbl\tkvmppc_check_wake_reason\n\n\t/* see if any other thread is already exiting */\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tbge\tkvm_novcpu_exit\n\n\t/* clear our bit in napping_threads */\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n4:\tlwarx\tr7, 0, r6\n\tandc\tr7, r7, r0\n\tstwcx.\tr7, 0, r6\n\tbne\t4b\n\n\t/* See if the wake reason means we need to exit */\n\tcmpdi\tr3, 0\n\tbge\tkvm_novcpu_exit\n\n\t/* See if our timeslice has expired (HDEC is negative) */\n\tmfspr\tr0, SPRN_HDEC\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n\tcmpwi\tr0, 0\n\tblt\tkvm_novcpu_exit\n\n\t/* Got an IPI but other vcpus aren't yet exiting, must be a latecomer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n#endif\n\tb\tkvmppc_got_guest\n\nkvm_novcpu_exit:\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\t13f\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n13:\tmr\tr3, r12\n\tstw\tr12, 112-4(r1)\n\tbl\tkvmhv_commence_exit\n\tnop\n\tlwz\tr12, 112-4(r1)\n\tb\tkvmhv_switch_to_host\n\n/*\n * We come in here when wakened from nap mode.\n * Relocation is off and most register values are lost.\n * r13 points to the PACA.\n */\n\t.globl\tkvm_start_guest\nkvm_start_guest:\n\n\t/* Set runlatch bit the minute you wake up from nap */\n\tmfspr\tr0, SPRN_CTRLF\n\tori \tr0, r0, 1\n\tmtspr\tSPRN_CTRLT, r0\n\n\tld\tr2,PACATOC(r13)\n\n\tli\tr0,KVM_HWTHREAD_IN_KVM\n\tstb\tr0,HSTATE_HWTHREAD_STATE(r13)\n\n\t/* NV GPR values from power7_idle() will no longer be valid */\n\tli\tr0,1\n\tstb\tr0,PACA_NAPSTATELOST(r13)\n\n\t/* were we napping due to cede? */\n\tlbz\tr0,HSTATE_NAPPING(r13)\n\tcmpwi\tr0,NAPPING_CEDE\n\tbeq\tkvm_end_cede\n\tcmpwi\tr0,NAPPING_NOVCPU\n\tbeq\tkvm_novcpu_wakeup\n\n\tld\tr1,PACAEMERGSP(r13)\n\tsubi\tr1,r1,STACK_FRAME_OVERHEAD\n\n\t/*\n\t * We weren't napping due to cede, so this must be a secondary\n\t * thread being woken up to run a guest, or being woken up due\n\t * to a stray IPI.  (Or due to some machine check or hypervisor\n\t * maintenance interrupt while the core is in KVM.)\n\t */\n\n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\tcmpdi\tr3, 0\n\tbge\tkvm_no_guest\n\n\t/* get vcore pointer, NULL if we have nothing to run */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5,0\n\t/* if we have no vcore to run, go back to sleep */\n\tbeq\tkvm_no_guest\n\nkvm_secondary_got_guest:\n\n\t/* Set HSTATE_DSCR(r13) to something sensible */\n\tld\tr6, PACA_DSCR_DEFAULT(r13)\n\tstd\tr6, HSTATE_DSCR(r13)\n\n\t/* On thread 0 of a subcore, set HDEC to max */\n\tlbz\tr4, HSTATE_PTID(r13)\n\tcmpwi\tr4, 0\n\tbne\t63f\n\tlis\tr6, 0x7fff\n\tori\tr6, r6, 0xffff\n\tmtspr\tSPRN_HDEC, r6\n\t/* and set per-LPAR registers, if doing dynamic micro-threading */\n\tld\tr6, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr6, 0\n\tbeq\t63f\n\tld\tr0, KVM_SPLIT_RPR(r6)\n\tmtspr\tSPRN_RPR, r0\n\tld\tr0, KVM_SPLIT_PMMAR(r6)\n\tmtspr\tSPRN_PMMAR, r0\n\tld\tr0, KVM_SPLIT_LDBAR(r6)\n\tmtspr\tSPRN_LDBAR, r0\n\tisync\n63:\n\t/* Order load of vcpu after load of vcore */\n\tlwsync\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from the guest, go back to nap */\n\t/* Clear our vcpu and vcore pointers so we don't come back in early */\n\tli\tr0, 0\n\tstd\tr0, HSTATE_KVM_VCPU(r13)\n\t/*\n\t * Once we clear HSTATE_KVM_VCORE(r13), the code in\n\t * kvmppc_run_core() is going to assume that all our vcpu\n\t * state is visible in memory.  This lwsync makes sure\n\t * that that is true.\n\t */\n\tlwsync\n\tstd\tr0, HSTATE_KVM_VCORE(r13)\n\n\t/*\n\t * All secondaries exiting guest will fall through this path.\n\t * Before proceeding, just check for HMI interrupt and\n\t * invoke opal hmi handler. By now we are sure that the\n\t * primary thread on this core/subcore has already made partition\n\t * switch/TB resync and we are good to call opal hmi handler.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\tkvm_no_guest\n\n\tli\tr3,0\t\t\t/* NULL argument */\n\tbl\thmi_exception_realmode\n/*\n * At this point we have finished executing in the guest.\n * We need to wait for hwthread_req to become zero, since\n * we may not turn on the MMU while hwthread_req is non-zero.\n * While waiting we also need to check if we get given a vcpu to run.\n */\nkvm_no_guest:\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t53f\n\tHMT_MEDIUM\n\tli\tr0, KVM_HWTHREAD_IN_KERNEL\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\t/* need to recheck hwthread_req after a barrier, to avoid race */\n\tsync\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t54f\n/*\n * We jump to power7_wakeup_loss, which will return to the caller\n * of power7_nap in the powernv cpu offline loop.  The value we\n * put in r3 becomes the return value for power7_nap.\n */\n\tli\tr3, LPCR_PECE0\n\tmfspr\tr4, SPRN_LPCR\n\trlwimi\tr4, r3, 0, LPCR_PECE0 | LPCR_PECE1\n\tmtspr\tSPRN_LPCR, r4\n\tli\tr3, 0\n\tb\tpower7_wakeup_loss\n\n53:\tHMT_LOW\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5, 0\n\tbne\t60f\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr3, 0\n\tbeq\tkvm_no_guest\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\tkvm_no_guest\n\tHMT_MEDIUM\n\tb\tkvm_unsplit_nap\n60:\tHMT_MEDIUM\n\tb\tkvm_secondary_got_guest\n\n54:\tli\tr0, KVM_HWTHREAD_IN_KVM\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\tb\tkvm_no_guest\n\n/*\n * Here the primary thread is trying to return the core to\n * whole-core mode, so we need to nap.\n */\nkvm_unsplit_nap:\n\t/*\n\t * When secondaries are napping in kvm_unsplit_nap() with\n\t * hwthread_req = 1, HMI goes ignored even though subcores are\n\t * already exited the guest. Hence HMI keeps waking up secondaries\n\t * from nap in a loop and secondaries always go back to nap since\n\t * no vcore is assigned to them. This makes impossible for primary\n\t * thread to get hold of secondary threads resulting into a soft\n\t * lockup in KVM path.\n\t *\n\t * Let us check if HMI is pending and handle it before we go to nap.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\t55f\n\tli\tr3, 0\t\t\t/* NULL argument */\n\tbl\thmi_exception_realmode\n55:\n\t/*\n\t * Ensure that secondary doesn't nap when it has\n\t * its vcore pointer set.\n\t */\n\tsync\t\t/* matches smp_mb() before setting split_info.do_nap */\n\tld\tr0, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr0, 0\n\tbne\tkvm_no_guest\n\t/* clear any pending message */\nBEGIN_FTR_SECTION\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\t/* Set kvm_split_mode.napped[tid] = 1 */\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tli\tr0, 1\n\tlhz\tr4, PACAPACAINDEX(r13)\n\tclrldi\tr4, r4, 61\t/* micro-threading => P8 => 8 threads/core */\n\taddi\tr4, r4, KVM_SPLIT_NAPPED\n\tstbx\tr0, r3, r4\n\t/* Check the do_nap flag again after setting napped[] */\n\tsync\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\t57f\n\tli\tr3, (LPCR_PECEDH | LPCR_PECE0) >> 4\n\tmfspr\tr4, SPRN_LPCR\n\trlwimi\tr4, r3, 4, (LPCR_PECEDP | LPCR_PECEDH | LPCR_PECE0 | LPCR_PECE1)\n\tmtspr\tSPRN_LPCR, r4\n\tisync\n\tstd\tr0, HSTATE_SCRATCH0(r13)\n\tptesync\n\tld\tr0, HSTATE_SCRATCH0(r13)\n1:\tcmpd\tr0, r0\n\tbne\t1b\n\tnap\n\tb\t.\n\n57:\tli\tr0, 0\n\tstbx\tr0, r3, r4\n\tb\tkvm_no_guest\n\n/******************************************************************************\n *                                                                            *\n *                               Entry code                                   *\n *                                                                            *\n *****************************************************************************/\n\n.global kvmppc_hv_entry\nkvmppc_hv_entry:\n\n\t/* Required state:\n\t *\n\t * R4 = vcpu pointer (or NULL)\n\t * MSR = ~IR|DR\n\t * R13 = PACA\n\t * R1 = host R1\n\t * R2 = TOC\n\t * all other volatile GPRS = free\n\t */\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -112(r1)\n\n\t/* Save R1 in the PACA */\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\tli\tr6, KVM_GUEST_MODE_HOST_HV\n\tstb\tr6, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\t/* Store initial timestamp */\n\tcmpdi\tr4, 0\n\tbeq\t1f\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n1:\n#endif\n\t/* Clear out SLB */\n\tli\tr6,0\n\tslbmte\tr6,r6\n\tslbia\n\tptesync\n\n\t/*\n\t * POWER7/POWER8 host -> guest partition switch code.\n\t * We don't have to lock against concurrent tlbies,\n\t * but we do have to coordinate across hardware threads.\n\t */\n\t/* Set bit in entry map iff exit map is zero. */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tli\tr7, 1\n\tlbz\tr6, HSTATE_PTID(r13)\n\tsld\tr7, r7, r6\n\taddi\tr9, r5, VCORE_ENTRY_EXIT\n21:\tlwarx\tr3, 0, r9\n\tcmpwi\tr3, 0x100\t\t/* any threads starting to exit? */\n\tbge\tsecondary_too_late\t/* if so we're too late to the party */\n\tor\tr3, r3, r7\n\tstwcx.\tr3, 0, r9\n\tbne\t21b\n\n\t/* Primary thread switches to guest partition. */\n\tld\tr9,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\tcmpwi\tr6,0\n\tbne\t10f\n\tld\tr6,KVM_SDR1(r9)\n\tlwz\tr7,KVM_LPID(r9)\n\tli\tr0,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r0\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to partition page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\n\t/* See if we need to flush the TLB */\n\tlhz\tr6,PACAPACAINDEX(r13)\t/* test_bit(cpu, need_tlb_flush) */\n\tclrldi\tr7,r6,64-6\t\t/* extract bit number (6 bits) */\n\tsrdi\tr6,r6,6\t\t\t/* doubleword number */\n\tsldi\tr6,r6,3\t\t\t/* address offset */\n\tadd\tr6,r6,r9\n\taddi\tr6,r6,KVM_NEED_FLUSH\t/* dword in kvm->arch.need_tlb_flush */\n\tli\tr0,1\n\tsld\tr0,r0,r7\n\tld\tr7,0(r6)\n\tand.\tr7,r7,r0\n\tbeq\t22f\n23:\tldarx\tr7,0,r6\t\t\t/* if set, clear the bit */\n\tandc\tr7,r7,r0\n\tstdcx.\tr7,0,r6\n\tbne\t23b\n\t/* Flush the TLB of any entries for this LPID */\n\t/* use arch 2.07S as a proxy for POWER8 */\nBEGIN_FTR_SECTION\n\tli\tr6,512\t\t\t/* POWER8 has 512 sets */\nFTR_SECTION_ELSE\n\tli\tr6,128\t\t\t/* POWER7 has 128 sets */\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)\n\tmtctr\tr6\n\tli\tr7,0x800\t\t/* IS field = 0b10 */\n\tptesync\n28:\ttlbiel\tr7\n\taddi\tr7,r7,0x1000\n\tbdnz\t28b\n\tptesync\n\n\t/* Add timebase offset onto timebase */\n22:\tld\tr8,VCORE_TB_OFFSET(r5)\n\tcmpdi\tr8,0\n\tbeq\t37f\n\tmftb\tr6\t\t/* current host timebase */\n\tadd\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t/* update upper 40 bits */\n\tmftb\tr7\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t37f\n\taddis\tr8,r8,0x100\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n\t/* Load guest PCR value to select appropriate compat mode */\n37:\tld\tr7, VCORE_PCR(r5)\n\tcmpdi\tr7, 0\n\tbeq\t38f\n\tmtspr\tSPRN_PCR, r7\n38:\n\nBEGIN_FTR_SECTION\n\t/* DPDES is shared between threads */\n\tld\tr8, VCORE_DPDES(r5)\n\tmtspr\tSPRN_DPDES, r8\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* Mark the subcore state as inside guest */\n\tbl\tkvmppc_subcore_enter_guest\n\tnop\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tli\tr0,1\n\tstb\tr0,VCORE_IN_GUEST(r5)\t/* signal secondaries to continue */\n\n\t/* Do we have a guest vcpu to run? */\n10:\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\nkvmppc_got_guest:\n\n\t/* Load up guest SLB entries */\n\tlwz\tr5,VCPU_SLB_MAX(r4)\n\tcmpwi\tr5,0\n\tbeq\t9f\n\tmtctr\tr5\n\taddi\tr6,r4,VCPU_SLB\n1:\tld\tr8,VCPU_SLB_E(r6)\n\tld\tr9,VCPU_SLB_V(r6)\n\tslbmte\tr9,r8\n\taddi\tr6,r6,VCPU_SLB_SIZE\n\tbdnz\t1b\n9:\n\t/* Increment yield count if they have a VPA */\n\tld\tr3, VCPU_VPA(r4)\n\tcmpdi\tr3, 0\n\tbeq\t25f\n\tli\tr6, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr5, r3, r6\n\taddi\tr5, r5, 1\n\tSTWX_BE\tr5, r3, r6\n\tli\tr6, 1\n\tstb\tr6, VCPU_VPA_DIRTY(r4)\n25:\n\n\t/* Save purr/spurr */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tstd\tr5,HSTATE_PURR(r13)\n\tstd\tr6,HSTATE_SPURR(r13)\n\tld\tr7,VCPU_PURR(r4)\n\tld\tr8,VCPU_SPURR(r4)\n\tmtspr\tSPRN_PURR,r7\n\tmtspr\tSPRN_SPURR,r8\n\nBEGIN_FTR_SECTION\n\t/* Set partition DABR */\n\t/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */\n\tlwz\tr5,VCPU_DABRX(r4)\n\tld\tr6,VCPU_DABR(r4)\n\tmtspr\tSPRN_DABRX,r5\n\tmtspr\tSPRN_DABR,r6\n\tisync\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tbl\tkvmppc_restore_tm\nEND_FTR_SECTION_IFSET(CPU_FTR_TM)\n#endif\n\n\t/* Load guest PMU registers */\n\t/* R4 is live here (vcpu pointer) */\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tisync\nBEGIN_FTR_SECTION\n\tld\tr3, VCPU_MMCR(r4)\n\tandi.\tr5, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr5, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, VCPU_PMC(r4)\t/* always load up guest PMU registers */\n\tlwz\tr5, VCPU_PMC + 4(r4)\t/* to prevent information leak */\n\tlwz\tr6, VCPU_PMC + 8(r4)\n\tlwz\tr7, VCPU_PMC + 12(r4)\n\tlwz\tr8, VCPU_PMC + 16(r4)\n\tlwz\tr9, VCPU_PMC + 20(r4)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r5\n\tmtspr\tSPRN_PMC3, r6\n\tmtspr\tSPRN_PMC4, r7\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, VCPU_MMCR(r4)\n\tld\tr5, VCPU_MMCR + 8(r4)\n\tld\tr6, VCPU_MMCR + 16(r4)\n\tld\tr7, VCPU_SIAR(r4)\n\tld\tr8, VCPU_SDAR(r4)\n\tmtspr\tSPRN_MMCR1, r5\n\tmtspr\tSPRN_MMCRA, r6\n\tmtspr\tSPRN_SIAR, r7\n\tmtspr\tSPRN_SDAR, r8\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_MMCR + 24(r4)\n\tld\tr6, VCPU_SIER(r4)\n\tlwz\tr7, VCPU_PMC + 24(r4)\n\tlwz\tr8, VCPU_PMC + 28(r4)\n\tld\tr9, VCPU_MMCR + 32(r4)\n\tmtspr\tSPRN_MMCR2, r5\n\tmtspr\tSPRN_SIER, r6\n\tmtspr\tSPRN_SPMC1, r7\n\tmtspr\tSPRN_SPMC2, r8\n\tmtspr\tSPRN_MMCRS, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n\n\t/* Load up FP, VMX and VSX registers */\n\tbl\tkvmppc_load_fp\n\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n\n\t/* Switch DSCR to guest value */\n\tld\tr5, VCPU_DSCR(r4)\n\tmtspr\tSPRN_DSCR, r5\n\nBEGIN_FTR_SECTION\n\t/* Skip next section on POWER7 */\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Load up POWER8-specific registers */\n\tld\tr5, VCPU_IAMR(r4)\n\tlwz\tr6, VCPU_PSPB(r4)\n\tld\tr7, VCPU_FSCR(r4)\n\tmtspr\tSPRN_IAMR, r5\n\tmtspr\tSPRN_PSPB, r6\n\tmtspr\tSPRN_FSCR, r7\n\tld\tr5, VCPU_DAWR(r4)\n\tld\tr6, VCPU_DAWRX(r4)\n\tld\tr7, VCPU_CIABR(r4)\n\tld\tr8, VCPU_TAR(r4)\n\tmtspr\tSPRN_DAWR, r5\n\tmtspr\tSPRN_DAWRX, r6\n\tmtspr\tSPRN_CIABR, r7\n\tmtspr\tSPRN_TAR, r8\n\tld\tr5, VCPU_IC(r4)\n\tld\tr6, VCPU_VTB(r4)\n\tmtspr\tSPRN_IC, r5\n\tmtspr\tSPRN_VTB, r6\n\tld\tr8, VCPU_EBBHR(r4)\n\tmtspr\tSPRN_EBBHR, r8\n\tld\tr5, VCPU_EBBRR(r4)\n\tld\tr6, VCPU_BESCR(r4)\n\tld\tr7, VCPU_CSIGR(r4)\n\tld\tr8, VCPU_TACR(r4)\n\tmtspr\tSPRN_EBBRR, r5\n\tmtspr\tSPRN_BESCR, r6\n\tmtspr\tSPRN_CSIGR, r7\n\tmtspr\tSPRN_TACR, r8\n\tld\tr5, VCPU_TCSCR(r4)\n\tld\tr6, VCPU_ACOP(r4)\n\tlwz\tr7, VCPU_GUEST_PID(r4)\n\tld\tr8, VCPU_WORT(r4)\n\tmtspr\tSPRN_TCSCR, r5\n\tmtspr\tSPRN_ACOP, r6\n\tmtspr\tSPRN_PID, r7\n\tmtspr\tSPRN_WORT, r8\n8:\n\n\t/*\n\t * Set the decrementer to the guest decrementer.\n\t */\n\tld\tr8,VCPU_DEC_EXPIRES(r4)\n\t/* r8 is a host timebase value here, convert to guest TB */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr6,VCORE_TB_OFFSET(r5)\n\tadd\tr8,r8,r6\n\tmftb\tr7\n\tsubf\tr3,r7,r8\n\tmtspr\tSPRN_DEC,r3\n\tstw\tr3,VCPU_DEC(r4)\n\n\tld\tr5, VCPU_SPRG0(r4)\n\tld\tr6, VCPU_SPRG1(r4)\n\tld\tr7, VCPU_SPRG2(r4)\n\tld\tr8, VCPU_SPRG3(r4)\n\tmtspr\tSPRN_SPRG0, r5\n\tmtspr\tSPRN_SPRG1, r6\n\tmtspr\tSPRN_SPRG2, r7\n\tmtspr\tSPRN_SPRG3, r8\n\n\t/* Load up DAR and DSISR */\n\tld\tr5, VCPU_DAR(r4)\n\tlwz\tr6, VCPU_DSISR(r4)\n\tmtspr\tSPRN_DAR, r5\n\tmtspr\tSPRN_DSISR, r6\n\n\t/* Restore AMR and UAMOR, set AMOR to all 1s */\n\tld\tr5,VCPU_AMR(r4)\n\tld\tr6,VCPU_UAMOR(r4)\n\tli\tr7,-1\n\tmtspr\tSPRN_AMR,r5\n\tmtspr\tSPRN_UAMOR,r6\n\tmtspr\tSPRN_AMOR,r7\n\n\t/* Restore state of CTRL run bit; assume 1 on entry */\n\tlwz\tr5,VCPU_CTRL(r4)\n\tandi.\tr5,r5,1\n\tbne\t4f\n\tmfspr\tr6,SPRN_CTRLF\n\tclrrdi\tr6,r6,1\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/* Secondary threads wait for primary to have done partition switch */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr6, HSTATE_PTID(r13)\n\tcmpwi\tr6, 0\n\tbeq\t21f\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbne\t21f\n\tHMT_LOW\n20:\tlwz\tr3, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr3, 0x100\n\tbge\tno_switch_exit\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t20b\n\tHMT_MEDIUM\n21:\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n\t/* Check if HDEC expires soon */\n\tmfspr\tr3, SPRN_HDEC\n\tcmpwi\tr3, 512\t\t/* 1 microsecond */\n\tblt\thdec_soon\n\n\tld\tr6, VCPU_CTR(r4)\n\tld\tr7, VCPU_XER(r4)\n\n\tmtctr\tr6\n\tmtxer\tr7\n\nkvmppc_cede_reentry:\t\t/* r4 = vcpu, r13 = paca */\n\tld\tr10, VCPU_PC(r4)\n\tld\tr11, VCPU_MSR(r4)\n\tld\tr6, VCPU_SRR0(r4)\n\tld\tr7, VCPU_SRR1(r4)\n\tmtspr\tSPRN_SRR0, r6\n\tmtspr\tSPRN_SRR1, r7\n\ndeliver_guest_interrupt:\n\t/* r11 = vcpu->arch.msr & ~MSR_HV */\n\trldicl\tr11, r11, 63 - MSR_HV_LG, 1\n\trotldi\tr11, r11, 1 + MSR_HV_LG\n\tori\tr11, r11, MSR_ME\n\n\t/* Check if we can deliver an external or decrementer interrupt now */\n\tld\tr0, VCPU_PENDING_EXC(r4)\n\trldicl\tr0, r0, 64 - BOOK3S_IRQPRIO_EXTERNAL_LEVEL, 63\n\tcmpdi\tcr1, r0, 0\n\tandi.\tr8, r11, MSR_EE\n\tmfspr\tr8, SPRN_LPCR\n\t/* Insert EXTERNAL_LEVEL bit into LPCR at the MER bit position */\n\trldimi\tr8, r0, LPCR_MER_SH, 63 - LPCR_MER_SH\n\tmtspr\tSPRN_LPCR, r8\n\tisync\n\tbeq\t5f\n\tli\tr0, BOOK3S_INTERRUPT_EXTERNAL\n\tbne\tcr1, 12f\n\tmfspr\tr0, SPRN_DEC\n\tcmpwi\tr0, 0\n\tli\tr0, BOOK3S_INTERRUPT_DECREMENTER\n\tbge\t5f\n\n12:\tmtspr\tSPRN_SRR0, r10\n\tmr\tr10,r0\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr9, r4\n\tbl\tkvmppc_msr_interrupt\n5:\n\n/*\n * Required state:\n * R4 = vcpu\n * R10: value for HSRR0\n * R11: value for HSRR1\n * R13 = PACA\n */\nfast_guest_return:\n\tli\tr0,0\n\tstb\tr0,VCPU_CEDED(r4)\t/* cancel cede */\n\tmtspr\tSPRN_HSRR0,r10\n\tmtspr\tSPRN_HSRR1,r11\n\n\t/* Activate guest mode, so faults get handled by KVM */\n\tli\tr9, KVM_GUEST_MODE_GUEST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\t/* Accumulate timing */\n\taddi\tr3, r4, VCPU_TB_GUEST\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\t/* Enter guest */\n\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_CFAR(r4)\n\tmtspr\tSPRN_CFAR, r5\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr0, VCPU_PPR(r4)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\tld\tr5, VCPU_LR(r4)\n\tlwz\tr6, VCPU_CR(r4)\n\tmtlr\tr5\n\tmtcr\tr6\n\n\tld\tr1, VCPU_GPR(R1)(r4)\n\tld\tr2, VCPU_GPR(R2)(r4)\n\tld\tr3, VCPU_GPR(R3)(r4)\n\tld\tr5, VCPU_GPR(R5)(r4)\n\tld\tr6, VCPU_GPR(R6)(r4)\n\tld\tr7, VCPU_GPR(R7)(r4)\n\tld\tr8, VCPU_GPR(R8)(r4)\n\tld\tr9, VCPU_GPR(R9)(r4)\n\tld\tr10, VCPU_GPR(R10)(r4)\n\tld\tr11, VCPU_GPR(R11)(r4)\n\tld\tr12, VCPU_GPR(R12)(r4)\n\tld\tr13, VCPU_GPR(R13)(r4)\n\nBEGIN_FTR_SECTION\n\tmtspr\tSPRN_PPR, r0\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\tld\tr0, VCPU_GPR(R0)(r4)\n\tld\tr4, VCPU_GPR(R4)(r4)\n\n\thrfid\n\tb\t.\n\nsecondary_too_late:\n\tli\tr12, 0\n\tcmpdi\tr4, 0\n\tbeq\t11f\n\tstw\tr12, VCPU_TRAP(r4)\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n11:\tb\tkvmhv_switch_to_host\n\nno_switch_exit:\n\tHMT_MEDIUM\n\tli\tr12, 0\n\tb\t12f\nhdec_soon:\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n12:\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n\tb\tguest_exit_cont\n\n/******************************************************************************\n *                                                                            *\n *                               Exit code                                    *\n *                                                                            *\n *****************************************************************************/\n\n/*\n * We come here from the first-level interrupt handlers.\n */\n\t.globl\tkvmppc_interrupt_hv\nkvmppc_interrupt_hv:\n\t/*\n\t * Register contents:\n\t * R12\t\t= interrupt vector\n\t * R13\t\t= PACA\n\t * guest CR, R12 saved in shadow VCPU SCRATCH1/0\n\t * guest R13 saved in SPRN_SCRATCH0\n\t */\n\tstd\tr9, HSTATE_SCRATCH2(r13)\n\n\tlbz\tr9, HSTATE_IN_GUEST(r13)\n\tcmpwi\tr9, KVM_GUEST_MODE_HOST_HV\n\tbeq\tkvmppc_bad_host_intr\n#ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE\n\tcmpwi\tr9, KVM_GUEST_MODE_GUEST\n\tld\tr9, HSTATE_SCRATCH2(r13)\n\tbeq\tkvmppc_interrupt_pr\n#endif\n\t/* We're now back in the host but in guest MMU context */\n\tli\tr9, KVM_GUEST_MODE_HOST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Save registers */\n\n\tstd\tr0, VCPU_GPR(R0)(r9)\n\tstd\tr1, VCPU_GPR(R1)(r9)\n\tstd\tr2, VCPU_GPR(R2)(r9)\n\tstd\tr3, VCPU_GPR(R3)(r9)\n\tstd\tr4, VCPU_GPR(R4)(r9)\n\tstd\tr5, VCPU_GPR(R5)(r9)\n\tstd\tr6, VCPU_GPR(R6)(r9)\n\tstd\tr7, VCPU_GPR(R7)(r9)\n\tstd\tr8, VCPU_GPR(R8)(r9)\n\tld\tr0, HSTATE_SCRATCH2(r13)\n\tstd\tr0, VCPU_GPR(R9)(r9)\n\tstd\tr10, VCPU_GPR(R10)(r9)\n\tstd\tr11, VCPU_GPR(R11)(r9)\n\tld\tr3, HSTATE_SCRATCH0(r13)\n\tlwz\tr4, HSTATE_SCRATCH1(r13)\n\tstd\tr3, VCPU_GPR(R12)(r9)\n\tstw\tr4, VCPU_CR(r9)\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_CFAR(r13)\n\tstd\tr3, VCPU_CFAR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr4, HSTATE_PPR(r13)\n\tstd\tr4, VCPU_PPR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\t/* Restore R1/R2 so we can handle faults */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr2, PACATOC(r13)\n\n\tmfspr\tr10, SPRN_SRR0\n\tmfspr\tr11, SPRN_SRR1\n\tstd\tr10, VCPU_SRR0(r9)\n\tstd\tr11, VCPU_SRR1(r9)\n\tandi.\tr0, r12, 2\t\t/* need to read HSRR0/1? */\n\tbeq\t1f\n\tmfspr\tr10, SPRN_HSRR0\n\tmfspr\tr11, SPRN_HSRR1\n\tclrrdi\tr12, r12, 2\n1:\tstd\tr10, VCPU_PC(r9)\n\tstd\tr11, VCPU_MSR(r9)\n\n\tGET_SCRATCH0(r3)\n\tmflr\tr4\n\tstd\tr3, VCPU_GPR(R13)(r9)\n\tstd\tr4, VCPU_LR(r9)\n\n\tstw\tr12,VCPU_TRAP(r9)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r9, VCPU_TB_RMINTR\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n\tld\tr5, VCPU_GPR(R5)(r9)\n\tld\tr6, VCPU_GPR(R6)(r9)\n\tld\tr7, VCPU_GPR(R7)(r9)\n\tld\tr8, VCPU_GPR(R8)(r9)\n#endif\n\n\t/* Save HEIR (HV emulation assist reg) in emul_inst\n\t   if this is an HEI (HV emulation interrupt, e40) */\n\tli\tr3,KVM_INST_FETCH_FAILED\n\tstw\tr3,VCPU_LAST_INST(r9)\n\tcmpwi\tr12,BOOK3S_INTERRUPT_H_EMUL_ASSIST\n\tbne\t11f\n\tmfspr\tr3,SPRN_HEIR\n11:\tstw\tr3,VCPU_HEIR(r9)\n\n\t/* these are volatile across C function calls */\n\tmfctr\tr3\n\tmfxer\tr4\n\tstd\tr3, VCPU_CTR(r9)\n\tstd\tr4, VCPU_XER(r9)\n\n\t/* If this is a page table miss then see if it's theirs or ours */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tbeq\tkvmppc_hdsi\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tbeq\tkvmppc_hisi\n\n\t/* See if this is a leftover HDEC interrupt */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_HV_DECREMENTER\n\tbne\t2f\n\tmfspr\tr3,SPRN_HDEC\n\tcmpwi\tr3,0\n\tmr\tr4,r9\n\tbge\tfast_guest_return\n2:\n\t/* See if this is an hcall we can handle in real mode */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tbeq\thcall_try_real_mode\n\n\t/* Hypervisor doorbell - exit only if host IPI flag set */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\tbne\t3f\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbeq\t4f\n\tb\tguest_exit_cont\n3:\n\t/* External interrupt ? */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbne+\tguest_exit_cont\n\n\t/* External interrupt, first check for host_ipi. If this is\n\t * set, we know the host wants us out so let's do it now\n\t */\n\tbl\tkvmppc_read_intr\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\n\t/* Check if any CPU is heading out to the host, if so head out too */\n4:\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tmr\tr4, r9\n\tblt\tdeliver_guest_interrupt\n\nguest_exit_cont:\t\t/* r9 = vcpu, r12 = trap, r13 = paca */\n\t/* Save more register state  */\n\tmfdar\tr6\n\tmfdsisr\tr7\n\tstd\tr6, VCPU_DAR(r9)\n\tstw\tr7, VCPU_DSISR(r9)\n\t/* don't overwrite fault_dar/fault_dsisr if HDSI */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tbeq\tmc_cont\n\tstd\tr6, VCPU_FAULT_DAR(r9)\n\tstw\tr7, VCPU_FAULT_DSISR(r9)\n\n\t/* See if it is a machine check */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tbeq\tmachine_check_realmode\nmc_cont:\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r9, VCPU_TB_RMEXIT\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\tmr \tr3, r12\n\t/* Increment exit count, poke other threads to exit */\n\tbl\tkvmhv_commence_exit\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tlwz\tr12, VCPU_TRAP(r9)\n\n\t/* Stop others sending VCPU interrupts to this physical CPU */\n\tli\tr0, -1\n\tstw\tr0, VCPU_CPU(r9)\n\tstw\tr0, VCPU_THREAD_CPU(r9)\n\n\t/* Save guest CTRL register, set runlatch to 1 */\n\tmfspr\tr6,SPRN_CTRLF\n\tstw\tr6,VCPU_CTRL(r9)\n\tandi.\tr0,r6,1\n\tbne\t4f\n\tori\tr6,r6,1\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/* Read the guest SLB and save it away */\n\tlwz\tr0,VCPU_SLB_NR(r9)\t/* number of entries in SLB */\n\tmtctr\tr0\n\tli\tr6,0\n\taddi\tr7,r9,VCPU_SLB\n\tli\tr5,0\n1:\tslbmfee\tr8,r6\n\tandis.\tr0,r8,SLB_ESID_V@h\n\tbeq\t2f\n\tadd\tr8,r8,r6\t\t/* put index in */\n\tslbmfev\tr3,r6\n\tstd\tr8,VCPU_SLB_E(r7)\n\tstd\tr3,VCPU_SLB_V(r7)\n\taddi\tr7,r7,VCPU_SLB_SIZE\n\taddi\tr5,r5,1\n2:\taddi\tr6,r6,1\n\tbdnz\t1b\n\tstw\tr5,VCPU_SLB_MAX(r9)\n\n\t/*\n\t * Save the guest PURR/SPURR\n\t */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tld\tr7,VCPU_PURR(r9)\n\tld\tr8,VCPU_SPURR(r9)\n\tstd\tr5,VCPU_PURR(r9)\n\tstd\tr6,VCPU_SPURR(r9)\n\tsubf\tr5,r7,r5\n\tsubf\tr6,r8,r6\n\n\t/*\n\t * Restore host PURR/SPURR and add guest times\n\t * so that the time in the guest gets accounted.\n\t */\n\tld\tr3,HSTATE_PURR(r13)\n\tld\tr4,HSTATE_SPURR(r13)\n\tadd\tr3,r3,r5\n\tadd\tr4,r4,r6\n\tmtspr\tSPRN_PURR,r3\n\tmtspr\tSPRN_SPURR,r4\n\n\t/* Save DEC */\n\tmfspr\tr5,SPRN_DEC\n\tmftb\tr6\n\textsw\tr5,r5\n\tadd\tr5,r5,r6\n\t/* r5 is a guest timebase value here, convert to host TB */\n\tld\tr3,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_TB_OFFSET(r3)\n\tsubf\tr5,r4,r5\n\tstd\tr5,VCPU_DEC_EXPIRES(r9)\n\nBEGIN_FTR_SECTION\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Save POWER8-specific registers */\n\tmfspr\tr5, SPRN_IAMR\n\tmfspr\tr6, SPRN_PSPB\n\tmfspr\tr7, SPRN_FSCR\n\tstd\tr5, VCPU_IAMR(r9)\n\tstw\tr6, VCPU_PSPB(r9)\n\tstd\tr7, VCPU_FSCR(r9)\n\tmfspr\tr5, SPRN_IC\n\tmfspr\tr6, SPRN_VTB\n\tmfspr\tr7, SPRN_TAR\n\tstd\tr5, VCPU_IC(r9)\n\tstd\tr6, VCPU_VTB(r9)\n\tstd\tr7, VCPU_TAR(r9)\n\tmfspr\tr8, SPRN_EBBHR\n\tstd\tr8, VCPU_EBBHR(r9)\n\tmfspr\tr5, SPRN_EBBRR\n\tmfspr\tr6, SPRN_BESCR\n\tmfspr\tr7, SPRN_CSIGR\n\tmfspr\tr8, SPRN_TACR\n\tstd\tr5, VCPU_EBBRR(r9)\n\tstd\tr6, VCPU_BESCR(r9)\n\tstd\tr7, VCPU_CSIGR(r9)\n\tstd\tr8, VCPU_TACR(r9)\n\tmfspr\tr5, SPRN_TCSCR\n\tmfspr\tr6, SPRN_ACOP\n\tmfspr\tr7, SPRN_PID\n\tmfspr\tr8, SPRN_WORT\n\tstd\tr5, VCPU_TCSCR(r9)\n\tstd\tr6, VCPU_ACOP(r9)\n\tstw\tr7, VCPU_GUEST_PID(r9)\n\tstd\tr8, VCPU_WORT(r9)\n\t/*\n\t * Restore various registers to 0, where non-zero values\n\t * set by the guest could disrupt the host.\n\t */\n\tli\tr0, 0\n\tmtspr\tSPRN_IAMR, r0\n\tmtspr\tSPRN_CIABR, r0\n\tmtspr\tSPRN_DAWRX, r0\n\tmtspr\tSPRN_TCSCR, r0\n\tmtspr\tSPRN_WORT, r0\n\t/* Set MMCRS to 1<<31 to freeze and disable the SPMC counters */\n\tli\tr0, 1\n\tsldi\tr0, r0, 31\n\tmtspr\tSPRN_MMCRS, r0\n8:\n\n\t/* Save and reset AMR and UAMOR before turning on the MMU */\n\tmfspr\tr5,SPRN_AMR\n\tmfspr\tr6,SPRN_UAMOR\n\tstd\tr5,VCPU_AMR(r9)\n\tstd\tr6,VCPU_UAMOR(r9)\n\tli\tr6,0\n\tmtspr\tSPRN_AMR,r6\n\n\t/* Switch DSCR back to host value */\n\tmfspr\tr8, SPRN_DSCR\n\tld\tr7, HSTATE_DSCR(r13)\n\tstd\tr8, VCPU_DSCR(r9)\n\tmtspr\tSPRN_DSCR, r7\n\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r9)\n\tstd\tr15, VCPU_GPR(R15)(r9)\n\tstd\tr16, VCPU_GPR(R16)(r9)\n\tstd\tr17, VCPU_GPR(R17)(r9)\n\tstd\tr18, VCPU_GPR(R18)(r9)\n\tstd\tr19, VCPU_GPR(R19)(r9)\n\tstd\tr20, VCPU_GPR(R20)(r9)\n\tstd\tr21, VCPU_GPR(R21)(r9)\n\tstd\tr22, VCPU_GPR(R22)(r9)\n\tstd\tr23, VCPU_GPR(R23)(r9)\n\tstd\tr24, VCPU_GPR(R24)(r9)\n\tstd\tr25, VCPU_GPR(R25)(r9)\n\tstd\tr26, VCPU_GPR(R26)(r9)\n\tstd\tr27, VCPU_GPR(R27)(r9)\n\tstd\tr28, VCPU_GPR(R28)(r9)\n\tstd\tr29, VCPU_GPR(R29)(r9)\n\tstd\tr30, VCPU_GPR(R30)(r9)\n\tstd\tr31, VCPU_GPR(R31)(r9)\n\n\t/* Save SPRGs */\n\tmfspr\tr3, SPRN_SPRG0\n\tmfspr\tr4, SPRN_SPRG1\n\tmfspr\tr5, SPRN_SPRG2\n\tmfspr\tr6, SPRN_SPRG3\n\tstd\tr3, VCPU_SPRG0(r9)\n\tstd\tr4, VCPU_SPRG1(r9)\n\tstd\tr5, VCPU_SPRG2(r9)\n\tstd\tr6, VCPU_SPRG3(r9)\n\n\t/* save FP state */\n\tmr\tr3, r9\n\tbl\tkvmppc_save_fp\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tbl\tkvmppc_save_tm\nEND_FTR_SECTION_IFSET(CPU_FTR_TM)\n#endif\n\n\t/* Increment yield count if they have a VPA */\n\tld\tr8, VCPU_VPA(r9)\t/* do they have a VPA? */\n\tcmpdi\tr8, 0\n\tbeq\t25f\n\tli\tr4, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr3, r8, r4\n\taddi\tr3, r3, 1\n\tSTWX_BE\tr3, r8, r4\n\tli\tr3, 1\n\tstb\tr3, VCPU_VPA_DIRTY(r9)\n25:\n\t/* Save PMU registers if requested */\n\t/* r8 and cr0.eq are live here */\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER8 seems to have a hardware bug where setting\n\t * MMCR0[PMAE] along with MMCR0[PMC1CE] and/or MMCR0[PMCjCE]\n\t * when some counters are already negative doesn't seem\n\t * to cause a performance monitor alert (and hence interrupt).\n\t * The effect of this is that when saving the PMU state,\n\t * if there is no PMU alert pending when we read MMCR0\n\t * before freezing the counters, but one becomes pending\n\t * before we read the counters, we lose it.\n\t * To work around this, we need a way to freeze the counters\n\t * before reading MMCR0.  Normally, freezing the counters\n\t * is done by writing MMCR0 (to set MMCR0[FC]) which\n\t * unavoidably writes MMCR0[PMA0] as well.  On POWER8,\n\t * we can also freeze the counters using MMCR2, by writing\n\t * 1s to all the counter freeze condition bits (there are\n\t * 9 bits each for 6 counters).\n\t */\n\tli\tr3, -1\t\t\t/* set all freeze bits */\n\tclrrdi\tr3, r3, 10\n\tmfspr\tr10, SPRN_MMCR2\n\tmtspr\tSPRN_MMCR2, r3\n\tisync\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmfspr\tr4, SPRN_MMCR0\t\t/* save MMCR0 */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tmfspr\tr6, SPRN_MMCRA\n\t/* Clear MMCRA in order to disable SDAR updates */\n\tli\tr7, 0\n\tmtspr\tSPRN_MMCRA, r7\n\tisync\n\tbeq\t21f\t\t\t/* if no VPA, save PMU stuff anyway */\n\tlbz\tr7, LPPACA_PMCINUSE(r8)\n\tcmpwi\tr7, 0\t\t\t/* did they ask for PMU stuff to be saved? */\n\tbne\t21f\n\tstd\tr3, VCPU_MMCR(r9)\t/* if not, set saved MMCR0 to FC */\n\tb\t22f\n21:\tmfspr\tr5, SPRN_MMCR1\n\tmfspr\tr7, SPRN_SIAR\n\tmfspr\tr8, SPRN_SDAR\n\tstd\tr4, VCPU_MMCR(r9)\n\tstd\tr5, VCPU_MMCR + 8(r9)\n\tstd\tr6, VCPU_MMCR + 16(r9)\nBEGIN_FTR_SECTION\n\tstd\tr10, VCPU_MMCR + 24(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr7, VCPU_SIAR(r9)\n\tstd\tr8, VCPU_SDAR(r9)\n\tmfspr\tr3, SPRN_PMC1\n\tmfspr\tr4, SPRN_PMC2\n\tmfspr\tr5, SPRN_PMC3\n\tmfspr\tr6, SPRN_PMC4\n\tmfspr\tr7, SPRN_PMC5\n\tmfspr\tr8, SPRN_PMC6\n\tstw\tr3, VCPU_PMC(r9)\n\tstw\tr4, VCPU_PMC + 4(r9)\n\tstw\tr5, VCPU_PMC + 8(r9)\n\tstw\tr6, VCPU_PMC + 12(r9)\n\tstw\tr7, VCPU_PMC + 16(r9)\n\tstw\tr8, VCPU_PMC + 20(r9)\nBEGIN_FTR_SECTION\n\tmfspr\tr5, SPRN_SIER\n\tmfspr\tr6, SPRN_SPMC1\n\tmfspr\tr7, SPRN_SPMC2\n\tmfspr\tr8, SPRN_MMCRS\n\tstd\tr5, VCPU_SIER(r9)\n\tstw\tr6, VCPU_PMC + 24(r9)\n\tstw\tr7, VCPU_PMC + 28(r9)\n\tstd\tr8, VCPU_MMCR + 32(r9)\n\tlis\tr4, 0x8000\n\tmtspr\tSPRN_MMCRS, r4\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n22:\n\t/* Clear out SLB */\n\tli\tr5,0\n\tslbmte\tr5,r5\n\tslbia\n\tptesync\n\n\t/*\n\t * POWER7/POWER8 guest -> host partition switch code.\n\t * We don't have to lock against tlbies but we do\n\t * have to coordinate the hardware threads.\n\t */\nkvmhv_switch_to_host:\n\t/* Secondary threads wait for primary to do partition switch */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\tlbz\tr3,HSTATE_PTID(r13)\n\tcmpwi\tr3,0\n\tbeq\t15f\n\tHMT_LOW\n13:\tlbz\tr3,VCORE_IN_GUEST(r5)\n\tcmpwi\tr3,0\n\tbne\t13b\n\tHMT_MEDIUM\n\tb\t16f\n\n\t/* Primary thread waits for all the secondaries to exit guest */\n15:\tlwz\tr3,VCORE_ENTRY_EXIT(r5)\n\trlwinm\tr0,r3,32-8,0xff\n\tclrldi\tr3,r3,56\n\tcmpw\tr3,r0\n\tbne\t15b\n\tisync\n\n\t/* Did we actually switch to the guest at all? */\n\tlbz\tr6, VCORE_IN_GUEST(r5)\n\tcmpwi\tr6, 0\n\tbeq\t19f\n\n\t/* Primary thread switches back to host partition */\n\tld\tr6,KVM_HOST_SDR1(r4)\n\tlwz\tr7,KVM_HOST_LPID(r4)\n\tli\tr8,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r8\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to partition page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\nBEGIN_FTR_SECTION\n\t/* DPDES is shared between threads */\n\tmfspr\tr7, SPRN_DPDES\n\tstd\tr7, VCORE_DPDES(r5)\n\t/* clear DPDES so we don't get guest doorbells in the host */\n\tli\tr8, 0\n\tmtspr\tSPRN_DPDES, r8\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* If HMI, call kvmppc_realmode_hmi_handler() */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\t27f\n\tbl\tkvmppc_realmode_hmi_handler\n\tnop\n\tli\tr12, BOOK3S_INTERRUPT_HMI\n\t/*\n\t * At this point kvmppc_realmode_hmi_handler would have resync-ed\n\t * the TB. Hence it is not required to subtract guest timebase\n\t * offset from timebase. So, skip it.\n\t *\n\t * Also, do not call kvmppc_subcore_exit_guest() because it has\n\t * been invoked as part of kvmppc_realmode_hmi_handler().\n\t */\n\tb\t30f\n\n27:\n\t/* Subtract timebase offset from timebase */\n\tld\tr8,VCORE_TB_OFFSET(r5)\n\tcmpdi\tr8,0\n\tbeq\t17f\n\tmftb\tr6\t\t\t/* current guest timebase */\n\tsubf\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t\t/* update upper 40 bits */\n\tmftb\tr7\t\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t17f\n\taddis\tr8,r8,0x100\t\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n17:\tbl\tkvmppc_subcore_exit_guest\n\tnop\n30:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\n\t/* Reset PCR */\n\tld\tr0, VCORE_PCR(r5)\n\tcmpdi\tr0, 0\n\tbeq\t18f\n\tli\tr0, 0\n\tmtspr\tSPRN_PCR, r0\n18:\n\t/* Signal secondary CPUs to continue */\n\tstb\tr0,VCORE_IN_GUEST(r5)\n19:\tlis\tr8,0x7fff\t\t/* MAX_INT@h */\n\tmtspr\tSPRN_HDEC,r8\n\n16:\tld\tr8,KVM_HOST_LPCR(r4)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n\t/* load host SLB entries */\n\tld\tr8,PACA_SLBSHADOWPTR(r13)\n\n\t.rept\tSLB_NUM_BOLTED\n\tli\tr3, SLBSHADOW_SAVEAREA\n\tLDX_BE\tr5, r8, r3\n\taddi\tr3, r3, 8\n\tLDX_BE\tr6, r8, r3\n\tandis.\tr7,r5,SLB_ESID_V@h\n\tbeq\t1f\n\tslbmte\tr6,r5\n1:\taddi\tr8,r8,16\n\t.endr\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\t/* Finish timing, if we have a vcpu */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tli\tr3, 0\n\tbeq\t2f\n\tbl\tkvmhv_accumulate_time\n2:\n#endif\n\t/* Unset guest mode */\n\tli\tr0, KVM_GUEST_MODE_NONE\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\tld\tr0, 112+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, 112\n\tmtlr\tr0\n\tblr\n\n/*\n * Check whether an HDSI is an HPTE not found fault or something else.\n * If it is an HPTE not found fault that is due to the guest accessing\n * a page that they have mapped but which we have paged out, then\n * we continue on with the guest exit path.  In all other cases,\n * reflect the HDSI to the guest as a DSI.\n */\nkvmppc_hdsi:\n\tmfspr\tr4, SPRN_HDAR\n\tmfspr\tr6, SPRN_HDSISR\n\t/* HPTE not found fault or protection fault? */\n\tandis.\tr0, r6, (DSISR_NOHPTE | DSISR_PROTFAULT)@h\n\tbeq\t1f\t\t\t/* if not, send it to the guest */\n\tandi.\tr0, r11, MSR_DR\t\t/* data relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r4, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_DATA_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\tstd\tr4, VCPU_FAULT_DAR(r9)\n\tstw\tr6, VCPU_FAULT_DSISR(r9)\n\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tli\tr7, 1\t\t\t/* data fault */\n\tbl\tkvmppc_hpte_hv_fault\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\t6f\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\tcmpdi\tr3, -2\t\t\t/* MMIO emulation; need instr word */\n\tbeq\t2f\n\n\t/* Synthesize a DSI (or DSegI) for the guest */\n\tld\tr4, VCPU_FAULT_DAR(r9)\n\tmr\tr6, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_DATA_STORAGE\n\tmtspr\tSPRN_DSISR, r6\n7:\tmtspr\tSPRN_DAR, r4\n\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\nfast_interrupt_c_return:\n6:\tld\tr7, VCPU_CTR(r9)\n\tld\tr8, VCPU_XER(r9)\n\tmtctr\tr7\n\tmtxer\tr8\n\tmr\tr4, r9\n\tb\tfast_guest_return\n\n3:\tld\tr5, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r5)\n\tb\t4b\n\n\t/* If this is for emulated MMIO, load the instruction word */\n2:\tli\tr8, KVM_INST_FETCH_FAILED\t/* In case lwz faults */\n\n\t/* Set guest mode to 'jump over instruction' so if lwz faults\n\t * we'll just continue at the next IP. */\n\tli\tr0, KVM_GUEST_MODE_SKIP\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\t/* Do the access with MSR:DR enabled */\n\tmfmsr\tr3\n\tori\tr4, r3, MSR_DR\t\t/* Enable paging for data */\n\tmtmsrd\tr4\n\tlwz\tr8, 0(r10)\n\tmtmsrd\tr3\n\n\t/* Store the result */\n\tstw\tr8, VCPU_LAST_INST(r9)\n\n\t/* Unset guest mode. */\n\tli\tr0, KVM_GUEST_MODE_HOST_HV\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\tb\tguest_exit_cont\n\n/*\n * Similarly for an HISI, reflect it to the guest as an ISI unless\n * it is an HPTE not found fault for a page that we have paged out.\n */\nkvmppc_hisi:\n\tandis.\tr0, r11, SRR1_ISI_NOPT@h\n\tbeq\t1f\n\tandi.\tr0, r11, MSR_IR\t\t/* instruction relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r10, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_INST_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tmr\tr4, r10\n\tmr\tr6, r11\n\tli\tr7, 0\t\t\t/* instruction fault */\n\tbl\tkvmppc_hpte_hv_fault\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\tfast_interrupt_c_return\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\n\t/* Synthesize an ISI (or ISegI) for the guest */\n\tmr\tr11, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_INST_STORAGE\n7:\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\n\tb\tfast_interrupt_c_return\n\n3:\tld\tr6, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r6)\n\tb\t4b\n\n/*\n * Try to handle an hcall in real mode.\n * Returns to the guest if we handle it, or continues on up to\n * the kernel if we can't (i.e. if we don't have a handler for\n * it, or if the handler returns H_TOO_HARD).\n *\n * r5 - r8 contain hcall args,\n * r9 = vcpu, r10 = pc, r11 = msr, r12 = trap, r13 = paca\n */\nhcall_try_real_mode:\n\tld\tr3,VCPU_GPR(R3)(r9)\n\tandi.\tr0,r11,MSR_PR\n\t/* sc 1 from userspace - reflect to guest syscall */\n\tbne\tsc_1_fast_return\n\tclrrdi\tr3,r3,2\n\tcmpldi\tr3,hcall_real_table_end - hcall_real_table\n\tbge\tguest_exit_cont\n\t/* See if this hcall is enabled for in-kernel handling */\n\tld\tr4, VCPU_KVM(r9)\n\tsrdi\tr0, r3, 8\t/* r0 = (r3 / 4) >> 6 */\n\tsldi\tr0, r0, 3\t/* index into kvm->arch.enabled_hcalls[] */\n\tadd\tr4, r4, r0\n\tld\tr0, KVM_ENABLED_HCALLS(r4)\n\trlwinm\tr4, r3, 32-2, 0x3f\t/* r4 = (r3 / 4) & 0x3f */\n\tsrd\tr0, r0, r4\n\tandi.\tr0, r0, 1\n\tbeq\tguest_exit_cont\n\t/* Get pointer to handler, if any, and call it */\n\tLOAD_REG_ADDR(r4, hcall_real_table)\n\tlwax\tr3,r3,r4\n\tcmpwi\tr3,0\n\tbeq\tguest_exit_cont\n\tadd\tr12,r3,r4\n\tmtctr\tr12\n\tmr\tr3,r9\t\t/* get vcpu pointer */\n\tld\tr4,VCPU_GPR(R4)(r9)\n\tbctrl\n\tcmpdi\tr3,H_TOO_HARD\n\tbeq\thcall_real_fallback\n\tld\tr4,HSTATE_KVM_VCPU(r13)\n\tstd\tr3,VCPU_GPR(R3)(r4)\n\tld\tr10,VCPU_PC(r4)\n\tld\tr11,VCPU_MSR(r4)\n\tb\tfast_guest_return\n\nsc_1_fast_return:\n\tmtspr\tSPRN_SRR0,r10\n\tmtspr\tSPRN_SRR1,r11\n\tli\tr10, BOOK3S_INTERRUPT_SYSCALL\n\tbl\tkvmppc_msr_interrupt\n\tmr\tr4,r9\n\tb\tfast_guest_return\n\n\t/* We've attempted a real mode hcall, but it's punted it back\n\t * to userspace.  We need to restore some clobbered volatiles\n\t * before resuming the pass-it-to-qemu path */\nhcall_real_fallback:\n\tli\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\tb\tguest_exit_cont\n\n\t.globl\thcall_real_table\nhcall_real_table:\n\t.long\t0\t\t/* 0 - unused */\n\t.long\tDOTSYM(kvmppc_h_remove) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_enter) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_read) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_mod) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_ref) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_protect) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_get_tce) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_put_tce) - hcall_real_table\n\t.long\t0\t\t/* 0x24 - H_SET_SPRG0 */\n\t.long\tDOTSYM(kvmppc_h_set_dabr) - hcall_real_table\n\t.long\t0\t\t/* 0x2c */\n\t.long\t0\t\t/* 0x30 */\n\t.long\t0\t\t/* 0x34 */\n\t.long\t0\t\t/* 0x38 */\n\t.long\t0\t\t/* 0x3c */\n\t.long\t0\t\t/* 0x40 */\n\t.long\t0\t\t/* 0x44 */\n\t.long\t0\t\t/* 0x48 */\n\t.long\t0\t\t/* 0x4c */\n\t.long\t0\t\t/* 0x50 */\n\t.long\t0\t\t/* 0x54 */\n\t.long\t0\t\t/* 0x58 */\n\t.long\t0\t\t/* 0x5c */\n\t.long\t0\t\t/* 0x60 */\n#ifdef CONFIG_KVM_XICS\n\t.long\tDOTSYM(kvmppc_rm_h_eoi) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_cppr) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_ipi) - hcall_real_table\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\tDOTSYM(kvmppc_rm_h_xirr) - hcall_real_table\n#else\n\t.long\t0\t\t/* 0x64 - H_EOI */\n\t.long\t0\t\t/* 0x68 - H_CPPR */\n\t.long\t0\t\t/* 0x6c - H_IPI */\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\t0\t\t/* 0x74 - H_XIRR */\n#endif\n\t.long\t0\t\t/* 0x78 */\n\t.long\t0\t\t/* 0x7c */\n\t.long\t0\t\t/* 0x80 */\n\t.long\t0\t\t/* 0x84 */\n\t.long\t0\t\t/* 0x88 */\n\t.long\t0\t\t/* 0x8c */\n\t.long\t0\t\t/* 0x90 */\n\t.long\t0\t\t/* 0x94 */\n\t.long\t0\t\t/* 0x98 */\n\t.long\t0\t\t/* 0x9c */\n\t.long\t0\t\t/* 0xa0 */\n\t.long\t0\t\t/* 0xa4 */\n\t.long\t0\t\t/* 0xa8 */\n\t.long\t0\t\t/* 0xac */\n\t.long\t0\t\t/* 0xb0 */\n\t.long\t0\t\t/* 0xb4 */\n\t.long\t0\t\t/* 0xb8 */\n\t.long\t0\t\t/* 0xbc */\n\t.long\t0\t\t/* 0xc0 */\n\t.long\t0\t\t/* 0xc4 */\n\t.long\t0\t\t/* 0xc8 */\n\t.long\t0\t\t/* 0xcc */\n\t.long\t0\t\t/* 0xd0 */\n\t.long\t0\t\t/* 0xd4 */\n\t.long\t0\t\t/* 0xd8 */\n\t.long\t0\t\t/* 0xdc */\n\t.long\tDOTSYM(kvmppc_h_cede) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_confer) - hcall_real_table\n\t.long\t0\t\t/* 0xe8 */\n\t.long\t0\t\t/* 0xec */\n\t.long\t0\t\t/* 0xf0 */\n\t.long\t0\t\t/* 0xf4 */\n\t.long\t0\t\t/* 0xf8 */\n\t.long\t0\t\t/* 0xfc */\n\t.long\t0\t\t/* 0x100 */\n\t.long\t0\t\t/* 0x104 */\n\t.long\t0\t\t/* 0x108 */\n\t.long\t0\t\t/* 0x10c */\n\t.long\t0\t\t/* 0x110 */\n\t.long\t0\t\t/* 0x114 */\n\t.long\t0\t\t/* 0x118 */\n\t.long\t0\t\t/* 0x11c */\n\t.long\t0\t\t/* 0x120 */\n\t.long\tDOTSYM(kvmppc_h_bulk_remove) - hcall_real_table\n\t.long\t0\t\t/* 0x128 */\n\t.long\t0\t\t/* 0x12c */\n\t.long\t0\t\t/* 0x130 */\n\t.long\tDOTSYM(kvmppc_h_set_xdabr) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_stuff_tce) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_put_tce_indirect) - hcall_real_table\n\t.long\t0\t\t/* 0x140 */\n\t.long\t0\t\t/* 0x144 */\n\t.long\t0\t\t/* 0x148 */\n\t.long\t0\t\t/* 0x14c */\n\t.long\t0\t\t/* 0x150 */\n\t.long\t0\t\t/* 0x154 */\n\t.long\t0\t\t/* 0x158 */\n\t.long\t0\t\t/* 0x15c */\n\t.long\t0\t\t/* 0x160 */\n\t.long\t0\t\t/* 0x164 */\n\t.long\t0\t\t/* 0x168 */\n\t.long\t0\t\t/* 0x16c */\n\t.long\t0\t\t/* 0x170 */\n\t.long\t0\t\t/* 0x174 */\n\t.long\t0\t\t/* 0x178 */\n\t.long\t0\t\t/* 0x17c */\n\t.long\t0\t\t/* 0x180 */\n\t.long\t0\t\t/* 0x184 */\n\t.long\t0\t\t/* 0x188 */\n\t.long\t0\t\t/* 0x18c */\n\t.long\t0\t\t/* 0x190 */\n\t.long\t0\t\t/* 0x194 */\n\t.long\t0\t\t/* 0x198 */\n\t.long\t0\t\t/* 0x19c */\n\t.long\t0\t\t/* 0x1a0 */\n\t.long\t0\t\t/* 0x1a4 */\n\t.long\t0\t\t/* 0x1a8 */\n\t.long\t0\t\t/* 0x1ac */\n\t.long\t0\t\t/* 0x1b0 */\n\t.long\t0\t\t/* 0x1b4 */\n\t.long\t0\t\t/* 0x1b8 */\n\t.long\t0\t\t/* 0x1bc */\n\t.long\t0\t\t/* 0x1c0 */\n\t.long\t0\t\t/* 0x1c4 */\n\t.long\t0\t\t/* 0x1c8 */\n\t.long\t0\t\t/* 0x1cc */\n\t.long\t0\t\t/* 0x1d0 */\n\t.long\t0\t\t/* 0x1d4 */\n\t.long\t0\t\t/* 0x1d8 */\n\t.long\t0\t\t/* 0x1dc */\n\t.long\t0\t\t/* 0x1e0 */\n\t.long\t0\t\t/* 0x1e4 */\n\t.long\t0\t\t/* 0x1e8 */\n\t.long\t0\t\t/* 0x1ec */\n\t.long\t0\t\t/* 0x1f0 */\n\t.long\t0\t\t/* 0x1f4 */\n\t.long\t0\t\t/* 0x1f8 */\n\t.long\t0\t\t/* 0x1fc */\n\t.long\t0\t\t/* 0x200 */\n\t.long\t0\t\t/* 0x204 */\n\t.long\t0\t\t/* 0x208 */\n\t.long\t0\t\t/* 0x20c */\n\t.long\t0\t\t/* 0x210 */\n\t.long\t0\t\t/* 0x214 */\n\t.long\t0\t\t/* 0x218 */\n\t.long\t0\t\t/* 0x21c */\n\t.long\t0\t\t/* 0x220 */\n\t.long\t0\t\t/* 0x224 */\n\t.long\t0\t\t/* 0x228 */\n\t.long\t0\t\t/* 0x22c */\n\t.long\t0\t\t/* 0x230 */\n\t.long\t0\t\t/* 0x234 */\n\t.long\t0\t\t/* 0x238 */\n\t.long\t0\t\t/* 0x23c */\n\t.long\t0\t\t/* 0x240 */\n\t.long\t0\t\t/* 0x244 */\n\t.long\t0\t\t/* 0x248 */\n\t.long\t0\t\t/* 0x24c */\n\t.long\t0\t\t/* 0x250 */\n\t.long\t0\t\t/* 0x254 */\n\t.long\t0\t\t/* 0x258 */\n\t.long\t0\t\t/* 0x25c */\n\t.long\t0\t\t/* 0x260 */\n\t.long\t0\t\t/* 0x264 */\n\t.long\t0\t\t/* 0x268 */\n\t.long\t0\t\t/* 0x26c */\n\t.long\t0\t\t/* 0x270 */\n\t.long\t0\t\t/* 0x274 */\n\t.long\t0\t\t/* 0x278 */\n\t.long\t0\t\t/* 0x27c */\n\t.long\t0\t\t/* 0x280 */\n\t.long\t0\t\t/* 0x284 */\n\t.long\t0\t\t/* 0x288 */\n\t.long\t0\t\t/* 0x28c */\n\t.long\t0\t\t/* 0x290 */\n\t.long\t0\t\t/* 0x294 */\n\t.long\t0\t\t/* 0x298 */\n\t.long\t0\t\t/* 0x29c */\n\t.long\t0\t\t/* 0x2a0 */\n\t.long\t0\t\t/* 0x2a4 */\n\t.long\t0\t\t/* 0x2a8 */\n\t.long\t0\t\t/* 0x2ac */\n\t.long\t0\t\t/* 0x2b0 */\n\t.long\t0\t\t/* 0x2b4 */\n\t.long\t0\t\t/* 0x2b8 */\n\t.long\t0\t\t/* 0x2bc */\n\t.long\t0\t\t/* 0x2c0 */\n\t.long\t0\t\t/* 0x2c4 */\n\t.long\t0\t\t/* 0x2c8 */\n\t.long\t0\t\t/* 0x2cc */\n\t.long\t0\t\t/* 0x2d0 */\n\t.long\t0\t\t/* 0x2d4 */\n\t.long\t0\t\t/* 0x2d8 */\n\t.long\t0\t\t/* 0x2dc */\n\t.long\t0\t\t/* 0x2e0 */\n\t.long\t0\t\t/* 0x2e4 */\n\t.long\t0\t\t/* 0x2e8 */\n\t.long\t0\t\t/* 0x2ec */\n\t.long\t0\t\t/* 0x2f0 */\n\t.long\t0\t\t/* 0x2f4 */\n\t.long\t0\t\t/* 0x2f8 */\n\t.long\t0\t\t/* 0x2fc */\n\t.long\tDOTSYM(kvmppc_h_random) - hcall_real_table\n\t.globl\thcall_real_table_end\nhcall_real_table_end:\n\n_GLOBAL(kvmppc_h_set_xdabr)\n\tandi.\tr0, r5, DABRX_USER | DABRX_KERNEL\n\tbeq\t6f\n\tli\tr0, DABRX_USER | DABRX_KERNEL | DABRX_BTI\n\tandc.\tr0, r5, r0\n\tbeq\t3f\n6:\tli\tr3, H_PARAMETER\n\tblr\n\n_GLOBAL(kvmppc_h_set_dabr)\n\tli\tr5, DABRX_USER | DABRX_KERNEL\n3:\nBEGIN_FTR_SECTION\n\tb\t2f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr4,VCPU_DABR(r3)\n\tstw\tr5, VCPU_DABRX(r3)\n\tmtspr\tSPRN_DABRX, r5\n\t/* Work around P7 bug where DABR can get corrupted on mtspr */\n1:\tmtspr\tSPRN_DABR,r4\n\tmfspr\tr5, SPRN_DABR\n\tcmpd\tr4, r5\n\tbne\t1b\n\tisync\n\tli\tr3,0\n\tblr\n\n\t/* Emulate H_SET_DABR/X on P8 for the sake of compat mode guests */\n2:\trlwimi\tr5, r4, 5, DAWRX_DR | DAWRX_DW\n\trlwimi\tr5, r4, 2, DAWRX_WT\n\tclrrdi\tr4, r4, 3\n\tstd\tr4, VCPU_DAWR(r3)\n\tstd\tr5, VCPU_DAWRX(r3)\n\tmtspr\tSPRN_DAWR, r4\n\tmtspr\tSPRN_DAWRX, r5\n\tli\tr3, 0\n\tblr\n\n_GLOBAL(kvmppc_h_cede)\t\t/* r3 = vcpu pointer, r11 = msr, r13 = paca */\n\tori\tr11,r11,MSR_EE\n\tstd\tr11,VCPU_MSR(r3)\n\tli\tr0,1\n\tstb\tr0,VCPU_CEDED(r3)\n\tsync\t\t\t/* order setting ceded vs. testing prodded */\n\tlbz\tr5,VCPU_PRODDED(r3)\n\tcmpwi\tr5,0\n\tbne\tkvm_cede_prodded\n\tli\tr12,0\t\t/* set trap to 0 to say hcall is handled */\n\tstw\tr12,VCPU_TRAP(r3)\n\tli\tr0,H_SUCCESS\n\tstd\tr0,VCPU_GPR(R3)(r3)\n\n\t/*\n\t * Set our bit in the bitmask of napping threads unless all the\n\t * other threads are already napping, in which case we send this\n\t * up to the host.\n\t */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr6,HSTATE_PTID(r13)\n\tlwz\tr8,VCORE_ENTRY_EXIT(r5)\n\tclrldi\tr8,r8,56\n\tli\tr0,1\n\tsld\tr0,r0,r6\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n31:\tlwarx\tr4,0,r6\n\tor\tr4,r4,r0\n\tcmpw\tr4,r8\n\tbeq\tkvm_cede_exit\n\tstwcx.\tr4,0,r6\n\tbne\t31b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr0,NAPPING_CEDE\n\tstb\tr0,HSTATE_NAPPING(r13)\n\tlwz\tr7,VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7,0x100\n\tbge\t33f\t\t/* another thread already exiting */\n\n/*\n * Although not specifically required by the architecture, POWER7\n * preserves the following registers in nap mode, even if an SMT mode\n * switch occurs: SLB entries, PURR, SPURR, AMOR, UAMOR, AMR, SPRG0-3,\n * DAR, DSISR, DABR, DABRX, DSCR, PMCx, MMCRx, SIAR, SDAR.\n */\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r3)\n\tstd\tr15, VCPU_GPR(R15)(r3)\n\tstd\tr16, VCPU_GPR(R16)(r3)\n\tstd\tr17, VCPU_GPR(R17)(r3)\n\tstd\tr18, VCPU_GPR(R18)(r3)\n\tstd\tr19, VCPU_GPR(R19)(r3)\n\tstd\tr20, VCPU_GPR(R20)(r3)\n\tstd\tr21, VCPU_GPR(R21)(r3)\n\tstd\tr22, VCPU_GPR(R22)(r3)\n\tstd\tr23, VCPU_GPR(R23)(r3)\n\tstd\tr24, VCPU_GPR(R24)(r3)\n\tstd\tr25, VCPU_GPR(R25)(r3)\n\tstd\tr26, VCPU_GPR(R26)(r3)\n\tstd\tr27, VCPU_GPR(R27)(r3)\n\tstd\tr28, VCPU_GPR(R28)(r3)\n\tstd\tr29, VCPU_GPR(R29)(r3)\n\tstd\tr30, VCPU_GPR(R30)(r3)\n\tstd\tr31, VCPU_GPR(R31)(r3)\n\n\t/* save FP state */\n\tbl\tkvmppc_save_fp\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_save_tm\nEND_FTR_SECTION_IFSET(CPU_FTR_TM)\n#endif\n\n\t/*\n\t * Set DEC to the smaller of DEC and HDEC, so that we wake\n\t * no later than the end of our timeslice (HDEC interrupts\n\t * don't wake us from nap).\n\t */\n\tmfspr\tr3, SPRN_DEC\n\tmfspr\tr4, SPRN_HDEC\n\tmftb\tr5\n\tcmpw\tr3, r4\n\tble\t67f\n\tmtspr\tSPRN_DEC, r4\n67:\n\t/* save expiry time of guest decrementer */\n\textsw\tr3, r3\n\tadd\tr3, r3, r5\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr6, VCORE_TB_OFFSET(r5)\n\tsubf\tr3, r6, r3\t/* convert to host TB value */\n\tstd\tr3, VCPU_DEC_EXPIRES(r4)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\taddi\tr3, r4, VCPU_TB_CEDE\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\tlis\tr3, LPCR_PECEDP@h\t/* Do wake on privileged doorbell */\n\n\t/*\n\t * Take a nap until a decrementer or external or doobell interrupt\n\t * occurs, with PECE1 and PECE0 set in LPCR.\n\t * On POWER8, set PECEDH, and if we are ceding, also set PECEDP.\n\t * Also clear the runlatch bit before napping.\n\t */\nkvm_do_nap:\n\tmfspr\tr0, SPRN_CTRLF\n\tclrrdi\tr0, r0, 1\n\tmtspr\tSPRN_CTRLT, r0\n\n\tli\tr0,1\n\tstb\tr0,HSTATE_HWTHREAD_REQ(r13)\n\tmfspr\tr5,SPRN_LPCR\n\tori\tr5,r5,LPCR_PECE0 | LPCR_PECE1\nBEGIN_FTR_SECTION\n\tori\tr5, r5, LPCR_PECEDH\n\trlwimi\tr5, r3, 0, LPCR_PECEDP\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_LPCR,r5\n\tisync\n\tli\tr0, 0\n\tstd\tr0, HSTATE_SCRATCH0(r13)\n\tptesync\n\tld\tr0, HSTATE_SCRATCH0(r13)\n1:\tcmpd\tr0, r0\n\tbne\t1b\n\tnap\n\tb\t.\n\n33:\tmr\tr4, r3\n\tli\tr3, 0\n\tli\tr12, 0\n\tb\t34f\n\nkvm_end_cede:\n\t/* get vcpu pointer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\n\t/* Woken by external or decrementer interrupt */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n\taddi\tr3, r4, VCPU_TB_RMINTR\n\tbl\tkvmhv_accumulate_time\n#endif\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tbl\tkvmppc_restore_tm\nEND_FTR_SECTION_IFSET(CPU_FTR_TM)\n#endif\n\n\t/* load up FP state */\n\tbl\tkvmppc_load_fp\n\n\t/* Restore guest decrementer */\n\tld\tr3, VCPU_DEC_EXPIRES(r4)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr6, VCORE_TB_OFFSET(r5)\n\tadd\tr3, r3, r6\t/* convert host TB to guest TB value */\n\tmftb\tr7\n\tsubf\tr3, r7, r3\n\tmtspr\tSPRN_DEC, r3\n\n\t/* Load NV GPRS */\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n \n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\n\t/* clear our bit in vcore->napping_threads */\n34:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr7,HSTATE_PTID(r13)\n\tli\tr0,1\n\tsld\tr0,r0,r7\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n32:\tlwarx\tr7,0,r6\n\tandc\tr7,r7,r0\n\tstwcx.\tr7,0,r6\n\tbne\t32b\n\tli\tr0,0\n\tstb\tr0,HSTATE_NAPPING(r13)\n\n\t/* See if the wake reason means we need to exit */\n\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\n\t/* see if any other thread is already exiting */\n\tlwz\tr0,VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0,0x100\n\tbge\tguest_exit_cont\n\n\tb\tkvmppc_cede_reentry\t/* if not go back to guest */\n\n\t/* cede when already previously prodded case */\nkvm_cede_prodded:\n\tli\tr0,0\n\tstb\tr0,VCPU_PRODDED(r3)\n\tsync\t\t\t/* order testing prodded vs. clearing ceded */\n\tstb\tr0,VCPU_CEDED(r3)\n\tli\tr3,H_SUCCESS\n\tblr\n\n\t/* we've ceded but we want to give control to the host */\nkvm_cede_exit:\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tb\tguest_exit_cont\n\n\t/* Try to handle a machine check in real mode */\nmachine_check_realmode:\n\tmr\tr3, r9\t\t/* get vcpu pointer */\n\tbl\tkvmppc_realmode_machine_check\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\t/*\n\t * Deliver unhandled/fatal (e.g. UE) MCE errors to guest through\n\t * machine check interrupt (set HSRR0 to 0x200). And for handled\n\t * errors (no-fatal), just go back to guest execution with current\n\t * HSRR0 instead of exiting guest. This new approach will inject\n\t * machine check to guest for fatal error causing guest to crash.\n\t *\n\t * The old code used to return to host for unhandled errors which\n\t * was causing guest to hang with soft lockups inside guest and\n\t * makes it difficult to recover guest instance.\n\t *\n\t * if we receive machine check with MSR(RI=0) then deliver it to\n\t * guest as machine check causing guest to crash.\n\t */\n\tld\tr11, VCPU_MSR(r9)\n\trldicl.\tr0, r11, 64-MSR_HV_LG, 63 /* check if it happened in HV mode */\n\tbne\tmc_cont\t\t\t/* if so, exit to host */\n\tandi.\tr10, r11, MSR_RI\t/* check for unrecoverable exception */\n\tbeq\t1f\t\t\t/* Deliver a machine check to guest */\n\tld\tr10, VCPU_PC(r9)\n\tcmpdi\tr3, 0\t\t/* Did we handle MCE ? */\n\tbne\t2f\t/* Continue guest execution. */\n\t/* If not, deliver a machine check.  SRR0/1 are already set */\n1:\tli\tr10, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tbl\tkvmppc_msr_interrupt\n2:\tb\tfast_interrupt_c_return\n\n/*\n * Check the reason we woke from nap, and take appropriate action.\n * Returns (in r3):\n *\t0 if nothing needs to be done\n *\t1 if something happened that needs to be handled by the host\n *\t-1 if there was a guest wakeup (IPI or msgsnd)\n *\n * Also sets r12 to the interrupt vector for any interrupt that needs\n * to be handled now by the host (0x500 for external interrupt), or zero.\n * Modifies r0, r6, r7, r8.\n */\nkvmppc_check_wake_reason:\n\tmfspr\tr6, SPRN_SRR1\nBEGIN_FTR_SECTION\n\trlwinm\tr6, r6, 45-31, 0xf\t/* extract wake reason field (P8) */\nFTR_SECTION_ELSE\n\trlwinm\tr6, r6, 45-31, 0xe\t/* P7 wake reason field is 3 bits */\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 8\t\t\t/* was it an external interrupt? */\n\tli\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbeq\tkvmppc_read_intr\t/* if so, see what it was */\n\tli\tr3, 0\n\tli\tr12, 0\n\tcmpwi\tr6, 6\t\t\t/* was it the decrementer? */\n\tbeq\t0f\nBEGIN_FTR_SECTION\n\tcmpwi\tr6, 5\t\t\t/* privileged doorbell? */\n\tbeq\t0f\n\tcmpwi\tr6, 3\t\t\t/* hypervisor doorbell? */\n\tbeq\t3f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 0xa\t\t\t/* Hypervisor maintenance ? */\n\tbeq\t4f\n\tli\tr3, 1\t\t\t/* anything else, return 1 */\n0:\tblr\n\n\t/* hypervisor doorbell */\n3:\tli\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\n\t/*\n\t * Clear the doorbell as we will invoke the handler\n\t * explicitly in the guest exit path.\n\t */\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\n\t/* see if it's a host IPI */\n\tli\tr3, 1\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbnelr\n\t/* if not, return -1 */\n\tli\tr3, -1\n\tblr\n\n\t/* Woken up due to Hypervisor maintenance interrupt */\n4:\tli\tr12, BOOK3S_INTERRUPT_HMI\n\tli\tr3, 1\n\tblr\n\n/*\n * Determine what sort of external interrupt is pending (if any).\n * Returns:\n *\t0 if no interrupt is pending\n *\t1 if an interrupt is pending that needs to be handled by the host\n *\t-1 if there was a guest wakeup IPI (which has now been cleared)\n * Modifies r0, r6, r7, r8, returns value in r3.\n */\nkvmppc_read_intr:\n\t/* see if a host IPI is pending */\n\tli\tr3, 1\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbne\t1f\n\n\t/* Now read the interrupt from the ICP */\n\tld\tr6, HSTATE_XICS_PHYS(r13)\n\tli\tr7, XICS_XIRR\n\tcmpdi\tr6, 0\n\tbeq-\t1f\n\tlwzcix\tr0, r6, r7\n\t/*\n\t * Save XIRR for later. Since we get in in reverse endian on LE\n\t * systems, save it byte reversed and fetch it back in host endian.\n\t */\n\tli\tr3, HSTATE_SAVED_XIRR\n\tSTWX_BE\tr0, r3, r13\n#ifdef __LITTLE_ENDIAN__\n\tlwz\tr3, HSTATE_SAVED_XIRR(r13)\n#else\n\tmr\tr3, r0\n#endif\n\trlwinm.\tr3, r3, 0, 0xffffff\n\tsync\n\tbeq\t1f\t\t\t/* if nothing pending in the ICP */\n\n\t/* We found something in the ICP...\n\t *\n\t * If it's not an IPI, stash it in the PACA and return to\n\t * the host, we don't (yet) handle directing real external\n\t * interrupts directly to the guest\n\t */\n\tcmpwi\tr3, XICS_IPI\t\t/* if there is, is it an IPI? */\n\tbne\t42f\n\n\t/* It's an IPI, clear the MFRR and EOI it */\n\tli\tr3, 0xff\n\tli\tr8, XICS_MFRR\n\tstbcix\tr3, r6, r8\t\t/* clear the IPI */\n\tstwcix\tr0, r6, r7\t\t/* EOI it */\n\tsync\n\n\t/* We need to re-check host IPI now in case it got set in the\n\t * meantime. If it's clear, we bounce the interrupt to the\n\t * guest\n\t */\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbne-\t43f\n\n\t/* OK, it's an IPI for us */\n\tli\tr12, 0\n\tli\tr3, -1\n1:\tblr\n\n42:\t/* It's not an IPI and it's for the host. We saved a copy of XIRR in\n\t * the PACA earlier, it will be picked up by the host ICP driver\n\t */\n\tli\tr3, 1\n\tb\t1b\n\n43:\t/* We raced with the host, we need to resend that IPI, bummer */\n\tli\tr0, IPI_PRIORITY\n\tstbcix\tr0, r6, r8\t\t/* set the IPI */\n\tsync\n\tli\tr3, 1\n\tb\t1b\n\n/*\n * Save away FP, VMX and VSX registers.\n * r3 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nkvmppc_save_fp:\n\tmflr\tr30\n\tmr\tr31,r3\n\tmfmsr\tr5\n\tori\tr8,r5,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r3,VCPU_FPRS\n\tbl\tstore_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tstore_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tmfspr\tr6,SPRN_VRSAVE\n\tstw\tr6,VCPU_VRSAVE(r31)\n\tmtlr\tr30\n\tblr\n\n/*\n * Load up FP, VMX and VSX registers\n * r4 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nkvmppc_load_fp:\n\tmflr\tr30\n\tmr\tr31,r4\n\tmfmsr\tr9\n\tori\tr8,r9,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r4,VCPU_FPRS\n\tbl\tload_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tload_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tlwz\tr7,VCPU_VRSAVE(r31)\n\tmtspr\tSPRN_VRSAVE,r7\n\tmtlr\tr30\n\tmr\tr4,r31\n\tblr\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n/*\n * Save transactional state and TM-related registers.\n * Called with r9 pointing to the vcpu struct.\n * This can modify all checkpointed registers, but\n * restores r1, r2 and r9 (vcpu pointer) before exit.\n */\nkvmppc_save_tm:\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\n\t/* Turn on TM. */\n\tmfmsr\tr8\n\tli\tr0, 1\n\trldimi\tr8, r0, MSR_TM_LG, 63-MSR_TM_LG\n\tmtmsrd\tr8\n\n\tld\tr5, VCPU_MSR(r9)\n\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n\tbeq\t1f\t/* TM not active in guest. */\n\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\tli\tr3, TM_CAUSE_KVM_RESCHED\n\n\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n\tli\tr5, 0\n\tmtmsrd\tr5, 1\n\n\t/* All GPRs are volatile at this point. */\n\tTRECLAIM(R3)\n\n\t/* Temporarily store r13 and r9 so we have some regs to play with */\n\tSET_SCRATCH0(r13)\n\tGET_PACA(r13)\n\tstd\tr9, PACATMSCRATCH(r13)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Get a few more GPRs free. */\n\tstd\tr29, VCPU_GPRS_TM(29)(r9)\n\tstd\tr30, VCPU_GPRS_TM(30)(r9)\n\tstd\tr31, VCPU_GPRS_TM(31)(r9)\n\n\t/* Save away PPR and DSCR soon so don't run with user values. */\n\tmfspr\tr31, SPRN_PPR\n\tHMT_MEDIUM\n\tmfspr\tr30, SPRN_DSCR\n\tld\tr29, HSTATE_DSCR(r13)\n\tmtspr\tSPRN_DSCR, r29\n\n\t/* Save all but r9, r13 & r29-r31 */\n\treg = 0\n\t.rept\t29\n\t.if (reg != 9) && (reg != 13)\n\tstd\treg, VCPU_GPRS_TM(reg)(r9)\n\t.endif\n\treg = reg + 1\n\t.endr\n\t/* ... now save r13 */\n\tGET_SCRATCH0(r4)\n\tstd\tr4, VCPU_GPRS_TM(13)(r9)\n\t/* ... and save r9 */\n\tld\tr4, PACATMSCRATCH(r13)\n\tstd\tr4, VCPU_GPRS_TM(9)(r9)\n\n\t/* Reload stack pointer and TOC. */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr2, PACATOC(r13)\n\n\t/* Set MSR RI now we have r1 and r13 back. */\n\tli\tr5, MSR_RI\n\tmtmsrd\tr5, 1\n\n\t/* Save away checkpinted SPRs. */\n\tstd\tr31, VCPU_PPR_TM(r9)\n\tstd\tr30, VCPU_DSCR_TM(r9)\n\tmflr\tr5\n\tmfcr\tr6\n\tmfctr\tr7\n\tmfspr\tr8, SPRN_AMR\n\tmfspr\tr10, SPRN_TAR\n\tstd\tr5, VCPU_LR_TM(r9)\n\tstw\tr6, VCPU_CR_TM(r9)\n\tstd\tr7, VCPU_CTR_TM(r9)\n\tstd\tr8, VCPU_AMR_TM(r9)\n\tstd\tr10, VCPU_TAR_TM(r9)\n\n\t/* Restore r12 as trap number. */\n\tlwz\tr12, VCPU_TRAP(r9)\n\n\t/* Save FP/VSX. */\n\taddi\tr3, r9, VCPU_FPRS_TM\n\tbl\tstore_fp_state\n\taddi\tr3, r9, VCPU_VRS_TM\n\tbl\tstore_vr_state\n\tmfspr\tr6, SPRN_VRSAVE\n\tstw\tr6, VCPU_VRSAVE_TM(r9)\n1:\n\t/*\n\t * We need to save these SPRs after the treclaim so that the software\n\t * error code is recorded correctly in the TEXASR.  Also the user may\n\t * change these outside of a transaction, so they must always be\n\t * context switched.\n\t */\n\tmfspr\tr5, SPRN_TFHAR\n\tmfspr\tr6, SPRN_TFIAR\n\tmfspr\tr7, SPRN_TEXASR\n\tstd\tr5, VCPU_TFHAR(r9)\n\tstd\tr6, VCPU_TFIAR(r9)\n\tstd\tr7, VCPU_TEXASR(r9)\n\n\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n\n/*\n * Restore transactional state and TM-related registers.\n * Called with r4 pointing to the vcpu struct.\n * This potentially modifies all checkpointed registers.\n * It restores r1, r2, r4 from the PACA.\n */\nkvmppc_restore_tm:\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\n\t/* Turn on TM/FP/VSX/VMX so we can restore them. */\n\tmfmsr\tr5\n\tli\tr6, MSR_TM >> 32\n\tsldi\tr6, r6, 32\n\tor\tr5, r5, r6\n\tori\tr5, r5, MSR_FP\n\toris\tr5, r5, (MSR_VEC | MSR_VSX)@h\n\tmtmsrd\tr5\n\n\t/*\n\t * The user may change these outside of a transaction, so they must\n\t * always be context switched.\n\t */\n\tld\tr5, VCPU_TFHAR(r4)\n\tld\tr6, VCPU_TFIAR(r4)\n\tld\tr7, VCPU_TEXASR(r4)\n\tmtspr\tSPRN_TFHAR, r5\n\tmtspr\tSPRN_TFIAR, r6\n\tmtspr\tSPRN_TEXASR, r7\n\n\tld\tr5, VCPU_MSR(r4)\n\trldicl. r5, r5, 64 - MSR_TS_S_LG, 62\n\tbeqlr\t\t/* TM not active in guest */\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\t/* Make sure the failure summary is set, otherwise we'll program check\n\t * when we trechkpt.  It's possible that this might have been not set\n\t * on a kvmppc_set_one_reg() call but we shouldn't let this crash the\n\t * host.\n\t */\n\toris\tr7, r7, (TEXASR_FS)@h\n\tmtspr\tSPRN_TEXASR, r7\n\n\t/*\n\t * We need to load up the checkpointed state for the guest.\n\t * We need to do this early as it will blow away any GPRs, VSRs and\n\t * some SPRs.\n\t */\n\n\tmr\tr31, r4\n\taddi\tr3, r31, VCPU_FPRS_TM\n\tbl\tload_fp_state\n\taddi\tr3, r31, VCPU_VRS_TM\n\tbl\tload_vr_state\n\tmr\tr4, r31\n\tlwz\tr7, VCPU_VRSAVE_TM(r4)\n\tmtspr\tSPRN_VRSAVE, r7\n\n\tld\tr5, VCPU_LR_TM(r4)\n\tlwz\tr6, VCPU_CR_TM(r4)\n\tld\tr7, VCPU_CTR_TM(r4)\n\tld\tr8, VCPU_AMR_TM(r4)\n\tld\tr9, VCPU_TAR_TM(r4)\n\tmtlr\tr5\n\tmtcr\tr6\n\tmtctr\tr7\n\tmtspr\tSPRN_AMR, r8\n\tmtspr\tSPRN_TAR, r9\n\n\t/*\n\t * Load up PPR and DSCR values but don't put them in the actual SPRs\n\t * till the last moment to avoid running with userspace PPR and DSCR for\n\t * too long.\n\t */\n\tld\tr29, VCPU_DSCR_TM(r4)\n\tld\tr30, VCPU_PPR_TM(r4)\n\n\tstd\tr2, PACATMSCRATCH(r13) /* Save TOC */\n\n\t/* Clear the MSR RI since r1, r13 are all going to be foobar. */\n\tli\tr5, 0\n\tmtmsrd\tr5, 1\n\n\t/* Load GPRs r0-r28 */\n\treg = 0\n\t.rept\t29\n\tld\treg, VCPU_GPRS_TM(reg)(r31)\n\treg = reg + 1\n\t.endr\n\n\tmtspr\tSPRN_DSCR, r29\n\tmtspr\tSPRN_PPR, r30\n\n\t/* Load final GPRs */\n\tld\t29, VCPU_GPRS_TM(29)(r31)\n\tld\t30, VCPU_GPRS_TM(30)(r31)\n\tld\t31, VCPU_GPRS_TM(31)(r31)\n\n\t/* TM checkpointed state is now setup.  All GPRs are now volatile. */\n\tTRECHKPT\n\n\t/* Now let's get back the state we need. */\n\tHMT_MEDIUM\n\tGET_PACA(r13)\n\tld\tr29, HSTATE_DSCR(r13)\n\tmtspr\tSPRN_DSCR, r29\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr2, PACATMSCRATCH(r13)\n\n\t/* Set the MSR RI since we have our registers back. */\n\tli\tr5, MSR_RI\n\tmtmsrd\tr5, 1\n\n\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n#endif\n\n/*\n * We come here if we get any exception or interrupt while we are\n * executing host real mode code while in guest MMU context.\n * For now just spin, but we should do something better.\n */\nkvmppc_bad_host_intr:\n\tb\t.\n\n/*\n * This mimics the MSR transition on IRQ delivery.  The new guest MSR is taken\n * from VCPU_INTR_MSR and is modified based on the required TM state changes.\n *   r11 has the guest MSR value (in/out)\n *   r9 has a vcpu pointer (in)\n *   r0 is used as a scratch register\n */\nkvmppc_msr_interrupt:\n\trldicl\tr0, r11, 64 - MSR_TS_S_LG, 62\n\tcmpwi\tr0, 2 /* Check if we are in transactional state..  */\n\tld\tr11, VCPU_INTR_MSR(r9)\n\tbne\t1f\n\t/* ... if transactional, change to suspended */\n\tli\tr0, 1\n1:\trldimi\tr11, r0, MSR_TS_S_LG, 63 - MSR_TS_T_LG\n\tblr\n\n/*\n * This works around a hardware bug on POWER8E processors, where\n * writing a 1 to the MMCR0[PMAO] bit doesn't generate a\n * performance monitor interrupt.  Instead, when we need to have\n * an interrupt pending, we have to arrange for a counter to overflow.\n */\nkvmppc_fix_pmao:\n\tli\tr3, 0\n\tmtspr\tSPRN_MMCR2, r3\n\tlis\tr3, (MMCR0_PMXE | MMCR0_FCECE)@h\n\tori\tr3, r3, MMCR0_PMCjCE | MMCR0_C56RUN\n\tmtspr\tSPRN_MMCR0, r3\n\tlis\tr3, 0x7fff\n\tori\tr3, r3, 0xffff\n\tmtspr\tSPRN_PMC6, r3\n\tisync\n\tblr\n\n#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING\n/*\n * Start timing an activity\n * r3 = pointer to time accumulation struct, r4 = vcpu\n */\nkvmhv_start_timing:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr6, VCORE_IN_GUEST(r5)\n\tcmpwi\tr6, 0\n\tbeq\t5f\t\t\t\t/* if in guest, need to */\n\tld\tr6, VCORE_TB_OFFSET(r5)\t\t/* subtract timebase offset */\n5:\tmftb\tr5\n\tsubf\tr5, r6, r5\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tstd\tr5, VCPU_ACTIVITY_START(r4)\n\tblr\n\n/*\n * Accumulate time to one activity and start another.\n * r3 = pointer to new time accumulation struct, r4 = vcpu\n */\nkvmhv_accumulate_time:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr8, VCORE_IN_GUEST(r5)\n\tcmpwi\tr8, 0\n\tbeq\t4f\t\t\t\t/* if in guest, need to */\n\tld\tr8, VCORE_TB_OFFSET(r5)\t\t/* subtract timebase offset */\n4:\tld\tr5, VCPU_CUR_ACTIVITY(r4)\n\tld\tr6, VCPU_ACTIVITY_START(r4)\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tmftb\tr7\n\tsubf\tr7, r8, r7\n\tstd\tr7, VCPU_ACTIVITY_START(r4)\n\tcmpdi\tr5, 0\n\tbeqlr\n\tsubf\tr3, r6, r7\n\tld\tr8, TAS_SEQCOUNT(r5)\n\tcmpdi\tr8, 0\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tlwsync\n\tld\tr7, TAS_TOTAL(r5)\n\tadd\tr7, r7, r3\n\tstd\tr7, TAS_TOTAL(r5)\n\tld\tr6, TAS_MIN(r5)\n\tld\tr7, TAS_MAX(r5)\n\tbeq\t3f\n\tcmpd\tr3, r6\n\tbge\t1f\n3:\tstd\tr3, TAS_MIN(r5)\n1:\tcmpd\tr3, r7\n\tble\t2f\n\tstd\tr3, TAS_MAX(r5)\n2:\tlwsync\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tblr\n#endif\n", "code_before": "/* SPDX-License-Identifier: GPL-2.0-only */\n/*\n *\n * Copyright 2011 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>\n *\n * Derived from book3s_rmhandlers.S and other files, which are:\n *\n * Copyright SUSE Linux Products GmbH 2009\n *\n * Authors: Alexander Graf <agraf@suse.de>\n */\n\n#include <linux/export.h>\n#include <linux/linkage.h>\n#include <linux/objtool.h>\n#include <asm/ppc_asm.h>\n#include <asm/code-patching-asm.h>\n#include <asm/kvm_asm.h>\n#include <asm/reg.h>\n#include <asm/mmu.h>\n#include <asm/page.h>\n#include <asm/ptrace.h>\n#include <asm/hvcall.h>\n#include <asm/asm-offsets.h>\n#include <asm/exception-64s.h>\n#include <asm/kvm_book3s_asm.h>\n#include <asm/book3s/64/mmu-hash.h>\n#include <asm/tm.h>\n#include <asm/opal.h>\n#include <asm/thread_info.h>\n#include <asm/asm-compat.h>\n#include <asm/feature-fixups.h>\n#include <asm/cpuidle.h>\n\n/* Values in HSTATE_NAPPING(r13) */\n#define NAPPING_CEDE\t1\n#define NAPPING_NOVCPU\t2\n#define NAPPING_UNSPLIT\t3\n\n/* Stack frame offsets for kvmppc_hv_entry */\n#define SFS\t\t\t160\n#define STACK_SLOT_TRAP\t\t(SFS-4)\n#define STACK_SLOT_TID\t\t(SFS-16)\n#define STACK_SLOT_PSSCR\t(SFS-24)\n#define STACK_SLOT_PID\t\t(SFS-32)\n#define STACK_SLOT_IAMR\t\t(SFS-40)\n#define STACK_SLOT_CIABR\t(SFS-48)\n#define STACK_SLOT_DAWR0\t(SFS-56)\n#define STACK_SLOT_DAWRX0\t(SFS-64)\n#define STACK_SLOT_HFSCR\t(SFS-72)\n#define STACK_SLOT_AMR\t\t(SFS-80)\n#define STACK_SLOT_UAMOR\t(SFS-88)\n#define STACK_SLOT_FSCR\t\t(SFS-96)\n\n/*\n * Use the last LPID (all implemented LPID bits = 1) for partition switching.\n * This is reserved in the LPID allocator. POWER7 only implements 0x3ff, but\n * we write 0xfff into the LPID SPR anyway, which seems to work and just\n * ignores the top bits.\n */\n#define   LPID_RSVD\t\t0xfff\n\n/*\n * Call kvmppc_hv_entry in real mode.\n * Must be called with interrupts hard-disabled.\n *\n * Input Registers:\n *\n * LR = return address to continue at after eventually re-enabling MMU\n */\n_GLOBAL_TOC(kvmppc_hv_entry_trampoline)\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -112(r1)\n\tmfmsr\tr10\n\tstd\tr10, HSTATE_HOST_MSR(r13)\n\tLOAD_REG_ADDR(r5, kvmppc_call_hv_entry)\n\tli\tr0,MSR_RI\n\tandc\tr0,r10,r0\n\tli\tr6,MSR_IR | MSR_DR\n\tandc\tr6,r10,r6\n\tmtmsrd\tr0,1\t\t/* clear RI in MSR */\n\tmtsrr0\tr5\n\tmtsrr1\tr6\n\tRFI_TO_KERNEL\n\nkvmppc_call_hv_entry:\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from guest - restore host state and return to caller */\n\nBEGIN_FTR_SECTION\n\t/* Restore host DABR and DABRX */\n\tld\tr5,HSTATE_DABR(r13)\n\tli\tr6,7\n\tmtspr\tSPRN_DABR,r5\n\tmtspr\tSPRN_DABRX,r6\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n\t/* Restore SPRG3 */\n\tld\tr3,PACA_SPRG_VDSO(r13)\n\tmtspr\tSPRN_SPRG_VDSO_WRITE,r3\n\n\t/* Reload the host's PMU registers */\n\tbl\tkvmhv_load_host_pmu\n\n\t/*\n\t * Reload DEC.  HDEC interrupts were disabled when\n\t * we reloaded the host's LPCR value.\n\t */\n\tld\tr3, HSTATE_DECEXP(r13)\n\tmftb\tr4\n\tsubf\tr4, r4, r3\n\tmtspr\tSPRN_DEC, r4\n\n\t/* hwthread_req may have got set by cede or no vcpu, so clear it */\n\tli\tr0, 0\n\tstb\tr0, HSTATE_HWTHREAD_REQ(r13)\n\n\t/*\n\t * For external interrupts we need to call the Linux\n\t * handler to process the interrupt. We do that by jumping\n\t * to absolute address 0x500 for external interrupts.\n\t * The [h]rfid at the end of the handler will return to\n\t * the book3s_hv_interrupts.S code. For other interrupts\n\t * we do the rfid to get back to the book3s_hv_interrupts.S\n\t * code here.\n\t */\n\tld\tr8, 112+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, 112\n\tld\tr7, HSTATE_HOST_MSR(r13)\n\n\t/* Return the trap number on this thread as the return value */\n\tmr\tr3, r12\n\n\t/* RFI into the highmem handler */\n\tmfmsr\tr6\n\tli\tr0, MSR_RI\n\tandc\tr6, r6, r0\n\tmtmsrd\tr6, 1\t\t\t/* Clear RI in MSR */\n\tmtsrr0\tr8\n\tmtsrr1\tr7\n\tRFI_TO_KERNEL\n\nkvmppc_primary_no_guest:\n\t/* We handle this much like a ceded vcpu */\n\t/* put the HDEC into the DEC, since HDEC interrupts don't wake us */\n\t/* HDEC may be larger than DEC for arch >= v3.00, but since the */\n\t/* HDEC value came from DEC in the first place, it will fit */\n\tmfspr\tr3, SPRN_HDEC\n\tmtspr\tSPRN_DEC, r3\n\t/*\n\t * Make sure the primary has finished the MMU switch.\n\t * We should never get here on a secondary thread, but\n\t * check it for robustness' sake.\n\t */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n65:\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t65b\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\t/* set our bit in napping_threads */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n1:\tlwarx\tr3, 0, r6\n\tor\tr3, r3, r0\n\tstwcx.\tr3, 0, r6\n\tbne\t1b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr12, 0\n\tlwz\tr7, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7, 0x100\n\tbge\tkvm_novcpu_exit\t/* another thread already exiting */\n\tli\tr3, NAPPING_NOVCPU\n\tstb\tr3, HSTATE_NAPPING(r13)\n\n\tli\tr3, 0\t\t/* Don't wake on privileged (OS) doorbell */\n\tb\tkvm_do_nap\n\n/*\n * kvm_novcpu_wakeup\n *\tEntered from kvm_start_guest if kvm_hstate.napping is set\n *\tto NAPPING_NOVCPU\n *\t\tr2 = kernel TOC\n *\t\tr13 = paca\n */\nkvm_novcpu_wakeup:\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tli\tr0, 0\n\tstb\tr0, HSTATE_NAPPING(r13)\n\n\t/* check the wake reason */\n\tbl\tkvmppc_check_wake_reason\n\n\t/*\n\t * Restore volatile registers since we could have called\n\t * a C routine in kvmppc_check_wake_reason.\n\t *\tr5 = VCORE\n\t */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\n\t/* see if any other thread is already exiting */\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tbge\tkvm_novcpu_exit\n\n\t/* clear our bit in napping_threads */\n\tlbz\tr7, HSTATE_PTID(r13)\n\tli\tr0, 1\n\tsld\tr0, r0, r7\n\taddi\tr6, r5, VCORE_NAPPING_THREADS\n4:\tlwarx\tr7, 0, r6\n\tandc\tr7, r7, r0\n\tstwcx.\tr7, 0, r6\n\tbne\t4b\n\n\t/* See if the wake reason means we need to exit */\n\tcmpdi\tr3, 0\n\tbge\tkvm_novcpu_exit\n\n\t/* See if our timeslice has expired (HDEC is negative) */\n\tmfspr\tr0, SPRN_HDEC\n\textsw\tr0, r0\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n\tcmpdi\tr0, 0\n\tblt\tkvm_novcpu_exit\n\n\t/* Got an IPI but other vcpus aren't yet exiting, must be a latecomer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n#endif\n\tb\tkvmppc_got_guest\n\nkvm_novcpu_exit:\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tbeq\t13f\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n13:\tmr\tr3, r12\n\tstw\tr12, STACK_SLOT_TRAP(r1)\n\tbl\tkvmhv_commence_exit\n\tnop\n\tb\tkvmhv_switch_to_host\n\n/*\n * We come in here when wakened from Linux offline idle code.\n * Relocation is off\n * r3 contains the SRR1 wakeup value, SRR1 is trashed.\n */\n_GLOBAL(idle_kvm_start_guest)\n\tmfcr\tr5\n\tmflr\tr0\n\tstd\tr5, 8(r1)\t// Save CR in caller's frame\n\tstd\tr0, 16(r1)\t// Save LR in caller's frame\n\t// Create frame on emergency stack\n\tld\tr4, PACAEMERGSP(r13)\n\tstdu\tr1, -SWITCH_FRAME_SIZE(r4)\n\t// Switch to new frame on emergency stack\n\tmr\tr1, r4\n\tstd\tr3, 32(r1)\t// Save SRR1 wakeup value\n\tSAVE_NVGPRS(r1)\n\n\t/*\n\t * Could avoid this and pass it through in r3. For now,\n\t * code expects it to be in SRR1.\n\t */\n\tmtspr\tSPRN_SRR1,r3\n\n\tli\tr0,0\n\tstb\tr0,PACA_FTRACE_ENABLED(r13)\n\n\tli\tr0,KVM_HWTHREAD_IN_KVM\n\tstb\tr0,HSTATE_HWTHREAD_STATE(r13)\n\n\t/* kvm cede / napping does not come through here */\n\tlbz\tr0,HSTATE_NAPPING(r13)\n\ttwnei\tr0,0\n\n\tb\t1f\n\nkvm_unsplit_wakeup:\n\tli\tr0, 0\n\tstb\tr0, HSTATE_NAPPING(r13)\n\n1:\n\n\t/*\n\t * We weren't napping due to cede, so this must be a secondary\n\t * thread being woken up to run a guest, or being woken up due\n\t * to a stray IPI.  (Or due to some machine check or hypervisor\n\t * maintenance interrupt while the core is in KVM.)\n\t */\n\n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\t/*\n\t * kvmppc_check_wake_reason could invoke a C routine, but we\n\t * have no volatile registers to restore when we return.\n\t */\n\n\tcmpdi\tr3, 0\n\tbge\tkvm_no_guest\n\n\t/* get vcore pointer, NULL if we have nothing to run */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5,0\n\t/* if we have no vcore to run, go back to sleep */\n\tbeq\tkvm_no_guest\n\nkvm_secondary_got_guest:\n\n\t// About to go to guest, clear saved SRR1\n\tli\tr0, 0\n\tstd\tr0, 32(r1)\n\n\t/* Set HSTATE_DSCR(r13) to something sensible */\n\tld\tr6, PACA_DSCR_DEFAULT(r13)\n\tstd\tr6, HSTATE_DSCR(r13)\n\n\t/* On thread 0 of a subcore, set HDEC to max */\n\tlbz\tr4, HSTATE_PTID(r13)\n\tcmpwi\tr4, 0\n\tbne\t63f\n\tlis\tr6,0x7fff\t\t/* MAX_INT@h */\n\tmtspr\tSPRN_HDEC, r6\n\t/* and set per-LPAR registers, if doing dynamic micro-threading */\n\tld\tr6, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr6, 0\n\tbeq\t63f\n\tld\tr0, KVM_SPLIT_RPR(r6)\n\tmtspr\tSPRN_RPR, r0\n\tld\tr0, KVM_SPLIT_PMMAR(r6)\n\tmtspr\tSPRN_PMMAR, r0\n\tld\tr0, KVM_SPLIT_LDBAR(r6)\n\tmtspr\tSPRN_LDBAR, r0\n\tisync\n63:\n\t/* Order load of vcpu after load of vcore */\n\tlwsync\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_hv_entry\n\n\t/* Back from the guest, go back to nap */\n\t/* Clear our vcpu and vcore pointers so we don't come back in early */\n\tli\tr0, 0\n\tstd\tr0, HSTATE_KVM_VCPU(r13)\n\t/*\n\t * Once we clear HSTATE_KVM_VCORE(r13), the code in\n\t * kvmppc_run_core() is going to assume that all our vcpu\n\t * state is visible in memory.  This lwsync makes sure\n\t * that that is true.\n\t */\n\tlwsync\n\tstd\tr0, HSTATE_KVM_VCORE(r13)\n\n\t/*\n\t * All secondaries exiting guest will fall through this path.\n\t * Before proceeding, just check for HMI interrupt and\n\t * invoke opal hmi handler. By now we are sure that the\n\t * primary thread on this core/subcore has already made partition\n\t * switch/TB resync and we are good to call opal hmi handler.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\tkvm_no_guest\n\n\tli\tr3,0\t\t\t/* NULL argument */\n\tbl\tCFUNC(hmi_exception_realmode)\n/*\n * At this point we have finished executing in the guest.\n * We need to wait for hwthread_req to become zero, since\n * we may not turn on the MMU while hwthread_req is non-zero.\n * While waiting we also need to check if we get given a vcpu to run.\n */\nkvm_no_guest:\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t53f\n\tHMT_MEDIUM\n\tli\tr0, KVM_HWTHREAD_IN_KERNEL\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\t/* need to recheck hwthread_req after a barrier, to avoid race */\n\tsync\n\tlbz\tr3, HSTATE_HWTHREAD_REQ(r13)\n\tcmpwi\tr3, 0\n\tbne\t54f\n\n\t/*\n\t * Jump to idle_return_gpr_loss, which returns to the\n\t * idle_kvm_start_guest caller.\n\t */\n\tli\tr3, LPCR_PECE0\n\tmfspr\tr4, SPRN_LPCR\n\trlwimi\tr4, r3, 0, LPCR_PECE0 | LPCR_PECE1\n\tmtspr\tSPRN_LPCR, r4\n\t// Return SRR1 wakeup value, or 0 if we went into the guest\n\tld\tr3, 32(r1)\n\tREST_NVGPRS(r1)\n\tld\tr1, 0(r1)\t// Switch back to caller stack\n\tld\tr0, 16(r1)\t// Reload LR\n\tld\tr5, 8(r1)\t// Reload CR\n\tmtlr\tr0\n\tmtcr\tr5\n\tblr\n\n53:\n\tHMT_LOW\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr5, 0\n\tbne\t60f\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tcmpdi\tr3, 0\n\tbeq\tkvm_no_guest\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\tkvm_no_guest\n\tHMT_MEDIUM\n\tb\tkvm_unsplit_nap\n60:\tHMT_MEDIUM\n\tb\tkvm_secondary_got_guest\n\n54:\tli\tr0, KVM_HWTHREAD_IN_KVM\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\tb\tkvm_no_guest\n\n/*\n * Here the primary thread is trying to return the core to\n * whole-core mode, so we need to nap.\n */\nkvm_unsplit_nap:\n\t/*\n\t * When secondaries are napping in kvm_unsplit_nap() with\n\t * hwthread_req = 1, HMI goes ignored even though subcores are\n\t * already exited the guest. Hence HMI keeps waking up secondaries\n\t * from nap in a loop and secondaries always go back to nap since\n\t * no vcore is assigned to them. This makes impossible for primary\n\t * thread to get hold of secondary threads resulting into a soft\n\t * lockup in KVM path.\n\t *\n\t * Let us check if HMI is pending and handle it before we go to nap.\n\t */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbne\t55f\n\tli\tr3, 0\t\t\t/* NULL argument */\n\tbl\tCFUNC(hmi_exception_realmode)\n55:\n\t/*\n\t * Ensure that secondary doesn't nap when it has\n\t * its vcore pointer set.\n\t */\n\tsync\t\t/* matches smp_mb() before setting split_info.do_nap */\n\tld\tr0, HSTATE_KVM_VCORE(r13)\n\tcmpdi\tr0, 0\n\tbne\tkvm_no_guest\n\t/* clear any pending message */\nBEGIN_FTR_SECTION\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\t/* Set kvm_split_mode.napped[tid] = 1 */\n\tld\tr3, HSTATE_SPLIT_MODE(r13)\n\tli\tr0, 1\n\tlhz\tr4, PACAPACAINDEX(r13)\n\tclrldi\tr4, r4, 61\t/* micro-threading => P8 => 8 threads/core */\n\taddi\tr4, r4, KVM_SPLIT_NAPPED\n\tstbx\tr0, r3, r4\n\t/* Check the do_nap flag again after setting napped[] */\n\tsync\n\tlbz\tr0, KVM_SPLIT_DO_NAP(r3)\n\tcmpwi\tr0, 0\n\tbeq\t57f\n\tli\tr3, NAPPING_UNSPLIT\n\tstb\tr3, HSTATE_NAPPING(r13)\n\tli\tr3, (LPCR_PECEDH | LPCR_PECE0) >> 4\n\tmfspr\tr5, SPRN_LPCR\n\trlwimi\tr5, r3, 4, (LPCR_PECEDP | LPCR_PECEDH | LPCR_PECE0 | LPCR_PECE1)\n\tb\tkvm_nap_sequence\n\n57:\tli\tr0, 0\n\tstbx\tr0, r3, r4\n\tb\tkvm_no_guest\n\n/******************************************************************************\n *                                                                            *\n *                               Entry code                                   *\n *                                                                            *\n *****************************************************************************/\n\nSYM_CODE_START_LOCAL(kvmppc_hv_entry)\n\n\t/* Required state:\n\t *\n\t * R4 = vcpu pointer (or NULL)\n\t * MSR = ~IR|DR\n\t * R13 = PACA\n\t * R1 = host R1\n\t * R2 = TOC\n\t * all other volatile GPRS = free\n\t * Does not preserve non-volatile GPRs or CR fields\n\t */\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -SFS(r1)\n\n\t/* Save R1 in the PACA */\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\tli\tr6, KVM_GUEST_MODE_HOST_HV\n\tstb\tr6, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\t/* Store initial timestamp */\n\tcmpdi\tr4, 0\n\tbeq\t1f\n\taddi\tr3, r4, VCPU_TB_RMENTRY\n\tbl\tkvmhv_start_timing\n1:\n#endif\n\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr9, VCORE_KVM(r5)\t/* pointer to struct kvm */\n\n\t/*\n\t * POWER7/POWER8 host -> guest partition switch code.\n\t * We don't have to lock against concurrent tlbies,\n\t * but we do have to coordinate across hardware threads.\n\t */\n\t/* Set bit in entry map iff exit map is zero. */\n\tli\tr7, 1\n\tlbz\tr6, HSTATE_PTID(r13)\n\tsld\tr7, r7, r6\n\taddi\tr8, r5, VCORE_ENTRY_EXIT\n21:\tlwarx\tr3, 0, r8\n\tcmpwi\tr3, 0x100\t\t/* any threads starting to exit? */\n\tbge\tsecondary_too_late\t/* if so we're too late to the party */\n\tor\tr3, r3, r7\n\tstwcx.\tr3, 0, r8\n\tbne\t21b\n\n\t/* Primary thread switches to guest partition. */\n\tcmpwi\tr6,0\n\tbne\t10f\n\n\tlwz\tr7,KVM_LPID(r9)\n\tld\tr6,KVM_SDR1(r9)\n\tli\tr0,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r0\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to partition page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\n\t/* See if we need to flush the TLB. */\n\tmr\tr3, r9\t\t\t/* kvm pointer */\n\tlhz\tr4, PACAPACAINDEX(r13)\t/* physical cpu number */\n\tli\tr5, 0\t\t\t/* nested vcpu pointer */\n\tbl\tkvmppc_check_need_tlb_flush\n\tnop\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\n\t/* Add timebase offset onto timebase */\n22:\tld\tr8,VCORE_TB_OFFSET(r5)\n\tcmpdi\tr8,0\n\tbeq\t37f\n\tstd\tr8, VCORE_TB_OFFSET_APPL(r5)\n\tmftb\tr6\t\t/* current host timebase */\n\tadd\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t/* update upper 40 bits */\n\tmftb\tr7\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t37f\n\taddis\tr8,r8,0x100\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n\t/* Load guest PCR value to select appropriate compat mode */\n37:\tld\tr7, VCORE_PCR(r5)\n\tLOAD_REG_IMMEDIATE(r6, PCR_MASK)\n\tcmpld\tr7, r6\n\tbeq\t38f\n\tor\tr7, r7, r6\n\tmtspr\tSPRN_PCR, r7\n38:\n\nBEGIN_FTR_SECTION\n\t/* DPDES and VTB are shared between threads */\n\tld\tr8, VCORE_DPDES(r5)\n\tld\tr7, VCORE_VTB(r5)\n\tmtspr\tSPRN_DPDES, r8\n\tmtspr\tSPRN_VTB, r7\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* Mark the subcore state as inside guest */\n\tbl\tkvmppc_subcore_enter_guest\n\tnop\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tli\tr0,1\n\tstb\tr0,VCORE_IN_GUEST(r5)\t/* signal secondaries to continue */\n\n\t/* Do we have a guest vcpu to run? */\n10:\tcmpdi\tr4, 0\n\tbeq\tkvmppc_primary_no_guest\nkvmppc_got_guest:\n\t/* Increment yield count if they have a VPA */\n\tld\tr3, VCPU_VPA(r4)\n\tcmpdi\tr3, 0\n\tbeq\t25f\n\tli\tr6, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr5, r3, r6\n\taddi\tr5, r5, 1\n\tSTWX_BE\tr5, r3, r6\n\tli\tr6, 1\n\tstb\tr6, VCPU_VPA_DIRTY(r4)\n25:\n\n\t/* Save purr/spurr */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tstd\tr5,HSTATE_PURR(r13)\n\tstd\tr6,HSTATE_SPURR(r13)\n\tld\tr7,VCPU_PURR(r4)\n\tld\tr8,VCPU_SPURR(r4)\n\tmtspr\tSPRN_PURR,r7\n\tmtspr\tSPRN_SPURR,r8\n\n\t/* Save host values of some registers */\nBEGIN_FTR_SECTION\n\tmfspr\tr5, SPRN_CIABR\n\tmfspr\tr6, SPRN_DAWR0\n\tmfspr\tr7, SPRN_DAWRX0\n\tmfspr\tr8, SPRN_IAMR\n\tstd\tr5, STACK_SLOT_CIABR(r1)\n\tstd\tr6, STACK_SLOT_DAWR0(r1)\n\tstd\tr7, STACK_SLOT_DAWRX0(r1)\n\tstd\tr8, STACK_SLOT_IAMR(r1)\n\tmfspr\tr5, SPRN_FSCR\n\tstd\tr5, STACK_SLOT_FSCR(r1)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\tmfspr\tr5, SPRN_AMR\n\tstd\tr5, STACK_SLOT_AMR(r1)\n\tmfspr\tr6, SPRN_UAMOR\n\tstd\tr6, STACK_SLOT_UAMOR(r1)\n\nBEGIN_FTR_SECTION\n\t/* Set partition DABR */\n\t/* Do this before re-enabling PMU to avoid P7 DABR corruption bug */\n\tlwz\tr5,VCPU_DABRX(r4)\n\tld\tr6,VCPU_DABR(r4)\n\tmtspr\tSPRN_DABRX,r5\n\tmtspr\tSPRN_DABR,r6\n\tisync\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tmr      r3, r4\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_restore_tm_hv\n\tnop\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n91:\n#endif\n\n\t/* Load guest PMU registers; r4 = vcpu pointer here */\n\tmr\tr3, r4\n\tbl\tkvmhv_load_guest_pmu\n\n\t/* Load up FP, VMX and VSX registers */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tbl\tkvmppc_load_fp\n\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n\n\t/* Switch DSCR to guest value */\n\tld\tr5, VCPU_DSCR(r4)\n\tmtspr\tSPRN_DSCR, r5\n\nBEGIN_FTR_SECTION\n\t/* Skip next section on POWER7 */\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Load up POWER8-specific registers */\n\tld\tr5, VCPU_IAMR(r4)\n\tlwz\tr6, VCPU_PSPB(r4)\n\tld\tr7, VCPU_FSCR(r4)\n\tmtspr\tSPRN_IAMR, r5\n\tmtspr\tSPRN_PSPB, r6\n\tmtspr\tSPRN_FSCR, r7\n\t/*\n\t * Handle broken DAWR case by not writing it. This means we\n\t * can still store the DAWR register for migration.\n\t */\n\tLOAD_REG_ADDR(r5, dawr_force_enable)\n\tlbz\tr5, 0(r5)\n\tcmpdi\tr5, 0\n\tbeq\t1f\n\tld\tr5, VCPU_DAWR0(r4)\n\tld\tr6, VCPU_DAWRX0(r4)\n\tmtspr\tSPRN_DAWR0, r5\n\tmtspr\tSPRN_DAWRX0, r6\n1:\n\tld\tr7, VCPU_CIABR(r4)\n\tld\tr8, VCPU_TAR(r4)\n\tmtspr\tSPRN_CIABR, r7\n\tmtspr\tSPRN_TAR, r8\n\tld\tr5, VCPU_IC(r4)\n\tld\tr8, VCPU_EBBHR(r4)\n\tmtspr\tSPRN_IC, r5\n\tmtspr\tSPRN_EBBHR, r8\n\tld\tr5, VCPU_EBBRR(r4)\n\tld\tr6, VCPU_BESCR(r4)\n\tlwz\tr7, VCPU_GUEST_PID(r4)\n\tld\tr8, VCPU_WORT(r4)\n\tmtspr\tSPRN_EBBRR, r5\n\tmtspr\tSPRN_BESCR, r6\n\tmtspr\tSPRN_PID, r7\n\tmtspr\tSPRN_WORT, r8\n\t/* POWER8-only registers */\n\tld\tr5, VCPU_TCSCR(r4)\n\tld\tr6, VCPU_ACOP(r4)\n\tld\tr7, VCPU_CSIGR(r4)\n\tld\tr8, VCPU_TACR(r4)\n\tmtspr\tSPRN_TCSCR, r5\n\tmtspr\tSPRN_ACOP, r6\n\tmtspr\tSPRN_CSIGR, r7\n\tmtspr\tSPRN_TACR, r8\n\tnop\n8:\n\n\tld\tr5, VCPU_SPRG0(r4)\n\tld\tr6, VCPU_SPRG1(r4)\n\tld\tr7, VCPU_SPRG2(r4)\n\tld\tr8, VCPU_SPRG3(r4)\n\tmtspr\tSPRN_SPRG0, r5\n\tmtspr\tSPRN_SPRG1, r6\n\tmtspr\tSPRN_SPRG2, r7\n\tmtspr\tSPRN_SPRG3, r8\n\n\t/* Load up DAR and DSISR */\n\tld\tr5, VCPU_DAR(r4)\n\tlwz\tr6, VCPU_DSISR(r4)\n\tmtspr\tSPRN_DAR, r5\n\tmtspr\tSPRN_DSISR, r6\n\n\t/* Restore AMR and UAMOR, set AMOR to all 1s */\n\tld\tr5,VCPU_AMR(r4)\n\tld\tr6,VCPU_UAMOR(r4)\n\tmtspr\tSPRN_AMR,r5\n\tmtspr\tSPRN_UAMOR,r6\n\n\t/* Restore state of CTRL run bit; the host currently has it set to 1 */\n\tlwz\tr5,VCPU_CTRL(r4)\n\tandi.\tr5,r5,1\n\tbne\t4f\n\tli\tr6,0\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/* Secondary threads wait for primary to have done partition switch */\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlbz\tr6, HSTATE_PTID(r13)\n\tcmpwi\tr6, 0\n\tbeq\t21f\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbne\t21f\n\tHMT_LOW\n20:\tlwz\tr3, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr3, 0x100\n\tbge\tno_switch_exit\n\tlbz\tr0, VCORE_IN_GUEST(r5)\n\tcmpwi\tr0, 0\n\tbeq\t20b\n\tHMT_MEDIUM\n21:\n\t/* Set LPCR. */\n\tld\tr8,VCORE_LPCR(r5)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n\t/*\n\t * Set the decrementer to the guest decrementer.\n\t */\n\tld\tr8,VCPU_DEC_EXPIRES(r4)\n\tmftb\tr7\n\tsubf\tr3,r7,r8\n\tmtspr\tSPRN_DEC,r3\n\n\t/* Check if HDEC expires soon */\n\tmfspr\tr3, SPRN_HDEC\n\textsw\tr3, r3\n\tcmpdi\tr3, 512\t\t/* 1 microsecond */\n\tblt\thdec_soon\n\n\t/* Clear out and reload the SLB */\n\tli\tr6, 0\n\tslbmte\tr6, r6\n\tPPC_SLBIA(6)\n\tptesync\n\n\t/* Load up guest SLB entries (N.B. slb_max will be 0 for radix) */\n\tlwz\tr5,VCPU_SLB_MAX(r4)\n\tcmpwi\tr5,0\n\tbeq\t9f\n\tmtctr\tr5\n\taddi\tr6,r4,VCPU_SLB\n1:\tld\tr8,VCPU_SLB_E(r6)\n\tld\tr9,VCPU_SLB_V(r6)\n\tslbmte\tr9,r8\n\taddi\tr6,r6,VCPU_SLB_SIZE\n\tbdnz\t1b\n9:\n\ndeliver_guest_interrupt:\t/* r4 = vcpu, r13 = paca */\n\t/* Check if we can deliver an external or decrementer interrupt now */\n\tld\tr0, VCPU_PENDING_EXC(r4)\n\tcmpdi\tr0, 0\n\tbeq\t71f\n\tmr\tr3, r4\n\tbl\tCFUNC(kvmppc_guest_entry_inject_int)\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n71:\n\tld\tr6, VCPU_SRR0(r4)\n\tld\tr7, VCPU_SRR1(r4)\n\tmtspr\tSPRN_SRR0, r6\n\tmtspr\tSPRN_SRR1, r7\n\n\tld\tr10, VCPU_PC(r4)\n\tld\tr11, VCPU_MSR(r4)\n\t/* r11 = vcpu->arch.msr & ~MSR_HV */\n\trldicl\tr11, r11, 63 - MSR_HV_LG, 1\n\trotldi\tr11, r11, 1 + MSR_HV_LG\n\tori\tr11, r11, MSR_ME\n\n\tld\tr6, VCPU_CTR(r4)\n\tld\tr7, VCPU_XER(r4)\n\tmtctr\tr6\n\tmtxer\tr7\n\n/*\n * Required state:\n * R4 = vcpu\n * R10: value for HSRR0\n * R11: value for HSRR1\n * R13 = PACA\n */\nfast_guest_return:\n\tli\tr0,0\n\tstb\tr0,VCPU_CEDED(r4)\t/* cancel cede */\n\tmtspr\tSPRN_HSRR0,r10\n\tmtspr\tSPRN_HSRR1,r11\n\n\t/* Activate guest mode, so faults get handled by KVM */\n\tli\tr9, KVM_GUEST_MODE_GUEST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\t/* Accumulate timing */\n\taddi\tr3, r4, VCPU_TB_GUEST\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\t/* Enter guest */\n\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_CFAR(r4)\n\tmtspr\tSPRN_CFAR, r5\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr0, VCPU_PPR(r4)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\tld\tr5, VCPU_LR(r4)\n\tmtlr\tr5\n\n\tld\tr1, VCPU_GPR(R1)(r4)\n\tld\tr5, VCPU_GPR(R5)(r4)\n\tld\tr8, VCPU_GPR(R8)(r4)\n\tld\tr9, VCPU_GPR(R9)(r4)\n\tld\tr10, VCPU_GPR(R10)(r4)\n\tld\tr11, VCPU_GPR(R11)(r4)\n\tld\tr12, VCPU_GPR(R12)(r4)\n\tld\tr13, VCPU_GPR(R13)(r4)\n\nBEGIN_FTR_SECTION\n\tmtspr\tSPRN_PPR, r0\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\tld\tr6, VCPU_GPR(R6)(r4)\n\tld\tr7, VCPU_GPR(R7)(r4)\n\n\tld\tr0, VCPU_CR(r4)\n\tmtcr\tr0\n\n\tld\tr0, VCPU_GPR(R0)(r4)\n\tld\tr2, VCPU_GPR(R2)(r4)\n\tld\tr3, VCPU_GPR(R3)(r4)\n\tld\tr4, VCPU_GPR(R4)(r4)\n\tHRFI_TO_GUEST\n\tb\t.\nSYM_CODE_END(kvmppc_hv_entry)\n\nsecondary_too_late:\n\tli\tr12, 0\n\tstw\tr12, STACK_SLOT_TRAP(r1)\n\tcmpdi\tr4, 0\n\tbeq\t11f\n\tstw\tr12, VCPU_TRAP(r4)\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n11:\tb\tkvmhv_switch_to_host\n\nno_switch_exit:\n\tHMT_MEDIUM\n\tli\tr12, 0\n\tb\t12f\nhdec_soon:\n\tli\tr12, BOOK3S_INTERRUPT_HV_DECREMENTER\n12:\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMEXIT\n\tbl\tkvmhv_accumulate_time\n#endif\n\tb\tguest_bypass\n\n/******************************************************************************\n *                                                                            *\n *                               Exit code                                    *\n *                                                                            *\n *****************************************************************************/\n\n/*\n * We come here from the first-level interrupt handlers.\n */\n\t.globl\tkvmppc_interrupt_hv\nkvmppc_interrupt_hv:\n\t/*\n\t * Register contents:\n\t * R9\t\t= HSTATE_IN_GUEST\n\t * R12\t\t= (guest CR << 32) | interrupt vector\n\t * R13\t\t= PACA\n\t * guest R12 saved in shadow VCPU SCRATCH0\n\t * guest R13 saved in SPRN_SCRATCH0\n\t * guest R9 saved in HSTATE_SCRATCH2\n\t */\n\t/* We're now back in the host but in guest MMU context */\n\tcmpwi\tr9,KVM_GUEST_MODE_HOST_HV\n\tbeq\tkvmppc_bad_host_intr\n\tli\tr9, KVM_GUEST_MODE_HOST_HV\n\tstb\tr9, HSTATE_IN_GUEST(r13)\n\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Save registers */\n\n\tstd\tr0, VCPU_GPR(R0)(r9)\n\tstd\tr1, VCPU_GPR(R1)(r9)\n\tstd\tr2, VCPU_GPR(R2)(r9)\n\tstd\tr3, VCPU_GPR(R3)(r9)\n\tstd\tr4, VCPU_GPR(R4)(r9)\n\tstd\tr5, VCPU_GPR(R5)(r9)\n\tstd\tr6, VCPU_GPR(R6)(r9)\n\tstd\tr7, VCPU_GPR(R7)(r9)\n\tstd\tr8, VCPU_GPR(R8)(r9)\n\tld\tr0, HSTATE_SCRATCH2(r13)\n\tstd\tr0, VCPU_GPR(R9)(r9)\n\tstd\tr10, VCPU_GPR(R10)(r9)\n\tstd\tr11, VCPU_GPR(R11)(r9)\n\tld\tr3, HSTATE_SCRATCH0(r13)\n\tstd\tr3, VCPU_GPR(R12)(r9)\n\t/* CR is in the high half of r12 */\n\tsrdi\tr4, r12, 32\n\tstd\tr4, VCPU_CR(r9)\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_CFAR(r13)\n\tstd\tr3, VCPU_CFAR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_CFAR)\nBEGIN_FTR_SECTION\n\tld\tr4, HSTATE_PPR(r13)\n\tstd\tr4, VCPU_PPR(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_HAS_PPR)\n\n\t/* Restore R1/R2 so we can handle faults */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tLOAD_PACA_TOC()\n\n\tmfspr\tr10, SPRN_SRR0\n\tmfspr\tr11, SPRN_SRR1\n\tstd\tr10, VCPU_SRR0(r9)\n\tstd\tr11, VCPU_SRR1(r9)\n\t/* trap is in the low half of r12, clear CR from the high half */\n\tclrldi\tr12, r12, 32\n\tandi.\tr0, r12, 2\t\t/* need to read HSRR0/1? */\n\tbeq\t1f\n\tmfspr\tr10, SPRN_HSRR0\n\tmfspr\tr11, SPRN_HSRR1\n\tclrrdi\tr12, r12, 2\n1:\tstd\tr10, VCPU_PC(r9)\n\tstd\tr11, VCPU_MSR(r9)\n\n\tGET_SCRATCH0(r3)\n\tmflr\tr4\n\tstd\tr3, VCPU_GPR(R13)(r9)\n\tstd\tr4, VCPU_LR(r9)\n\n\tstw\tr12,VCPU_TRAP(r9)\n\n\t/*\n\t * Now that we have saved away SRR0/1 and HSRR0/1,\n\t * interrupts are recoverable in principle, so set MSR_RI.\n\t * This becomes important for relocation-on interrupts from\n\t * the guest, which we can get in radix mode on POWER9.\n\t */\n\tli\tr0, MSR_RI\n\tmtmsrd\tr0, 1\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r9, VCPU_TB_RMINTR\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n\tld\tr5, VCPU_GPR(R5)(r9)\n\tld\tr6, VCPU_GPR(R6)(r9)\n\tld\tr7, VCPU_GPR(R7)(r9)\n\tld\tr8, VCPU_GPR(R8)(r9)\n#endif\n\n\t/* Save HEIR (HV emulation assist reg) in emul_inst\n\t   if this is an HEI (HV emulation interrupt, e40) */\n\tli\tr3,KVM_INST_FETCH_FAILED\n\tstd\tr3,VCPU_LAST_INST(r9)\n\tcmpwi\tr12,BOOK3S_INTERRUPT_H_EMUL_ASSIST\n\tbne\t11f\n\tmfspr\tr3,SPRN_HEIR\n11:\tstd\tr3,VCPU_HEIR(r9)\n\n\t/* these are volatile across C function calls */\n\tmfctr\tr3\n\tmfxer\tr4\n\tstd\tr3, VCPU_CTR(r9)\n\tstd\tr4, VCPU_XER(r9)\n\n\t/* Save more register state  */\n\tmfdar\tr3\n\tmfdsisr\tr4\n\tstd\tr3, VCPU_DAR(r9)\n\tstw\tr4, VCPU_DSISR(r9)\n\n\t/* If this is a page table miss then see if it's theirs or ours */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tbeq\tkvmppc_hdsi\n\tstd\tr3, VCPU_FAULT_DAR(r9)\n\tstw\tr4, VCPU_FAULT_DSISR(r9)\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tbeq\tkvmppc_hisi\n\n\t/* See if this is a leftover HDEC interrupt */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_HV_DECREMENTER\n\tbne\t2f\n\tmfspr\tr3,SPRN_HDEC\n\textsw\tr3, r3\n\tcmpdi\tr3,0\n\tmr\tr4,r9\n\tbge\tfast_guest_return\n2:\n\t/* See if this is an hcall we can handle in real mode */\n\tcmpwi\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tbeq\thcall_try_real_mode\n\n\t/* Hypervisor doorbell - exit only if host IPI flag set */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\tbne\t3f\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbeq\tmaybe_reenter_guest\n\tb\tguest_exit_cont\n3:\n\t/* If it's a hypervisor facility unavailable interrupt, save HFSCR */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_H_FAC_UNAVAIL\n\tbne\t14f\n\tmfspr\tr3, SPRN_HFSCR\n\tstd\tr3, VCPU_HFSCR(r9)\n\tb\tguest_exit_cont\n14:\n\t/* External interrupt ? */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tbeq\tkvmppc_guest_external\n\t/* See if it is a machine check */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tbeq\tmachine_check_realmode\n\t/* Or a hypervisor maintenance interrupt */\n\tcmpwi\tr12, BOOK3S_INTERRUPT_HMI\n\tbeq\thmi_realmode\n\nguest_exit_cont:\t\t/* r9 = vcpu, r12 = trap, r13 = paca */\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r9, VCPU_TB_RMEXIT\n\tmr\tr4, r9\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\t/*\n\t * Possibly flush the link stack here, before we do a blr in\n\t * kvmhv_switch_to_host.\n\t */\n1:\tnop\n\tpatch_site 1b patch__call_kvm_flush_link_stack\n\n\t/* For hash guest, read the guest SLB and save it away */\n\tli\tr5, 0\n\tlwz\tr0,VCPU_SLB_NR(r9)\t/* number of entries in SLB */\n\tmtctr\tr0\n\tli\tr6,0\n\taddi\tr7,r9,VCPU_SLB\n1:\tslbmfee\tr8,r6\n\tandis.\tr0,r8,SLB_ESID_V@h\n\tbeq\t2f\n\tadd\tr8,r8,r6\t\t/* put index in */\n\tslbmfev\tr3,r6\n\tstd\tr8,VCPU_SLB_E(r7)\n\tstd\tr3,VCPU_SLB_V(r7)\n\taddi\tr7,r7,VCPU_SLB_SIZE\n\taddi\tr5,r5,1\n2:\taddi\tr6,r6,1\n\tbdnz\t1b\n\t/* Finally clear out the SLB */\n\tli\tr0,0\n\tslbmte\tr0,r0\n\tPPC_SLBIA(6)\n\tptesync\n\tstw\tr5,VCPU_SLB_MAX(r9)\n\n\t/* load host SLB entries */\n\tld\tr8,PACA_SLBSHADOWPTR(r13)\n\n\t.rept\tSLB_NUM_BOLTED\n\tli\tr3, SLBSHADOW_SAVEAREA\n\tLDX_BE\tr5, r8, r3\n\taddi\tr3, r3, 8\n\tLDX_BE\tr6, r8, r3\n\tandis.\tr7,r5,SLB_ESID_V@h\n\tbeq\t1f\n\tslbmte\tr6,r5\n1:\taddi\tr8,r8,16\n\t.endr\n\nguest_bypass:\n\tstw\tr12, STACK_SLOT_TRAP(r1)\n\n\t/* Save DEC */\n\t/* Do this before kvmhv_commence_exit so we know TB is guest TB */\n\tld\tr3, HSTATE_KVM_VCORE(r13)\n\tmfspr\tr5,SPRN_DEC\n\tmftb\tr6\n\textsw\tr5,r5\n16:\tadd\tr5,r5,r6\n\tstd\tr5,VCPU_DEC_EXPIRES(r9)\n\n\t/* Increment exit count, poke other threads to exit */\n\tmr \tr3, r12\n\tbl\tkvmhv_commence_exit\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Stop others sending VCPU interrupts to this physical CPU */\n\tli\tr0, -1\n\tstw\tr0, VCPU_CPU(r9)\n\tstw\tr0, VCPU_THREAD_CPU(r9)\n\n\t/* Save guest CTRL register, set runlatch to 1 if it was clear */\n\tmfspr\tr6,SPRN_CTRLF\n\tstw\tr6,VCPU_CTRL(r9)\n\tandi.\tr0,r6,1\n\tbne\t4f\n\tli\tr6,1\n\tmtspr\tSPRN_CTRLT,r6\n4:\n\t/*\n\t * Save the guest PURR/SPURR\n\t */\n\tmfspr\tr5,SPRN_PURR\n\tmfspr\tr6,SPRN_SPURR\n\tld\tr7,VCPU_PURR(r9)\n\tld\tr8,VCPU_SPURR(r9)\n\tstd\tr5,VCPU_PURR(r9)\n\tstd\tr6,VCPU_SPURR(r9)\n\tsubf\tr5,r7,r5\n\tsubf\tr6,r8,r6\n\n\t/*\n\t * Restore host PURR/SPURR and add guest times\n\t * so that the time in the guest gets accounted.\n\t */\n\tld\tr3,HSTATE_PURR(r13)\n\tld\tr4,HSTATE_SPURR(r13)\n\tadd\tr3,r3,r5\n\tadd\tr4,r4,r6\n\tmtspr\tSPRN_PURR,r3\n\tmtspr\tSPRN_SPURR,r4\n\nBEGIN_FTR_SECTION\n\tb\t8f\nEND_FTR_SECTION_IFCLR(CPU_FTR_ARCH_207S)\n\t/* Save POWER8-specific registers */\n\tmfspr\tr5, SPRN_IAMR\n\tmfspr\tr6, SPRN_PSPB\n\tmfspr\tr7, SPRN_FSCR\n\tstd\tr5, VCPU_IAMR(r9)\n\tstw\tr6, VCPU_PSPB(r9)\n\tstd\tr7, VCPU_FSCR(r9)\n\tmfspr\tr5, SPRN_IC\n\tmfspr\tr7, SPRN_TAR\n\tstd\tr5, VCPU_IC(r9)\n\tstd\tr7, VCPU_TAR(r9)\n\tmfspr\tr8, SPRN_EBBHR\n\tstd\tr8, VCPU_EBBHR(r9)\n\tmfspr\tr5, SPRN_EBBRR\n\tmfspr\tr6, SPRN_BESCR\n\tmfspr\tr7, SPRN_PID\n\tmfspr\tr8, SPRN_WORT\n\tstd\tr5, VCPU_EBBRR(r9)\n\tstd\tr6, VCPU_BESCR(r9)\n\tstw\tr7, VCPU_GUEST_PID(r9)\n\tstd\tr8, VCPU_WORT(r9)\n\tmfspr\tr5, SPRN_TCSCR\n\tmfspr\tr6, SPRN_ACOP\n\tmfspr\tr7, SPRN_CSIGR\n\tmfspr\tr8, SPRN_TACR\n\tstd\tr5, VCPU_TCSCR(r9)\n\tstd\tr6, VCPU_ACOP(r9)\n\tstd\tr7, VCPU_CSIGR(r9)\n\tstd\tr8, VCPU_TACR(r9)\nBEGIN_FTR_SECTION\n\tld\tr5, STACK_SLOT_FSCR(r1)\n\tmtspr\tSPRN_FSCR, r5\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\t/*\n\t * Restore various registers to 0, where non-zero values\n\t * set by the guest could disrupt the host.\n\t */\n\tli\tr0, 0\n\tmtspr\tSPRN_PSPB, r0\n\tmtspr\tSPRN_WORT, r0\n\tmtspr\tSPRN_TCSCR, r0\n\t/* Set MMCRS to 1<<31 to freeze and disable the SPMC counters */\n\tli\tr0, 1\n\tsldi\tr0, r0, 31\n\tmtspr\tSPRN_MMCRS, r0\n\n\t/* Save and restore AMR, IAMR and UAMOR before turning on the MMU */\n\tld\tr8, STACK_SLOT_IAMR(r1)\n\tmtspr\tSPRN_IAMR, r8\n\n8:\t/* Power7 jumps back in here */\n\tmfspr\tr5,SPRN_AMR\n\tmfspr\tr6,SPRN_UAMOR\n\tstd\tr5,VCPU_AMR(r9)\n\tstd\tr6,VCPU_UAMOR(r9)\n\tld\tr5,STACK_SLOT_AMR(r1)\n\tld\tr6,STACK_SLOT_UAMOR(r1)\n\tmtspr\tSPRN_AMR, r5\n\tmtspr\tSPRN_UAMOR, r6\n\n\t/* Switch DSCR back to host value */\n\tmfspr\tr8, SPRN_DSCR\n\tld\tr7, HSTATE_DSCR(r13)\n\tstd\tr8, VCPU_DSCR(r9)\n\tmtspr\tSPRN_DSCR, r7\n\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r9)\n\tstd\tr15, VCPU_GPR(R15)(r9)\n\tstd\tr16, VCPU_GPR(R16)(r9)\n\tstd\tr17, VCPU_GPR(R17)(r9)\n\tstd\tr18, VCPU_GPR(R18)(r9)\n\tstd\tr19, VCPU_GPR(R19)(r9)\n\tstd\tr20, VCPU_GPR(R20)(r9)\n\tstd\tr21, VCPU_GPR(R21)(r9)\n\tstd\tr22, VCPU_GPR(R22)(r9)\n\tstd\tr23, VCPU_GPR(R23)(r9)\n\tstd\tr24, VCPU_GPR(R24)(r9)\n\tstd\tr25, VCPU_GPR(R25)(r9)\n\tstd\tr26, VCPU_GPR(R26)(r9)\n\tstd\tr27, VCPU_GPR(R27)(r9)\n\tstd\tr28, VCPU_GPR(R28)(r9)\n\tstd\tr29, VCPU_GPR(R29)(r9)\n\tstd\tr30, VCPU_GPR(R30)(r9)\n\tstd\tr31, VCPU_GPR(R31)(r9)\n\n\t/* Save SPRGs */\n\tmfspr\tr3, SPRN_SPRG0\n\tmfspr\tr4, SPRN_SPRG1\n\tmfspr\tr5, SPRN_SPRG2\n\tmfspr\tr6, SPRN_SPRG3\n\tstd\tr3, VCPU_SPRG0(r9)\n\tstd\tr4, VCPU_SPRG1(r9)\n\tstd\tr5, VCPU_SPRG2(r9)\n\tstd\tr6, VCPU_SPRG3(r9)\n\n\t/* save FP state */\n\tmr\tr3, r9\n\tbl\tkvmppc_save_fp\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tmr      r3, r9\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_save_tm_hv\n\tnop\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n91:\n#endif\n\n\t/* Increment yield count if they have a VPA */\n\tld\tr8, VCPU_VPA(r9)\t/* do they have a VPA? */\n\tcmpdi\tr8, 0\n\tbeq\t25f\n\tli\tr4, LPPACA_YIELDCOUNT\n\tLWZX_BE\tr3, r8, r4\n\taddi\tr3, r3, 1\n\tSTWX_BE\tr3, r8, r4\n\tli\tr3, 1\n\tstb\tr3, VCPU_VPA_DIRTY(r9)\n25:\n\t/* Save PMU registers if requested */\n\t/* r8 and cr0.eq are live here */\n\tmr\tr3, r9\n\tli\tr4, 1\n\tbeq\t21f\t\t\t/* if no VPA, save PMU stuff anyway */\n\tlbz\tr4, LPPACA_PMCINUSE(r8)\n21:\tbl\tkvmhv_save_guest_pmu\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\t/* Restore host values of some registers */\nBEGIN_FTR_SECTION\n\tld\tr5, STACK_SLOT_CIABR(r1)\n\tld\tr6, STACK_SLOT_DAWR0(r1)\n\tld\tr7, STACK_SLOT_DAWRX0(r1)\n\tmtspr\tSPRN_CIABR, r5\n\t/*\n\t * If the DAWR doesn't work, it's ok to write these here as\n\t * this value should always be zero\n\t*/\n\tmtspr\tSPRN_DAWR0, r6\n\tmtspr\tSPRN_DAWRX0, r7\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/*\n\t * POWER7/POWER8 guest -> host partition switch code.\n\t * We don't have to lock against tlbies but we do\n\t * have to coordinate the hardware threads.\n\t * Here STACK_SLOT_TRAP(r1) contains the trap number.\n\t */\nkvmhv_switch_to_host:\n\t/* Secondary threads wait for primary to do partition switch */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\tlbz\tr3,HSTATE_PTID(r13)\n\tcmpwi\tr3,0\n\tbeq\t15f\n\tHMT_LOW\n13:\tlbz\tr3,VCORE_IN_GUEST(r5)\n\tcmpwi\tr3,0\n\tbne\t13b\n\tHMT_MEDIUM\n\tb\t16f\n\n\t/* Primary thread waits for all the secondaries to exit guest */\n15:\tlwz\tr3,VCORE_ENTRY_EXIT(r5)\n\trlwinm\tr0,r3,32-8,0xff\n\tclrldi\tr3,r3,56\n\tcmpw\tr3,r0\n\tbne\t15b\n\tisync\n\n\t/* Did we actually switch to the guest at all? */\n\tlbz\tr6, VCORE_IN_GUEST(r5)\n\tcmpwi\tr6, 0\n\tbeq\t19f\n\n\t/* Primary thread switches back to host partition */\n\tlwz\tr7,KVM_HOST_LPID(r4)\n\tld\tr6,KVM_HOST_SDR1(r4)\n\tli\tr8,LPID_RSVD\t\t/* switch to reserved LPID */\n\tmtspr\tSPRN_LPID,r8\n\tptesync\n\tmtspr\tSPRN_SDR1,r6\t\t/* switch to host page table */\n\tmtspr\tSPRN_LPID,r7\n\tisync\n\nBEGIN_FTR_SECTION\n\t/* DPDES and VTB are shared between threads */\n\tmfspr\tr7, SPRN_DPDES\n\tmfspr\tr8, SPRN_VTB\n\tstd\tr7, VCORE_DPDES(r5)\n\tstd\tr8, VCORE_VTB(r5)\n\t/* clear DPDES so we don't get guest doorbells in the host */\n\tli\tr8, 0\n\tmtspr\tSPRN_DPDES, r8\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\n\t/* Subtract timebase offset from timebase */\n\tld\tr8, VCORE_TB_OFFSET_APPL(r5)\n\tcmpdi\tr8,0\n\tbeq\t17f\n\tli\tr0, 0\n\tstd\tr0, VCORE_TB_OFFSET_APPL(r5)\n\tmftb\tr6\t\t\t/* current guest timebase */\n\tsubf\tr8,r8,r6\n\tmtspr\tSPRN_TBU40,r8\t\t/* update upper 40 bits */\n\tmftb\tr7\t\t\t/* check if lower 24 bits overflowed */\n\tclrldi\tr6,r6,40\n\tclrldi\tr7,r7,40\n\tcmpld\tr7,r6\n\tbge\t17f\n\taddis\tr8,r8,0x100\t\t/* if so, increment upper 40 bits */\n\tmtspr\tSPRN_TBU40,r8\n\n17:\n\t/*\n\t * If this is an HMI, we called kvmppc_realmode_hmi_handler\n\t * above, which may or may not have already called\n\t * kvmppc_subcore_exit_guest.  Fortunately, all that\n\t * kvmppc_subcore_exit_guest does is clear a flag, so calling\n\t * it again here is benign even if kvmppc_realmode_hmi_handler\n\t * has already called it.\n\t */\n\tbl\tkvmppc_subcore_exit_guest\n\tnop\n30:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tld\tr4,VCORE_KVM(r5)\t/* pointer to struct kvm */\n\n\t/* Reset PCR */\n\tld\tr0, VCORE_PCR(r5)\n\tLOAD_REG_IMMEDIATE(r6, PCR_MASK)\n\tcmpld\tr0, r6\n\tbeq\t18f\n\tmtspr\tSPRN_PCR, r6\n18:\n\t/* Signal secondary CPUs to continue */\n\tli\tr0, 0\n\tstb\tr0,VCORE_IN_GUEST(r5)\n19:\tlis\tr8,0x7fff\t\t/* MAX_INT@h */\n\tmtspr\tSPRN_HDEC,r8\n\n16:\tld\tr8,KVM_HOST_LPCR(r4)\n\tmtspr\tSPRN_LPCR,r8\n\tisync\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\t/* Finish timing, if we have a vcpu */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tcmpdi\tr4, 0\n\tli\tr3, 0\n\tbeq\t2f\n\tbl\tkvmhv_accumulate_time\n2:\n#endif\n\t/* Unset guest mode */\n\tli\tr0, KVM_GUEST_MODE_NONE\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\tlwz\tr12, STACK_SLOT_TRAP(r1)\t/* return trap # in r12 */\n\tld\tr0, SFS+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, SFS\n\tmtlr\tr0\n\tblr\n\n.balign 32\n.global kvm_flush_link_stack\nkvm_flush_link_stack:\n\t/* Save LR into r0 */\n\tmflr\tr0\n\n\t/* Flush the link stack. On Power8 it's up to 32 entries in size. */\n\t.rept 32\n\tANNOTATE_INTRA_FUNCTION_CALL\n\tbl\t.+4\n\t.endr\n\n\t/* And on Power9 it's up to 64. */\nBEGIN_FTR_SECTION\n\t.rept 32\n\tANNOTATE_INTRA_FUNCTION_CALL\n\tbl\t.+4\n\t.endr\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_300)\n\n\t/* Restore LR */\n\tmtlr\tr0\n\tblr\n\nkvmppc_guest_external:\n\t/* External interrupt, first check for host_ipi. If this is\n\t * set, we know the host wants us out so let's do it now\n\t */\n\tbl\tCFUNC(kvmppc_read_intr)\n\n\t/*\n\t * Restore the active volatile registers after returning from\n\t * a C function.\n\t */\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\n\t/*\n\t * kvmppc_read_intr return codes:\n\t *\n\t * Exit to host (r3 > 0)\n\t *   1 An interrupt is pending that needs to be handled by the host\n\t *     Exit guest and return to host by branching to guest_exit_cont\n\t *\n\t *   2 Passthrough that needs completion in the host\n\t *     Exit guest and return to host by branching to guest_exit_cont\n\t *     However, we also set r12 to BOOK3S_INTERRUPT_HV_RM_HARD\n\t *     to indicate to the host to complete handling the interrupt\n\t *\n\t * Before returning to guest, we check if any CPU is heading out\n\t * to the host and if so, we head out also. If no CPUs are heading\n\t * check return values <= 0.\n\t *\n\t * Return to guest (r3 <= 0)\n\t *  0 No external interrupt is pending\n\t * -1 A guest wakeup IPI (which has now been cleared)\n\t *    In either case, we return to guest to deliver any pending\n\t *    guest interrupts.\n\t *\n\t * -2 A PCI passthrough external interrupt was handled\n\t *    (interrupt was delivered directly to guest)\n\t *    Return to guest to deliver any pending guest interrupts.\n\t */\n\n\tcmpdi\tr3, 1\n\tble\t1f\n\n\t/* Return code = 2 */\n\tli\tr12, BOOK3S_INTERRUPT_HV_RM_HARD\n\tstw\tr12, VCPU_TRAP(r9)\n\tb\tguest_exit_cont\n\n1:\t/* Return code <= 1 */\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\n\t/* Return code <= 0 */\nmaybe_reenter_guest:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tlwz\tr0, VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr0, 0x100\n\tmr\tr4, r9\n\tblt\tdeliver_guest_interrupt\n\tb\tguest_exit_cont\n\n/*\n * Check whether an HDSI is an HPTE not found fault or something else.\n * If it is an HPTE not found fault that is due to the guest accessing\n * a page that they have mapped but which we have paged out, then\n * we continue on with the guest exit path.  In all other cases,\n * reflect the HDSI to the guest as a DSI.\n */\nkvmppc_hdsi:\n\tmfspr\tr4, SPRN_HDAR\n\tmfspr\tr6, SPRN_HDSISR\n\t/* HPTE not found fault or protection fault? */\n\tandis.\tr0, r6, (DSISR_NOHPTE | DSISR_PROTFAULT)@h\n\tbeq\t1f\t\t\t/* if not, send it to the guest */\n\tandi.\tr0, r11, MSR_DR\t\t/* data relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r4, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_DATA_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\tstd\tr4, VCPU_FAULT_DAR(r9)\n\tstw\tr6, VCPU_FAULT_DSISR(r9)\n\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tli\tr7, 1\t\t\t/* data fault */\n\tbl\tCFUNC(kvmppc_hpte_hv_fault)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_DATA_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\t6f\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\tcmpdi\tr3, -2\t\t\t/* MMIO emulation; need instr word */\n\tbeq\t2f\n\n\t/* Synthesize a DSI (or DSegI) for the guest */\n\tld\tr4, VCPU_FAULT_DAR(r9)\n\tmr\tr6, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_DATA_STORAGE\n\tmtspr\tSPRN_DSISR, r6\n7:\tmtspr\tSPRN_DAR, r4\n\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\nfast_interrupt_c_return:\n6:\tld\tr7, VCPU_CTR(r9)\n\tld\tr8, VCPU_XER(r9)\n\tmtctr\tr7\n\tmtxer\tr8\n\tmr\tr4, r9\n\tb\tfast_guest_return\n\n3:\tld\tr5, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r5)\n\tb\t4b\n\n\t/* If this is for emulated MMIO, load the instruction word */\n2:\tli\tr8, KVM_INST_FETCH_FAILED\t/* In case lwz faults */\n\n\t/* Set guest mode to 'jump over instruction' so if lwz faults\n\t * we'll just continue at the next IP. */\n\tli\tr0, KVM_GUEST_MODE_SKIP\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\n\t/* Do the access with MSR:DR enabled */\n\tmfmsr\tr3\n\tori\tr4, r3, MSR_DR\t\t/* Enable paging for data */\n\tmtmsrd\tr4\n\tlwz\tr8, 0(r10)\n\tmtmsrd\tr3\n\n\t/* Store the result */\n\tstd\tr8, VCPU_LAST_INST(r9)\n\n\t/* Unset guest mode. */\n\tli\tr0, KVM_GUEST_MODE_HOST_HV\n\tstb\tr0, HSTATE_IN_GUEST(r13)\n\tb\tguest_exit_cont\n\n/*\n * Similarly for an HISI, reflect it to the guest as an ISI unless\n * it is an HPTE not found fault for a page that we have paged out.\n */\nkvmppc_hisi:\n\tandis.\tr0, r11, SRR1_ISI_NOPT@h\n\tbeq\t1f\n\tandi.\tr0, r11, MSR_IR\t\t/* instruction relocation enabled? */\n\tbeq\t3f\n\tclrrdi\tr0, r10, 28\n\tPPC_SLBFEE_DOT(R5, R0)\t\t/* if so, look up SLB */\n\tli\tr0, BOOK3S_INTERRUPT_INST_SEGMENT\n\tbne\t7f\t\t\t/* if no SLB entry found */\n4:\n\t/* Search the hash table. */\n\tmr\tr3, r9\t\t\t/* vcpu pointer */\n\tmr\tr4, r10\n\tmr\tr6, r11\n\tli\tr7, 0\t\t\t/* instruction fault */\n\tbl\tCFUNC(kvmppc_hpte_hv_fault)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tld\tr10, VCPU_PC(r9)\n\tld\tr11, VCPU_MSR(r9)\n\tli\tr12, BOOK3S_INTERRUPT_H_INST_STORAGE\n\tcmpdi\tr3, 0\t\t\t/* retry the instruction */\n\tbeq\tfast_interrupt_c_return\n\tcmpdi\tr3, -1\t\t\t/* handle in kernel mode */\n\tbeq\tguest_exit_cont\n\n\t/* Synthesize an ISI (or ISegI) for the guest */\n\tmr\tr11, r3\n1:\tli\tr0, BOOK3S_INTERRUPT_INST_STORAGE\n7:\tmtspr\tSPRN_SRR0, r10\n\tmtspr\tSPRN_SRR1, r11\n\tmr\tr10, r0\n\tbl\tkvmppc_msr_interrupt\n\tb\tfast_interrupt_c_return\n\n3:\tld\tr6, VCPU_KVM(r9)\t/* not relocated, use VRMA */\n\tld\tr5, KVM_VRMA_SLB_V(r6)\n\tb\t4b\n\n/*\n * Try to handle an hcall in real mode.\n * Returns to the guest if we handle it, or continues on up to\n * the kernel if we can't (i.e. if we don't have a handler for\n * it, or if the handler returns H_TOO_HARD).\n *\n * r5 - r8 contain hcall args,\n * r9 = vcpu, r10 = pc, r11 = msr, r12 = trap, r13 = paca\n */\nhcall_try_real_mode:\n\tld\tr3,VCPU_GPR(R3)(r9)\n\tandi.\tr0,r11,MSR_PR\n\t/* sc 1 from userspace - reflect to guest syscall */\n\tbne\tsc_1_fast_return\n\tclrrdi\tr3,r3,2\n\tcmpldi\tr3,hcall_real_table_end - hcall_real_table\n\tbge\tguest_exit_cont\n\t/* See if this hcall is enabled for in-kernel handling */\n\tld\tr4, VCPU_KVM(r9)\n\tsrdi\tr0, r3, 8\t/* r0 = (r3 / 4) >> 6 */\n\tsldi\tr0, r0, 3\t/* index into kvm->arch.enabled_hcalls[] */\n\tadd\tr4, r4, r0\n\tld\tr0, KVM_ENABLED_HCALLS(r4)\n\trlwinm\tr4, r3, 32-2, 0x3f\t/* r4 = (r3 / 4) & 0x3f */\n\tsrd\tr0, r0, r4\n\tandi.\tr0, r0, 1\n\tbeq\tguest_exit_cont\n\t/* Get pointer to handler, if any, and call it */\n\tLOAD_REG_ADDR(r4, hcall_real_table)\n\tlwax\tr3,r3,r4\n\tcmpwi\tr3,0\n\tbeq\tguest_exit_cont\n\tadd\tr12,r3,r4\n\tmtctr\tr12\n\tmr\tr3,r9\t\t/* get vcpu pointer */\n\tld\tr4,VCPU_GPR(R4)(r9)\n\tbctrl\n\tcmpdi\tr3,H_TOO_HARD\n\tbeq\thcall_real_fallback\n\tld\tr4,HSTATE_KVM_VCPU(r13)\n\tstd\tr3,VCPU_GPR(R3)(r4)\n\tld\tr10,VCPU_PC(r4)\n\tld\tr11,VCPU_MSR(r4)\n\tb\tfast_guest_return\n\nsc_1_fast_return:\n\tmtspr\tSPRN_SRR0,r10\n\tmtspr\tSPRN_SRR1,r11\n\tli\tr10, BOOK3S_INTERRUPT_SYSCALL\n\tbl\tkvmppc_msr_interrupt\n\tmr\tr4,r9\n\tb\tfast_guest_return\n\n\t/* We've attempted a real mode hcall, but it's punted it back\n\t * to userspace.  We need to restore some clobbered volatiles\n\t * before resuming the pass-it-to-qemu path */\nhcall_real_fallback:\n\tli\tr12,BOOK3S_INTERRUPT_SYSCALL\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\n\tb\tguest_exit_cont\n\n\t.globl\thcall_real_table\nhcall_real_table:\n\t.long\t0\t\t/* 0 - unused */\n\t.long\tDOTSYM(kvmppc_h_remove) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_enter) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_read) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_mod) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_clear_ref) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_h_protect) - hcall_real_table\n\t.long\t0\t\t/* 0x1c */\n\t.long\t0\t\t/* 0x20 */\n\t.long\t0\t\t/* 0x24 - H_SET_SPRG0 */\n\t.long\tDOTSYM(kvmppc_h_set_dabr) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_page_init) - hcall_real_table\n\t.long\t0\t\t/* 0x30 */\n\t.long\t0\t\t/* 0x34 */\n\t.long\t0\t\t/* 0x38 */\n\t.long\t0\t\t/* 0x3c */\n\t.long\t0\t\t/* 0x40 */\n\t.long\t0\t\t/* 0x44 */\n\t.long\t0\t\t/* 0x48 */\n\t.long\t0\t\t/* 0x4c */\n\t.long\t0\t\t/* 0x50 */\n\t.long\t0\t\t/* 0x54 */\n\t.long\t0\t\t/* 0x58 */\n\t.long\t0\t\t/* 0x5c */\n\t.long\t0\t\t/* 0x60 */\n#ifdef CONFIG_KVM_XICS\n\t.long\tDOTSYM(xics_rm_h_eoi) - hcall_real_table\n\t.long\tDOTSYM(xics_rm_h_cppr) - hcall_real_table\n\t.long\tDOTSYM(xics_rm_h_ipi) - hcall_real_table\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\tDOTSYM(xics_rm_h_xirr) - hcall_real_table\n#else\n\t.long\t0\t\t/* 0x64 - H_EOI */\n\t.long\t0\t\t/* 0x68 - H_CPPR */\n\t.long\t0\t\t/* 0x6c - H_IPI */\n\t.long\t0\t\t/* 0x70 - H_IPOLL */\n\t.long\t0\t\t/* 0x74 - H_XIRR */\n#endif\n\t.long\t0\t\t/* 0x78 */\n\t.long\t0\t\t/* 0x7c */\n\t.long\t0\t\t/* 0x80 */\n\t.long\t0\t\t/* 0x84 */\n\t.long\t0\t\t/* 0x88 */\n\t.long\t0\t\t/* 0x8c */\n\t.long\t0\t\t/* 0x90 */\n\t.long\t0\t\t/* 0x94 */\n\t.long\t0\t\t/* 0x98 */\n\t.long\t0\t\t/* 0x9c */\n\t.long\t0\t\t/* 0xa0 */\n\t.long\t0\t\t/* 0xa4 */\n\t.long\t0\t\t/* 0xa8 */\n\t.long\t0\t\t/* 0xac */\n\t.long\t0\t\t/* 0xb0 */\n\t.long\t0\t\t/* 0xb4 */\n\t.long\t0\t\t/* 0xb8 */\n\t.long\t0\t\t/* 0xbc */\n\t.long\t0\t\t/* 0xc0 */\n\t.long\t0\t\t/* 0xc4 */\n\t.long\t0\t\t/* 0xc8 */\n\t.long\t0\t\t/* 0xcc */\n\t.long\t0\t\t/* 0xd0 */\n\t.long\t0\t\t/* 0xd4 */\n\t.long\t0\t\t/* 0xd8 */\n\t.long\t0\t\t/* 0xdc */\n\t.long\tDOTSYM(kvmppc_h_cede) - hcall_real_table\n\t.long\tDOTSYM(kvmppc_rm_h_confer) - hcall_real_table\n\t.long\t0\t\t/* 0xe8 */\n\t.long\t0\t\t/* 0xec */\n\t.long\t0\t\t/* 0xf0 */\n\t.long\t0\t\t/* 0xf4 */\n\t.long\t0\t\t/* 0xf8 */\n\t.long\t0\t\t/* 0xfc */\n\t.long\t0\t\t/* 0x100 */\n\t.long\t0\t\t/* 0x104 */\n\t.long\t0\t\t/* 0x108 */\n\t.long\t0\t\t/* 0x10c */\n\t.long\t0\t\t/* 0x110 */\n\t.long\t0\t\t/* 0x114 */\n\t.long\t0\t\t/* 0x118 */\n\t.long\t0\t\t/* 0x11c */\n\t.long\t0\t\t/* 0x120 */\n\t.long\tDOTSYM(kvmppc_h_bulk_remove) - hcall_real_table\n\t.long\t0\t\t/* 0x128 */\n\t.long\t0\t\t/* 0x12c */\n\t.long\t0\t\t/* 0x130 */\n\t.long\tDOTSYM(kvmppc_h_set_xdabr) - hcall_real_table\n\t.long\t0\t\t/* 0x138 */\n\t.long\t0\t\t/* 0x13c */\n\t.long\t0\t\t/* 0x140 */\n\t.long\t0\t\t/* 0x144 */\n\t.long\t0\t\t/* 0x148 */\n\t.long\t0\t\t/* 0x14c */\n\t.long\t0\t\t/* 0x150 */\n\t.long\t0\t\t/* 0x154 */\n\t.long\t0\t\t/* 0x158 */\n\t.long\t0\t\t/* 0x15c */\n\t.long\t0\t\t/* 0x160 */\n\t.long\t0\t\t/* 0x164 */\n\t.long\t0\t\t/* 0x168 */\n\t.long\t0\t\t/* 0x16c */\n\t.long\t0\t\t/* 0x170 */\n\t.long\t0\t\t/* 0x174 */\n\t.long\t0\t\t/* 0x178 */\n\t.long\t0\t\t/* 0x17c */\n\t.long\t0\t\t/* 0x180 */\n\t.long\t0\t\t/* 0x184 */\n\t.long\t0\t\t/* 0x188 */\n\t.long\t0\t\t/* 0x18c */\n\t.long\t0\t\t/* 0x190 */\n\t.long\t0\t\t/* 0x194 */\n\t.long\t0\t\t/* 0x198 */\n\t.long\t0\t\t/* 0x19c */\n\t.long\t0\t\t/* 0x1a0 */\n\t.long\t0\t\t/* 0x1a4 */\n\t.long\t0\t\t/* 0x1a8 */\n\t.long\t0\t\t/* 0x1ac */\n\t.long\t0\t\t/* 0x1b0 */\n\t.long\t0\t\t/* 0x1b4 */\n\t.long\t0\t\t/* 0x1b8 */\n\t.long\t0\t\t/* 0x1bc */\n\t.long\t0\t\t/* 0x1c0 */\n\t.long\t0\t\t/* 0x1c4 */\n\t.long\t0\t\t/* 0x1c8 */\n\t.long\t0\t\t/* 0x1cc */\n\t.long\t0\t\t/* 0x1d0 */\n\t.long\t0\t\t/* 0x1d4 */\n\t.long\t0\t\t/* 0x1d8 */\n\t.long\t0\t\t/* 0x1dc */\n\t.long\t0\t\t/* 0x1e0 */\n\t.long\t0\t\t/* 0x1e4 */\n\t.long\t0\t\t/* 0x1e8 */\n\t.long\t0\t\t/* 0x1ec */\n\t.long\t0\t\t/* 0x1f0 */\n\t.long\t0\t\t/* 0x1f4 */\n\t.long\t0\t\t/* 0x1f8 */\n\t.long\t0\t\t/* 0x1fc */\n\t.long\t0\t\t/* 0x200 */\n\t.long\t0\t\t/* 0x204 */\n\t.long\t0\t\t/* 0x208 */\n\t.long\t0\t\t/* 0x20c */\n\t.long\t0\t\t/* 0x210 */\n\t.long\t0\t\t/* 0x214 */\n\t.long\t0\t\t/* 0x218 */\n\t.long\t0\t\t/* 0x21c */\n\t.long\t0\t\t/* 0x220 */\n\t.long\t0\t\t/* 0x224 */\n\t.long\t0\t\t/* 0x228 */\n\t.long\t0\t\t/* 0x22c */\n\t.long\t0\t\t/* 0x230 */\n\t.long\t0\t\t/* 0x234 */\n\t.long\t0\t\t/* 0x238 */\n\t.long\t0\t\t/* 0x23c */\n\t.long\t0\t\t/* 0x240 */\n\t.long\t0\t\t/* 0x244 */\n\t.long\t0\t\t/* 0x248 */\n\t.long\t0\t\t/* 0x24c */\n\t.long\t0\t\t/* 0x250 */\n\t.long\t0\t\t/* 0x254 */\n\t.long\t0\t\t/* 0x258 */\n\t.long\t0\t\t/* 0x25c */\n\t.long\t0\t\t/* 0x260 */\n\t.long\t0\t\t/* 0x264 */\n\t.long\t0\t\t/* 0x268 */\n\t.long\t0\t\t/* 0x26c */\n\t.long\t0\t\t/* 0x270 */\n\t.long\t0\t\t/* 0x274 */\n\t.long\t0\t\t/* 0x278 */\n\t.long\t0\t\t/* 0x27c */\n\t.long\t0\t\t/* 0x280 */\n\t.long\t0\t\t/* 0x284 */\n\t.long\t0\t\t/* 0x288 */\n\t.long\t0\t\t/* 0x28c */\n\t.long\t0\t\t/* 0x290 */\n\t.long\t0\t\t/* 0x294 */\n\t.long\t0\t\t/* 0x298 */\n\t.long\t0\t\t/* 0x29c */\n\t.long\t0\t\t/* 0x2a0 */\n\t.long\t0\t\t/* 0x2a4 */\n\t.long\t0\t\t/* 0x2a8 */\n\t.long\t0\t\t/* 0x2ac */\n\t.long\t0\t\t/* 0x2b0 */\n\t.long\t0\t\t/* 0x2b4 */\n\t.long\t0\t\t/* 0x2b8 */\n\t.long\t0\t\t/* 0x2bc */\n\t.long\t0\t\t/* 0x2c0 */\n\t.long\t0\t\t/* 0x2c4 */\n\t.long\t0\t\t/* 0x2c8 */\n\t.long\t0\t\t/* 0x2cc */\n\t.long\t0\t\t/* 0x2d0 */\n\t.long\t0\t\t/* 0x2d4 */\n\t.long\t0\t\t/* 0x2d8 */\n\t.long\t0\t\t/* 0x2dc */\n\t.long\t0\t\t/* 0x2e0 */\n\t.long\t0\t\t/* 0x2e4 */\n\t.long\t0\t\t/* 0x2e8 */\n\t.long\t0\t\t/* 0x2ec */\n\t.long\t0\t\t/* 0x2f0 */\n\t.long\t0\t\t/* 0x2f4 */\n\t.long\t0\t\t/* 0x2f8 */\n#ifdef CONFIG_KVM_XICS\n\t.long\tDOTSYM(xics_rm_h_xirr_x) - hcall_real_table\n#else\n\t.long\t0\t\t/* 0x2fc - H_XIRR_X*/\n#endif\n\t.long\tDOTSYM(kvmppc_rm_h_random) - hcall_real_table\n\t.globl\thcall_real_table_end\nhcall_real_table_end:\n\n_GLOBAL_TOC(kvmppc_h_set_xdabr)\nEXPORT_SYMBOL_GPL(kvmppc_h_set_xdabr)\n\tandi.\tr0, r5, DABRX_USER | DABRX_KERNEL\n\tbeq\t6f\n\tli\tr0, DABRX_USER | DABRX_KERNEL | DABRX_BTI\n\tandc.\tr0, r5, r0\n\tbeq\t3f\n6:\tli\tr3, H_PARAMETER\n\tblr\n\n_GLOBAL_TOC(kvmppc_h_set_dabr)\nEXPORT_SYMBOL_GPL(kvmppc_h_set_dabr)\n\tli\tr5, DABRX_USER | DABRX_KERNEL\n3:\nBEGIN_FTR_SECTION\n\tb\t2f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr4,VCPU_DABR(r3)\n\tstw\tr5, VCPU_DABRX(r3)\n\tmtspr\tSPRN_DABRX, r5\n\t/* Work around P7 bug where DABR can get corrupted on mtspr */\n1:\tmtspr\tSPRN_DABR,r4\n\tmfspr\tr5, SPRN_DABR\n\tcmpd\tr4, r5\n\tbne\t1b\n\tisync\n\tli\tr3,0\n\tblr\n\n2:\n\tLOAD_REG_ADDR(r11, dawr_force_enable)\n\tlbz\tr11, 0(r11)\n\tcmpdi\tr11, 0\n\tbne\t3f\n\tli\tr3, H_HARDWARE\n\tblr\n3:\n\t/* Emulate H_SET_DABR/X on P8 for the sake of compat mode guests */\n\trlwimi\tr5, r4, 5, DAWRX_DR | DAWRX_DW\n\trlwimi\tr5, r4, 2, DAWRX_WT\n\tclrrdi\tr4, r4, 3\n\tstd\tr4, VCPU_DAWR0(r3)\n\tstd\tr5, VCPU_DAWRX0(r3)\n\t/*\n\t * If came in through the real mode hcall handler then it is necessary\n\t * to write the registers since the return path won't. Otherwise it is\n\t * sufficient to store then in the vcpu struct as they will be loaded\n\t * next time the vcpu is run.\n\t */\n\tmfmsr\tr6\n\tandi.\tr6, r6, MSR_DR\t\t/* in real mode? */\n\tbne\t4f\n\tmtspr\tSPRN_DAWR0, r4\n\tmtspr\tSPRN_DAWRX0, r5\n4:\tli\tr3, 0\n\tblr\n\n_GLOBAL(kvmppc_h_cede)\t\t/* r3 = vcpu pointer, r11 = msr, r13 = paca */\n\tori\tr11,r11,MSR_EE\n\tstd\tr11,VCPU_MSR(r3)\n\tli\tr0,1\n\tstb\tr0,VCPU_CEDED(r3)\n\tsync\t\t\t/* order setting ceded vs. testing prodded */\n\tlbz\tr5,VCPU_PRODDED(r3)\n\tcmpwi\tr5,0\n\tbne\tkvm_cede_prodded\n\tli\tr12,0\t\t/* set trap to 0 to say hcall is handled */\n\tstw\tr12,VCPU_TRAP(r3)\n\tli\tr0,H_SUCCESS\n\tstd\tr0,VCPU_GPR(R3)(r3)\n\n\t/*\n\t * Set our bit in the bitmask of napping threads unless all the\n\t * other threads are already napping, in which case we send this\n\t * up to the host.\n\t */\n\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr6,HSTATE_PTID(r13)\n\tlwz\tr8,VCORE_ENTRY_EXIT(r5)\n\tclrldi\tr8,r8,56\n\tli\tr0,1\n\tsld\tr0,r0,r6\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n31:\tlwarx\tr4,0,r6\n\tor\tr4,r4,r0\n\tcmpw\tr4,r8\n\tbeq\tkvm_cede_exit\n\tstwcx.\tr4,0,r6\n\tbne\t31b\n\t/* order napping_threads update vs testing entry_exit_map */\n\tisync\n\tli\tr0,NAPPING_CEDE\n\tstb\tr0,HSTATE_NAPPING(r13)\n\tlwz\tr7,VCORE_ENTRY_EXIT(r5)\n\tcmpwi\tr7,0x100\n\tbge\t33f\t\t/* another thread already exiting */\n\n/*\n * Although not specifically required by the architecture, POWER7\n * preserves the following registers in nap mode, even if an SMT mode\n * switch occurs: SLB entries, PURR, SPURR, AMOR, UAMOR, AMR, SPRG0-3,\n * DAR, DSISR, DABR, DABRX, DSCR, PMCx, MMCRx, SIAR, SDAR.\n */\n\t/* Save non-volatile GPRs */\n\tstd\tr14, VCPU_GPR(R14)(r3)\n\tstd\tr15, VCPU_GPR(R15)(r3)\n\tstd\tr16, VCPU_GPR(R16)(r3)\n\tstd\tr17, VCPU_GPR(R17)(r3)\n\tstd\tr18, VCPU_GPR(R18)(r3)\n\tstd\tr19, VCPU_GPR(R19)(r3)\n\tstd\tr20, VCPU_GPR(R20)(r3)\n\tstd\tr21, VCPU_GPR(R21)(r3)\n\tstd\tr22, VCPU_GPR(R22)(r3)\n\tstd\tr23, VCPU_GPR(R23)(r3)\n\tstd\tr24, VCPU_GPR(R24)(r3)\n\tstd\tr25, VCPU_GPR(R25)(r3)\n\tstd\tr26, VCPU_GPR(R26)(r3)\n\tstd\tr27, VCPU_GPR(R27)(r3)\n\tstd\tr28, VCPU_GPR(R28)(r3)\n\tstd\tr29, VCPU_GPR(R29)(r3)\n\tstd\tr30, VCPU_GPR(R30)(r3)\n\tstd\tr31, VCPU_GPR(R31)(r3)\n\n\t/* save FP state */\n\tbl\tkvmppc_save_fp\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tld\tr3, HSTATE_KVM_VCPU(r13)\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_save_tm_hv\n\tnop\n91:\n#endif\n\n\t/*\n\t * Set DEC to the smaller of DEC and HDEC, so that we wake\n\t * no later than the end of our timeslice (HDEC interrupts\n\t * don't wake us from nap).\n\t */\n\tmfspr\tr3, SPRN_DEC\n\tmfspr\tr4, SPRN_HDEC\n\tmftb\tr5\n\textsw\tr3, r3\n\textsw\tr4, r4\n\tcmpd\tr3, r4\n\tble\t67f\n\tmtspr\tSPRN_DEC, r4\n67:\n\t/* save expiry time of guest decrementer */\n\tadd\tr3, r3, r5\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\tstd\tr3, VCPU_DEC_EXPIRES(r4)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\taddi\tr3, r4, VCPU_TB_CEDE\n\tbl\tkvmhv_accumulate_time\n#endif\n\n\tlis\tr3, LPCR_PECEDP@h\t/* Do wake on privileged doorbell */\n\n\t/* Go back to host stack */\n\tld\tr1, HSTATE_HOST_R1(r13)\n\n\t/*\n\t * Take a nap until a decrementer or external or doobell interrupt\n\t * occurs, with PECE1 and PECE0 set in LPCR.\n\t * On POWER8, set PECEDH, and if we are ceding, also set PECEDP.\n\t * Also clear the runlatch bit before napping.\n\t */\nkvm_do_nap:\n\tli\tr0,0\n\tmtspr\tSPRN_CTRLT, r0\n\n\tli\tr0,1\n\tstb\tr0,HSTATE_HWTHREAD_REQ(r13)\n\tmfspr\tr5,SPRN_LPCR\n\tori\tr5,r5,LPCR_PECE0 | LPCR_PECE1\nBEGIN_FTR_SECTION\n\tori\tr5, r5, LPCR_PECEDH\n\trlwimi\tr5, r3, 0, LPCR_PECEDP\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\nkvm_nap_sequence:\t\t/* desired LPCR value in r5 */\n\tli\tr3, PNV_THREAD_NAP\n\tmtspr\tSPRN_LPCR,r5\n\tisync\n\n\tbl\tisa206_idle_insn_mayloss\n\n\tli\tr0,1\n\tmtspr\tSPRN_CTRLT, r0\n\n\tmtspr\tSPRN_SRR1, r3\n\n\tli\tr0, 0\n\tstb\tr0, PACA_FTRACE_ENABLED(r13)\n\n\tli\tr0, KVM_HWTHREAD_IN_KVM\n\tstb\tr0, HSTATE_HWTHREAD_STATE(r13)\n\n\tlbz\tr0, HSTATE_NAPPING(r13)\n\tcmpwi\tr0, NAPPING_CEDE\n\tbeq\tkvm_end_cede\n\tcmpwi\tr0, NAPPING_NOVCPU\n\tbeq\tkvm_novcpu_wakeup\n\tcmpwi\tr0, NAPPING_UNSPLIT\n\tbeq\tkvm_unsplit_wakeup\n\ttwi\t31,0,0 /* Nap state must not be zero */\n\n33:\tmr\tr4, r3\n\tli\tr3, 0\n\tli\tr12, 0\n\tb\t34f\n\nkvm_end_cede:\n\t/* Woken by external or decrementer interrupt */\n\n\t/* get vcpu pointer */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n\taddi\tr3, r4, VCPU_TB_RMINTR\n\tbl\tkvmhv_accumulate_time\n#endif\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\nBEGIN_FTR_SECTION\n\tb\t91f\nEND_FTR_SECTION_IFCLR(CPU_FTR_TM)\n\t/*\n\t * NOTE THAT THIS TRASHES ALL NON-VOLATILE REGISTERS (but not CR)\n\t */\n\tmr      r3, r4\n\tld      r4, VCPU_MSR(r3)\n\tli\tr5, 0\t\t\t/* don't preserve non-vol regs */\n\tbl\tkvmppc_restore_tm_hv\n\tnop\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n91:\n#endif\n\n\t/* load up FP state */\n\tbl\tkvmppc_load_fp\n\n\t/* Restore guest decrementer */\n\tld\tr3, VCPU_DEC_EXPIRES(r4)\n\tmftb\tr7\n\tsubf\tr3, r7, r3\n\tmtspr\tSPRN_DEC, r3\n\n\t/* Load NV GPRS */\n\tld\tr14, VCPU_GPR(R14)(r4)\n\tld\tr15, VCPU_GPR(R15)(r4)\n\tld\tr16, VCPU_GPR(R16)(r4)\n\tld\tr17, VCPU_GPR(R17)(r4)\n\tld\tr18, VCPU_GPR(R18)(r4)\n\tld\tr19, VCPU_GPR(R19)(r4)\n\tld\tr20, VCPU_GPR(R20)(r4)\n\tld\tr21, VCPU_GPR(R21)(r4)\n\tld\tr22, VCPU_GPR(R22)(r4)\n\tld\tr23, VCPU_GPR(R23)(r4)\n\tld\tr24, VCPU_GPR(R24)(r4)\n\tld\tr25, VCPU_GPR(R25)(r4)\n\tld\tr26, VCPU_GPR(R26)(r4)\n\tld\tr27, VCPU_GPR(R27)(r4)\n\tld\tr28, VCPU_GPR(R28)(r4)\n\tld\tr29, VCPU_GPR(R29)(r4)\n\tld\tr30, VCPU_GPR(R30)(r4)\n\tld\tr31, VCPU_GPR(R31)(r4)\n\n\t/* Check the wake reason in SRR1 to see why we got here */\n\tbl\tkvmppc_check_wake_reason\n\n\t/*\n\t * Restore volatile registers since we could have called a\n\t * C routine in kvmppc_check_wake_reason\n\t *\tr4 = VCPU\n\t * r3 tells us whether we need to return to host or not\n\t * WARNING: it gets checked further down:\n\t * should not modify r3 until this check is done.\n\t */\n\tld\tr4, HSTATE_KVM_VCPU(r13)\n\n\t/* clear our bit in vcore->napping_threads */\n34:\tld\tr5,HSTATE_KVM_VCORE(r13)\n\tlbz\tr7,HSTATE_PTID(r13)\n\tli\tr0,1\n\tsld\tr0,r0,r7\n\taddi\tr6,r5,VCORE_NAPPING_THREADS\n32:\tlwarx\tr7,0,r6\n\tandc\tr7,r7,r0\n\tstwcx.\tr7,0,r6\n\tbne\t32b\n\tli\tr0,0\n\tstb\tr0,HSTATE_NAPPING(r13)\n\n\t/* See if the wake reason saved in r3 means we need to exit */\n\tstw\tr12, VCPU_TRAP(r4)\n\tmr\tr9, r4\n\tcmpdi\tr3, 0\n\tbgt\tguest_exit_cont\n\tb\tmaybe_reenter_guest\n\n\t/* cede when already previously prodded case */\nkvm_cede_prodded:\n\tli\tr0,0\n\tstb\tr0,VCPU_PRODDED(r3)\n\tsync\t\t\t/* order testing prodded vs. clearing ceded */\n\tstb\tr0,VCPU_CEDED(r3)\n\tli\tr3,H_SUCCESS\n\tblr\n\n\t/* we've ceded but we want to give control to the host */\nkvm_cede_exit:\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tb\tguest_exit_cont\n\n\t/* Try to do machine check recovery in real mode */\nmachine_check_realmode:\n\tmr\tr3, r9\t\t/* get vcpu pointer */\n\tbl\tkvmppc_realmode_machine_check\n\tnop\n\t/* all machine checks go to virtual mode for further handling */\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_MACHINE_CHECK\n\tb\tguest_exit_cont\n\n/*\n * Call C code to handle a HMI in real mode.\n * Only the primary thread does the call, secondary threads are handled\n * by calling hmi_exception_realmode() after kvmppc_hv_entry returns.\n * r9 points to the vcpu on entry\n */\nhmi_realmode:\n\tlbz\tr0, HSTATE_PTID(r13)\n\tcmpwi\tr0, 0\n\tbne\tguest_exit_cont\n\tbl\tCFUNC(kvmppc_realmode_hmi_handler)\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tli\tr12, BOOK3S_INTERRUPT_HMI\n\tb\tguest_exit_cont\n\n/*\n * Check the reason we woke from nap, and take appropriate action.\n * Returns (in r3):\n *\t0 if nothing needs to be done\n *\t1 if something happened that needs to be handled by the host\n *\t-1 if there was a guest wakeup (IPI or msgsnd)\n *\t-2 if we handled a PCI passthrough interrupt (returned by\n *\t\tkvmppc_read_intr only)\n *\n * Also sets r12 to the interrupt vector for any interrupt that needs\n * to be handled now by the host (0x500 for external interrupt), or zero.\n * Modifies all volatile registers (since it may call a C function).\n * This routine calls kvmppc_read_intr, a C function, if an external\n * interrupt is pending.\n */\nSYM_FUNC_START_LOCAL(kvmppc_check_wake_reason)\n\tmfspr\tr6, SPRN_SRR1\nBEGIN_FTR_SECTION\n\trlwinm\tr6, r6, 45-31, 0xf\t/* extract wake reason field (P8) */\nFTR_SECTION_ELSE\n\trlwinm\tr6, r6, 45-31, 0xe\t/* P7 wake reason field is 3 bits */\nALT_FTR_SECTION_END_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 8\t\t\t/* was it an external interrupt? */\n\tbeq\t7f\t\t\t/* if so, see what it was */\n\tli\tr3, 0\n\tli\tr12, 0\n\tcmpwi\tr6, 6\t\t\t/* was it the decrementer? */\n\tbeq\t0f\nBEGIN_FTR_SECTION\n\tcmpwi\tr6, 5\t\t\t/* privileged doorbell? */\n\tbeq\t0f\n\tcmpwi\tr6, 3\t\t\t/* hypervisor doorbell? */\n\tbeq\t3f\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tcmpwi\tr6, 0xa\t\t\t/* Hypervisor maintenance ? */\n\tbeq\t4f\n\tli\tr3, 1\t\t\t/* anything else, return 1 */\n0:\tblr\n\n\t/* hypervisor doorbell */\n3:\tli\tr12, BOOK3S_INTERRUPT_H_DOORBELL\n\n\t/*\n\t * Clear the doorbell as we will invoke the handler\n\t * explicitly in the guest exit path.\n\t */\n\tlis\tr6, (PPC_DBELL_SERVER << (63-36))@h\n\tPPC_MSGCLR(6)\n\t/* see if it's a host IPI */\n\tli\tr3, 1\n\tlbz\tr0, HSTATE_HOST_IPI(r13)\n\tcmpwi\tr0, 0\n\tbnelr\n\t/* if not, return -1 */\n\tli\tr3, -1\n\tblr\n\n\t/* Woken up due to Hypervisor maintenance interrupt */\n4:\tli\tr12, BOOK3S_INTERRUPT_HMI\n\tli\tr3, 1\n\tblr\n\n\t/* external interrupt - create a stack frame so we can call C */\n7:\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -PPC_MIN_STKFRM(r1)\n\tbl\tCFUNC(kvmppc_read_intr)\n\tnop\n\tli\tr12, BOOK3S_INTERRUPT_EXTERNAL\n\tcmpdi\tr3, 1\n\tble\t1f\n\n\t/*\n\t * Return code of 2 means PCI passthrough interrupt, but\n\t * we need to return back to host to complete handling the\n\t * interrupt. Trap reason is expected in r12 by guest\n\t * exit code.\n\t */\n\tli\tr12, BOOK3S_INTERRUPT_HV_RM_HARD\n1:\n\tld\tr0, PPC_MIN_STKFRM+PPC_LR_STKOFF(r1)\n\taddi\tr1, r1, PPC_MIN_STKFRM\n\tmtlr\tr0\n\tblr\nSYM_FUNC_END(kvmppc_check_wake_reason)\n\n/*\n * Save away FP, VMX and VSX registers.\n * r3 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nSYM_FUNC_START_LOCAL(kvmppc_save_fp)\n\tmflr\tr30\n\tmr\tr31,r3\n\tmfmsr\tr5\n\tori\tr8,r5,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r3,VCPU_FPRS\n\tbl\tstore_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tstore_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tmfspr\tr6,SPRN_VRSAVE\n\tstw\tr6,VCPU_VRSAVE(r31)\n\tmtlr\tr30\n\tblr\nSYM_FUNC_END(kvmppc_save_fp)\n\n/*\n * Load up FP, VMX and VSX registers\n * r4 = vcpu pointer\n * N.B. r30 and r31 are volatile across this function,\n * thus it is not callable from C.\n */\nSYM_FUNC_START_LOCAL(kvmppc_load_fp)\n\tmflr\tr30\n\tmr\tr31,r4\n\tmfmsr\tr9\n\tori\tr8,r9,MSR_FP\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VEC@h\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n#ifdef CONFIG_VSX\nBEGIN_FTR_SECTION\n\toris\tr8,r8,MSR_VSX@h\nEND_FTR_SECTION_IFSET(CPU_FTR_VSX)\n#endif\n\tmtmsrd\tr8\n\taddi\tr3,r4,VCPU_FPRS\n\tbl\tload_fp_state\n#ifdef CONFIG_ALTIVEC\nBEGIN_FTR_SECTION\n\taddi\tr3,r31,VCPU_VRS\n\tbl\tload_vr_state\nEND_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)\n#endif\n\tlwz\tr7,VCPU_VRSAVE(r31)\n\tmtspr\tSPRN_VRSAVE,r7\n\tmtlr\tr30\n\tmr\tr4,r31\n\tblr\nSYM_FUNC_END(kvmppc_load_fp)\n\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n/*\n * Save transactional state and TM-related registers.\n * Called with r3 pointing to the vcpu struct and r4 containing\n * the guest MSR value.\n * r5 is non-zero iff non-volatile register state needs to be maintained.\n * If r5 == 0, this can modify all checkpointed registers, but\n * restores r1 and r2 before exit.\n */\n_GLOBAL_TOC(kvmppc_save_tm_hv)\nEXPORT_SYMBOL_GPL(kvmppc_save_tm_hv)\n\t/* See if we need to handle fake suspend mode */\nBEGIN_FTR_SECTION\n\tb\t__kvmppc_save_tm\nEND_FTR_SECTION_IFCLR(CPU_FTR_P9_TM_HV_ASSIST)\n\n\tlbz\tr0, HSTATE_FAKE_SUSPEND(r13) /* Were we fake suspended? */\n\tcmpwi\tr0, 0\n\tbeq\t__kvmppc_save_tm\n\n\t/* The following code handles the fake_suspend = 1 case */\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\tstdu\tr1, -TM_FRAME_SIZE(r1)\n\n\t/* Turn on TM. */\n\tmfmsr\tr8\n\tli\tr0, 1\n\trldimi\tr8, r0, MSR_TM_LG, 63-MSR_TM_LG\n\tmtmsrd\tr8\n\n\trldicl. r8, r8, 64 - MSR_TS_S_LG, 62 /* Did we actually hrfid? */\n\tbeq\t4f\nBEGIN_FTR_SECTION\n\tbl\tpnv_power9_force_smt4_catch\nEND_FTR_SECTION_IFSET(CPU_FTR_P9_TM_XER_SO_BUG)\n\tnop\n\n\t/*\n\t * It's possible that treclaim. may modify registers, if we have lost\n\t * track of fake-suspend state in the guest due to it using rfscv.\n\t * Save and restore registers in case this occurs.\n\t */\n\tmfspr\tr3, SPRN_DSCR\n\tmfspr\tr4, SPRN_XER\n\tmfspr\tr5, SPRN_AMR\n\t/* SPRN_TAR would need to be saved here if the kernel ever used it */\n\tmfcr\tr12\n\tSAVE_NVGPRS(r1)\n\tSAVE_GPR(2, r1)\n\tSAVE_GPR(3, r1)\n\tSAVE_GPR(4, r1)\n\tSAVE_GPR(5, r1)\n\tstw\tr12, 8(r1)\n\tstd\tr1, HSTATE_HOST_R1(r13)\n\n\t/* We have to treclaim here because that's the only way to do S->N */\n\tli\tr3, TM_CAUSE_KVM_RESCHED\n\tTRECLAIM(R3)\n\n\tGET_PACA(r13)\n\tld\tr1, HSTATE_HOST_R1(r13)\n\tREST_GPR(2, r1)\n\tREST_GPR(3, r1)\n\tREST_GPR(4, r1)\n\tREST_GPR(5, r1)\n\tlwz\tr12, 8(r1)\n\tREST_NVGPRS(r1)\n\tmtspr\tSPRN_DSCR, r3\n\tmtspr\tSPRN_XER, r4\n\tmtspr\tSPRN_AMR, r5\n\tmtcr\tr12\n\tHMT_MEDIUM\n\n\t/*\n\t * We were in fake suspend, so we are not going to save the\n\t * register state as the guest checkpointed state (since\n\t * we already have it), therefore we can now use any volatile GPR.\n\t * In fact treclaim in fake suspend state doesn't modify\n\t * any registers.\n\t */\n\nBEGIN_FTR_SECTION\n\tbl\tpnv_power9_force_smt4_release\nEND_FTR_SECTION_IFSET(CPU_FTR_P9_TM_XER_SO_BUG)\n\tnop\n\n4:\n\tmfspr\tr3, SPRN_PSSCR\n\t/* PSSCR_FAKE_SUSPEND is a write-only bit, but clear it anyway */\n\tli\tr0, PSSCR_FAKE_SUSPEND\n\tandc\tr3, r3, r0\n\tmtspr\tSPRN_PSSCR, r3\n\n\t/* Don't save TEXASR, use value from last exit in real suspend state */\n\tld\tr9, HSTATE_KVM_VCPU(r13)\n\tmfspr\tr5, SPRN_TFHAR\n\tmfspr\tr6, SPRN_TFIAR\n\tstd\tr5, VCPU_TFHAR(r9)\n\tstd\tr6, VCPU_TFIAR(r9)\n\n\taddi\tr1, r1, TM_FRAME_SIZE\n\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n\n/*\n * Restore transactional state and TM-related registers.\n * Called with r3 pointing to the vcpu struct\n * and r4 containing the guest MSR value.\n * r5 is non-zero iff non-volatile register state needs to be maintained.\n * This potentially modifies all checkpointed registers.\n * It restores r1 and r2 from the PACA.\n */\n_GLOBAL_TOC(kvmppc_restore_tm_hv)\nEXPORT_SYMBOL_GPL(kvmppc_restore_tm_hv)\n\t/*\n\t * If we are doing TM emulation for the guest on a POWER9 DD2,\n\t * then we don't actually do a trechkpt -- we either set up\n\t * fake-suspend mode, or emulate a TM rollback.\n\t */\nBEGIN_FTR_SECTION\n\tb\t__kvmppc_restore_tm\nEND_FTR_SECTION_IFCLR(CPU_FTR_P9_TM_HV_ASSIST)\n\tmflr\tr0\n\tstd\tr0, PPC_LR_STKOFF(r1)\n\n\tli\tr0, 0\n\tstb\tr0, HSTATE_FAKE_SUSPEND(r13)\n\n\t/* Turn on TM so we can restore TM SPRs */\n\tmfmsr\tr5\n\tli\tr0, 1\n\trldimi\tr5, r0, MSR_TM_LG, 63-MSR_TM_LG\n\tmtmsrd\tr5\n\n\t/*\n\t * The user may change these outside of a transaction, so they must\n\t * always be context switched.\n\t */\n\tld\tr5, VCPU_TFHAR(r3)\n\tld\tr6, VCPU_TFIAR(r3)\n\tld\tr7, VCPU_TEXASR(r3)\n\tmtspr\tSPRN_TFHAR, r5\n\tmtspr\tSPRN_TFIAR, r6\n\tmtspr\tSPRN_TEXASR, r7\n\n\trldicl. r5, r4, 64 - MSR_TS_S_LG, 62\n\tbeqlr\t\t/* TM not active in guest */\n\n\t/* Make sure the failure summary is set */\n\toris\tr7, r7, (TEXASR_FS)@h\n\tmtspr\tSPRN_TEXASR, r7\n\n\tcmpwi\tr5, 1\t\t/* check for suspended state */\n\tbgt\t10f\n\tstb\tr5, HSTATE_FAKE_SUSPEND(r13)\n\tb\t9f\t\t/* and return */\n10:\tstdu\tr1, -PPC_MIN_STKFRM(r1)\n\t/* guest is in transactional state, so simulate rollback */\n\tbl\tkvmhv_emulate_tm_rollback\n\tnop\n\taddi\tr1, r1, PPC_MIN_STKFRM\n9:\tld\tr0, PPC_LR_STKOFF(r1)\n\tmtlr\tr0\n\tblr\n#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */\n\n/*\n * We come here if we get any exception or interrupt while we are\n * executing host real mode code while in guest MMU context.\n * r12 is (CR << 32) | vector\n * r13 points to our PACA\n * r12 is saved in HSTATE_SCRATCH0(r13)\n * r9 is saved in HSTATE_SCRATCH2(r13)\n * r13 is saved in HSPRG1\n * cfar is saved in HSTATE_CFAR(r13)\n * ppr is saved in HSTATE_PPR(r13)\n */\nkvmppc_bad_host_intr:\n\t/*\n\t * Switch to the emergency stack, but start half-way down in\n\t * case we were already on it.\n\t */\n\tmr\tr9, r1\n\tstd\tr1, PACAR1(r13)\n\tld\tr1, PACAEMERGSP(r13)\n\tsubi\tr1, r1, THREAD_SIZE/2 + INT_FRAME_SIZE\n\tstd\tr9, 0(r1)\n\tstd\tr0, GPR0(r1)\n\tstd\tr9, GPR1(r1)\n\tstd\tr2, GPR2(r1)\n\tSAVE_GPRS(3, 8, r1)\n\tsrdi\tr0, r12, 32\n\tclrldi\tr12, r12, 32\n\tstd\tr0, _CCR(r1)\n\tstd\tr12, _TRAP(r1)\n\tandi.\tr0, r12, 2\n\tbeq\t1f\n\tmfspr\tr3, SPRN_HSRR0\n\tmfspr\tr4, SPRN_HSRR1\n\tmfspr\tr5, SPRN_HDAR\n\tmfspr\tr6, SPRN_HDSISR\n\tb\t2f\n1:\tmfspr\tr3, SPRN_SRR0\n\tmfspr\tr4, SPRN_SRR1\n\tmfspr\tr5, SPRN_DAR\n\tmfspr\tr6, SPRN_DSISR\n2:\tstd\tr3, _NIP(r1)\n\tstd\tr4, _MSR(r1)\n\tstd\tr5, _DAR(r1)\n\tstd\tr6, _DSISR(r1)\n\tld\tr9, HSTATE_SCRATCH2(r13)\n\tld\tr12, HSTATE_SCRATCH0(r13)\n\tGET_SCRATCH0(r0)\n\tSAVE_GPRS(9, 12, r1)\n\tstd\tr0, GPR13(r1)\n\tSAVE_NVGPRS(r1)\n\tld\tr5, HSTATE_CFAR(r13)\n\tstd\tr5, ORIG_GPR3(r1)\n\tmflr\tr3\n\tmfctr\tr4\n\tmfxer\tr5\n\tlbz\tr6, PACAIRQSOFTMASK(r13)\n\tstd\tr3, _LINK(r1)\n\tstd\tr4, _CTR(r1)\n\tstd\tr5, _XER(r1)\n\tstd\tr6, SOFTE(r1)\n\tLOAD_PACA_TOC()\n\tLOAD_REG_IMMEDIATE(3, STACK_FRAME_REGS_MARKER)\n\tstd\tr3, STACK_INT_FRAME_MARKER(r1)\n\n\t/*\n\t * XXX On POWER7 and POWER8, we just spin here since we don't\n\t * know what the other threads are doing (and we don't want to\n\t * coordinate with them) - but at least we now have register state\n\t * in memory that we might be able to look at from another CPU.\n\t */\n\tb\t.\n\n/*\n * This mimics the MSR transition on IRQ delivery.  The new guest MSR is taken\n * from VCPU_INTR_MSR and is modified based on the required TM state changes.\n *   r11 has the guest MSR value (in/out)\n *   r9 has a vcpu pointer (in)\n *   r0 is used as a scratch register\n */\nSYM_FUNC_START_LOCAL(kvmppc_msr_interrupt)\n\trldicl\tr0, r11, 64 - MSR_TS_S_LG, 62\n\tcmpwi\tr0, 2 /* Check if we are in transactional state..  */\n\tld\tr11, VCPU_INTR_MSR(r9)\n\tbne\t1f\n\t/* ... if transactional, change to suspended */\n\tli\tr0, 1\n1:\trldimi\tr11, r0, MSR_TS_S_LG, 63 - MSR_TS_T_LG\n\tblr\nSYM_FUNC_END(kvmppc_msr_interrupt)\n\n/*\n * void kvmhv_load_guest_pmu(struct kvm_vcpu *vcpu)\n *\n * Load up guest PMU state.  R3 points to the vcpu struct.\n */\nSYM_FUNC_START_LOCAL(kvmhv_load_guest_pmu)\n\tmr\tr4, r3\n\tmflr\tr0\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tisync\nBEGIN_FTR_SECTION\n\tld\tr3, VCPU_MMCR(r4)\n\tandi.\tr5, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr5, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, VCPU_PMC(r4)\t/* always load up guest PMU registers */\n\tlwz\tr5, VCPU_PMC + 4(r4)\t/* to prevent information leak */\n\tlwz\tr6, VCPU_PMC + 8(r4)\n\tlwz\tr7, VCPU_PMC + 12(r4)\n\tlwz\tr8, VCPU_PMC + 16(r4)\n\tlwz\tr9, VCPU_PMC + 20(r4)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r5\n\tmtspr\tSPRN_PMC3, r6\n\tmtspr\tSPRN_PMC4, r7\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, VCPU_MMCR(r4)\n\tld\tr5, VCPU_MMCR + 8(r4)\n\tld\tr6, VCPU_MMCRA(r4)\n\tld\tr7, VCPU_SIAR(r4)\n\tld\tr8, VCPU_SDAR(r4)\n\tmtspr\tSPRN_MMCR1, r5\n\tmtspr\tSPRN_MMCRA, r6\n\tmtspr\tSPRN_SIAR, r7\n\tmtspr\tSPRN_SDAR, r8\nBEGIN_FTR_SECTION\n\tld\tr5, VCPU_MMCR + 16(r4)\n\tld\tr6, VCPU_SIER(r4)\n\tmtspr\tSPRN_MMCR2, r5\n\tmtspr\tSPRN_SIER, r6\n\tlwz\tr7, VCPU_PMC + 24(r4)\n\tlwz\tr8, VCPU_PMC + 28(r4)\n\tld\tr9, VCPU_MMCRS(r4)\n\tmtspr\tSPRN_SPMC1, r7\n\tmtspr\tSPRN_SPMC2, r8\n\tmtspr\tSPRN_MMCRS, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n\tmtlr\tr0\n\tblr\nSYM_FUNC_END(kvmhv_load_guest_pmu)\n\n/*\n * void kvmhv_load_host_pmu(void)\n *\n * Reload host PMU state saved in the PACA by kvmhv_save_host_pmu.\n */\nSYM_FUNC_START_LOCAL(kvmhv_load_host_pmu)\n\tmflr\tr0\n\tlbz\tr4, PACA_PMCINUSE(r13) /* is the host using the PMU? */\n\tcmpwi\tr4, 0\n\tbeq\t23f\t\t\t/* skip if not */\nBEGIN_FTR_SECTION\n\tld\tr3, HSTATE_MMCR0(r13)\n\tandi.\tr4, r3, MMCR0_PMAO_SYNC | MMCR0_PMAO\n\tcmpwi\tr4, MMCR0_PMAO\n\tbeql\tkvmppc_fix_pmao\nEND_FTR_SECTION_IFSET(CPU_FTR_PMAO_BUG)\n\tlwz\tr3, HSTATE_PMC1(r13)\n\tlwz\tr4, HSTATE_PMC2(r13)\n\tlwz\tr5, HSTATE_PMC3(r13)\n\tlwz\tr6, HSTATE_PMC4(r13)\n\tlwz\tr8, HSTATE_PMC5(r13)\n\tlwz\tr9, HSTATE_PMC6(r13)\n\tmtspr\tSPRN_PMC1, r3\n\tmtspr\tSPRN_PMC2, r4\n\tmtspr\tSPRN_PMC3, r5\n\tmtspr\tSPRN_PMC4, r6\n\tmtspr\tSPRN_PMC5, r8\n\tmtspr\tSPRN_PMC6, r9\n\tld\tr3, HSTATE_MMCR0(r13)\n\tld\tr4, HSTATE_MMCR1(r13)\n\tld\tr5, HSTATE_MMCRA(r13)\n\tld\tr6, HSTATE_SIAR(r13)\n\tld\tr7, HSTATE_SDAR(r13)\n\tmtspr\tSPRN_MMCR1, r4\n\tmtspr\tSPRN_MMCRA, r5\n\tmtspr\tSPRN_SIAR, r6\n\tmtspr\tSPRN_SDAR, r7\nBEGIN_FTR_SECTION\n\tld\tr8, HSTATE_MMCR2(r13)\n\tld\tr9, HSTATE_SIER(r13)\n\tmtspr\tSPRN_MMCR2, r8\n\tmtspr\tSPRN_SIER, r9\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tmtspr\tSPRN_MMCR0, r3\n\tisync\n\tmtlr\tr0\n23:\tblr\nSYM_FUNC_END(kvmhv_load_host_pmu)\n\n/*\n * void kvmhv_save_guest_pmu(struct kvm_vcpu *vcpu, bool pmu_in_use)\n *\n * Save guest PMU state into the vcpu struct.\n * r3 = vcpu, r4 = full save flag (PMU in use flag set in VPA)\n */\nSYM_FUNC_START_LOCAL(kvmhv_save_guest_pmu)\n\tmr\tr9, r3\n\tmr\tr8, r4\nBEGIN_FTR_SECTION\n\t/*\n\t * POWER8 seems to have a hardware bug where setting\n\t * MMCR0[PMAE] along with MMCR0[PMC1CE] and/or MMCR0[PMCjCE]\n\t * when some counters are already negative doesn't seem\n\t * to cause a performance monitor alert (and hence interrupt).\n\t * The effect of this is that when saving the PMU state,\n\t * if there is no PMU alert pending when we read MMCR0\n\t * before freezing the counters, but one becomes pending\n\t * before we read the counters, we lose it.\n\t * To work around this, we need a way to freeze the counters\n\t * before reading MMCR0.  Normally, freezing the counters\n\t * is done by writing MMCR0 (to set MMCR0[FC]) which\n\t * unavoidably writes MMCR0[PMA0] as well.  On POWER8,\n\t * we can also freeze the counters using MMCR2, by writing\n\t * 1s to all the counter freeze condition bits (there are\n\t * 9 bits each for 6 counters).\n\t */\n\tli\tr3, -1\t\t\t/* set all freeze bits */\n\tclrrdi\tr3, r3, 10\n\tmfspr\tr10, SPRN_MMCR2\n\tmtspr\tSPRN_MMCR2, r3\n\tisync\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tli\tr3, 1\n\tsldi\tr3, r3, 31\t\t/* MMCR0_FC (freeze counters) bit */\n\tmfspr\tr4, SPRN_MMCR0\t\t/* save MMCR0 */\n\tmtspr\tSPRN_MMCR0, r3\t\t/* freeze all counters, disable ints */\n\tmfspr\tr6, SPRN_MMCRA\n\t/* Clear MMCRA in order to disable SDAR updates */\n\tli\tr7, 0\n\tmtspr\tSPRN_MMCRA, r7\n\tisync\n\tcmpwi\tr8, 0\t\t\t/* did they ask for PMU stuff to be saved? */\n\tbne\t21f\n\tstd\tr3, VCPU_MMCR(r9)\t/* if not, set saved MMCR0 to FC */\n\tb\t22f\n21:\tmfspr\tr5, SPRN_MMCR1\n\tmfspr\tr7, SPRN_SIAR\n\tmfspr\tr8, SPRN_SDAR\n\tstd\tr4, VCPU_MMCR(r9)\n\tstd\tr5, VCPU_MMCR + 8(r9)\n\tstd\tr6, VCPU_MMCRA(r9)\nBEGIN_FTR_SECTION\n\tstd\tr10, VCPU_MMCR + 16(r9)\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n\tstd\tr7, VCPU_SIAR(r9)\n\tstd\tr8, VCPU_SDAR(r9)\n\tmfspr\tr3, SPRN_PMC1\n\tmfspr\tr4, SPRN_PMC2\n\tmfspr\tr5, SPRN_PMC3\n\tmfspr\tr6, SPRN_PMC4\n\tmfspr\tr7, SPRN_PMC5\n\tmfspr\tr8, SPRN_PMC6\n\tstw\tr3, VCPU_PMC(r9)\n\tstw\tr4, VCPU_PMC + 4(r9)\n\tstw\tr5, VCPU_PMC + 8(r9)\n\tstw\tr6, VCPU_PMC + 12(r9)\n\tstw\tr7, VCPU_PMC + 16(r9)\n\tstw\tr8, VCPU_PMC + 20(r9)\nBEGIN_FTR_SECTION\n\tmfspr\tr5, SPRN_SIER\n\tstd\tr5, VCPU_SIER(r9)\n\tmfspr\tr6, SPRN_SPMC1\n\tmfspr\tr7, SPRN_SPMC2\n\tmfspr\tr8, SPRN_MMCRS\n\tstw\tr6, VCPU_PMC + 24(r9)\n\tstw\tr7, VCPU_PMC + 28(r9)\n\tstd\tr8, VCPU_MMCRS(r9)\n\tlis\tr4, 0x8000\n\tmtspr\tSPRN_MMCRS, r4\nEND_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)\n22:\tblr\nSYM_FUNC_END(kvmhv_save_guest_pmu)\n\n/*\n * This works around a hardware bug on POWER8E processors, where\n * writing a 1 to the MMCR0[PMAO] bit doesn't generate a\n * performance monitor interrupt.  Instead, when we need to have\n * an interrupt pending, we have to arrange for a counter to overflow.\n */\nkvmppc_fix_pmao:\n\tli\tr3, 0\n\tmtspr\tSPRN_MMCR2, r3\n\tlis\tr3, (MMCR0_PMXE | MMCR0_FCECE)@h\n\tori\tr3, r3, MMCR0_PMCjCE | MMCR0_C56RUN\n\tmtspr\tSPRN_MMCR0, r3\n\tlis\tr3, 0x7fff\n\tori\tr3, r3, 0xffff\n\tmtspr\tSPRN_PMC6, r3\n\tisync\n\tblr\n\n#ifdef CONFIG_KVM_BOOK3S_HV_P8_TIMING\n/*\n * Start timing an activity\n * r3 = pointer to time accumulation struct, r4 = vcpu\n */\nkvmhv_start_timing:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr6, VCORE_TB_OFFSET_APPL(r5)\n\tmftb\tr5\n\tsubf\tr5, r6, r5\t/* subtract current timebase offset */\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tstd\tr5, VCPU_ACTIVITY_START(r4)\n\tblr\n\n/*\n * Accumulate time to one activity and start another.\n * r3 = pointer to new time accumulation struct, r4 = vcpu\n */\nkvmhv_accumulate_time:\n\tld\tr5, HSTATE_KVM_VCORE(r13)\n\tld\tr8, VCORE_TB_OFFSET_APPL(r5)\n\tld\tr5, VCPU_CUR_ACTIVITY(r4)\n\tld\tr6, VCPU_ACTIVITY_START(r4)\n\tstd\tr3, VCPU_CUR_ACTIVITY(r4)\n\tmftb\tr7\n\tsubf\tr7, r8, r7\t/* subtract current timebase offset */\n\tstd\tr7, VCPU_ACTIVITY_START(r4)\n\tcmpdi\tr5, 0\n\tbeqlr\n\tsubf\tr3, r6, r7\n\tld\tr8, TAS_SEQCOUNT(r5)\n\tcmpdi\tr8, 0\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tlwsync\n\tld\tr7, TAS_TOTAL(r5)\n\tadd\tr7, r7, r3\n\tstd\tr7, TAS_TOTAL(r5)\n\tld\tr6, TAS_MIN(r5)\n\tld\tr7, TAS_MAX(r5)\n\tbeq\t3f\n\tcmpd\tr3, r6\n\tbge\t1f\n3:\tstd\tr3, TAS_MIN(r5)\n1:\tcmpd\tr3, r7\n\tble\t2f\n\tstd\tr3, TAS_MAX(r5)\n2:\tlwsync\n\taddi\tr8, r8, 1\n\tstd\tr8, TAS_SEQCOUNT(r5)\n\tblr\n#endif\n", "patch": "@@ -2093,6 +2093,13 @@ _GLOBAL(kvmppc_h_cede)\t\t/* r3 = vcpu pointer, r11 = msr, r13 = paca */\n \t/* save FP state */\n \tbl\tkvmppc_save_fp\n \n+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n+BEGIN_FTR_SECTION\n+\tld\tr9, HSTATE_KVM_VCPU(r13)\n+\tbl\tkvmppc_save_tm\n+END_FTR_SECTION_IFSET(CPU_FTR_TM)\n+#endif\n+\n \t/*\n \t * Set DEC to the smaller of DEC and HDEC, so that we wake\n \t * no later than the end of our timeslice (HDEC interrupts\n@@ -2169,6 +2176,12 @@ kvm_end_cede:\n \tbl\tkvmhv_accumulate_time\n #endif\n \n+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\n+BEGIN_FTR_SECTION\n+\tbl\tkvmppc_restore_tm\n+END_FTR_SECTION_IFSET(CPU_FTR_TM)\n+#endif\n+\n \t/* load up FP state */\n \tbl\tkvmppc_load_fp\n ", "file_path": "files/2016_8\\104", "file_language": "S", "file_name": "arch/powerpc/kvm/book3s_hv_rmhandlers.S", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 43, "cve_id": "CVE-2016-6516", "cwe_id": ["CWE-119", "CWE-362"], "cve_language": "C", "cve_description": "Race condition in the ioctl_file_dedupe_range function in fs/ioctl.c in the Linux kernel through 4.7 allows local users to cause a denial of service (heap-based buffer overflow) or possibly gain privileges by changing a certain count value, aka a \"double fetch\" vulnerability.", "cvss": "7.4", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "HIGH", "PR": "NONE", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "10eec60ce79187686e052092e5383c99b4420a20", "commit_message": "vfs: ioctl: prevent double-fetch in dedupe ioctl\n\nThis prevents a double-fetch from user space that can lead to to an\nundersized allocation and heap overflow.\n\nFixes: 54dbc1517237 (\"vfs: hoist the btrfs deduplication ioctl to the vfs\")\nSigned-off-by: Scott Bauer <sbauer@plzdonthack.me>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "commit_date": "2016-07-28T22:23:12Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/10eec60ce79187686e052092e5383c99b4420a20", "html_url": "https://github.com/torvalds/linux/commit/10eec60ce79187686e052092e5383c99b4420a20", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "884316deb4c9fdf9becfa31831a9e40717e3026c", "url_before": "https://api.github.com/repos/torvalds/linux/commits/884316deb4c9fdf9becfa31831a9e40717e3026c", "html_url_before": "https://github.com/torvalds/linux/commit/884316deb4c9fdf9becfa31831a9e40717e3026c"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/10eec60ce79187686e052092e5383c99b4420a20/fs/ioctl.c", "code": "/*\n *  linux/fs/ioctl.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#include <linux/syscalls.h>\n#include <linux/mm.h>\n#include <linux/capability.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/security.h>\n#include <linux/export.h>\n#include <linux/uaccess.h>\n#include <linux/writeback.h>\n#include <linux/buffer_head.h>\n#include <linux/falloc.h>\n#include \"internal.h\"\n\n#include <asm/ioctls.h>\n\n/* So that the fiemap access checks can't overflow on 32 bit machines. */\n#define FIEMAP_MAX_EXTENTS\t(UINT_MAX / sizeof(struct fiemap_extent))\n\n/**\n * vfs_ioctl - call filesystem specific ioctl methods\n * @filp:\topen file to invoke ioctl method on\n * @cmd:\tioctl command to execute\n * @arg:\tcommand-specific argument for ioctl\n *\n * Invokes filesystem specific ->unlocked_ioctl, if one exists; otherwise\n * returns -ENOTTY.\n *\n * Returns 0 on success, -errno on error.\n */\nlong vfs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tint error = -ENOTTY;\n\n\tif (!filp->f_op->unlocked_ioctl)\n\t\tgoto out;\n\n\terror = filp->f_op->unlocked_ioctl(filp, cmd, arg);\n\tif (error == -ENOIOCTLCMD)\n\t\terror = -ENOTTY;\n out:\n\treturn error;\n}\n\nstatic int ioctl_fibmap(struct file *filp, int __user *p)\n{\n\tstruct address_space *mapping = filp->f_mapping;\n\tint res, block;\n\n\t/* do we support this mess? */\n\tif (!mapping->a_ops->bmap)\n\t\treturn -EINVAL;\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\tres = get_user(block, p);\n\tif (res)\n\t\treturn res;\n\tres = mapping->a_ops->bmap(mapping, block);\n\treturn put_user(res, p);\n}\n\n/**\n * fiemap_fill_next_extent - Fiemap helper function\n * @fieinfo:\tFiemap context passed into ->fiemap\n * @logical:\tExtent logical start offset, in bytes\n * @phys:\tExtent physical start offset, in bytes\n * @len:\tExtent length, in bytes\n * @flags:\tFIEMAP_EXTENT flags that describe this extent\n *\n * Called from file system ->fiemap callback. Will populate extent\n * info as passed in via arguments and copy to user memory. On\n * success, extent count on fieinfo is incremented.\n *\n * Returns 0 on success, -errno on error, 1 if this was the last\n * extent that will fit in user array.\n */\n#define SET_UNKNOWN_FLAGS\t(FIEMAP_EXTENT_DELALLOC)\n#define SET_NO_UNMOUNTED_IO_FLAGS\t(FIEMAP_EXTENT_DATA_ENCRYPTED)\n#define SET_NOT_ALIGNED_FLAGS\t(FIEMAP_EXTENT_DATA_TAIL|FIEMAP_EXTENT_DATA_INLINE)\nint fiemap_fill_next_extent(struct fiemap_extent_info *fieinfo, u64 logical,\n\t\t\t    u64 phys, u64 len, u32 flags)\n{\n\tstruct fiemap_extent extent;\n\tstruct fiemap_extent __user *dest = fieinfo->fi_extents_start;\n\n\t/* only count the extents */\n\tif (fieinfo->fi_extents_max == 0) {\n\t\tfieinfo->fi_extents_mapped++;\n\t\treturn (flags & FIEMAP_EXTENT_LAST) ? 1 : 0;\n\t}\n\n\tif (fieinfo->fi_extents_mapped >= fieinfo->fi_extents_max)\n\t\treturn 1;\n\n\tif (flags & SET_UNKNOWN_FLAGS)\n\t\tflags |= FIEMAP_EXTENT_UNKNOWN;\n\tif (flags & SET_NO_UNMOUNTED_IO_FLAGS)\n\t\tflags |= FIEMAP_EXTENT_ENCODED;\n\tif (flags & SET_NOT_ALIGNED_FLAGS)\n\t\tflags |= FIEMAP_EXTENT_NOT_ALIGNED;\n\n\tmemset(&extent, 0, sizeof(extent));\n\textent.fe_logical = logical;\n\textent.fe_physical = phys;\n\textent.fe_length = len;\n\textent.fe_flags = flags;\n\n\tdest += fieinfo->fi_extents_mapped;\n\tif (copy_to_user(dest, &extent, sizeof(extent)))\n\t\treturn -EFAULT;\n\n\tfieinfo->fi_extents_mapped++;\n\tif (fieinfo->fi_extents_mapped == fieinfo->fi_extents_max)\n\t\treturn 1;\n\treturn (flags & FIEMAP_EXTENT_LAST) ? 1 : 0;\n}\nEXPORT_SYMBOL(fiemap_fill_next_extent);\n\n/**\n * fiemap_check_flags - check validity of requested flags for fiemap\n * @fieinfo:\tFiemap context passed into ->fiemap\n * @fs_flags:\tSet of fiemap flags that the file system understands\n *\n * Called from file system ->fiemap callback. This will compute the\n * intersection of valid fiemap flags and those that the fs supports. That\n * value is then compared against the user supplied flags. In case of bad user\n * flags, the invalid values will be written into the fieinfo structure, and\n * -EBADR is returned, which tells ioctl_fiemap() to return those values to\n * userspace. For this reason, a return code of -EBADR should be preserved.\n *\n * Returns 0 on success, -EBADR on bad flags.\n */\nint fiemap_check_flags(struct fiemap_extent_info *fieinfo, u32 fs_flags)\n{\n\tu32 incompat_flags;\n\n\tincompat_flags = fieinfo->fi_flags & ~(FIEMAP_FLAGS_COMPAT & fs_flags);\n\tif (incompat_flags) {\n\t\tfieinfo->fi_flags = incompat_flags;\n\t\treturn -EBADR;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(fiemap_check_flags);\n\nstatic int fiemap_check_ranges(struct super_block *sb,\n\t\t\t       u64 start, u64 len, u64 *new_len)\n{\n\tu64 maxbytes = (u64) sb->s_maxbytes;\n\n\t*new_len = len;\n\n\tif (len == 0)\n\t\treturn -EINVAL;\n\n\tif (start > maxbytes)\n\t\treturn -EFBIG;\n\n\t/*\n\t * Shrink request scope to what the fs can actually handle.\n\t */\n\tif (len > maxbytes || (maxbytes - len) < start)\n\t\t*new_len = maxbytes - start;\n\n\treturn 0;\n}\n\nstatic int ioctl_fiemap(struct file *filp, unsigned long arg)\n{\n\tstruct fiemap fiemap;\n\tstruct fiemap __user *ufiemap = (struct fiemap __user *) arg;\n\tstruct fiemap_extent_info fieinfo = { 0, };\n\tstruct inode *inode = file_inode(filp);\n\tstruct super_block *sb = inode->i_sb;\n\tu64 len;\n\tint error;\n\n\tif (!inode->i_op->fiemap)\n\t\treturn -EOPNOTSUPP;\n\n\tif (copy_from_user(&fiemap, ufiemap, sizeof(fiemap)))\n\t\treturn -EFAULT;\n\n\tif (fiemap.fm_extent_count > FIEMAP_MAX_EXTENTS)\n\t\treturn -EINVAL;\n\n\terror = fiemap_check_ranges(sb, fiemap.fm_start, fiemap.fm_length,\n\t\t\t\t    &len);\n\tif (error)\n\t\treturn error;\n\n\tfieinfo.fi_flags = fiemap.fm_flags;\n\tfieinfo.fi_extents_max = fiemap.fm_extent_count;\n\tfieinfo.fi_extents_start = ufiemap->fm_extents;\n\n\tif (fiemap.fm_extent_count != 0 &&\n\t    !access_ok(VERIFY_WRITE, fieinfo.fi_extents_start,\n\t\t       fieinfo.fi_extents_max * sizeof(struct fiemap_extent)))\n\t\treturn -EFAULT;\n\n\tif (fieinfo.fi_flags & FIEMAP_FLAG_SYNC)\n\t\tfilemap_write_and_wait(inode->i_mapping);\n\n\terror = inode->i_op->fiemap(inode, &fieinfo, fiemap.fm_start, len);\n\tfiemap.fm_flags = fieinfo.fi_flags;\n\tfiemap.fm_mapped_extents = fieinfo.fi_extents_mapped;\n\tif (copy_to_user(ufiemap, &fiemap, sizeof(fiemap)))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\nstatic long ioctl_file_clone(struct file *dst_file, unsigned long srcfd,\n\t\t\t     u64 off, u64 olen, u64 destoff)\n{\n\tstruct fd src_file = fdget(srcfd);\n\tint ret;\n\n\tif (!src_file.file)\n\t\treturn -EBADF;\n\tret = vfs_clone_file_range(src_file.file, off, dst_file, destoff, olen);\n\tfdput(src_file);\n\treturn ret;\n}\n\nstatic long ioctl_file_clone_range(struct file *file, void __user *argp)\n{\n\tstruct file_clone_range args;\n\n\tif (copy_from_user(&args, argp, sizeof(args)))\n\t\treturn -EFAULT;\n\treturn ioctl_file_clone(file, args.src_fd, args.src_offset,\n\t\t\t\targs.src_length, args.dest_offset);\n}\n\n#ifdef CONFIG_BLOCK\n\nstatic inline sector_t logical_to_blk(struct inode *inode, loff_t offset)\n{\n\treturn (offset >> inode->i_blkbits);\n}\n\nstatic inline loff_t blk_to_logical(struct inode *inode, sector_t blk)\n{\n\treturn (blk << inode->i_blkbits);\n}\n\n/**\n * __generic_block_fiemap - FIEMAP for block based inodes (no locking)\n * @inode: the inode to map\n * @fieinfo: the fiemap info struct that will be passed back to userspace\n * @start: where to start mapping in the inode\n * @len: how much space to map\n * @get_block: the fs's get_block function\n *\n * This does FIEMAP for block based inodes.  Basically it will just loop\n * through get_block until we hit the number of extents we want to map, or we\n * go past the end of the file and hit a hole.\n *\n * If it is possible to have data blocks beyond a hole past @inode->i_size, then\n * please do not use this function, it will stop at the first unmapped block\n * beyond i_size.\n *\n * If you use this function directly, you need to do your own locking. Use\n * generic_block_fiemap if you want the locking done for you.\n */\n\nint __generic_block_fiemap(struct inode *inode,\n\t\t\t   struct fiemap_extent_info *fieinfo, loff_t start,\n\t\t\t   loff_t len, get_block_t *get_block)\n{\n\tstruct buffer_head map_bh;\n\tsector_t start_blk, last_blk;\n\tloff_t isize = i_size_read(inode);\n\tu64 logical = 0, phys = 0, size = 0;\n\tu32 flags = FIEMAP_EXTENT_MERGED;\n\tbool past_eof = false, whole_file = false;\n\tint ret = 0;\n\n\tret = fiemap_check_flags(fieinfo, FIEMAP_FLAG_SYNC);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Either the i_mutex or other appropriate locking needs to be held\n\t * since we expect isize to not change at all through the duration of\n\t * this call.\n\t */\n\tif (len >= isize) {\n\t\twhole_file = true;\n\t\tlen = isize;\n\t}\n\n\t/*\n\t * Some filesystems can't deal with being asked to map less than\n\t * blocksize, so make sure our len is at least block length.\n\t */\n\tif (logical_to_blk(inode, len) == 0)\n\t\tlen = blk_to_logical(inode, 1);\n\n\tstart_blk = logical_to_blk(inode, start);\n\tlast_blk = logical_to_blk(inode, start + len - 1);\n\n\tdo {\n\t\t/*\n\t\t * we set b_size to the total size we want so it will map as\n\t\t * many contiguous blocks as possible at once\n\t\t */\n\t\tmemset(&map_bh, 0, sizeof(struct buffer_head));\n\t\tmap_bh.b_size = len;\n\n\t\tret = get_block(inode, start_blk, &map_bh, 0);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\t/* HOLE */\n\t\tif (!buffer_mapped(&map_bh)) {\n\t\t\tstart_blk++;\n\n\t\t\t/*\n\t\t\t * We want to handle the case where there is an\n\t\t\t * allocated block at the front of the file, and then\n\t\t\t * nothing but holes up to the end of the file properly,\n\t\t\t * to make sure that extent at the front gets properly\n\t\t\t * marked with FIEMAP_EXTENT_LAST\n\t\t\t */\n\t\t\tif (!past_eof &&\n\t\t\t    blk_to_logical(inode, start_blk) >= isize)\n\t\t\t\tpast_eof = 1;\n\n\t\t\t/*\n\t\t\t * First hole after going past the EOF, this is our\n\t\t\t * last extent\n\t\t\t */\n\t\t\tif (past_eof && size) {\n\t\t\t\tflags = FIEMAP_EXTENT_MERGED|FIEMAP_EXTENT_LAST;\n\t\t\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\t\t\t\t      phys, size,\n\t\t\t\t\t\t\t      flags);\n\t\t\t} else if (size) {\n\t\t\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\t\t\t\t      phys, size, flags);\n\t\t\t\tsize = 0;\n\t\t\t}\n\n\t\t\t/* if we have holes up to/past EOF then we're done */\n\t\t\tif (start_blk > last_blk || past_eof || ret)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/*\n\t\t\t * We have gone over the length of what we wanted to\n\t\t\t * map, and it wasn't the entire file, so add the extent\n\t\t\t * we got last time and exit.\n\t\t\t *\n\t\t\t * This is for the case where say we want to map all the\n\t\t\t * way up to the second to the last block in a file, but\n\t\t\t * the last block is a hole, making the second to last\n\t\t\t * block FIEMAP_EXTENT_LAST.  In this case we want to\n\t\t\t * see if there is a hole after the second to last block\n\t\t\t * so we can mark it properly.  If we found data after\n\t\t\t * we exceeded the length we were requesting, then we\n\t\t\t * are good to go, just add the extent to the fieinfo\n\t\t\t * and break\n\t\t\t */\n\t\t\tif (start_blk > last_blk && !whole_file) {\n\t\t\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\t\t\t\t      phys, size,\n\t\t\t\t\t\t\t      flags);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * if size != 0 then we know we already have an extent\n\t\t\t * to add, so add it.\n\t\t\t */\n\t\t\tif (size) {\n\t\t\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\t\t\t\t      phys, size,\n\t\t\t\t\t\t\t      flags);\n\t\t\t\tif (ret)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tlogical = blk_to_logical(inode, start_blk);\n\t\t\tphys = blk_to_logical(inode, map_bh.b_blocknr);\n\t\t\tsize = map_bh.b_size;\n\t\t\tflags = FIEMAP_EXTENT_MERGED;\n\n\t\t\tstart_blk += logical_to_blk(inode, size);\n\n\t\t\t/*\n\t\t\t * If we are past the EOF, then we need to make sure as\n\t\t\t * soon as we find a hole that the last extent we found\n\t\t\t * is marked with FIEMAP_EXTENT_LAST\n\t\t\t */\n\t\t\tif (!past_eof && logical + size >= isize)\n\t\t\t\tpast_eof = true;\n\t\t}\n\t\tcond_resched();\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t} while (1);\n\n\t/* If ret is 1 then we just hit the end of the extent array */\n\tif (ret == 1)\n\t\tret = 0;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__generic_block_fiemap);\n\n/**\n * generic_block_fiemap - FIEMAP for block based inodes\n * @inode: The inode to map\n * @fieinfo: The mapping information\n * @start: The initial block to map\n * @len: The length of the extect to attempt to map\n * @get_block: The block mapping function for the fs\n *\n * Calls __generic_block_fiemap to map the inode, after taking\n * the inode's mutex lock.\n */\n\nint generic_block_fiemap(struct inode *inode,\n\t\t\t struct fiemap_extent_info *fieinfo, u64 start,\n\t\t\t u64 len, get_block_t *get_block)\n{\n\tint ret;\n\tinode_lock(inode);\n\tret = __generic_block_fiemap(inode, fieinfo, start, len, get_block);\n\tinode_unlock(inode);\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_block_fiemap);\n\n#endif  /*  CONFIG_BLOCK  */\n\n/*\n * This provides compatibility with legacy XFS pre-allocation ioctls\n * which predate the fallocate syscall.\n *\n * Only the l_start, l_len and l_whence fields of the 'struct space_resv'\n * are used here, rest are ignored.\n */\nint ioctl_preallocate(struct file *filp, void __user *argp)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct space_resv sr;\n\n\tif (copy_from_user(&sr, argp, sizeof(sr)))\n\t\treturn -EFAULT;\n\n\tswitch (sr.l_whence) {\n\tcase SEEK_SET:\n\t\tbreak;\n\tcase SEEK_CUR:\n\t\tsr.l_start += filp->f_pos;\n\t\tbreak;\n\tcase SEEK_END:\n\t\tsr.l_start += i_size_read(inode);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn vfs_fallocate(filp, FALLOC_FL_KEEP_SIZE, sr.l_start, sr.l_len);\n}\n\nstatic int file_ioctl(struct file *filp, unsigned int cmd,\n\t\tunsigned long arg)\n{\n\tstruct inode *inode = file_inode(filp);\n\tint __user *p = (int __user *)arg;\n\n\tswitch (cmd) {\n\tcase FIBMAP:\n\t\treturn ioctl_fibmap(filp, p);\n\tcase FIONREAD:\n\t\treturn put_user(i_size_read(inode) - filp->f_pos, p);\n\tcase FS_IOC_RESVSP:\n\tcase FS_IOC_RESVSP64:\n\t\treturn ioctl_preallocate(filp, p);\n\t}\n\n\treturn vfs_ioctl(filp, cmd, arg);\n}\n\nstatic int ioctl_fionbio(struct file *filp, int __user *argp)\n{\n\tunsigned int flag;\n\tint on, error;\n\n\terror = get_user(on, argp);\n\tif (error)\n\t\treturn error;\n\tflag = O_NONBLOCK;\n#ifdef __sparc__\n\t/* SunOS compatibility item. */\n\tif (O_NONBLOCK != O_NDELAY)\n\t\tflag |= O_NDELAY;\n#endif\n\tspin_lock(&filp->f_lock);\n\tif (on)\n\t\tfilp->f_flags |= flag;\n\telse\n\t\tfilp->f_flags &= ~flag;\n\tspin_unlock(&filp->f_lock);\n\treturn error;\n}\n\nstatic int ioctl_fioasync(unsigned int fd, struct file *filp,\n\t\t\t  int __user *argp)\n{\n\tunsigned int flag;\n\tint on, error;\n\n\terror = get_user(on, argp);\n\tif (error)\n\t\treturn error;\n\tflag = on ? FASYNC : 0;\n\n\t/* Did FASYNC state change ? */\n\tif ((flag ^ filp->f_flags) & FASYNC) {\n\t\tif (filp->f_op->fasync)\n\t\t\t/* fasync() adjusts filp->f_flags */\n\t\t\terror = filp->f_op->fasync(fd, filp, on);\n\t\telse\n\t\t\terror = -ENOTTY;\n\t}\n\treturn error < 0 ? error : 0;\n}\n\nstatic int ioctl_fsfreeze(struct file *filp)\n{\n\tstruct super_block *sb = file_inode(filp)->i_sb;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* If filesystem doesn't support freeze feature, return. */\n\tif (sb->s_op->freeze_fs == NULL && sb->s_op->freeze_super == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Freeze */\n\tif (sb->s_op->freeze_super)\n\t\treturn sb->s_op->freeze_super(sb);\n\treturn freeze_super(sb);\n}\n\nstatic int ioctl_fsthaw(struct file *filp)\n{\n\tstruct super_block *sb = file_inode(filp)->i_sb;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* Thaw */\n\tif (sb->s_op->thaw_super)\n\t\treturn sb->s_op->thaw_super(sb);\n\treturn thaw_super(sb);\n}\n\nstatic long ioctl_file_dedupe_range(struct file *file, void __user *arg)\n{\n\tstruct file_dedupe_range __user *argp = arg;\n\tstruct file_dedupe_range *same = NULL;\n\tint ret;\n\tunsigned long size;\n\tu16 count;\n\n\tif (get_user(count, &argp->dest_count)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tsize = offsetof(struct file_dedupe_range __user, info[count]);\n\n\tsame = memdup_user(argp, size);\n\tif (IS_ERR(same)) {\n\t\tret = PTR_ERR(same);\n\t\tsame = NULL;\n\t\tgoto out;\n\t}\n\n\tsame->dest_count = count;\n\tret = vfs_dedupe_file_range(file, same);\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(argp, same, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(same);\n\treturn ret;\n}\n\n/*\n * When you add any new common ioctls to the switches above and below\n * please update compat_sys_ioctl() too.\n *\n * do_vfs_ioctl() is not for drivers and not intended to be EXPORT_SYMBOL()'d.\n * It's just a simple helper for sys_ioctl and compat_sys_ioctl.\n */\nint do_vfs_ioctl(struct file *filp, unsigned int fd, unsigned int cmd,\n\t     unsigned long arg)\n{\n\tint error = 0;\n\tint __user *argp = (int __user *)arg;\n\tstruct inode *inode = file_inode(filp);\n\n\tswitch (cmd) {\n\tcase FIOCLEX:\n\t\tset_close_on_exec(fd, 1);\n\t\tbreak;\n\n\tcase FIONCLEX:\n\t\tset_close_on_exec(fd, 0);\n\t\tbreak;\n\n\tcase FIONBIO:\n\t\terror = ioctl_fionbio(filp, argp);\n\t\tbreak;\n\n\tcase FIOASYNC:\n\t\terror = ioctl_fioasync(fd, filp, argp);\n\t\tbreak;\n\n\tcase FIOQSIZE:\n\t\tif (S_ISDIR(inode->i_mode) || S_ISREG(inode->i_mode) ||\n\t\t    S_ISLNK(inode->i_mode)) {\n\t\t\tloff_t res = inode_get_bytes(inode);\n\t\t\terror = copy_to_user(argp, &res, sizeof(res)) ?\n\t\t\t\t\t-EFAULT : 0;\n\t\t} else\n\t\t\terror = -ENOTTY;\n\t\tbreak;\n\n\tcase FIFREEZE:\n\t\terror = ioctl_fsfreeze(filp);\n\t\tbreak;\n\n\tcase FITHAW:\n\t\terror = ioctl_fsthaw(filp);\n\t\tbreak;\n\n\tcase FS_IOC_FIEMAP:\n\t\treturn ioctl_fiemap(filp, arg);\n\n\tcase FIGETBSZ:\n\t\treturn put_user(inode->i_sb->s_blocksize, argp);\n\n\tcase FICLONE:\n\t\treturn ioctl_file_clone(filp, arg, 0, 0, 0);\n\n\tcase FICLONERANGE:\n\t\treturn ioctl_file_clone_range(filp, argp);\n\n\tcase FIDEDUPERANGE:\n\t\treturn ioctl_file_dedupe_range(filp, argp);\n\n\tdefault:\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\terror = file_ioctl(filp, cmd, arg);\n\t\telse\n\t\t\terror = vfs_ioctl(filp, cmd, arg);\n\t\tbreak;\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)\n{\n\tint error;\n\tstruct fd f = fdget(fd);\n\n\tif (!f.file)\n\t\treturn -EBADF;\n\terror = security_file_ioctl(f.file, cmd, arg);\n\tif (!error)\n\t\terror = do_vfs_ioctl(f.file, fd, cmd, arg);\n\tfdput(f);\n\treturn error;\n}\n", "code_before": "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/ioctl.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#include <linux/syscalls.h>\n#include <linux/mm.h>\n#include <linux/capability.h>\n#include <linux/compat.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/security.h>\n#include <linux/export.h>\n#include <linux/uaccess.h>\n#include <linux/writeback.h>\n#include <linux/buffer_head.h>\n#include <linux/falloc.h>\n#include <linux/sched/signal.h>\n#include <linux/fiemap.h>\n#include <linux/mount.h>\n#include <linux/fscrypt.h>\n#include <linux/fileattr.h>\n\n#include \"internal.h\"\n\n#include <asm/ioctls.h>\n\n/* So that the fiemap access checks can't overflow on 32 bit machines. */\n#define FIEMAP_MAX_EXTENTS\t(UINT_MAX / sizeof(struct fiemap_extent))\n\n/**\n * vfs_ioctl - call filesystem specific ioctl methods\n * @filp:\topen file to invoke ioctl method on\n * @cmd:\tioctl command to execute\n * @arg:\tcommand-specific argument for ioctl\n *\n * Invokes filesystem specific ->unlocked_ioctl, if one exists; otherwise\n * returns -ENOTTY.\n *\n * Returns 0 on success, -errno on error.\n */\nint vfs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tint error = -ENOTTY;\n\n\tif (!filp->f_op->unlocked_ioctl)\n\t\tgoto out;\n\n\terror = filp->f_op->unlocked_ioctl(filp, cmd, arg);\n\tif (error == -ENOIOCTLCMD)\n\t\terror = -ENOTTY;\n out:\n\treturn error;\n}\nEXPORT_SYMBOL(vfs_ioctl);\n\nstatic int ioctl_fibmap(struct file *filp, int __user *p)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct super_block *sb = inode->i_sb;\n\tint error, ur_block;\n\tsector_t block;\n\n\tif (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\terror = get_user(ur_block, p);\n\tif (error)\n\t\treturn error;\n\n\tif (ur_block < 0)\n\t\treturn -EINVAL;\n\n\tblock = ur_block;\n\terror = bmap(inode, &block);\n\n\tif (block > INT_MAX) {\n\t\terror = -ERANGE;\n\t\tpr_warn_ratelimited(\"[%s/%d] FS: %s File: %pD4 would truncate fibmap result\\n\",\n\t\t\t\t    current->comm, task_pid_nr(current),\n\t\t\t\t    sb->s_id, filp);\n\t}\n\n\tif (error)\n\t\tur_block = 0;\n\telse\n\t\tur_block = block;\n\n\tif (put_user(ur_block, p))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\n/**\n * fiemap_fill_next_extent - Fiemap helper function\n * @fieinfo:\tFiemap context passed into ->fiemap\n * @logical:\tExtent logical start offset, in bytes\n * @phys:\tExtent physical start offset, in bytes\n * @len:\tExtent length, in bytes\n * @flags:\tFIEMAP_EXTENT flags that describe this extent\n *\n * Called from file system ->fiemap callback. Will populate extent\n * info as passed in via arguments and copy to user memory. On\n * success, extent count on fieinfo is incremented.\n *\n * Returns 0 on success, -errno on error, 1 if this was the last\n * extent that will fit in user array.\n */\nint fiemap_fill_next_extent(struct fiemap_extent_info *fieinfo, u64 logical,\n\t\t\t    u64 phys, u64 len, u32 flags)\n{\n\tstruct fiemap_extent extent;\n\tstruct fiemap_extent __user *dest = fieinfo->fi_extents_start;\n\n\t/* only count the extents */\n\tif (fieinfo->fi_extents_max == 0) {\n\t\tfieinfo->fi_extents_mapped++;\n\t\treturn (flags & FIEMAP_EXTENT_LAST) ? 1 : 0;\n\t}\n\n\tif (fieinfo->fi_extents_mapped >= fieinfo->fi_extents_max)\n\t\treturn 1;\n\n#define SET_UNKNOWN_FLAGS\t(FIEMAP_EXTENT_DELALLOC)\n#define SET_NO_UNMOUNTED_IO_FLAGS\t(FIEMAP_EXTENT_DATA_ENCRYPTED)\n#define SET_NOT_ALIGNED_FLAGS\t(FIEMAP_EXTENT_DATA_TAIL|FIEMAP_EXTENT_DATA_INLINE)\n\n\tif (flags & SET_UNKNOWN_FLAGS)\n\t\tflags |= FIEMAP_EXTENT_UNKNOWN;\n\tif (flags & SET_NO_UNMOUNTED_IO_FLAGS)\n\t\tflags |= FIEMAP_EXTENT_ENCODED;\n\tif (flags & SET_NOT_ALIGNED_FLAGS)\n\t\tflags |= FIEMAP_EXTENT_NOT_ALIGNED;\n\n\tmemset(&extent, 0, sizeof(extent));\n\textent.fe_logical = logical;\n\textent.fe_physical = phys;\n\textent.fe_length = len;\n\textent.fe_flags = flags;\n\n\tdest += fieinfo->fi_extents_mapped;\n\tif (copy_to_user(dest, &extent, sizeof(extent)))\n\t\treturn -EFAULT;\n\n\tfieinfo->fi_extents_mapped++;\n\tif (fieinfo->fi_extents_mapped == fieinfo->fi_extents_max)\n\t\treturn 1;\n\treturn (flags & FIEMAP_EXTENT_LAST) ? 1 : 0;\n}\nEXPORT_SYMBOL(fiemap_fill_next_extent);\n\n/**\n * fiemap_prep - check validity of requested flags for fiemap\n * @inode:\tInode to operate on\n * @fieinfo:\tFiemap context passed into ->fiemap\n * @start:\tStart of the mapped range\n * @len:\tLength of the mapped range, can be truncated by this function.\n * @supported_flags:\tSet of fiemap flags that the file system understands\n *\n * This function must be called from each ->fiemap instance to validate the\n * fiemap request against the file system parameters.\n *\n * Returns 0 on success, or a negative error on failure.\n */\nint fiemap_prep(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\tu64 start, u64 *len, u32 supported_flags)\n{\n\tu64 maxbytes = inode->i_sb->s_maxbytes;\n\tu32 incompat_flags;\n\tint ret = 0;\n\n\tif (*len == 0)\n\t\treturn -EINVAL;\n\tif (start >= maxbytes)\n\t\treturn -EFBIG;\n\n\t/*\n\t * Shrink request scope to what the fs can actually handle.\n\t */\n\tif (*len > maxbytes || (maxbytes - *len) < start)\n\t\t*len = maxbytes - start;\n\n\tsupported_flags |= FIEMAP_FLAG_SYNC;\n\tsupported_flags &= FIEMAP_FLAGS_COMPAT;\n\tincompat_flags = fieinfo->fi_flags & ~supported_flags;\n\tif (incompat_flags) {\n\t\tfieinfo->fi_flags = incompat_flags;\n\t\treturn -EBADR;\n\t}\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_SYNC)\n\t\tret = filemap_write_and_wait(inode->i_mapping);\n\treturn ret;\n}\nEXPORT_SYMBOL(fiemap_prep);\n\nstatic int ioctl_fiemap(struct file *filp, struct fiemap __user *ufiemap)\n{\n\tstruct fiemap fiemap;\n\tstruct fiemap_extent_info fieinfo = { 0, };\n\tstruct inode *inode = file_inode(filp);\n\tint error;\n\n\tif (!inode->i_op->fiemap)\n\t\treturn -EOPNOTSUPP;\n\n\tif (copy_from_user(&fiemap, ufiemap, sizeof(fiemap)))\n\t\treturn -EFAULT;\n\n\tif (fiemap.fm_extent_count > FIEMAP_MAX_EXTENTS)\n\t\treturn -EINVAL;\n\n\tfieinfo.fi_flags = fiemap.fm_flags;\n\tfieinfo.fi_extents_max = fiemap.fm_extent_count;\n\tfieinfo.fi_extents_start = ufiemap->fm_extents;\n\n\terror = inode->i_op->fiemap(inode, &fieinfo, fiemap.fm_start,\n\t\t\tfiemap.fm_length);\n\n\tfiemap.fm_flags = fieinfo.fi_flags;\n\tfiemap.fm_mapped_extents = fieinfo.fi_extents_mapped;\n\tif (copy_to_user(ufiemap, &fiemap, sizeof(fiemap)))\n\t\terror = -EFAULT;\n\n\treturn error;\n}\n\nstatic int ioctl_file_clone(struct file *dst_file, unsigned long srcfd,\n\t\t\t    u64 off, u64 olen, u64 destoff)\n{\n\tCLASS(fd, src_file)(srcfd);\n\tloff_t cloned;\n\tint ret;\n\n\tif (fd_empty(src_file))\n\t\treturn -EBADF;\n\tcloned = vfs_clone_file_range(fd_file(src_file), off, dst_file, destoff,\n\t\t\t\t      olen, 0);\n\tif (cloned < 0)\n\t\tret = cloned;\n\telse if (olen && cloned != olen)\n\t\tret = -EINVAL;\n\telse\n\t\tret = 0;\n\treturn ret;\n}\n\nstatic int ioctl_file_clone_range(struct file *file,\n\t\t\t\t  struct file_clone_range __user *argp)\n{\n\tstruct file_clone_range args;\n\n\tif (copy_from_user(&args, argp, sizeof(args)))\n\t\treturn -EFAULT;\n\treturn ioctl_file_clone(file, args.src_fd, args.src_offset,\n\t\t\t\targs.src_length, args.dest_offset);\n}\n\n/*\n * This provides compatibility with legacy XFS pre-allocation ioctls\n * which predate the fallocate syscall.\n *\n * Only the l_start, l_len and l_whence fields of the 'struct space_resv'\n * are used here, rest are ignored.\n */\nstatic int ioctl_preallocate(struct file *filp, int mode, void __user *argp)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct space_resv sr;\n\n\tif (copy_from_user(&sr, argp, sizeof(sr)))\n\t\treturn -EFAULT;\n\n\tswitch (sr.l_whence) {\n\tcase SEEK_SET:\n\t\tbreak;\n\tcase SEEK_CUR:\n\t\tsr.l_start += filp->f_pos;\n\t\tbreak;\n\tcase SEEK_END:\n\t\tsr.l_start += i_size_read(inode);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn vfs_fallocate(filp, mode | FALLOC_FL_KEEP_SIZE, sr.l_start,\n\t\t\tsr.l_len);\n}\n\n/* on ia32 l_start is on a 32-bit boundary */\n#if defined CONFIG_COMPAT && defined(CONFIG_X86_64)\n/* just account for different alignment */\nstatic int compat_ioctl_preallocate(struct file *file, int mode,\n\t\t\t\t    struct space_resv_32 __user *argp)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct space_resv_32 sr;\n\n\tif (copy_from_user(&sr, argp, sizeof(sr)))\n\t\treturn -EFAULT;\n\n\tswitch (sr.l_whence) {\n\tcase SEEK_SET:\n\t\tbreak;\n\tcase SEEK_CUR:\n\t\tsr.l_start += file->f_pos;\n\t\tbreak;\n\tcase SEEK_END:\n\t\tsr.l_start += i_size_read(inode);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn vfs_fallocate(file, mode | FALLOC_FL_KEEP_SIZE, sr.l_start, sr.l_len);\n}\n#endif\n\nstatic int file_ioctl(struct file *filp, unsigned int cmd, int __user *p)\n{\n\tswitch (cmd) {\n\tcase FIBMAP:\n\t\treturn ioctl_fibmap(filp, p);\n\tcase FS_IOC_RESVSP:\n\tcase FS_IOC_RESVSP64:\n\t\treturn ioctl_preallocate(filp, 0, p);\n\tcase FS_IOC_UNRESVSP:\n\tcase FS_IOC_UNRESVSP64:\n\t\treturn ioctl_preallocate(filp, FALLOC_FL_PUNCH_HOLE, p);\n\tcase FS_IOC_ZERO_RANGE:\n\t\treturn ioctl_preallocate(filp, FALLOC_FL_ZERO_RANGE, p);\n\t}\n\n\treturn -ENOIOCTLCMD;\n}\n\nstatic int ioctl_fionbio(struct file *filp, int __user *argp)\n{\n\tunsigned int flag;\n\tint on, error;\n\n\terror = get_user(on, argp);\n\tif (error)\n\t\treturn error;\n\tflag = O_NONBLOCK;\n#ifdef __sparc__\n\t/* SunOS compatibility item. */\n\tif (O_NONBLOCK != O_NDELAY)\n\t\tflag |= O_NDELAY;\n#endif\n\tspin_lock(&filp->f_lock);\n\tif (on)\n\t\tfilp->f_flags |= flag;\n\telse\n\t\tfilp->f_flags &= ~flag;\n\tspin_unlock(&filp->f_lock);\n\treturn error;\n}\n\nstatic int ioctl_fioasync(unsigned int fd, struct file *filp,\n\t\t\t  int __user *argp)\n{\n\tunsigned int flag;\n\tint on, error;\n\n\terror = get_user(on, argp);\n\tif (error)\n\t\treturn error;\n\tflag = on ? FASYNC : 0;\n\n\t/* Did FASYNC state change ? */\n\tif ((flag ^ filp->f_flags) & FASYNC) {\n\t\tif (filp->f_op->fasync)\n\t\t\t/* fasync() adjusts filp->f_flags */\n\t\t\terror = filp->f_op->fasync(fd, filp, on);\n\t\telse\n\t\t\terror = -ENOTTY;\n\t}\n\treturn error < 0 ? error : 0;\n}\n\nstatic int ioctl_fsfreeze(struct file *filp)\n{\n\tstruct super_block *sb = file_inode(filp)->i_sb;\n\n\tif (!ns_capable(sb->s_user_ns, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* If filesystem doesn't support freeze feature, return. */\n\tif (sb->s_op->freeze_fs == NULL && sb->s_op->freeze_super == NULL)\n\t\treturn -EOPNOTSUPP;\n\n\t/* Freeze */\n\tif (sb->s_op->freeze_super)\n\t\treturn sb->s_op->freeze_super(sb, FREEZE_HOLDER_USERSPACE, NULL);\n\treturn freeze_super(sb, FREEZE_HOLDER_USERSPACE, NULL);\n}\n\nstatic int ioctl_fsthaw(struct file *filp)\n{\n\tstruct super_block *sb = file_inode(filp)->i_sb;\n\n\tif (!ns_capable(sb->s_user_ns, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* Thaw */\n\tif (sb->s_op->thaw_super)\n\t\treturn sb->s_op->thaw_super(sb, FREEZE_HOLDER_USERSPACE, NULL);\n\treturn thaw_super(sb, FREEZE_HOLDER_USERSPACE, NULL);\n}\n\nstatic int ioctl_file_dedupe_range(struct file *file,\n\t\t\t\t   struct file_dedupe_range __user *argp)\n{\n\tstruct file_dedupe_range *same = NULL;\n\tint ret;\n\tunsigned long size;\n\tu16 count;\n\n\tif (get_user(count, &argp->dest_count)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tsize = struct_size(same, info, count);\n\tif (size > PAGE_SIZE) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsame = memdup_user(argp, size);\n\tif (IS_ERR(same)) {\n\t\tret = PTR_ERR(same);\n\t\tsame = NULL;\n\t\tgoto out;\n\t}\n\n\tsame->dest_count = count;\n\tret = vfs_dedupe_file_range(file, same);\n\tif (ret)\n\t\tgoto out;\n\n\tret = copy_to_user(argp, same, size);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout:\n\tkfree(same);\n\treturn ret;\n}\n\nstatic int ioctl_getfsuuid(struct file *file, void __user *argp)\n{\n\tstruct super_block *sb = file_inode(file)->i_sb;\n\tstruct fsuuid2 u = { .len = sb->s_uuid_len, };\n\n\tif (!sb->s_uuid_len)\n\t\treturn -ENOTTY;\n\n\tmemcpy(&u.uuid[0], &sb->s_uuid, sb->s_uuid_len);\n\n\treturn copy_to_user(argp, &u, sizeof(u)) ? -EFAULT : 0;\n}\n\nstatic int ioctl_get_fs_sysfs_path(struct file *file, void __user *argp)\n{\n\tstruct super_block *sb = file_inode(file)->i_sb;\n\n\tif (!strlen(sb->s_sysfs_name))\n\t\treturn -ENOTTY;\n\n\tstruct fs_sysfs_path u = {};\n\n\tu.len = scnprintf(u.name, sizeof(u.name), \"%s/%s\", sb->s_type->name, sb->s_sysfs_name);\n\n\treturn copy_to_user(argp, &u, sizeof(u)) ? -EFAULT : 0;\n}\n\n/*\n * do_vfs_ioctl() is not for drivers and not intended to be EXPORT_SYMBOL()'d.\n * It's just a simple helper for sys_ioctl and compat_sys_ioctl.\n *\n * When you add any new common ioctls to the switches above and below,\n * please ensure they have compatible arguments in compat mode.\n *\n * The LSM mailing list should also be notified of any command additions or\n * changes, as specific LSMs may be affected.\n */\nstatic int do_vfs_ioctl(struct file *filp, unsigned int fd,\n\t\t\tunsigned int cmd, unsigned long arg)\n{\n\tvoid __user *argp = (void __user *)arg;\n\tstruct inode *inode = file_inode(filp);\n\n\tswitch (cmd) {\n\tcase FIOCLEX:\n\t\tset_close_on_exec(fd, 1);\n\t\treturn 0;\n\n\tcase FIONCLEX:\n\t\tset_close_on_exec(fd, 0);\n\t\treturn 0;\n\n\tcase FIONBIO:\n\t\treturn ioctl_fionbio(filp, argp);\n\n\tcase FIOASYNC:\n\t\treturn ioctl_fioasync(fd, filp, argp);\n\n\tcase FIOQSIZE:\n\t\tif (S_ISDIR(inode->i_mode) ||\n\t\t    (S_ISREG(inode->i_mode) && !IS_ANON_FILE(inode)) ||\n\t\t    S_ISLNK(inode->i_mode)) {\n\t\t\tloff_t res = inode_get_bytes(inode);\n\t\t\treturn copy_to_user(argp, &res, sizeof(res)) ?\n\t\t\t\t\t    -EFAULT : 0;\n\t\t}\n\n\t\treturn -ENOTTY;\n\n\tcase FIFREEZE:\n\t\treturn ioctl_fsfreeze(filp);\n\n\tcase FITHAW:\n\t\treturn ioctl_fsthaw(filp);\n\n\tcase FS_IOC_FIEMAP:\n\t\treturn ioctl_fiemap(filp, argp);\n\n\tcase FIGETBSZ:\n\t\t/* anon_bdev filesystems may not have a block size */\n\t\tif (!inode->i_sb->s_blocksize)\n\t\t\treturn -EINVAL;\n\n\t\treturn put_user(inode->i_sb->s_blocksize, (int __user *)argp);\n\n\tcase FICLONE:\n\t\treturn ioctl_file_clone(filp, arg, 0, 0, 0);\n\n\tcase FICLONERANGE:\n\t\treturn ioctl_file_clone_range(filp, argp);\n\n\tcase FIDEDUPERANGE:\n\t\treturn ioctl_file_dedupe_range(filp, argp);\n\n\tcase FIONREAD:\n\t\tif (!S_ISREG(inode->i_mode) || IS_ANON_FILE(inode))\n\t\t\treturn vfs_ioctl(filp, cmd, arg);\n\n\t\treturn put_user(i_size_read(inode) - filp->f_pos,\n\t\t\t\t(int __user *)argp);\n\n\tcase FS_IOC_GETFLAGS:\n\t\treturn ioctl_getflags(filp, argp);\n\n\tcase FS_IOC_SETFLAGS:\n\t\treturn ioctl_setflags(filp, argp);\n\n\tcase FS_IOC_FSGETXATTR:\n\t\treturn ioctl_fsgetxattr(filp, argp);\n\n\tcase FS_IOC_FSSETXATTR:\n\t\treturn ioctl_fssetxattr(filp, argp);\n\n\tcase FS_IOC_GETFSUUID:\n\t\treturn ioctl_getfsuuid(filp, argp);\n\n\tcase FS_IOC_GETFSSYSFSPATH:\n\t\treturn ioctl_get_fs_sysfs_path(filp, argp);\n\n\tdefault:\n\t\tif (S_ISREG(inode->i_mode) && !IS_ANON_FILE(inode))\n\t\t\treturn file_ioctl(filp, cmd, argp);\n\t\tbreak;\n\t}\n\n\treturn -ENOIOCTLCMD;\n}\n\nSYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)\n{\n\tCLASS(fd, f)(fd);\n\tint error;\n\n\tif (fd_empty(f))\n\t\treturn -EBADF;\n\n\terror = security_file_ioctl(fd_file(f), cmd, arg);\n\tif (error)\n\t\treturn error;\n\n\terror = do_vfs_ioctl(fd_file(f), fd, cmd, arg);\n\tif (error == -ENOIOCTLCMD)\n\t\terror = vfs_ioctl(fd_file(f), cmd, arg);\n\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT\n/**\n * compat_ptr_ioctl - generic implementation of .compat_ioctl file operation\n * @file: The file to operate on.\n * @cmd: The ioctl command number.\n * @arg: The argument to the ioctl.\n *\n * This is not normally called as a function, but instead set in struct\n * file_operations as\n *\n *     .compat_ioctl = compat_ptr_ioctl,\n *\n * On most architectures, the compat_ptr_ioctl() just passes all arguments\n * to the corresponding ->ioctl handler. The exception is arch/s390, where\n * compat_ptr() clears the top bit of a 32-bit pointer value, so user space\n * pointers to the second 2GB alias the first 2GB, as is the case for\n * native 32-bit s390 user space.\n *\n * The compat_ptr_ioctl() function must therefore be used only with ioctl\n * functions that either ignore the argument or pass a pointer to a\n * compatible data type.\n *\n * If any ioctl command handled by fops->unlocked_ioctl passes a plain\n * integer instead of a pointer, or any of the passed data types\n * is incompatible between 32-bit and 64-bit architectures, a proper\n * handler is required instead of compat_ptr_ioctl.\n */\nlong compat_ptr_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tif (!file->f_op->unlocked_ioctl)\n\t\treturn -ENOIOCTLCMD;\n\n\treturn file->f_op->unlocked_ioctl(file, cmd, (unsigned long)compat_ptr(arg));\n}\nEXPORT_SYMBOL(compat_ptr_ioctl);\n\nCOMPAT_SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd,\n\t\t       compat_ulong_t, arg)\n{\n\tCLASS(fd, f)(fd);\n\tint error;\n\n\tif (fd_empty(f))\n\t\treturn -EBADF;\n\n\terror = security_file_ioctl_compat(fd_file(f), cmd, arg);\n\tif (error)\n\t\treturn error;\n\n\tswitch (cmd) {\n\t/* FICLONE takes an int argument, so don't use compat_ptr() */\n\tcase FICLONE:\n\t\terror = ioctl_file_clone(fd_file(f), arg, 0, 0, 0);\n\t\tbreak;\n\n#if defined(CONFIG_X86_64)\n\t/* these get messy on amd64 due to alignment differences */\n\tcase FS_IOC_RESVSP_32:\n\tcase FS_IOC_RESVSP64_32:\n\t\terror = compat_ioctl_preallocate(fd_file(f), 0, compat_ptr(arg));\n\t\tbreak;\n\tcase FS_IOC_UNRESVSP_32:\n\tcase FS_IOC_UNRESVSP64_32:\n\t\terror = compat_ioctl_preallocate(fd_file(f), FALLOC_FL_PUNCH_HOLE,\n\t\t\t\tcompat_ptr(arg));\n\t\tbreak;\n\tcase FS_IOC_ZERO_RANGE_32:\n\t\terror = compat_ioctl_preallocate(fd_file(f), FALLOC_FL_ZERO_RANGE,\n\t\t\t\tcompat_ptr(arg));\n\t\tbreak;\n#endif\n\n\t/*\n\t * These access 32-bit values anyway so no further handling is\n\t * necessary.\n\t */\n\tcase FS_IOC32_GETFLAGS:\n\tcase FS_IOC32_SETFLAGS:\n\t\tcmd = (cmd == FS_IOC32_GETFLAGS) ?\n\t\t\tFS_IOC_GETFLAGS : FS_IOC_SETFLAGS;\n\t\tfallthrough;\n\t/*\n\t * everything else in do_vfs_ioctl() takes either a compatible\n\t * pointer argument or no argument -- call it with a modified\n\t * argument.\n\t */\n\tdefault:\n\t\terror = do_vfs_ioctl(fd_file(f), fd, cmd,\n\t\t\t\t     (unsigned long)compat_ptr(arg));\n\t\tif (error != -ENOIOCTLCMD)\n\t\t\tbreak;\n\n\t\tif (fd_file(f)->f_op->compat_ioctl)\n\t\t\terror = fd_file(f)->f_op->compat_ioctl(fd_file(f), cmd, arg);\n\t\tif (error == -ENOIOCTLCMD)\n\t\t\terror = -ENOTTY;\n\t\tbreak;\n\t}\n\treturn error;\n}\n#endif\n", "patch": "@@ -590,6 +590,7 @@ static long ioctl_file_dedupe_range(struct file *file, void __user *arg)\n \t\tgoto out;\n \t}\n \n+\tsame->dest_count = count;\n \tret = vfs_dedupe_file_range(file, same);\n \tif (ret)\n \t\tgoto out;", "file_path": "files/2016_8\\105", "file_language": "c", "file_name": "fs/ioctl.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 44, "cve_id": "CVE-2016-3070", "cwe_id": ["CWE-476"], "cve_language": "C", "cve_description": "The trace_writeback_dirty_page implementation in include/trace/events/writeback.h in the Linux kernel before 4.4 improperly interacts with mm/migrate.c, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by triggering a certain page move.", "cvss": "7.8", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "42cb14b110a5698ccf26ce59c4441722605a3743", "commit_message": "mm: migrate dirty page without clear_page_dirty_for_io etc\n\nclear_page_dirty_for_io() has accumulated writeback and memcg subtleties\nsince v2.6.16 first introduced page migration; and the set_page_dirty()\nwhich completed its migration of PageDirty, later had to be moderated to\n__set_page_dirty_nobuffers(); then PageSwapBacked had to skip that too.\n\nNo actual problems seen with this procedure recently, but if you look into\nwhat the clear_page_dirty_for_io(page)+set_page_dirty(newpage) is actually\nachieving, it turns out to be nothing more than moving the PageDirty flag,\nand its NR_FILE_DIRTY stat from one zone to another.\n\nIt would be good to avoid a pile of irrelevant decrementations and\nincrementations, and improper event counting, and unnecessary descent of\nthe radix_tree under tree_lock (to set the PAGECACHE_TAG_DIRTY which\nradix_tree_replace_slot() left in place anyway).\n\nDo the NR_FILE_DIRTY movement, like the other stats movements, while\ninterrupts still disabled in migrate_page_move_mapping(); and don't even\nbother if the zone is the same.  Do the PageDirty movement there under\ntree_lock too, where old page is frozen and newpage not yet visible:\nbearing in mind that as soon as newpage becomes visible in radix_tree, an\nun-page-locked set_page_dirty() might interfere (or perhaps that's just\nnot possible: anything doing so should already hold an additional\nreference to the old page, preventing its migration; but play safe).\n\nBut we do still need to transfer PageDirty in migrate_page_copy(), for\nthose who don't go the mapping route through migrate_page_move_mapping().\n\nSigned-off-by: Hugh Dickins <hughd@google.com>\nCc: Christoph Lameter <cl@linux.com>\nCc: \"Kirill A. Shutemov\" <kirill.shutemov@linux.intel.com>\nCc: Rik van Riel <riel@redhat.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Davidlohr Bueso <dave@stgolabs.net>\nCc: Oleg Nesterov <oleg@redhat.com>\nCc: Sasha Levin <sasha.levin@oracle.com>\nCc: Dmitry Vyukov <dvyukov@google.com>\nCc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "commit_date": "2015-11-06T03:34:48Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/42cb14b110a5698ccf26ce59c4441722605a3743", "html_url": "https://github.com/torvalds/linux/commit/42cb14b110a5698ccf26ce59c4441722605a3743", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "cf4b769abb8aef01f887543cb8308c0d8671367c", "url_before": "https://api.github.com/repos/torvalds/linux/commits/cf4b769abb8aef01f887543cb8308c0d8671367c", "html_url_before": "https://github.com/torvalds/linux/commit/cf4b769abb8aef01f887543cb8308c0d8671367c"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/42cb14b110a5698ccf26ce59c4441722605a3743/mm/migrate.c", "code": "/*\n * Memory Migration functionality - linux/mm/migrate.c\n *\n * Copyright (C) 2006 Silicon Graphics, Inc., Christoph Lameter\n *\n * Page migration was first developed in the context of the memory hotplug\n * project. The main authors of the migration code are:\n *\n * IWAMOTO Toshihiro <iwamoto@valinux.co.jp>\n * Hirokazu Takahashi <taka@valinux.co.jp>\n * Dave Hansen <haveblue@us.ibm.com>\n * Christoph Lameter\n */\n\n#include <linux/migrate.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/buffer_head.h>\n#include <linux/mm_inline.h>\n#include <linux/nsproxy.h>\n#include <linux/pagevec.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/topology.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/writeback.h>\n#include <linux/mempolicy.h>\n#include <linux/vmalloc.h>\n#include <linux/security.h>\n#include <linux/backing-dev.h>\n#include <linux/syscalls.h>\n#include <linux/hugetlb.h>\n#include <linux/hugetlb_cgroup.h>\n#include <linux/gfp.h>\n#include <linux/balloon_compaction.h>\n#include <linux/mmu_notifier.h>\n#include <linux/page_idle.h>\n\n#include <asm/tlbflush.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/migrate.h>\n\n#include \"internal.h\"\n\n/*\n * migrate_prep() needs to be called before we start compiling a list of pages\n * to be migrated using isolate_lru_page(). If scheduling work on other CPUs is\n * undesirable, use migrate_prep_local()\n */\nint migrate_prep(void)\n{\n\t/*\n\t * Clear the LRU lists so pages can be isolated.\n\t * Note that pages may be moved off the LRU after we have\n\t * drained them. Those pages will fail to migrate like other\n\t * pages that may be busy.\n\t */\n\tlru_add_drain_all();\n\n\treturn 0;\n}\n\n/* Do the necessary work of migrate_prep but not if it involves other CPUs */\nint migrate_prep_local(void)\n{\n\tlru_add_drain();\n\n\treturn 0;\n}\n\n/*\n * Put previously isolated pages back onto the appropriate lists\n * from where they were once taken off for compaction/migration.\n *\n * This function shall be used whenever the isolated pageset has been\n * built from lru, balloon, hugetlbfs page. See isolate_migratepages_range()\n * and isolate_huge_page().\n */\nvoid putback_movable_pages(struct list_head *l)\n{\n\tstruct page *page;\n\tstruct page *page2;\n\n\tlist_for_each_entry_safe(page, page2, l, lru) {\n\t\tif (unlikely(PageHuge(page))) {\n\t\t\tputback_active_hugepage(page);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del(&page->lru);\n\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\tpage_is_file_cache(page));\n\t\tif (unlikely(isolated_balloon_page(page)))\n\t\t\tballoon_page_putback(page);\n\t\telse\n\t\t\tputback_lru_page(page);\n\t}\n}\n\n/*\n * Restore a potential migration pte to a working pte entry\n */\nstatic int remove_migration_pte(struct page *new, struct vm_area_struct *vma,\n\t\t\t\t unsigned long addr, void *old)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tswp_entry_t entry;\n \tpmd_t *pmd;\n\tpte_t *ptep, pte;\n \tspinlock_t *ptl;\n\n\tif (unlikely(PageHuge(new))) {\n\t\tptep = huge_pte_offset(mm, addr);\n\t\tif (!ptep)\n\t\t\tgoto out;\n\t\tptl = huge_pte_lockptr(hstate_vma(vma), mm, ptep);\n\t} else {\n\t\tpmd = mm_find_pmd(mm, addr);\n\t\tif (!pmd)\n\t\t\tgoto out;\n\n\t\tptep = pte_offset_map(pmd, addr);\n\n\t\t/*\n\t\t * Peek to check is_swap_pte() before taking ptlock?  No, we\n\t\t * can race mremap's move_ptes(), which skips anon_vma lock.\n\t\t */\n\n\t\tptl = pte_lockptr(mm, pmd);\n\t}\n\n \tspin_lock(ptl);\n\tpte = *ptep;\n\tif (!is_swap_pte(pte))\n\t\tgoto unlock;\n\n\tentry = pte_to_swp_entry(pte);\n\n\tif (!is_migration_entry(entry) ||\n\t    migration_entry_to_page(entry) != old)\n\t\tgoto unlock;\n\n\tget_page(new);\n\tpte = pte_mkold(mk_pte(new, vma->vm_page_prot));\n\tif (pte_swp_soft_dirty(*ptep))\n\t\tpte = pte_mksoft_dirty(pte);\n\n\t/* Recheck VMA as permissions can change since migration started  */\n\tif (is_write_migration_entry(entry))\n\t\tpte = maybe_mkwrite(pte, vma);\n\n#ifdef CONFIG_HUGETLB_PAGE\n\tif (PageHuge(new)) {\n\t\tpte = pte_mkhuge(pte);\n\t\tpte = arch_make_huge_pte(pte, vma, new, 0);\n\t}\n#endif\n\tflush_dcache_page(new);\n\tset_pte_at(mm, addr, ptep, pte);\n\n\tif (PageHuge(new)) {\n\t\tif (PageAnon(new))\n\t\t\thugepage_add_anon_rmap(new, vma, addr);\n\t\telse\n\t\t\tpage_dup_rmap(new);\n\t} else if (PageAnon(new))\n\t\tpage_add_anon_rmap(new, vma, addr);\n\telse\n\t\tpage_add_file_rmap(new);\n\n\tif (vma->vm_flags & VM_LOCKED)\n\t\tmlock_vma_page(new);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(vma, addr, ptep);\nunlock:\n\tpte_unmap_unlock(ptep, ptl);\nout:\n\treturn SWAP_AGAIN;\n}\n\n/*\n * Get rid of all migration entries and replace them by\n * references to the indicated page.\n */\nstatic void remove_migration_ptes(struct page *old, struct page *new)\n{\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = remove_migration_pte,\n\t\t.arg = old,\n\t};\n\n\trmap_walk(new, &rwc);\n}\n\n/*\n * Something used the pte of a page under migration. We need to\n * get to the page and wait until migration is finished.\n * When we return from this function the fault will be retried.\n */\nvoid __migration_entry_wait(struct mm_struct *mm, pte_t *ptep,\n\t\t\t\tspinlock_t *ptl)\n{\n\tpte_t pte;\n\tswp_entry_t entry;\n\tstruct page *page;\n\n\tspin_lock(ptl);\n\tpte = *ptep;\n\tif (!is_swap_pte(pte))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(pte);\n\tif (!is_migration_entry(entry))\n\t\tgoto out;\n\n\tpage = migration_entry_to_page(entry);\n\n\t/*\n\t * Once radix-tree replacement of page migration started, page_count\n\t * *must* be zero. And, we don't want to call wait_on_page_locked()\n\t * against a page without get_page().\n\t * So, we use get_page_unless_zero(), here. Even failed, page fault\n\t * will occur again.\n\t */\n\tif (!get_page_unless_zero(page))\n\t\tgoto out;\n\tpte_unmap_unlock(ptep, ptl);\n\twait_on_page_locked(page);\n\tput_page(page);\n\treturn;\nout:\n\tpte_unmap_unlock(ptep, ptl);\n}\n\nvoid migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t\tunsigned long address)\n{\n\tspinlock_t *ptl = pte_lockptr(mm, pmd);\n\tpte_t *ptep = pte_offset_map(pmd, address);\n\t__migration_entry_wait(mm, ptep, ptl);\n}\n\nvoid migration_entry_wait_huge(struct vm_area_struct *vma,\n\t\tstruct mm_struct *mm, pte_t *pte)\n{\n\tspinlock_t *ptl = huge_pte_lockptr(hstate_vma(vma), mm, pte);\n\t__migration_entry_wait(mm, pte, ptl);\n}\n\n#ifdef CONFIG_BLOCK\n/* Returns true if all buffers are successfully locked */\nstatic bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\tstruct buffer_head *bh = head;\n\n\t/* Simple case, sync compaction */\n\tif (mode != MIGRATE_ASYNC) {\n\t\tdo {\n\t\t\tget_bh(bh);\n\t\t\tlock_buffer(bh);\n\t\t\tbh = bh->b_this_page;\n\n\t\t} while (bh != head);\n\n\t\treturn true;\n\t}\n\n\t/* async case, we cannot block on lock_buffer so use trylock_buffer */\n\tdo {\n\t\tget_bh(bh);\n\t\tif (!trylock_buffer(bh)) {\n\t\t\t/*\n\t\t\t * We failed to lock the buffer and cannot stall in\n\t\t\t * async migration. Release the taken locks\n\t\t\t */\n\t\t\tstruct buffer_head *failed_bh = bh;\n\t\t\tput_bh(failed_bh);\n\t\t\tbh = head;\n\t\t\twhile (bh != failed_bh) {\n\t\t\t\tunlock_buffer(bh);\n\t\t\t\tput_bh(bh);\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\treturn true;\n}\n#else\nstatic inline bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\treturn true;\n}\n#endif /* CONFIG_BLOCK */\n\n/*\n * Replace the page in the mapping.\n *\n * The number of remaining references must be:\n * 1 for anonymous pages without a mapping\n * 2 for pages with a mapping\n * 3 for pages with a mapping and PagePrivate/PagePrivate2 set.\n */\nint migrate_page_move_mapping(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tstruct buffer_head *head, enum migrate_mode mode,\n\t\tint extra_count)\n{\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tint expected_count = 1 + extra_count;\n\tvoid **pslot;\n\n\tif (!mapping) {\n\t\t/* Anonymous page without mapping */\n\t\tif (page_count(page) != expected_count)\n\t\t\treturn -EAGAIN;\n\n\t\t/* No turning back from here */\n\t\tset_page_memcg(newpage, page_memcg(page));\n\t\tnewpage->index = page->index;\n\t\tnewpage->mapping = page->mapping;\n\t\tif (PageSwapBacked(page))\n\t\t\tSetPageSwapBacked(newpage);\n\n\t\treturn MIGRATEPAGE_SUCCESS;\n\t}\n\n\toldzone = page_zone(page);\n\tnewzone = page_zone(newpage);\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n \t\t\t\t\tpage_index(page));\n\n\texpected_count += 1 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * In the async migration case of moving a page with buffers, lock the\n\t * buffers using trylock before the mapping is moved. If the mapping\n\t * was moved, we later failed to lock the buffers and could not move\n\t * the mapping back due to an elevated page count, we would have to\n\t * block waiting on other references to be dropped.\n\t */\n\tif (mode == MIGRATE_ASYNC && head &&\n\t\t\t!buffer_migrate_lock_buffers(head, mode)) {\n\t\tpage_unfreeze_refs(page, expected_count);\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * Now we know that no one else is looking at the page:\n\t * no turning back from here.\n\t */\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tif (PageSwapBacked(page))\n\t\tSetPageSwapBacked(newpage);\n\n\tget_page(newpage);\t/* add cache reference */\n\tif (PageSwapCache(page)) {\n\t\tSetPageSwapCache(newpage);\n\t\tset_page_private(newpage, page_private(page));\n\t}\n\n\t/* Move dirty while page refs frozen and newpage not yet exposed */\n\tdirty = PageDirty(page);\n\tif (dirty) {\n\t\tClearPageDirty(page);\n\t\tSetPageDirty(newpage);\n\t}\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\t/*\n\t * Drop cache reference from old page by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock(&mapping->tree_lock);\n\t/* Leave irq disabled to prevent preemption while updating stats */\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the page for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new page and drop references to the old page.\n\t *\n\t * Note that anonymous pages are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n\t * are mapped to swap space.\n\t */\n\tif (newzone != oldzone) {\n\t\t__dec_zone_state(oldzone, NR_FILE_PAGES);\n\t\t__inc_zone_state(newzone, NR_FILE_PAGES);\n\t\tif (PageSwapBacked(page) && !PageSwapCache(page)) {\n\t\t\t__dec_zone_state(oldzone, NR_SHMEM);\n\t\t\t__inc_zone_state(newzone, NR_SHMEM);\n\t\t}\n\t\tif (dirty && mapping_cap_account_dirty(mapping)) {\n\t\t\t__dec_zone_state(oldzone, NR_FILE_DIRTY);\n\t\t\t__inc_zone_state(newzone, NR_FILE_DIRTY);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n/*\n * The expected number of remaining references is the same as that\n * of migrate_page_move_mapping().\n */\nint migrate_huge_page_move_mapping(struct address_space *mapping,\n\t\t\t\t   struct page *newpage, struct page *page)\n{\n\tint expected_count;\n\tvoid **pslot;\n\n\tspin_lock_irq(&mapping->tree_lock);\n\n\tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n\t\t\t\t\tpage_index(page));\n\n\texpected_count = 2 + page_has_private(page);\n\tif (page_count(page) != expected_count ||\n\t\tradix_tree_deref_slot_protected(pslot, &mapping->tree_lock) != page) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!page_freeze_refs(page, expected_count)) {\n\t\tspin_unlock_irq(&mapping->tree_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tset_page_memcg(newpage, page_memcg(page));\n\tnewpage->index = page->index;\n\tnewpage->mapping = page->mapping;\n\tget_page(newpage);\n\n\tradix_tree_replace_slot(pslot, newpage);\n\n\tpage_unfreeze_refs(page, expected_count - 1);\n\n\tspin_unlock_irq(&mapping->tree_lock);\n\treturn MIGRATEPAGE_SUCCESS;\n}\n\n/*\n * Gigantic pages are so large that we do not guarantee that page++ pointer\n * arithmetic will work across the entire page.  We need something more\n * specialized.\n */\nstatic void __copy_gigantic_page(struct page *dst, struct page *src,\n\t\t\t\tint nr_pages)\n{\n\tint i;\n\tstruct page *dst_base = dst;\n\tstruct page *src_base = src;\n\n\tfor (i = 0; i < nr_pages; ) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst, src);\n\n\t\ti++;\n\t\tdst = mem_map_next(dst, dst_base, i);\n\t\tsrc = mem_map_next(src, src_base, i);\n\t}\n}\n\nstatic void copy_huge_page(struct page *dst, struct page *src)\n{\n\tint i;\n\tint nr_pages;\n\n\tif (PageHuge(src)) {\n\t\t/* hugetlbfs page */\n\t\tstruct hstate *h = page_hstate(src);\n\t\tnr_pages = pages_per_huge_page(h);\n\n\t\tif (unlikely(nr_pages > MAX_ORDER_NR_PAGES)) {\n\t\t\t__copy_gigantic_page(dst, src, nr_pages);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\t/* thp page */\n\t\tBUG_ON(!PageTransHuge(src));\n\t\tnr_pages = hpage_nr_pages(src);\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst + i, src + i);\n\t}\n}\n\n/*\n * Copy the page to its new location\n */\nvoid migrate_page_copy(struct page *newpage, struct page *page)\n{\n\tint cpupid;\n\n\tif (PageHuge(page) || PageTransHuge(page))\n\t\tcopy_huge_page(newpage, page);\n\telse\n\t\tcopy_highpage(newpage, page);\n\n\tif (PageError(page))\n\t\tSetPageError(newpage);\n\tif (PageReferenced(page))\n\t\tSetPageReferenced(newpage);\n\tif (PageUptodate(page))\n\t\tSetPageUptodate(newpage);\n\tif (TestClearPageActive(page)) {\n\t\tVM_BUG_ON_PAGE(PageUnevictable(page), page);\n\t\tSetPageActive(newpage);\n\t} else if (TestClearPageUnevictable(page))\n\t\tSetPageUnevictable(newpage);\n\tif (PageChecked(page))\n\t\tSetPageChecked(newpage);\n\tif (PageMappedToDisk(page))\n\t\tSetPageMappedToDisk(newpage);\n\n\t/* Move dirty on pages not done by migrate_page_move_mapping() */\n\tif (PageDirty(page))\n\t\tSetPageDirty(newpage);\n\n\tif (page_is_young(page))\n\t\tset_page_young(newpage);\n\tif (page_is_idle(page))\n\t\tset_page_idle(newpage);\n\n\t/*\n\t * Copy NUMA information to the new page, to prevent over-eager\n\t * future migrations of this same page.\n\t */\n\tcpupid = page_cpupid_xchg_last(page, -1);\n\tpage_cpupid_xchg_last(newpage, cpupid);\n\n\tksm_migrate_page(newpage, page);\n\t/*\n\t * Please do not reorder this without considering how mm/ksm.c's\n\t * get_ksm_page() depends upon ksm_migrate_page() and PageSwapCache().\n\t */\n\tif (PageSwapCache(page))\n\t\tClearPageSwapCache(page);\n\tClearPagePrivate(page);\n\tset_page_private(page, 0);\n\n\t/*\n\t * If any waiters have accumulated on the new page then\n\t * wake them up.\n\t */\n\tif (PageWriteback(newpage))\n\t\tend_page_writeback(newpage);\n}\n\n/************************************************************\n *                    Migration functions\n ***********************************************************/\n\n/*\n * Common logic to directly migrate a single page suitable for\n * pages that do not use PagePrivate/PagePrivate2.\n *\n * Pages are locked upon entry and exit.\n */\nint migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page,\n\t\tenum migrate_mode mode)\n{\n\tint rc;\n\n\tBUG_ON(PageWriteback(page));\t/* Writeback must be complete */\n\n\trc = migrate_page_move_mapping(mapping, newpage, page, NULL, mode, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\tmigrate_page_copy(newpage, page);\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL(migrate_page);\n\n#ifdef CONFIG_BLOCK\n/*\n * Migration function for pages with buffers. This function can only be used\n * if the underlying filesystem guarantees that no other references to \"page\"\n * exist.\n */\nint buffer_migrate_page(struct address_space *mapping,\n\t\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tstruct buffer_head *bh, *head;\n\tint rc;\n\n\tif (!page_has_buffers(page))\n\t\treturn migrate_page(mapping, newpage, page, mode);\n\n\thead = page_buffers(page);\n\n\trc = migrate_page_move_mapping(mapping, newpage, page, head, mode, 0);\n\n\tif (rc != MIGRATEPAGE_SUCCESS)\n\t\treturn rc;\n\n\t/*\n\t * In the async case, migrate_page_move_mapping locked the buffers\n\t * with an IRQ-safe spinlock held. In the sync case, the buffers\n\t * need to be locked now\n\t */\n\tif (mode != MIGRATE_ASYNC)\n\t\tBUG_ON(!buffer_migrate_lock_buffers(head, mode));\n\n\tClearPagePrivate(page);\n\tset_page_private(newpage, page_private(page));\n\tset_page_private(page, 0);\n\tput_page(page);\n\tget_page(newpage);\n\n\tbh = head;\n\tdo {\n\t\tset_bh_page(bh, newpage, bh_offset(bh));\n\t\tbh = bh->b_this_page;\n\n\t} while (bh != head);\n\n\tSetPagePrivate(newpage);\n\n\tmigrate_page_copy(newpage, page);\n\n\tbh = head;\n\tdo {\n\t\tunlock_buffer(bh);\n \t\tput_bh(bh);\n\t\tbh = bh->b_this_page;\n\n\t} while (bh != head);\n\n\treturn MIGRATEPAGE_SUCCESS;\n}\nEXPORT_SYMBOL(buffer_migrate_page);\n#endif\n\n/*\n * Writeback a page to clean the dirty state\n */\nstatic int writeout(struct address_space *mapping, struct page *page)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t\t.nr_to_write = 1,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t\t.for_reclaim = 1\n\t};\n\tint rc;\n\n\tif (!mapping->a_ops->writepage)\n\t\t/* No write method for the address space */\n\t\treturn -EINVAL;\n\n\tif (!clear_page_dirty_for_io(page))\n\t\t/* Someone else already triggered a write */\n\t\treturn -EAGAIN;\n\n\t/*\n\t * A dirty page may imply that the underlying filesystem has\n\t * the page on some queue. So the page must be clean for\n\t * migration. Writeout may mean we loose the lock and the\n\t * page state is no longer what we checked for earlier.\n\t * At this point we know that the migration attempt cannot\n\t * be successful.\n\t */\n\tremove_migration_ptes(page, page);\n\n\trc = mapping->a_ops->writepage(page, &wbc);\n\n\tif (rc != AOP_WRITEPAGE_ACTIVATE)\n\t\t/* unlocked. Relock */\n\t\tlock_page(page);\n\n\treturn (rc < 0) ? -EIO : -EAGAIN;\n}\n\n/*\n * Default handling if a filesystem does not provide a migration function.\n */\nstatic int fallback_migrate_page(struct address_space *mapping,\n\tstruct page *newpage, struct page *page, enum migrate_mode mode)\n{\n\tif (PageDirty(page)) {\n\t\t/* Only writeback pages in full synchronous migration */\n\t\tif (mode != MIGRATE_SYNC)\n\t\t\treturn -EBUSY;\n\t\treturn writeout(mapping, page);\n\t}\n\n\t/*\n\t * Buffers may be managed in a filesystem specific way.\n\t * We must have no buffers or drop them.\n\t */\n\tif (page_has_private(page) &&\n\t    !try_to_release_page(page, GFP_KERNEL))\n\t\treturn -EAGAIN;\n\n\treturn migrate_page(mapping, newpage, page, mode);\n}\n\n/*\n * Move a page to a newly allocated page\n * The page is locked and all ptes have been successfully removed.\n *\n * The new page will have replaced the old page if this function\n * is successful.\n *\n * Return value:\n *   < 0 - error code\n *  MIGRATEPAGE_SUCCESS - success\n */\nstatic int move_to_new_page(struct page *newpage, struct page *page,\n\t\t\t\tenum migrate_mode mode)\n{\n\tstruct address_space *mapping;\n\tint rc;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(newpage), newpage);\n\n\tmapping = page_mapping(page);\n\tif (!mapping)\n\t\trc = migrate_page(mapping, newpage, page, mode);\n\telse if (mapping->a_ops->migratepage)\n\t\t/*\n\t\t * Most pages have a mapping and most filesystems provide a\n\t\t * migratepage callback. Anonymous pages are part of swap\n\t\t * space which also has its own migratepage callback. This\n\t\t * is the most common path for page migration.\n\t\t */\n\t\trc = mapping->a_ops->migratepage(mapping, newpage, page, mode);\n\telse\n\t\trc = fallback_migrate_page(mapping, newpage, page, mode);\n\n\t/*\n\t * When successful, old pagecache page->mapping must be cleared before\n\t * page is freed; but stats require that PageAnon be left as PageAnon.\n\t */\n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\tset_page_memcg(page, NULL);\n\t\tif (!PageAnon(page))\n\t\t\tpage->mapping = NULL;\n\t}\n\treturn rc;\n}\n\nstatic int __unmap_and_move(struct page *page, struct page *newpage,\n\t\t\t\tint force, enum migrate_mode mode)\n{\n\tint rc = -EAGAIN;\n\tint page_was_mapped = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\n\tif (!trylock_page(page)) {\n\t\tif (!force || mode == MIGRATE_ASYNC)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * It's not safe for direct compaction to call lock_page.\n\t\t * For example, during page readahead pages are added locked\n\t\t * to the LRU. Later, when the IO completes the pages are\n\t\t * marked uptodate and unlocked. However, the queueing\n\t\t * could be merging multiple pages for one bio (e.g.\n\t\t * mpage_readpages). If an allocation happens for the\n\t\t * second or third page, the process can end up locking\n\t\t * the same page twice and deadlocking. Rather than\n\t\t * trying to be clever about what pages can be locked,\n\t\t * avoid the use of lock_page for direct compaction\n\t\t * altogether.\n\t\t */\n\t\tif (current->flags & PF_MEMALLOC)\n\t\t\tgoto out;\n\n\t\tlock_page(page);\n\t}\n\n\tif (PageWriteback(page)) {\n\t\t/*\n\t\t * Only in the case of a full synchronous migration is it\n\t\t * necessary to wait for PageWriteback. In the async case,\n\t\t * the retry loop is too short and in the sync-light case,\n\t\t * the overhead of stalling is too much\n\t\t */\n\t\tif (mode != MIGRATE_SYNC) {\n\t\t\trc = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (!force)\n\t\t\tgoto out_unlock;\n\t\twait_on_page_writeback(page);\n\t}\n\n\t/*\n\t * By try_to_unmap(), page->mapcount goes down to 0 here. In this case,\n\t * we cannot notice that anon_vma is freed while we migrates a page.\n\t * This get_anon_vma() delays freeing anon_vma pointer until the end\n\t * of migration. File cache pages are no problem because of page_lock()\n\t * File Caches may use write_page() or lock_page() in migration, then,\n\t * just care Anon page here.\n\t *\n\t * Only page_get_anon_vma() understands the subtleties of\n\t * getting a hold on an anon_vma from outside one of its mms.\n\t * But if we cannot get anon_vma, then we won't need it anyway,\n\t * because that implies that the anon page is no longer mapped\n\t * (and cannot be remapped so long as we hold the page lock).\n\t */\n\tif (PageAnon(page) && !PageKsm(page))\n\t\tanon_vma = page_get_anon_vma(page);\n\n\t/*\n\t * Block others from accessing the new page when we get around to\n\t * establishing additional references. We are usually the only one\n\t * holding a reference to newpage at this point. We used to have a BUG\n\t * here if trylock_page(newpage) fails, but would like to allow for\n\t * cases where there might be a race with the previous use of newpage.\n\t * This is much like races on refcount of oldpage: just don't BUG().\n\t */\n\tif (unlikely(!trylock_page(newpage)))\n\t\tgoto out_unlock;\n\n\tif (unlikely(isolated_balloon_page(page))) {\n\t\t/*\n\t\t * A ballooned page does not need any special attention from\n\t\t * physical to virtual reverse mapping procedures.\n\t\t * Skip any attempt to unmap PTEs or to remap swap cache,\n\t\t * in order to avoid burning cycles at rmap level, and perform\n\t\t * the page migration right away (proteced by page lock).\n\t\t */\n\t\trc = balloon_page_migrate(newpage, page, mode);\n\t\tgoto out_unlock_both;\n\t}\n\n\t/*\n\t * Corner case handling:\n\t * 1. When a new swap-cache page is read into, it is added to the LRU\n\t * and treated as swapcache but it has no rmap yet.\n\t * Calling try_to_unmap() against a page->mapping==NULL page will\n\t * trigger a BUG.  So handle it here.\n\t * 2. An orphaned page (see truncate_complete_page) might have\n\t * fs-private metadata. The page can be picked up due to memory\n\t * offlining.  Everywhere else except page reclaim, the page is\n\t * invisible to the vm, so the page can not be migrated.  So try to\n\t * free the metadata, so the page can be freed.\n\t */\n\tif (!page->mapping) {\n\t\tVM_BUG_ON_PAGE(PageAnon(page), page);\n\t\tif (page_has_private(page)) {\n\t\t\ttry_to_free_buffers(page);\n\t\t\tgoto out_unlock_both;\n\t\t}\n\t} else if (page_mapped(page)) {\n\t\t/* Establish migration ptes */\n\t\tVM_BUG_ON_PAGE(PageAnon(page) && !PageKsm(page) && !anon_vma,\n\t\t\t\tpage);\n\t\ttry_to_unmap(page,\n\t\t\tTTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);\n\t\tpage_was_mapped = 1;\n\t}\n\n\tif (!page_mapped(page))\n\t\trc = move_to_new_page(newpage, page, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(page,\n\t\t\trc == MIGRATEPAGE_SUCCESS ? newpage : page);\n\nout_unlock_both:\n\tunlock_page(newpage);\nout_unlock:\n\t/* Drop an anon_vma reference if we took one */\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\tunlock_page(page);\nout:\n\treturn rc;\n}\n\n/*\n * gcc 4.7 and 4.8 on arm get an ICEs when inlining unmap_and_move().  Work\n * around it.\n */\n#if (GCC_VERSION >= 40700 && GCC_VERSION < 40900) && defined(CONFIG_ARM)\n#define ICE_noinline noinline\n#else\n#define ICE_noinline\n#endif\n\n/*\n * Obtain the lock on page, remove all ptes and migrate the page\n * to the newly allocated page in newpage.\n */\nstatic ICE_noinline int unmap_and_move(new_page_t get_new_page,\n\t\t\t\t   free_page_t put_new_page,\n\t\t\t\t   unsigned long private, struct page *page,\n\t\t\t\t   int force, enum migrate_mode mode,\n\t\t\t\t   enum migrate_reason reason)\n{\n\tint rc = MIGRATEPAGE_SUCCESS;\n\tint *result = NULL;\n\tstruct page *newpage;\n\n\tnewpage = get_new_page(page, private, &result);\n\tif (!newpage)\n\t\treturn -ENOMEM;\n\n\tif (page_count(page) == 1) {\n\t\t/* page was freed from under us. So we are done. */\n\t\tgoto out;\n\t}\n\n\tif (unlikely(PageTransHuge(page)))\n\t\tif (unlikely(split_huge_page(page)))\n\t\t\tgoto out;\n\n\trc = __unmap_and_move(page, newpage, force, mode);\n\tif (rc == MIGRATEPAGE_SUCCESS)\n\t\tput_new_page = NULL;\n\nout:\n\tif (rc != -EAGAIN) {\n\t\t/*\n\t\t * A page that has been migrated has all references\n\t\t * removed and will be freed. A page that has not been\n\t\t * migrated will have kepts its references and be\n\t\t * restored.\n\t\t */\n\t\tlist_del(&page->lru);\n\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\tpage_is_file_cache(page));\n\t\t/* Soft-offlined page shouldn't go through lru cache list */\n\t\tif (reason == MR_MEMORY_FAILURE) {\n\t\t\tput_page(page);\n\t\t\tif (!test_set_page_hwpoison(page))\n\t\t\t\tnum_poisoned_pages_inc();\n\t\t} else\n\t\t\tputback_lru_page(page);\n\t}\n\n\t/*\n\t * If migration was not successful and there's a freeing callback, use\n\t * it.  Otherwise, putback_lru_page() will drop the reference grabbed\n\t * during isolation.\n\t */\n\tif (put_new_page)\n\t\tput_new_page(newpage, private);\n\telse if (unlikely(__is_movable_balloon_page(newpage))) {\n\t\t/* drop our reference, page already in the balloon */\n\t\tput_page(newpage);\n\t} else\n\t\tputback_lru_page(newpage);\n\n\tif (result) {\n\t\tif (rc)\n\t\t\t*result = rc;\n\t\telse\n\t\t\t*result = page_to_nid(newpage);\n\t}\n\treturn rc;\n}\n\n/*\n * Counterpart of unmap_and_move_page() for hugepage migration.\n *\n * This function doesn't wait the completion of hugepage I/O\n * because there is no race between I/O and migration for hugepage.\n * Note that currently hugepage I/O occurs only in direct I/O\n * where no lock is held and PG_writeback is irrelevant,\n * and writeback status of all subpages are counted in the reference\n * count of the head page (i.e. if all subpages of a 2MB hugepage are\n * under direct I/O, the reference of the head page is 512 and a bit more.)\n * This means that when we try to migrate hugepage whose subpages are\n * doing direct I/O, some references remain after try_to_unmap() and\n * hugepage migration fails without data corruption.\n *\n * There is also no race when direct I/O is issued on the page under migration,\n * because then pte is replaced with migration swap entry and direct I/O code\n * will wait in the page fault for migration to complete.\n */\nstatic int unmap_and_move_huge_page(new_page_t get_new_page,\n\t\t\t\tfree_page_t put_new_page, unsigned long private,\n\t\t\t\tstruct page *hpage, int force,\n\t\t\t\tenum migrate_mode mode)\n{\n\tint rc = -EAGAIN;\n\tint *result = NULL;\n\tint page_was_mapped = 0;\n\tstruct page *new_hpage;\n\tstruct anon_vma *anon_vma = NULL;\n\n\t/*\n\t * Movability of hugepages depends on architectures and hugepage size.\n\t * This check is necessary because some callers of hugepage migration\n\t * like soft offline and memory hotremove don't walk through page\n\t * tables or check whether the hugepage is pmd-based or not before\n\t * kicking migration.\n\t */\n\tif (!hugepage_migration_supported(page_hstate(hpage))) {\n\t\tputback_active_hugepage(hpage);\n\t\treturn -ENOSYS;\n\t}\n\n\tnew_hpage = get_new_page(hpage, private, &result);\n\tif (!new_hpage)\n\t\treturn -ENOMEM;\n\n\tif (!trylock_page(hpage)) {\n\t\tif (!force || mode != MIGRATE_SYNC)\n\t\t\tgoto out;\n\t\tlock_page(hpage);\n\t}\n\n\tif (PageAnon(hpage))\n\t\tanon_vma = page_get_anon_vma(hpage);\n\n\tif (unlikely(!trylock_page(new_hpage)))\n\t\tgoto put_anon;\n\n\tif (page_mapped(hpage)) {\n\t\ttry_to_unmap(hpage,\n\t\t\tTTU_MIGRATION|TTU_IGNORE_MLOCK|TTU_IGNORE_ACCESS);\n\t\tpage_was_mapped = 1;\n\t}\n\n\tif (!page_mapped(hpage))\n\t\trc = move_to_new_page(new_hpage, hpage, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(hpage,\n\t\t\trc == MIGRATEPAGE_SUCCESS ? new_hpage : hpage);\n\n\tunlock_page(new_hpage);\n\nput_anon:\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\n\tif (rc == MIGRATEPAGE_SUCCESS) {\n\t\thugetlb_cgroup_migrate(hpage, new_hpage);\n\t\tput_new_page = NULL;\n\t}\n\n\tunlock_page(hpage);\nout:\n\tif (rc != -EAGAIN)\n\t\tputback_active_hugepage(hpage);\n\n\t/*\n\t * If migration was not successful and there's a freeing callback, use\n\t * it.  Otherwise, put_page() will drop the reference grabbed during\n\t * isolation.\n\t */\n\tif (put_new_page)\n\t\tput_new_page(new_hpage, private);\n\telse\n\t\tputback_active_hugepage(new_hpage);\n\n\tif (result) {\n\t\tif (rc)\n\t\t\t*result = rc;\n\t\telse\n\t\t\t*result = page_to_nid(new_hpage);\n\t}\n\treturn rc;\n}\n\n/*\n * migrate_pages - migrate the pages specified in a list, to the free pages\n *\t\t   supplied as the target for the page migration\n *\n * @from:\t\tThe list of pages to be migrated.\n * @get_new_page:\tThe function used to allocate free pages to be used\n *\t\t\tas the target of the page migration.\n * @put_new_page:\tThe function used to free target pages if migration\n *\t\t\tfails, or NULL if no special handling is necessary.\n * @private:\t\tPrivate data to be passed on to get_new_page()\n * @mode:\t\tThe migration mode that specifies the constraints for\n *\t\t\tpage migration, if any.\n * @reason:\t\tThe reason for page migration.\n *\n * The function returns after 10 attempts or if no pages are movable any more\n * because the list has become empty or no retryable pages exist any more.\n * The caller should call putback_movable_pages() to return pages to the LRU\n * or free list only if ret != 0.\n *\n * Returns the number of pages that were not migrated, or an error code.\n */\nint migrate_pages(struct list_head *from, new_page_t get_new_page,\n\t\tfree_page_t put_new_page, unsigned long private,\n\t\tenum migrate_mode mode, int reason)\n{\n\tint retry = 1;\n\tint nr_failed = 0;\n\tint nr_succeeded = 0;\n\tint pass = 0;\n\tstruct page *page;\n\tstruct page *page2;\n\tint swapwrite = current->flags & PF_SWAPWRITE;\n\tint rc;\n\n\tif (!swapwrite)\n\t\tcurrent->flags |= PF_SWAPWRITE;\n\n\tfor(pass = 0; pass < 10 && retry; pass++) {\n\t\tretry = 0;\n\n\t\tlist_for_each_entry_safe(page, page2, from, lru) {\n\t\t\tcond_resched();\n\n\t\t\tif (PageHuge(page))\n\t\t\t\trc = unmap_and_move_huge_page(get_new_page,\n\t\t\t\t\t\tput_new_page, private, page,\n\t\t\t\t\t\tpass > 2, mode);\n\t\t\telse\n\t\t\t\trc = unmap_and_move(get_new_page, put_new_page,\n\t\t\t\t\t\tprivate, page, pass > 2, mode,\n\t\t\t\t\t\treason);\n\n\t\t\tswitch(rc) {\n\t\t\tcase -ENOMEM:\n\t\t\t\tgoto out;\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tbreak;\n\t\t\tcase MIGRATEPAGE_SUCCESS:\n\t\t\t\tnr_succeeded++;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/*\n\t\t\t\t * Permanent failure (-EBUSY, -ENOSYS, etc.):\n\t\t\t\t * unlike -EAGAIN case, the failed page is\n\t\t\t\t * removed from migration page list and not\n\t\t\t\t * retried in the next outer loop.\n\t\t\t\t */\n\t\t\t\tnr_failed++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tnr_failed += retry;\n\trc = nr_failed;\nout:\n\tif (nr_succeeded)\n\t\tcount_vm_events(PGMIGRATE_SUCCESS, nr_succeeded);\n\tif (nr_failed)\n\t\tcount_vm_events(PGMIGRATE_FAIL, nr_failed);\n\ttrace_mm_migrate_pages(nr_succeeded, nr_failed, mode, reason);\n\n\tif (!swapwrite)\n\t\tcurrent->flags &= ~PF_SWAPWRITE;\n\n\treturn rc;\n}\n\n#ifdef CONFIG_NUMA\n/*\n * Move a list of individual pages\n */\nstruct page_to_node {\n\tunsigned long addr;\n\tstruct page *page;\n\tint node;\n\tint status;\n};\n\nstatic struct page *new_page_node(struct page *p, unsigned long private,\n\t\tint **result)\n{\n\tstruct page_to_node *pm = (struct page_to_node *)private;\n\n\twhile (pm->node != MAX_NUMNODES && pm->page != p)\n\t\tpm++;\n\n\tif (pm->node == MAX_NUMNODES)\n\t\treturn NULL;\n\n\t*result = &pm->status;\n\n\tif (PageHuge(p))\n\t\treturn alloc_huge_page_node(page_hstate(compound_head(p)),\n\t\t\t\t\tpm->node);\n\telse\n\t\treturn __alloc_pages_node(pm->node,\n\t\t\t\tGFP_HIGHUSER_MOVABLE | __GFP_THISNODE, 0);\n}\n\n/*\n * Move a set of pages as indicated in the pm array. The addr\n * field must be set to the virtual address of the page to be moved\n * and the node number must contain a valid target node.\n * The pm array ends with node = MAX_NUMNODES.\n */\nstatic int do_move_page_to_node_array(struct mm_struct *mm,\n\t\t\t\t      struct page_to_node *pm,\n\t\t\t\t      int migrate_all)\n{\n\tint err;\n\tstruct page_to_node *pp;\n\tLIST_HEAD(pagelist);\n\n\tdown_read(&mm->mmap_sem);\n\n\t/*\n\t * Build a list of pages to migrate\n\t */\n\tfor (pp = pm; pp->node != MAX_NUMNODES; pp++) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct page *page;\n\n\t\terr = -EFAULT;\n\t\tvma = find_vma(mm, pp->addr);\n\t\tif (!vma || pp->addr < vma->vm_start || !vma_migratable(vma))\n\t\t\tgoto set_status;\n\n\t\t/* FOLL_DUMP to ignore special (like zero) pages */\n\t\tpage = follow_page(vma, pp->addr,\n\t\t\t\tFOLL_GET | FOLL_SPLIT | FOLL_DUMP);\n\n\t\terr = PTR_ERR(page);\n\t\tif (IS_ERR(page))\n\t\t\tgoto set_status;\n\n\t\terr = -ENOENT;\n\t\tif (!page)\n\t\t\tgoto set_status;\n\n\t\tpp->page = page;\n\t\terr = page_to_nid(page);\n\n\t\tif (err == pp->node)\n\t\t\t/*\n\t\t\t * Node already in the right place\n\t\t\t */\n\t\t\tgoto put_and_set;\n\n\t\terr = -EACCES;\n\t\tif (page_mapcount(page) > 1 &&\n\t\t\t\t!migrate_all)\n\t\t\tgoto put_and_set;\n\n\t\tif (PageHuge(page)) {\n\t\t\tif (PageHead(page))\n\t\t\t\tisolate_huge_page(page, &pagelist);\n\t\t\tgoto put_and_set;\n\t\t}\n\n\t\terr = isolate_lru_page(page);\n\t\tif (!err) {\n\t\t\tlist_add_tail(&page->lru, &pagelist);\n\t\t\tinc_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\t    page_is_file_cache(page));\n\t\t}\nput_and_set:\n\t\t/*\n\t\t * Either remove the duplicate refcount from\n\t\t * isolate_lru_page() or drop the page ref if it was\n\t\t * not isolated.\n\t\t */\n\t\tput_page(page);\nset_status:\n\t\tpp->status = err;\n\t}\n\n\terr = 0;\n\tif (!list_empty(&pagelist)) {\n\t\terr = migrate_pages(&pagelist, new_page_node, NULL,\n\t\t\t\t(unsigned long)pm, MIGRATE_SYNC, MR_SYSCALL);\n\t\tif (err)\n\t\t\tputback_movable_pages(&pagelist);\n\t}\n\n\tup_read(&mm->mmap_sem);\n\treturn err;\n}\n\n/*\n * Migrate an array of page address onto an array of nodes and fill\n * the corresponding array of status.\n */\nstatic int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n\t\t\t unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t const int __user *nodes,\n\t\t\t int __user *status, int flags)\n{\n\tstruct page_to_node *pm;\n\tunsigned long chunk_nr_pages;\n\tunsigned long chunk_start;\n\tint err;\n\n\terr = -ENOMEM;\n\tpm = (struct page_to_node *)__get_free_page(GFP_KERNEL);\n\tif (!pm)\n\t\tgoto out;\n\n\tmigrate_prep();\n\n\t/*\n\t * Store a chunk of page_to_node array in a page,\n\t * but keep the last one as a marker\n\t */\n\tchunk_nr_pages = (PAGE_SIZE / sizeof(struct page_to_node)) - 1;\n\n\tfor (chunk_start = 0;\n\t     chunk_start < nr_pages;\n\t     chunk_start += chunk_nr_pages) {\n\t\tint j;\n\n\t\tif (chunk_start + chunk_nr_pages > nr_pages)\n\t\t\tchunk_nr_pages = nr_pages - chunk_start;\n\n\t\t/* fill the chunk pm with addrs and nodes from user-space */\n\t\tfor (j = 0; j < chunk_nr_pages; j++) {\n\t\t\tconst void __user *p;\n\t\t\tint node;\n\n\t\t\terr = -EFAULT;\n\t\t\tif (get_user(p, pages + j + chunk_start))\n\t\t\t\tgoto out_pm;\n\t\t\tpm[j].addr = (unsigned long) p;\n\n\t\t\tif (get_user(node, nodes + j + chunk_start))\n\t\t\t\tgoto out_pm;\n\n\t\t\terr = -ENODEV;\n\t\t\tif (node < 0 || node >= MAX_NUMNODES)\n\t\t\t\tgoto out_pm;\n\n\t\t\tif (!node_state(node, N_MEMORY))\n\t\t\t\tgoto out_pm;\n\n\t\t\terr = -EACCES;\n\t\t\tif (!node_isset(node, task_nodes))\n\t\t\t\tgoto out_pm;\n\n\t\t\tpm[j].node = node;\n\t\t}\n\n\t\t/* End marker for this chunk */\n\t\tpm[chunk_nr_pages].node = MAX_NUMNODES;\n\n\t\t/* Migrate this chunk */\n\t\terr = do_move_page_to_node_array(mm, pm,\n\t\t\t\t\t\t flags & MPOL_MF_MOVE_ALL);\n\t\tif (err < 0)\n\t\t\tgoto out_pm;\n\n\t\t/* Return status information */\n\t\tfor (j = 0; j < chunk_nr_pages; j++)\n\t\t\tif (put_user(pm[j].status, status + j + chunk_start)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto out_pm;\n\t\t\t}\n\t}\n\terr = 0;\n\nout_pm:\n\tfree_page((unsigned long)pm);\nout:\n\treturn err;\n}\n\n/*\n * Determine the nodes of an array of pages and store it in an array of status.\n */\nstatic void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t\tconst void __user **pages, int *status)\n{\n\tunsigned long i;\n\n\tdown_read(&mm->mmap_sem);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long addr = (unsigned long)(*pages);\n\t\tstruct vm_area_struct *vma;\n\t\tstruct page *page;\n\t\tint err = -EFAULT;\n\n\t\tvma = find_vma(mm, addr);\n\t\tif (!vma || addr < vma->vm_start)\n\t\t\tgoto set_status;\n\n\t\t/* FOLL_DUMP to ignore special (like zero) pages */\n\t\tpage = follow_page(vma, addr, FOLL_DUMP);\n\n\t\terr = PTR_ERR(page);\n\t\tif (IS_ERR(page))\n\t\t\tgoto set_status;\n\n\t\terr = page ? page_to_nid(page) : -ENOENT;\nset_status:\n\t\t*status = err;\n\n\t\tpages++;\n\t\tstatus++;\n\t}\n\n\tup_read(&mm->mmap_sem);\n}\n\n/*\n * Determine the nodes of a user array of pages and store it in\n * a user array of status.\n */\nstatic int do_pages_stat(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t int __user *status)\n{\n#define DO_PAGES_STAT_CHUNK_NR 16\n\tconst void __user *chunk_pages[DO_PAGES_STAT_CHUNK_NR];\n\tint chunk_status[DO_PAGES_STAT_CHUNK_NR];\n\n\twhile (nr_pages) {\n\t\tunsigned long chunk_nr;\n\n\t\tchunk_nr = nr_pages;\n\t\tif (chunk_nr > DO_PAGES_STAT_CHUNK_NR)\n\t\t\tchunk_nr = DO_PAGES_STAT_CHUNK_NR;\n\n\t\tif (copy_from_user(chunk_pages, pages, chunk_nr * sizeof(*chunk_pages)))\n\t\t\tbreak;\n\n\t\tdo_pages_stat_array(mm, chunk_nr, chunk_pages, chunk_status);\n\n\t\tif (copy_to_user(status, chunk_status, chunk_nr * sizeof(*status)))\n\t\t\tbreak;\n\n\t\tpages += chunk_nr;\n\t\tstatus += chunk_nr;\n\t\tnr_pages -= chunk_nr;\n\t}\n\treturn nr_pages ? -EFAULT : 0;\n}\n\n/*\n * Move a list of pages in the address space of the currently executing\n * process.\n */\nSYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,\n\t\tconst void __user * __user *, pages,\n\t\tconst int __user *, nodes,\n\t\tint __user *, status, int, flags)\n{\n\tconst struct cred *cred = current_cred(), *tcred;\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\tint err;\n\tnodemask_t task_nodes;\n\n\t/* Check flags */\n\tif (flags & ~(MPOL_MF_MOVE|MPOL_MF_MOVE_ALL))\n\t\treturn -EINVAL;\n\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\t/* Find the mm_struct */\n\trcu_read_lock();\n\ttask = pid ? find_task_by_vpid(pid) : current;\n\tif (!task) {\n\t\trcu_read_unlock();\n\t\treturn -ESRCH;\n\t}\n\tget_task_struct(task);\n\n\t/*\n\t * Check if this process has the right to modify the specified\n\t * process. The right exists if the process has administrative\n\t * capabilities, superuser privileges or the same\n\t * userid as the target process.\n\t */\n\ttcred = __task_cred(task);\n\tif (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&\n\t    !capable(CAP_SYS_NICE)) {\n\t\trcu_read_unlock();\n\t\terr = -EPERM;\n\t\tgoto out;\n\t}\n\trcu_read_unlock();\n\n \terr = security_task_movememory(task);\n \tif (err)\n\t\tgoto out;\n\n\ttask_nodes = cpuset_mems_allowed(task);\n\tmm = get_task_mm(task);\n\tput_task_struct(task);\n\n\tif (!mm)\n\t\treturn -EINVAL;\n\n\tif (nodes)\n\t\terr = do_pages_move(mm, task_nodes, nr_pages, pages,\n\t\t\t\t    nodes, status, flags);\n\telse\n\t\terr = do_pages_stat(mm, nr_pages, pages, status);\n\n\tmmput(mm);\n\treturn err;\n\nout:\n\tput_task_struct(task);\n\treturn err;\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns true if this is a safe migration target node for misplaced NUMA\n * pages. Currently it only checks the watermarks which crude\n */\nstatic bool migrate_balanced_pgdat(struct pglist_data *pgdat,\n\t\t\t\t   unsigned long nr_migrate_pages)\n{\n\tint z;\n\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\tstruct zone *zone = pgdat->node_zones + z;\n\n\t\tif (!populated_zone(zone))\n\t\t\tcontinue;\n\n\t\tif (!zone_reclaimable(zone))\n\t\t\tcontinue;\n\n\t\t/* Avoid waking kswapd by allocating pages_to_migrate pages. */\n\t\tif (!zone_watermark_ok(zone, 0,\n\t\t\t\t       high_wmark_pages(zone) +\n\t\t\t\t       nr_migrate_pages,\n\t\t\t\t       0, 0))\n\t\t\tcontinue;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct page *alloc_misplaced_dst_page(struct page *page,\n\t\t\t\t\t   unsigned long data,\n\t\t\t\t\t   int **result)\n{\n\tint nid = (int) data;\n\tstruct page *newpage;\n\n\tnewpage = __alloc_pages_node(nid,\n\t\t\t\t\t (GFP_HIGHUSER_MOVABLE |\n\t\t\t\t\t  __GFP_THISNODE | __GFP_NOMEMALLOC |\n\t\t\t\t\t  __GFP_NORETRY | __GFP_NOWARN) &\n\t\t\t\t\t ~GFP_IOFS, 0);\n\n\treturn newpage;\n}\n\n/*\n * page migration rate limiting control.\n * Do not migrate more than @pages_to_migrate in a @migrate_interval_millisecs\n * window of time. Default here says do not migrate more than 1280M per second.\n */\nstatic unsigned int migrate_interval_millisecs __read_mostly = 100;\nstatic unsigned int ratelimit_pages __read_mostly = 128 << (20 - PAGE_SHIFT);\n\n/* Returns true if the node is migrate rate-limited after the update */\nstatic bool numamigrate_update_ratelimit(pg_data_t *pgdat,\n\t\t\t\t\tunsigned long nr_pages)\n{\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (time_after(jiffies, pgdat->numabalancing_migrate_next_window)) {\n\t\tspin_lock(&pgdat->numabalancing_migrate_lock);\n\t\tpgdat->numabalancing_migrate_nr_pages = 0;\n\t\tpgdat->numabalancing_migrate_next_window = jiffies +\n\t\t\tmsecs_to_jiffies(migrate_interval_millisecs);\n\t\tspin_unlock(&pgdat->numabalancing_migrate_lock);\n\t}\n\tif (pgdat->numabalancing_migrate_nr_pages > ratelimit_pages) {\n\t\ttrace_mm_numa_migrate_ratelimit(current, pgdat->node_id,\n\t\t\t\t\t\t\t\tnr_pages);\n\t\treturn true;\n\t}\n\n\t/*\n\t * This is an unlocked non-atomic update so errors are possible.\n\t * The consequences are failing to migrate when we potentiall should\n\t * have which is not severe enough to warrant locking. If it is ever\n\t * a problem, it can be converted to a per-cpu counter.\n\t */\n\tpgdat->numabalancing_migrate_nr_pages += nr_pages;\n\treturn false;\n}\n\nstatic int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)\n{\n\tint page_lru;\n\n\tVM_BUG_ON_PAGE(compound_order(page) && !PageTransHuge(page), page);\n\n\t/* Avoid migrating to a node that is nearly full */\n\tif (!migrate_balanced_pgdat(pgdat, 1UL << compound_order(page)))\n\t\treturn 0;\n\n\tif (isolate_lru_page(page))\n\t\treturn 0;\n\n\t/*\n\t * migrate_misplaced_transhuge_page() skips page migration's usual\n\t * check on page_count(), so we must do it here, now that the page\n\t * has been isolated: a GUP pin, or any other pin, prevents migration.\n\t * The expected page count is 3: 1 for page's mapcount and 1 for the\n\t * caller's pin and 1 for the reference taken by isolate_lru_page().\n\t */\n\tif (PageTransHuge(page) && page_count(page) != 3) {\n\t\tputback_lru_page(page);\n\t\treturn 0;\n\t}\n\n\tpage_lru = page_is_file_cache(page);\n\tmod_zone_page_state(page_zone(page), NR_ISOLATED_ANON + page_lru,\n\t\t\t\thpage_nr_pages(page));\n\n\t/*\n\t * Isolating the page has taken another reference, so the\n\t * caller's reference can be safely dropped without the page\n\t * disappearing underneath us during migration.\n\t */\n\tput_page(page);\n\treturn 1;\n}\n\nbool pmd_trans_migrating(pmd_t pmd)\n{\n\tstruct page *page = pmd_page(pmd);\n\treturn PageLocked(page);\n}\n\n/*\n * Attempt to migrate a misplaced page to the specified destination\n * node. Caller is expected to have an elevated reference count on\n * the page that will be dropped by this function before returning.\n */\nint migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,\n\t\t\t   int node)\n{\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint isolated;\n\tint nr_remaining;\n\tLIST_HEAD(migratepages);\n\n\t/*\n\t * Don't migrate file pages that are mapped in multiple processes\n\t * with execute permissions as they are probably shared libraries.\n\t */\n\tif (page_mapcount(page) != 1 && page_is_file_cache(page) &&\n\t    (vma->vm_flags & VM_EXEC))\n\t\tgoto out;\n\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (numamigrate_update_ratelimit(pgdat, 1))\n\t\tgoto out;\n\n\tisolated = numamigrate_isolate_page(pgdat, page);\n\tif (!isolated)\n\t\tgoto out;\n\n\tlist_add(&page->lru, &migratepages);\n\tnr_remaining = migrate_pages(&migratepages, alloc_misplaced_dst_page,\n\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n\t\t\t\t     MR_NUMA_MISPLACED);\n\tif (nr_remaining) {\n\t\tif (!list_empty(&migratepages)) {\n\t\t\tlist_del(&page->lru);\n\t\t\tdec_zone_page_state(page, NR_ISOLATED_ANON +\n\t\t\t\t\tpage_is_file_cache(page));\n\t\t\tputback_lru_page(page);\n\t\t}\n\t\tisolated = 0;\n\t} else\n\t\tcount_vm_numa_event(NUMA_PAGE_MIGRATE);\n\tBUG_ON(!list_empty(&migratepages));\n\treturn isolated;\n\nout:\n\tput_page(page);\n\treturn 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#if defined(CONFIG_NUMA_BALANCING) && defined(CONFIG_TRANSPARENT_HUGEPAGE)\n/*\n * Migrates a THP to a given target node. page must be locked and is unlocked\n * before returning.\n */\nint migrate_misplaced_transhuge_page(struct mm_struct *mm,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tpmd_t *pmd, pmd_t entry,\n\t\t\t\tunsigned long address,\n\t\t\t\tstruct page *page, int node)\n{\n\tspinlock_t *ptl;\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint isolated = 0;\n\tstruct page *new_page = NULL;\n\tint page_lru = page_is_file_cache(page);\n\tunsigned long mmun_start = address & HPAGE_PMD_MASK;\n\tunsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;\n\tpmd_t orig_entry;\n\n\t/*\n\t * Rate-limit the amount of data that is being migrated to a node.\n\t * Optimal placement is no good if the memory bus is saturated and\n\t * all the time is being spent migrating!\n\t */\n\tif (numamigrate_update_ratelimit(pgdat, HPAGE_PMD_NR))\n\t\tgoto out_dropref;\n\n\tnew_page = alloc_pages_node(node,\n\t\t(GFP_TRANSHUGE | __GFP_THISNODE) & ~__GFP_WAIT,\n\t\tHPAGE_PMD_ORDER);\n\tif (!new_page)\n\t\tgoto out_fail;\n\n\tisolated = numamigrate_isolate_page(pgdat, page);\n\tif (!isolated) {\n\t\tput_page(new_page);\n\t\tgoto out_fail;\n\t}\n\n\tif (mm_tlb_flush_pending(mm))\n\t\tflush_tlb_range(vma, mmun_start, mmun_end);\n\n\t/* Prepare a page as a migration target */\n\t__set_page_locked(new_page);\n\tSetPageSwapBacked(new_page);\n\n\t/* anon mapping, we can simply copy page->mapping to the new page: */\n\tnew_page->mapping = page->mapping;\n\tnew_page->index = page->index;\n\tmigrate_page_copy(new_page, page);\n\tWARN_ON(PageLRU(new_page));\n\n\t/* Recheck the target PMD */\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\tptl = pmd_lock(mm, pmd);\n\tif (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {\nfail_putback:\n\t\tspin_unlock(ptl);\n\t\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\n\t\t/* Reverse changes made by migrate_page_copy() */\n\t\tif (TestClearPageActive(new_page))\n\t\t\tSetPageActive(page);\n\t\tif (TestClearPageUnevictable(new_page))\n\t\t\tSetPageUnevictable(page);\n\n\t\tunlock_page(new_page);\n\t\tput_page(new_page);\t\t/* Free it */\n\n\t\t/* Retake the callers reference and putback on LRU */\n\t\tget_page(page);\n\t\tputback_lru_page(page);\n\t\tmod_zone_page_state(page_zone(page),\n\t\t\t NR_ISOLATED_ANON + page_lru, -HPAGE_PMD_NR);\n\n\t\tgoto out_unlock;\n\t}\n\n\torig_entry = *pmd;\n\tentry = mk_pmd(new_page, vma->vm_page_prot);\n\tentry = pmd_mkhuge(entry);\n\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\n\t/*\n\t * Clear the old entry under pagetable lock and establish the new PTE.\n\t * Any parallel GUP will either observe the old page blocking on the\n\t * page lock, block on the page table lock or observe the new page.\n\t * The SetPageUptodate on the new page and page_add_new_anon_rmap\n\t * guarantee the copy is visible before the pagetable update.\n\t */\n\tflush_cache_range(vma, mmun_start, mmun_end);\n\tpage_add_anon_rmap(new_page, vma, mmun_start);\n\tpmdp_huge_clear_flush_notify(vma, mmun_start, pmd);\n\tset_pmd_at(mm, mmun_start, pmd, entry);\n\tflush_tlb_range(vma, mmun_start, mmun_end);\n\tupdate_mmu_cache_pmd(vma, address, &entry);\n\n\tif (page_count(page) != 2) {\n\t\tset_pmd_at(mm, mmun_start, pmd, orig_entry);\n\t\tflush_tlb_range(vma, mmun_start, mmun_end);\n\t\tmmu_notifier_invalidate_range(mm, mmun_start, mmun_end);\n\t\tupdate_mmu_cache_pmd(vma, address, &entry);\n\t\tpage_remove_rmap(new_page);\n\t\tgoto fail_putback;\n\t}\n\n\tmlock_migrate_page(new_page, page);\n\tset_page_memcg(new_page, page_memcg(page));\n\tset_page_memcg(page, NULL);\n\tpage_remove_rmap(page);\n\n\tspin_unlock(ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\n\t/* Take an \"isolate\" reference and put new page on the LRU. */\n\tget_page(new_page);\n\tputback_lru_page(new_page);\n\n\tunlock_page(new_page);\n\tunlock_page(page);\n\tput_page(page);\t\t\t/* Drop the rmap reference */\n\tput_page(page);\t\t\t/* Drop the LRU isolation reference */\n\n\tcount_vm_events(PGMIGRATE_SUCCESS, HPAGE_PMD_NR);\n\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, HPAGE_PMD_NR);\n\n\tmod_zone_page_state(page_zone(page),\n\t\t\tNR_ISOLATED_ANON + page_lru,\n\t\t\t-HPAGE_PMD_NR);\n\treturn isolated;\n\nout_fail:\n\tcount_vm_events(PGMIGRATE_FAIL, HPAGE_PMD_NR);\nout_dropref:\n\tptl = pmd_lock(mm, pmd);\n\tif (pmd_same(*pmd, entry)) {\n\t\tentry = pmd_modify(entry, vma->vm_page_prot);\n\t\tset_pmd_at(mm, mmun_start, pmd, entry);\n\t\tupdate_mmu_cache_pmd(vma, address, &entry);\n\t}\n\tspin_unlock(ptl);\n\nout_unlock:\n\tunlock_page(page);\n\tput_page(page);\n\treturn 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n\n#endif /* CONFIG_NUMA */\n", "code_before": "// SPDX-License-Identifier: GPL-2.0\n/*\n * Memory Migration functionality - linux/mm/migrate.c\n *\n * Copyright (C) 2006 Silicon Graphics, Inc., Christoph Lameter\n *\n * Page migration was first developed in the context of the memory hotplug\n * project. The main authors of the migration code are:\n *\n * IWAMOTO Toshihiro <iwamoto@valinux.co.jp>\n * Hirokazu Takahashi <taka@valinux.co.jp>\n * Dave Hansen <haveblue@us.ibm.com>\n * Christoph Lameter\n */\n\n#include <linux/migrate.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/buffer_head.h>\n#include <linux/mm_inline.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/topology.h>\n#include <linux/cpu.h>\n#include <linux/cpuset.h>\n#include <linux/writeback.h>\n#include <linux/mempolicy.h>\n#include <linux/vmalloc.h>\n#include <linux/security.h>\n#include <linux/backing-dev.h>\n#include <linux/compaction.h>\n#include <linux/syscalls.h>\n#include <linux/compat.h>\n#include <linux/hugetlb.h>\n#include <linux/gfp.h>\n#include <linux/page_idle.h>\n#include <linux/page_owner.h>\n#include <linux/sched/mm.h>\n#include <linux/ptrace.h>\n#include <linux/memory.h>\n#include <linux/sched/sysctl.h>\n#include <linux/memory-tiers.h>\n#include <linux/pagewalk.h>\n\n#include <asm/tlbflush.h>\n\n#include <trace/events/migrate.h>\n\n#include \"internal.h\"\n#include \"swap.h\"\n\nstatic const struct movable_operations *offline_movable_ops;\nstatic const struct movable_operations *zsmalloc_movable_ops;\n\nint set_movable_ops(const struct movable_operations *ops, enum pagetype type)\n{\n\t/*\n\t * We only allow for selected types and don't handle concurrent\n\t * registration attempts yet.\n\t */\n\tswitch (type) {\n\tcase PGTY_offline:\n\t\tif (offline_movable_ops && ops)\n\t\t\treturn -EBUSY;\n\t\toffline_movable_ops = ops;\n\t\tbreak;\n\tcase PGTY_zsmalloc:\n\t\tif (zsmalloc_movable_ops && ops)\n\t\t\treturn -EBUSY;\n\t\tzsmalloc_movable_ops = ops;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(set_movable_ops);\n\nstatic const struct movable_operations *page_movable_ops(struct page *page)\n{\n\tVM_WARN_ON_ONCE_PAGE(!page_has_movable_ops(page), page);\n\n\t/*\n\t * If we enable page migration for a page of a certain type by marking\n\t * it as movable, the page type must be sticky until the page gets freed\n\t * back to the buddy.\n\t */\n\tif (PageOffline(page))\n\t\t/* Only balloon compaction sets PageOffline pages movable. */\n\t\treturn offline_movable_ops;\n\tif (PageZsmalloc(page))\n\t\treturn zsmalloc_movable_ops;\n\n\treturn NULL;\n}\n\n/**\n * isolate_movable_ops_page - isolate a movable_ops page for migration\n * @page: The page.\n * @mode: The isolation mode.\n *\n * Try to isolate a movable_ops page for migration. Will fail if the page is\n * not a movable_ops page, if the page is already isolated for migration\n * or if the page was just was released by its owner.\n *\n * Once isolated, the page cannot get freed until it is either putback\n * or migrated.\n *\n * Returns true if isolation succeeded, otherwise false.\n */\nbool isolate_movable_ops_page(struct page *page, isolate_mode_t mode)\n{\n\t/*\n\t * TODO: these pages will not be folios in the future. All\n\t * folio dependencies will have to be removed.\n\t */\n\tstruct folio *folio = folio_get_nontail_page(page);\n\tconst struct movable_operations *mops;\n\n\t/*\n\t * Avoid burning cycles with pages that are yet under __free_pages(),\n\t * or just got freed under us.\n\t *\n\t * In case we 'win' a race for a movable page being freed under us and\n\t * raise its refcount preventing __free_pages() from doing its job\n\t * the put_page() at the end of this block will take care of\n\t * release this page, thus avoiding a nasty leakage.\n\t */\n\tif (!folio)\n\t\tgoto out;\n\n\t/*\n\t * Check for movable_ops pages before taking the page lock because\n\t * we use non-atomic bitops on newly allocated page flags so\n\t * unconditionally grabbing the lock ruins page's owner side.\n\t *\n\t * Note that once a page has movable_ops, it will stay that way\n\t * until the page was freed.\n\t */\n\tif (unlikely(!page_has_movable_ops(page)))\n\t\tgoto out_putfolio;\n\n\t/*\n\t * As movable pages are not isolated from LRU lists, concurrent\n\t * compaction threads can race against page migration functions\n\t * as well as race against the releasing a page.\n\t *\n\t * In order to avoid having an already isolated movable page\n\t * being (wrongly) re-isolated while it is under migration,\n\t * or to avoid attempting to isolate pages being released,\n\t * lets be sure we have the page lock\n\t * before proceeding with the movable page isolation steps.\n\t */\n\tif (unlikely(!folio_trylock(folio)))\n\t\tgoto out_putfolio;\n\n\tVM_WARN_ON_ONCE_PAGE(!page_has_movable_ops(page), page);\n\tif (PageMovableOpsIsolated(page))\n\t\tgoto out_no_isolated;\n\n\tmops = page_movable_ops(page);\n\tif (WARN_ON_ONCE(!mops))\n\t\tgoto out_no_isolated;\n\n\tif (!mops->isolate_page(page, mode))\n\t\tgoto out_no_isolated;\n\n\t/* Driver shouldn't use the isolated flag */\n\tVM_WARN_ON_ONCE_PAGE(PageMovableOpsIsolated(page), page);\n\tSetPageMovableOpsIsolated(page);\n\tfolio_unlock(folio);\n\n\treturn true;\n\nout_no_isolated:\n\tfolio_unlock(folio);\nout_putfolio:\n\tfolio_put(folio);\nout:\n\treturn false;\n}\n\n/**\n * putback_movable_ops_page - putback an isolated movable_ops page\n * @page: The isolated page.\n *\n * Putback an isolated movable_ops page.\n *\n * After the page was putback, it might get freed instantly.\n */\nstatic void putback_movable_ops_page(struct page *page)\n{\n\t/*\n\t * TODO: these pages will not be folios in the future. All\n\t * folio dependencies will have to be removed.\n\t */\n\tstruct folio *folio = page_folio(page);\n\n\tVM_WARN_ON_ONCE_PAGE(!page_has_movable_ops(page), page);\n\tVM_WARN_ON_ONCE_PAGE(!PageMovableOpsIsolated(page), page);\n\tfolio_lock(folio);\n\tpage_movable_ops(page)->putback_page(page);\n\tClearPageMovableOpsIsolated(page);\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n}\n\n/**\n * migrate_movable_ops_page - migrate an isolated movable_ops page\n * @dst: The destination page.\n * @src: The source page.\n * @mode: The migration mode.\n *\n * Migrate an isolated movable_ops page.\n *\n * If the src page was already released by its owner, the src page is\n * un-isolated (putback) and migration succeeds; the migration core will be the\n * owner of both pages.\n *\n * If the src page was not released by its owner and the migration was\n * successful, the owner of the src page and the dst page are swapped and\n * the src page is un-isolated.\n *\n * If migration fails, the ownership stays unmodified and the src page\n * remains isolated: migration may be retried later or the page can be putback.\n *\n * TODO: migration core will treat both pages as folios and lock them before\n * this call to unlock them after this call. Further, the folio refcounts on\n * src and dst are also released by migration core. These pages will not be\n * folios in the future, so that must be reworked.\n *\n * Returns 0 on success, otherwise a negative error code.\n */\nstatic int migrate_movable_ops_page(struct page *dst, struct page *src,\n\t\tenum migrate_mode mode)\n{\n\tint rc;\n\n\tVM_WARN_ON_ONCE_PAGE(!page_has_movable_ops(src), src);\n\tVM_WARN_ON_ONCE_PAGE(!PageMovableOpsIsolated(src), src);\n\trc = page_movable_ops(src)->migrate_page(dst, src, mode);\n\tif (!rc)\n\t\tClearPageMovableOpsIsolated(src);\n\treturn rc;\n}\n\n/*\n * Put previously isolated pages back onto the appropriate lists\n * from where they were once taken off for compaction/migration.\n *\n * This function shall be used whenever the isolated pageset has been\n * built from lru, balloon, hugetlbfs page. See isolate_migratepages_range()\n * and folio_isolate_hugetlb().\n */\nvoid putback_movable_pages(struct list_head *l)\n{\n\tstruct folio *folio;\n\tstruct folio *folio2;\n\n\tlist_for_each_entry_safe(folio, folio2, l, lru) {\n\t\tif (unlikely(folio_test_hugetlb(folio))) {\n\t\t\tfolio_putback_hugetlb(folio);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del(&folio->lru);\n\t\tif (unlikely(page_has_movable_ops(&folio->page))) {\n\t\t\tputback_movable_ops_page(&folio->page);\n\t\t} else {\n\t\t\tnode_stat_mod_folio(folio, NR_ISOLATED_ANON +\n\t\t\t\t\tfolio_is_file_lru(folio), -folio_nr_pages(folio));\n\t\t\tfolio_putback_lru(folio);\n\t\t}\n\t}\n}\n\n/* Must be called with an elevated refcount on the non-hugetlb folio */\nbool isolate_folio_to_list(struct folio *folio, struct list_head *list)\n{\n\tif (folio_test_hugetlb(folio))\n\t\treturn folio_isolate_hugetlb(folio, list);\n\n\tif (page_has_movable_ops(&folio->page)) {\n\t\tif (!isolate_movable_ops_page(&folio->page,\n\t\t\t\t\t      ISOLATE_UNEVICTABLE))\n\t\t\treturn false;\n\t} else {\n\t\tif (!folio_isolate_lru(folio))\n\t\t\treturn false;\n\t\tnode_stat_add_folio(folio, NR_ISOLATED_ANON +\n\t\t\t\t    folio_is_file_lru(folio));\n\t}\n\tlist_add(&folio->lru, list);\n\treturn true;\n}\n\nstatic bool try_to_map_unused_to_zeropage(struct page_vma_mapped_walk *pvmw,\n\t\t\t\t\t  struct folio *folio,\n\t\t\t\t\t  unsigned long idx)\n{\n\tstruct page *page = folio_page(folio, idx);\n\tpte_t newpte;\n\n\tif (PageCompound(page))\n\t\treturn false;\n\tVM_BUG_ON_PAGE(!PageAnon(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON_PAGE(pte_present(ptep_get(pvmw->pte)), page);\n\n\tif (folio_test_mlocked(folio) || (pvmw->vma->vm_flags & VM_LOCKED) ||\n\t    mm_forbids_zeropage(pvmw->vma->vm_mm))\n\t\treturn false;\n\n\t/*\n\t * The pmd entry mapping the old thp was flushed and the pte mapping\n\t * this subpage has been non present. If the subpage is only zero-filled\n\t * then map it to the shared zeropage.\n\t */\n\tif (!pages_identical(page, ZERO_PAGE(0)))\n\t\treturn false;\n\n\tnewpte = pte_mkspecial(pfn_pte(my_zero_pfn(pvmw->address),\n\t\t\t\t\tpvmw->vma->vm_page_prot));\n\tset_pte_at(pvmw->vma->vm_mm, pvmw->address, pvmw->pte, newpte);\n\n\tdec_mm_counter(pvmw->vma->vm_mm, mm_counter(folio));\n\treturn true;\n}\n\nstruct rmap_walk_arg {\n\tstruct folio *folio;\n\tbool map_unused_to_zeropage;\n};\n\n/*\n * Restore a potential migration pte to a working pte entry\n */\nstatic bool remove_migration_pte(struct folio *folio,\n\t\tstruct vm_area_struct *vma, unsigned long addr, void *arg)\n{\n\tstruct rmap_walk_arg *rmap_walk_arg = arg;\n\tDEFINE_FOLIO_VMA_WALK(pvmw, rmap_walk_arg->folio, vma, addr, PVMW_SYNC | PVMW_MIGRATION);\n\n\twhile (page_vma_mapped_walk(&pvmw)) {\n\t\trmap_t rmap_flags = RMAP_NONE;\n\t\tpte_t old_pte;\n\t\tpte_t pte;\n\t\tswp_entry_t entry;\n\t\tstruct page *new;\n\t\tunsigned long idx = 0;\n\n\t\t/* pgoff is invalid for ksm pages, but they are never large */\n\t\tif (folio_test_large(folio) && !folio_test_hugetlb(folio))\n\t\t\tidx = linear_page_index(vma, pvmw.address) - pvmw.pgoff;\n\t\tnew = folio_page(folio, idx);\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\t/* PMD-mapped THP migration entry */\n\t\tif (!pvmw.pte) {\n\t\t\tVM_BUG_ON_FOLIO(folio_test_hugetlb(folio) ||\n\t\t\t\t\t!folio_test_pmd_mappable(folio), folio);\n\t\t\tremove_migration_pmd(&pvmw, new);\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tif (rmap_walk_arg->map_unused_to_zeropage &&\n\t\t    try_to_map_unused_to_zeropage(&pvmw, folio, idx))\n\t\t\tcontinue;\n\n\t\tfolio_get(folio);\n\t\tpte = mk_pte(new, READ_ONCE(vma->vm_page_prot));\n\t\told_pte = ptep_get(pvmw.pte);\n\n\t\tentry = pte_to_swp_entry(old_pte);\n\t\tif (!is_migration_entry_young(entry))\n\t\t\tpte = pte_mkold(pte);\n\t\tif (folio_test_dirty(folio) && is_migration_entry_dirty(entry))\n\t\t\tpte = pte_mkdirty(pte);\n\t\tif (pte_swp_soft_dirty(old_pte))\n\t\t\tpte = pte_mksoft_dirty(pte);\n\t\telse\n\t\t\tpte = pte_clear_soft_dirty(pte);\n\n\t\tif (is_writable_migration_entry(entry))\n\t\t\tpte = pte_mkwrite(pte, vma);\n\t\telse if (pte_swp_uffd_wp(old_pte))\n\t\t\tpte = pte_mkuffd_wp(pte);\n\n\t\tif (folio_test_anon(folio) && !is_readable_migration_entry(entry))\n\t\t\trmap_flags |= RMAP_EXCLUSIVE;\n\n\t\tif (unlikely(is_device_private_page(new))) {\n\t\t\tif (pte_write(pte))\n\t\t\t\tentry = make_writable_device_private_entry(\n\t\t\t\t\t\t\tpage_to_pfn(new));\n\t\t\telse\n\t\t\t\tentry = make_readable_device_private_entry(\n\t\t\t\t\t\t\tpage_to_pfn(new));\n\t\t\tpte = swp_entry_to_pte(entry);\n\t\t\tif (pte_swp_soft_dirty(old_pte))\n\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n\t\t\tif (pte_swp_uffd_wp(old_pte))\n\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n\t\t}\n\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tif (folio_test_hugetlb(folio)) {\n\t\t\tstruct hstate *h = hstate_vma(vma);\n\t\t\tunsigned int shift = huge_page_shift(h);\n\t\t\tunsigned long psize = huge_page_size(h);\n\n\t\t\tpte = arch_make_huge_pte(pte, shift, vma->vm_flags);\n\t\t\tif (folio_test_anon(folio))\n\t\t\t\thugetlb_add_anon_rmap(folio, vma, pvmw.address,\n\t\t\t\t\t\t      rmap_flags);\n\t\t\telse\n\t\t\t\thugetlb_add_file_rmap(folio);\n\t\t\tset_huge_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte,\n\t\t\t\t\tpsize);\n\t\t} else\n#endif\n\t\t{\n\t\t\tif (folio_test_anon(folio))\n\t\t\t\tfolio_add_anon_rmap_pte(folio, new, vma,\n\t\t\t\t\t\t\tpvmw.address, rmap_flags);\n\t\t\telse\n\t\t\t\tfolio_add_file_rmap_pte(folio, new, vma);\n\t\t\tset_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);\n\t\t}\n\t\tif (READ_ONCE(vma->vm_flags) & VM_LOCKED)\n\t\t\tmlock_drain_local();\n\n\t\ttrace_remove_migration_pte(pvmw.address, pte_val(pte),\n\t\t\t\t\t   compound_order(new));\n\n\t\t/* No need to invalidate - it was non-present before */\n\t\tupdate_mmu_cache(vma, pvmw.address, pvmw.pte);\n\t}\n\n\treturn true;\n}\n\n/*\n * Get rid of all migration entries and replace them by\n * references to the indicated page.\n */\nvoid remove_migration_ptes(struct folio *src, struct folio *dst, int flags)\n{\n\tstruct rmap_walk_arg rmap_walk_arg = {\n\t\t.folio = src,\n\t\t.map_unused_to_zeropage = flags & RMP_USE_SHARED_ZEROPAGE,\n\t};\n\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = remove_migration_pte,\n\t\t.arg = &rmap_walk_arg,\n\t};\n\n\tVM_BUG_ON_FOLIO((flags & RMP_USE_SHARED_ZEROPAGE) && (src != dst), src);\n\n\tif (flags & RMP_LOCKED)\n\t\trmap_walk_locked(dst, &rwc);\n\telse\n\t\trmap_walk(dst, &rwc);\n}\n\n/*\n * Something used the pte of a page under migration. We need to\n * get to the page and wait until migration is finished.\n * When we return from this function the fault will be retried.\n */\nvoid migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t  unsigned long address)\n{\n\tspinlock_t *ptl;\n\tpte_t *ptep;\n\tpte_t pte;\n\tswp_entry_t entry;\n\n\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tif (!ptep)\n\t\treturn;\n\n\tpte = ptep_get(ptep);\n\tpte_unmap(ptep);\n\n\tif (!is_swap_pte(pte))\n\t\tgoto out;\n\n\tentry = pte_to_swp_entry(pte);\n\tif (!is_migration_entry(entry))\n\t\tgoto out;\n\n\tmigration_entry_wait_on_locked(entry, ptl);\n\treturn;\nout:\n\tspin_unlock(ptl);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * The vma read lock must be held upon entry. Holding that lock prevents either\n * the pte or the ptl from being freed.\n *\n * This function will release the vma lock before returning.\n */\nvoid migration_entry_wait_huge(struct vm_area_struct *vma, unsigned long addr, pte_t *ptep)\n{\n\tspinlock_t *ptl = huge_pte_lockptr(hstate_vma(vma), vma->vm_mm, ptep);\n\tpte_t pte;\n\n\thugetlb_vma_assert_locked(vma);\n\tspin_lock(ptl);\n\tpte = huge_ptep_get(vma->vm_mm, addr, ptep);\n\n\tif (unlikely(!is_hugetlb_entry_migration(pte))) {\n\t\tspin_unlock(ptl);\n\t\thugetlb_vma_unlock_read(vma);\n\t} else {\n\t\t/*\n\t\t * If migration entry existed, safe to release vma lock\n\t\t * here because the pgtable page won't be freed without the\n\t\t * pgtable lock released.  See comment right above pgtable\n\t\t * lock release in migration_entry_wait_on_locked().\n\t\t */\n\t\thugetlb_vma_unlock_read(vma);\n\t\tmigration_entry_wait_on_locked(pte_to_swp_entry(pte), ptl);\n\t}\n}\n#endif\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\nvoid pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)\n{\n\tspinlock_t *ptl;\n\n\tptl = pmd_lock(mm, pmd);\n\tif (!is_pmd_migration_entry(*pmd))\n\t\tgoto unlock;\n\tmigration_entry_wait_on_locked(pmd_to_swp_entry(*pmd), ptl);\n\treturn;\nunlock:\n\tspin_unlock(ptl);\n}\n#endif\n\n/*\n * Replace the folio in the mapping.\n *\n * The number of remaining references must be:\n * 1 for anonymous folios without a mapping\n * 2 for folios with a mapping\n * 3 for folios with a mapping and the private flag set.\n */\nstatic int __folio_migrate_mapping(struct address_space *mapping,\n\t\tstruct folio *newfolio, struct folio *folio, int expected_count)\n{\n\tXA_STATE(xas, &mapping->i_pages, folio_index(folio));\n\tstruct swap_cluster_info *ci = NULL;\n\tstruct zone *oldzone, *newzone;\n\tint dirty;\n\tlong nr = folio_nr_pages(folio);\n\n\tif (!mapping) {\n\t\t/* Take off deferred split queue while frozen and memcg set */\n\t\tif (folio_test_large(folio) &&\n\t\t    folio_test_large_rmappable(folio)) {\n\t\t\tif (!folio_ref_freeze(folio, expected_count))\n\t\t\t\treturn -EAGAIN;\n\t\t\tfolio_unqueue_deferred_split(folio);\n\t\t\tfolio_ref_unfreeze(folio, expected_count);\n\t\t}\n\n\t\t/* No turning back from here */\n\t\tnewfolio->index = folio->index;\n\t\tnewfolio->mapping = folio->mapping;\n\t\tif (folio_test_anon(folio) && folio_test_large(folio))\n\t\t\tmod_mthp_stat(folio_order(folio), MTHP_STAT_NR_ANON, 1);\n\t\tif (folio_test_swapbacked(folio))\n\t\t\t__folio_set_swapbacked(newfolio);\n\n\t\treturn 0;\n\t}\n\n\toldzone = folio_zone(folio);\n\tnewzone = folio_zone(newfolio);\n\n\tif (folio_test_swapcache(folio))\n\t\tci = swap_cluster_get_and_lock_irq(folio);\n\telse\n\t\txas_lock_irq(&xas);\n\n\tif (!folio_ref_freeze(folio, expected_count)) {\n\t\tif (ci)\n\t\t\tswap_cluster_unlock_irq(ci);\n\t\telse\n\t\t\txas_unlock_irq(&xas);\n\t\treturn -EAGAIN;\n\t}\n\n\t/* Take off deferred split queue while frozen and memcg set */\n\tfolio_unqueue_deferred_split(folio);\n\n\t/*\n\t * Now we know that no one else is looking at the folio:\n\t * no turning back from here.\n\t */\n\tnewfolio->index = folio->index;\n\tnewfolio->mapping = folio->mapping;\n\tif (folio_test_anon(folio) && folio_test_large(folio))\n\t\tmod_mthp_stat(folio_order(folio), MTHP_STAT_NR_ANON, 1);\n\tfolio_ref_add(newfolio, nr); /* add cache reference */\n\tif (folio_test_swapbacked(folio))\n\t\t__folio_set_swapbacked(newfolio);\n\tif (folio_test_swapcache(folio)) {\n\t\tfolio_set_swapcache(newfolio);\n\t\tnewfolio->private = folio_get_private(folio);\n\t}\n\n\t/* Move dirty while folio refs frozen and newfolio not yet exposed */\n\tdirty = folio_test_dirty(folio);\n\tif (dirty) {\n\t\tfolio_clear_dirty(folio);\n\t\tfolio_set_dirty(newfolio);\n\t}\n\n\tif (folio_test_swapcache(folio))\n\t\t__swap_cache_replace_folio(ci, folio, newfolio);\n\telse\n\t\txas_store(&xas, newfolio);\n\n\t/*\n\t * Drop cache reference from old folio by unfreezing\n\t * to one less reference.\n\t * We know this isn't the last reference.\n\t */\n\tfolio_ref_unfreeze(folio, expected_count - nr);\n\n\t/* Leave irq disabled to prevent preemption while updating stats */\n\tif (ci)\n\t\tswap_cluster_unlock(ci);\n\telse\n\t\txas_unlock(&xas);\n\n\t/*\n\t * If moved to a different zone then also account\n\t * the folio for that zone. Other VM counters will be\n\t * taken care of when we establish references to the\n\t * new folio and drop references to the old folio.\n\t *\n\t * Note that anonymous folios are accounted for\n\t * via NR_FILE_PAGES and NR_ANON_MAPPED if they\n\t * are mapped to swap space.\n\t */\n\tif (newzone != oldzone) {\n\t\tstruct lruvec *old_lruvec, *new_lruvec;\n\t\tstruct mem_cgroup *memcg;\n\n\t\tmemcg = folio_memcg(folio);\n\t\told_lruvec = mem_cgroup_lruvec(memcg, oldzone->zone_pgdat);\n\t\tnew_lruvec = mem_cgroup_lruvec(memcg, newzone->zone_pgdat);\n\n\t\t__mod_lruvec_state(old_lruvec, NR_FILE_PAGES, -nr);\n\t\t__mod_lruvec_state(new_lruvec, NR_FILE_PAGES, nr);\n\t\tif (folio_test_swapbacked(folio) && !folio_test_swapcache(folio)) {\n\t\t\t__mod_lruvec_state(old_lruvec, NR_SHMEM, -nr);\n\t\t\t__mod_lruvec_state(new_lruvec, NR_SHMEM, nr);\n\n\t\t\tif (folio_test_pmd_mappable(folio)) {\n\t\t\t\t__mod_lruvec_state(old_lruvec, NR_SHMEM_THPS, -nr);\n\t\t\t\t__mod_lruvec_state(new_lruvec, NR_SHMEM_THPS, nr);\n\t\t\t}\n\t\t}\n#ifdef CONFIG_SWAP\n\t\tif (folio_test_swapcache(folio)) {\n\t\t\t__mod_lruvec_state(old_lruvec, NR_SWAPCACHE, -nr);\n\t\t\t__mod_lruvec_state(new_lruvec, NR_SWAPCACHE, nr);\n\t\t}\n#endif\n\t\tif (dirty && mapping_can_writeback(mapping)) {\n\t\t\t__mod_lruvec_state(old_lruvec, NR_FILE_DIRTY, -nr);\n\t\t\t__mod_zone_page_state(oldzone, NR_ZONE_WRITE_PENDING, -nr);\n\t\t\t__mod_lruvec_state(new_lruvec, NR_FILE_DIRTY, nr);\n\t\t\t__mod_zone_page_state(newzone, NR_ZONE_WRITE_PENDING, nr);\n\t\t}\n\t}\n\tlocal_irq_enable();\n\n\treturn 0;\n}\n\nint folio_migrate_mapping(struct address_space *mapping,\n\t\tstruct folio *newfolio, struct folio *folio, int extra_count)\n{\n\tint expected_count = folio_expected_ref_count(folio) + extra_count + 1;\n\n\tif (folio_ref_count(folio) != expected_count)\n\t\treturn -EAGAIN;\n\n\treturn __folio_migrate_mapping(mapping, newfolio, folio, expected_count);\n}\nEXPORT_SYMBOL(folio_migrate_mapping);\n\n/*\n * The expected number of remaining references is the same as that\n * of folio_migrate_mapping().\n */\nint migrate_huge_page_move_mapping(struct address_space *mapping,\n\t\t\t\t   struct folio *dst, struct folio *src)\n{\n\tXA_STATE(xas, &mapping->i_pages, folio_index(src));\n\tint rc, expected_count = folio_expected_ref_count(src) + 1;\n\n\tif (folio_ref_count(src) != expected_count)\n\t\treturn -EAGAIN;\n\n\trc = folio_mc_copy(dst, src);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\txas_lock_irq(&xas);\n\tif (!folio_ref_freeze(src, expected_count)) {\n\t\txas_unlock_irq(&xas);\n\t\treturn -EAGAIN;\n\t}\n\n\tdst->index = src->index;\n\tdst->mapping = src->mapping;\n\n\tfolio_ref_add(dst, folio_nr_pages(dst));\n\n\txas_store(&xas, dst);\n\n\tfolio_ref_unfreeze(src, expected_count - folio_nr_pages(src));\n\n\txas_unlock_irq(&xas);\n\n\treturn 0;\n}\n\n/*\n * Copy the flags and some other ancillary information\n */\nvoid folio_migrate_flags(struct folio *newfolio, struct folio *folio)\n{\n\tint cpupid;\n\n\tif (folio_test_referenced(folio))\n\t\tfolio_set_referenced(newfolio);\n\tif (folio_test_uptodate(folio))\n\t\tfolio_mark_uptodate(newfolio);\n\tif (folio_test_clear_active(folio)) {\n\t\tVM_BUG_ON_FOLIO(folio_test_unevictable(folio), folio);\n\t\tfolio_set_active(newfolio);\n\t} else if (folio_test_clear_unevictable(folio))\n\t\tfolio_set_unevictable(newfolio);\n\tif (folio_test_workingset(folio))\n\t\tfolio_set_workingset(newfolio);\n\tif (folio_test_checked(folio))\n\t\tfolio_set_checked(newfolio);\n\t/*\n\t * PG_anon_exclusive (-> PG_mappedtodisk) is always migrated via\n\t * migration entries. We can still have PG_anon_exclusive set on an\n\t * effectively unmapped and unreferenced first sub-pages of an\n\t * anonymous THP: we can simply copy it here via PG_mappedtodisk.\n\t */\n\tif (folio_test_mappedtodisk(folio))\n\t\tfolio_set_mappedtodisk(newfolio);\n\n\t/* Move dirty on pages not done by folio_migrate_mapping() */\n\tif (folio_test_dirty(folio))\n\t\tfolio_set_dirty(newfolio);\n\n\tif (folio_test_young(folio))\n\t\tfolio_set_young(newfolio);\n\tif (folio_test_idle(folio))\n\t\tfolio_set_idle(newfolio);\n\n\tfolio_migrate_refs(newfolio, folio);\n\t/*\n\t * Copy NUMA information to the new page, to prevent over-eager\n\t * future migrations of this same page.\n\t */\n\tcpupid = folio_xchg_last_cpupid(folio, -1);\n\t/*\n\t * For memory tiering mode, when migrate between slow and fast\n\t * memory node, reset cpupid, because that is used to record\n\t * page access time in slow memory node.\n\t */\n\tif (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) {\n\t\tbool f_toptier = node_is_toptier(folio_nid(folio));\n\t\tbool t_toptier = node_is_toptier(folio_nid(newfolio));\n\n\t\tif (f_toptier != t_toptier)\n\t\t\tcpupid = -1;\n\t}\n\tfolio_xchg_last_cpupid(newfolio, cpupid);\n\n\tfolio_migrate_ksm(newfolio, folio);\n\t/*\n\t * Please do not reorder this without considering how mm/ksm.c's\n\t * ksm_get_folio() depends upon ksm_migrate_page() and the\n\t * swapcache flag.\n\t */\n\tif (folio_test_swapcache(folio))\n\t\tfolio_clear_swapcache(folio);\n\tfolio_clear_private(folio);\n\n\t/* page->private contains hugetlb specific flags */\n\tif (!folio_test_hugetlb(folio))\n\t\tfolio->private = NULL;\n\n\t/*\n\t * If any waiters have accumulated on the new page then\n\t * wake them up.\n\t */\n\tif (folio_test_writeback(newfolio))\n\t\tfolio_end_writeback(newfolio);\n\n\t/*\n\t * PG_readahead shares the same bit with PG_reclaim.  The above\n\t * end_page_writeback() may clear PG_readahead mistakenly, so set the\n\t * bit after that.\n\t */\n\tif (folio_test_readahead(folio))\n\t\tfolio_set_readahead(newfolio);\n\n\tfolio_copy_owner(newfolio, folio);\n\tpgalloc_tag_swap(newfolio, folio);\n\n\tmem_cgroup_migrate(folio, newfolio);\n}\nEXPORT_SYMBOL(folio_migrate_flags);\n\n/************************************************************\n *                    Migration functions\n ***********************************************************/\n\nstatic int __migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\t\t   struct folio *src, void *src_private,\n\t\t\t   enum migrate_mode mode)\n{\n\tint rc, expected_count = folio_expected_ref_count(src) + 1;\n\n\t/* Check whether src does not have extra refs before we do more work */\n\tif (folio_ref_count(src) != expected_count)\n\t\treturn -EAGAIN;\n\n\trc = folio_mc_copy(dst, src);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\trc = __folio_migrate_mapping(mapping, dst, src, expected_count);\n\tif (rc)\n\t\treturn rc;\n\n\tif (src_private)\n\t\tfolio_attach_private(dst, folio_detach_private(src));\n\n\tfolio_migrate_flags(dst, src);\n\treturn 0;\n}\n\n/**\n * migrate_folio() - Simple folio migration.\n * @mapping: The address_space containing the folio.\n * @dst: The folio to migrate the data to.\n * @src: The folio containing the current data.\n * @mode: How to migrate the page.\n *\n * Common logic to directly migrate a single LRU folio suitable for\n * folios that do not have private data.\n *\n * Folios are locked upon entry and exit.\n */\nint migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\t  struct folio *src, enum migrate_mode mode)\n{\n\tBUG_ON(folio_test_writeback(src));\t/* Writeback must be complete */\n\treturn __migrate_folio(mapping, dst, src, NULL, mode);\n}\nEXPORT_SYMBOL(migrate_folio);\n\n#ifdef CONFIG_BUFFER_HEAD\n/* Returns true if all buffers are successfully locked */\nstatic bool buffer_migrate_lock_buffers(struct buffer_head *head,\n\t\t\t\t\t\t\tenum migrate_mode mode)\n{\n\tstruct buffer_head *bh = head;\n\tstruct buffer_head *failed_bh;\n\n\tdo {\n\t\tif (!trylock_buffer(bh)) {\n\t\t\tif (mode == MIGRATE_ASYNC)\n\t\t\t\tgoto unlock;\n\t\t\tif (mode == MIGRATE_SYNC_LIGHT && !buffer_uptodate(bh))\n\t\t\t\tgoto unlock;\n\t\t\tlock_buffer(bh);\n\t\t}\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\treturn true;\n\nunlock:\n\t/* We failed to lock the buffer and cannot stall. */\n\tfailed_bh = bh;\n\tbh = head;\n\twhile (bh != failed_bh) {\n\t\tunlock_buffer(bh);\n\t\tbh = bh->b_this_page;\n\t}\n\n\treturn false;\n}\n\nstatic int __buffer_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode,\n\t\tbool check_refs)\n{\n\tstruct buffer_head *bh, *head;\n\tint rc;\n\tint expected_count;\n\n\thead = folio_buffers(src);\n\tif (!head)\n\t\treturn migrate_folio(mapping, dst, src, mode);\n\n\t/* Check whether page does not have extra refs before we do more work */\n\texpected_count = folio_expected_ref_count(src) + 1;\n\tif (folio_ref_count(src) != expected_count)\n\t\treturn -EAGAIN;\n\n\tif (!buffer_migrate_lock_buffers(head, mode))\n\t\treturn -EAGAIN;\n\n\tif (check_refs) {\n\t\tbool busy, migrating;\n\t\tbool invalidated = false;\n\n\t\tmigrating = test_and_set_bit_lock(BH_Migrate, &head->b_state);\n\t\tVM_WARN_ON_ONCE(migrating);\nrecheck_buffers:\n\t\tbusy = false;\n\t\tspin_lock(&mapping->i_private_lock);\n\t\tbh = head;\n\t\tdo {\n\t\t\tif (atomic_read(&bh->b_count)) {\n\t\t\t\tbusy = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbh = bh->b_this_page;\n\t\t} while (bh != head);\n\t\tspin_unlock(&mapping->i_private_lock);\n\t\tif (busy) {\n\t\t\tif (invalidated) {\n\t\t\t\trc = -EAGAIN;\n\t\t\t\tgoto unlock_buffers;\n\t\t\t}\n\t\t\tinvalidate_bh_lrus();\n\t\t\tinvalidated = true;\n\t\t\tgoto recheck_buffers;\n\t\t}\n\t}\n\n\trc = filemap_migrate_folio(mapping, dst, src, mode);\n\tif (rc)\n\t\tgoto unlock_buffers;\n\n\tbh = head;\n\tdo {\n\t\tfolio_set_bh(bh, dst, bh_offset(bh));\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\nunlock_buffers:\n\tif (check_refs)\n\t\tclear_bit_unlock(BH_Migrate, &head->b_state);\n\tbh = head;\n\tdo {\n\t\tunlock_buffer(bh);\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\treturn rc;\n}\n\n/**\n * buffer_migrate_folio() - Migration function for folios with buffers.\n * @mapping: The address space containing @src.\n * @dst: The folio to migrate to.\n * @src: The folio to migrate from.\n * @mode: How to migrate the folio.\n *\n * This function can only be used if the underlying filesystem guarantees\n * that no other references to @src exist. For example attached buffer\n * heads are accessed only under the folio lock.  If your filesystem cannot\n * provide this guarantee, buffer_migrate_folio_norefs() may be more\n * appropriate.\n *\n * Return: 0 on success or a negative errno on failure.\n */\nint buffer_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\treturn __buffer_migrate_folio(mapping, dst, src, mode, false);\n}\nEXPORT_SYMBOL(buffer_migrate_folio);\n\n/**\n * buffer_migrate_folio_norefs() - Migration function for folios with buffers.\n * @mapping: The address space containing @src.\n * @dst: The folio to migrate to.\n * @src: The folio to migrate from.\n * @mode: How to migrate the folio.\n *\n * Like buffer_migrate_folio() except that this variant is more careful\n * and checks that there are also no buffer head references. This function\n * is the right one for mappings where buffer heads are directly looked\n * up and referenced (such as block device mappings).\n *\n * Return: 0 on success or a negative errno on failure.\n */\nint buffer_migrate_folio_norefs(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\treturn __buffer_migrate_folio(mapping, dst, src, mode, true);\n}\nEXPORT_SYMBOL_GPL(buffer_migrate_folio_norefs);\n#endif /* CONFIG_BUFFER_HEAD */\n\nint filemap_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\treturn __migrate_folio(mapping, dst, src, folio_get_private(src), mode);\n}\nEXPORT_SYMBOL_GPL(filemap_migrate_folio);\n\n/*\n * Default handling if a filesystem does not provide a migration function.\n */\nstatic int fallback_migrate_folio(struct address_space *mapping,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode mode)\n{\n\tWARN_ONCE(mapping->a_ops->writepages,\n\t\t\t\"%ps does not implement migrate_folio\\n\",\n\t\t\tmapping->a_ops);\n\tif (folio_test_dirty(src))\n\t\treturn -EBUSY;\n\n\t/*\n\t * Filesystem may have private data at folio->private that we\n\t * can't migrate automatically.\n\t */\n\tif (!filemap_release_folio(src, GFP_KERNEL))\n\t\treturn mode == MIGRATE_SYNC ? -EAGAIN : -EBUSY;\n\n\treturn migrate_folio(mapping, dst, src, mode);\n}\n\n/*\n * Move a src folio to a newly allocated dst folio.\n *\n * The src and dst folios are locked and the src folios was unmapped from\n * the page tables.\n *\n * On success, the src folio was replaced by the dst folio.\n *\n * Return value:\n *   < 0 - error code\n *     0 - success\n */\nstatic int move_to_new_folio(struct folio *dst, struct folio *src,\n\t\t\t\tenum migrate_mode mode)\n{\n\tstruct address_space *mapping = folio_mapping(src);\n\tint rc = -EAGAIN;\n\n\tVM_BUG_ON_FOLIO(!folio_test_locked(src), src);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(dst), dst);\n\n\tif (!mapping)\n\t\trc = migrate_folio(mapping, dst, src, mode);\n\telse if (mapping_inaccessible(mapping))\n\t\trc = -EOPNOTSUPP;\n\telse if (mapping->a_ops->migrate_folio)\n\t\t/*\n\t\t * Most folios have a mapping and most filesystems\n\t\t * provide a migrate_folio callback. Anonymous folios\n\t\t * are part of swap space which also has its own\n\t\t * migrate_folio callback. This is the most common path\n\t\t * for page migration.\n\t\t */\n\t\trc = mapping->a_ops->migrate_folio(mapping, dst, src,\n\t\t\t\t\t\t\tmode);\n\telse\n\t\trc = fallback_migrate_folio(mapping, dst, src, mode);\n\n\tif (!rc) {\n\t\t/*\n\t\t * For pagecache folios, src->mapping must be cleared before src\n\t\t * is freed. Anonymous folios must stay anonymous until freed.\n\t\t */\n\t\tif (!folio_test_anon(src))\n\t\t\tsrc->mapping = NULL;\n\n\t\tif (likely(!folio_is_zone_device(dst)))\n\t\t\tflush_dcache_folio(dst);\n\t}\n\treturn rc;\n}\n\n/*\n * To record some information during migration, we use unused private\n * field of struct folio of the newly allocated destination folio.\n * This is safe because nobody is using it except us.\n */\nenum {\n\tPAGE_WAS_MAPPED = BIT(0),\n\tPAGE_WAS_MLOCKED = BIT(1),\n\tPAGE_OLD_STATES = PAGE_WAS_MAPPED | PAGE_WAS_MLOCKED,\n};\n\nstatic void __migrate_folio_record(struct folio *dst,\n\t\t\t\t   int old_page_state,\n\t\t\t\t   struct anon_vma *anon_vma)\n{\n\tdst->private = (void *)anon_vma + old_page_state;\n}\n\nstatic void __migrate_folio_extract(struct folio *dst,\n\t\t\t\t   int *old_page_state,\n\t\t\t\t   struct anon_vma **anon_vmap)\n{\n\tunsigned long private = (unsigned long)dst->private;\n\n\t*anon_vmap = (struct anon_vma *)(private & ~PAGE_OLD_STATES);\n\t*old_page_state = private & PAGE_OLD_STATES;\n\tdst->private = NULL;\n}\n\n/* Restore the source folio to the original state upon failure */\nstatic void migrate_folio_undo_src(struct folio *src,\n\t\t\t\t   int page_was_mapped,\n\t\t\t\t   struct anon_vma *anon_vma,\n\t\t\t\t   bool locked,\n\t\t\t\t   struct list_head *ret)\n{\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(src, src, 0);\n\t/* Drop an anon_vma reference if we took one */\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\tif (locked)\n\t\tfolio_unlock(src);\n\tif (ret)\n\t\tlist_move_tail(&src->lru, ret);\n}\n\n/* Restore the destination folio to the original state upon failure */\nstatic void migrate_folio_undo_dst(struct folio *dst, bool locked,\n\t\tfree_folio_t put_new_folio, unsigned long private)\n{\n\tif (locked)\n\t\tfolio_unlock(dst);\n\tif (put_new_folio)\n\t\tput_new_folio(dst, private);\n\telse\n\t\tfolio_put(dst);\n}\n\n/* Cleanup src folio upon migration success */\nstatic void migrate_folio_done(struct folio *src,\n\t\t\t       enum migrate_reason reason)\n{\n\tif (likely(!page_has_movable_ops(&src->page)) && reason != MR_DEMOTION)\n\t\tmod_node_page_state(folio_pgdat(src), NR_ISOLATED_ANON +\n\t\t\t\t    folio_is_file_lru(src), -folio_nr_pages(src));\n\n\tif (reason != MR_MEMORY_FAILURE)\n\t\t/* We release the page in page_handle_poison. */\n\t\tfolio_put(src);\n}\n\n/* Obtain the lock on page, remove all ptes. */\nstatic int migrate_folio_unmap(new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tstruct folio *src, struct folio **dstp, enum migrate_mode mode,\n\t\tstruct list_head *ret)\n{\n\tstruct folio *dst;\n\tint rc = -EAGAIN;\n\tint old_page_state = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\tbool locked = false;\n\tbool dst_locked = false;\n\n\tdst = get_new_folio(src, private);\n\tif (!dst)\n\t\treturn -ENOMEM;\n\t*dstp = dst;\n\n\tdst->private = NULL;\n\n\tif (!folio_trylock(src)) {\n\t\tif (mode == MIGRATE_ASYNC)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * It's not safe for direct compaction to call lock_page.\n\t\t * For example, during page readahead pages are added locked\n\t\t * to the LRU. Later, when the IO completes the pages are\n\t\t * marked uptodate and unlocked. However, the queueing\n\t\t * could be merging multiple pages for one bio (e.g.\n\t\t * mpage_readahead). If an allocation happens for the\n\t\t * second or third page, the process can end up locking\n\t\t * the same page twice and deadlocking. Rather than\n\t\t * trying to be clever about what pages can be locked,\n\t\t * avoid the use of lock_page for direct compaction\n\t\t * altogether.\n\t\t */\n\t\tif (current->flags & PF_MEMALLOC)\n\t\t\tgoto out;\n\n\t\t/*\n\t\t * In \"light\" mode, we can wait for transient locks (eg\n\t\t * inserting a page into the page table), but it's not\n\t\t * worth waiting for I/O.\n\t\t */\n\t\tif (mode == MIGRATE_SYNC_LIGHT && !folio_test_uptodate(src))\n\t\t\tgoto out;\n\n\t\tfolio_lock(src);\n\t}\n\tlocked = true;\n\tif (folio_test_mlocked(src))\n\t\told_page_state |= PAGE_WAS_MLOCKED;\n\n\tif (folio_test_writeback(src)) {\n\t\t/*\n\t\t * Only in the case of a full synchronous migration is it\n\t\t * necessary to wait for PageWriteback. In the async case,\n\t\t * the retry loop is too short and in the sync-light case,\n\t\t * the overhead of stalling is too much\n\t\t */\n\t\tswitch (mode) {\n\t\tcase MIGRATE_SYNC:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trc = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t\tfolio_wait_writeback(src);\n\t}\n\n\t/*\n\t * By try_to_migrate(), src->mapcount goes down to 0 here. In this case,\n\t * we cannot notice that anon_vma is freed while we migrate a page.\n\t * This get_anon_vma() delays freeing anon_vma pointer until the end\n\t * of migration. File cache pages are no problem because of page_lock()\n\t * File Caches may use write_page() or lock_page() in migration, then,\n\t * just care Anon page here.\n\t *\n\t * Only folio_get_anon_vma() understands the subtleties of\n\t * getting a hold on an anon_vma from outside one of its mms.\n\t * But if we cannot get anon_vma, then we won't need it anyway,\n\t * because that implies that the anon page is no longer mapped\n\t * (and cannot be remapped so long as we hold the page lock).\n\t */\n\tif (folio_test_anon(src) && !folio_test_ksm(src))\n\t\tanon_vma = folio_get_anon_vma(src);\n\n\t/*\n\t * Block others from accessing the new page when we get around to\n\t * establishing additional references. We are usually the only one\n\t * holding a reference to dst at this point. We used to have a BUG\n\t * here if folio_trylock(dst) fails, but would like to allow for\n\t * cases where there might be a race with the previous use of dst.\n\t * This is much like races on refcount of oldpage: just don't BUG().\n\t */\n\tif (unlikely(!folio_trylock(dst)))\n\t\tgoto out;\n\tdst_locked = true;\n\n\tif (unlikely(page_has_movable_ops(&src->page))) {\n\t\t__migrate_folio_record(dst, old_page_state, anon_vma);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Corner case handling:\n\t * 1. When a new swap-cache page is read into, it is added to the LRU\n\t * and treated as swapcache but it has no rmap yet.\n\t * Calling try_to_unmap() against a src->mapping==NULL page will\n\t * trigger a BUG.  So handle it here.\n\t * 2. An orphaned page (see truncate_cleanup_page) might have\n\t * fs-private metadata. The page can be picked up due to memory\n\t * offlining.  Everywhere else except page reclaim, the page is\n\t * invisible to the vm, so the page can not be migrated.  So try to\n\t * free the metadata, so the page can be freed.\n\t */\n\tif (!src->mapping) {\n\t\tif (folio_test_private(src)) {\n\t\t\ttry_to_free_buffers(src);\n\t\t\tgoto out;\n\t\t}\n\t} else if (folio_mapped(src)) {\n\t\t/* Establish migration ptes */\n\t\tVM_BUG_ON_FOLIO(folio_test_anon(src) &&\n\t\t\t       !folio_test_ksm(src) && !anon_vma, src);\n\t\ttry_to_migrate(src, mode == MIGRATE_ASYNC ? TTU_BATCH_FLUSH : 0);\n\t\told_page_state |= PAGE_WAS_MAPPED;\n\t}\n\n\tif (!folio_mapped(src)) {\n\t\t__migrate_folio_record(dst, old_page_state, anon_vma);\n\t\treturn 0;\n\t}\n\nout:\n\t/*\n\t * A folio that has not been unmapped will be restored to\n\t * right list unless we want to retry.\n\t */\n\tif (rc == -EAGAIN)\n\t\tret = NULL;\n\n\tmigrate_folio_undo_src(src, old_page_state & PAGE_WAS_MAPPED,\n\t\t\t       anon_vma, locked, ret);\n\tmigrate_folio_undo_dst(dst, dst_locked, put_new_folio, private);\n\n\treturn rc;\n}\n\n/* Migrate the folio to the newly allocated folio in dst. */\nstatic int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n\t\t\t      struct folio *src, struct folio *dst,\n\t\t\t      enum migrate_mode mode, enum migrate_reason reason,\n\t\t\t      struct list_head *ret)\n{\n\tint rc;\n\tint old_page_state = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct list_head *prev;\n\n\t__migrate_folio_extract(dst, &old_page_state, &anon_vma);\n\tprev = dst->lru.prev;\n\tlist_del(&dst->lru);\n\n\tif (unlikely(page_has_movable_ops(&src->page))) {\n\t\trc = migrate_movable_ops_page(&dst->page, &src->page, mode);\n\t\tif (rc)\n\t\t\tgoto out;\n\t\tgoto out_unlock_both;\n\t}\n\n\trc = move_to_new_folio(dst, src, mode);\n\tif (rc)\n\t\tgoto out;\n\n\t/*\n\t * When successful, push dst to LRU immediately: so that if it\n\t * turns out to be an mlocked page, remove_migration_ptes() will\n\t * automatically build up the correct dst->mlock_count for it.\n\t *\n\t * We would like to do something similar for the old page, when\n\t * unsuccessful, and other cases when a page has been temporarily\n\t * isolated from the unevictable LRU: but this case is the easiest.\n\t */\n\tfolio_add_lru(dst);\n\tif (old_page_state & PAGE_WAS_MLOCKED)\n\t\tlru_add_drain();\n\n\tif (old_page_state & PAGE_WAS_MAPPED)\n\t\tremove_migration_ptes(src, dst, 0);\n\nout_unlock_both:\n\tfolio_unlock(dst);\n\tfolio_set_owner_migrate_reason(dst, reason);\n\t/*\n\t * If migration is successful, decrease refcount of dst,\n\t * which will not free the page because new page owner increased\n\t * refcounter.\n\t */\n\tfolio_put(dst);\n\n\t/*\n\t * A folio that has been migrated has all references removed\n\t * and will be freed.\n\t */\n\tlist_del(&src->lru);\n\t/* Drop an anon_vma reference if we took one */\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\tfolio_unlock(src);\n\tmigrate_folio_done(src, reason);\n\n\treturn rc;\nout:\n\t/*\n\t * A folio that has not been migrated will be restored to\n\t * right list unless we want to retry.\n\t */\n\tif (rc == -EAGAIN) {\n\t\tlist_add(&dst->lru, prev);\n\t\t__migrate_folio_record(dst, old_page_state, anon_vma);\n\t\treturn rc;\n\t}\n\n\tmigrate_folio_undo_src(src, old_page_state & PAGE_WAS_MAPPED,\n\t\t\t       anon_vma, true, ret);\n\tmigrate_folio_undo_dst(dst, true, put_new_folio, private);\n\n\treturn rc;\n}\n\n/*\n * Counterpart of unmap_and_move_page() for hugepage migration.\n *\n * This function doesn't wait the completion of hugepage I/O\n * because there is no race between I/O and migration for hugepage.\n * Note that currently hugepage I/O occurs only in direct I/O\n * where no lock is held and PG_writeback is irrelevant,\n * and writeback status of all subpages are counted in the reference\n * count of the head page (i.e. if all subpages of a 2MB hugepage are\n * under direct I/O, the reference of the head page is 512 and a bit more.)\n * This means that when we try to migrate hugepage whose subpages are\n * doing direct I/O, some references remain after try_to_unmap() and\n * hugepage migration fails without data corruption.\n *\n * There is also no race when direct I/O is issued on the page under migration,\n * because then pte is replaced with migration swap entry and direct I/O code\n * will wait in the page fault for migration to complete.\n */\nstatic int unmap_and_move_huge_page(new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tstruct folio *src, int force, enum migrate_mode mode,\n\t\tint reason, struct list_head *ret)\n{\n\tstruct folio *dst;\n\tint rc = -EAGAIN;\n\tint page_was_mapped = 0;\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct address_space *mapping = NULL;\n\n\tif (folio_ref_count(src) == 1) {\n\t\t/* page was freed from under us. So we are done. */\n\t\tfolio_putback_hugetlb(src);\n\t\treturn 0;\n\t}\n\n\tdst = get_new_folio(src, private);\n\tif (!dst)\n\t\treturn -ENOMEM;\n\n\tif (!folio_trylock(src)) {\n\t\tif (!force)\n\t\t\tgoto out;\n\t\tswitch (mode) {\n\t\tcase MIGRATE_SYNC:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto out;\n\t\t}\n\t\tfolio_lock(src);\n\t}\n\n\t/*\n\t * Check for pages which are in the process of being freed.  Without\n\t * folio_mapping() set, hugetlbfs specific move page routine will not\n\t * be called and we could leak usage counts for subpools.\n\t */\n\tif (hugetlb_folio_subpool(src) && !folio_mapping(src)) {\n\t\trc = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tif (folio_test_anon(src))\n\t\tanon_vma = folio_get_anon_vma(src);\n\n\tif (unlikely(!folio_trylock(dst)))\n\t\tgoto put_anon;\n\n\tif (folio_mapped(src)) {\n\t\tenum ttu_flags ttu = 0;\n\n\t\tif (!folio_test_anon(src)) {\n\t\t\t/*\n\t\t\t * In shared mappings, try_to_unmap could potentially\n\t\t\t * call huge_pmd_unshare.  Because of this, take\n\t\t\t * semaphore in write mode here and set TTU_RMAP_LOCKED\n\t\t\t * to let lower levels know we have taken the lock.\n\t\t\t */\n\t\t\tmapping = hugetlb_folio_mapping_lock_write(src);\n\t\t\tif (unlikely(!mapping))\n\t\t\t\tgoto unlock_put_anon;\n\n\t\t\tttu = TTU_RMAP_LOCKED;\n\t\t}\n\n\t\ttry_to_migrate(src, ttu);\n\t\tpage_was_mapped = 1;\n\n\t\tif (ttu & TTU_RMAP_LOCKED)\n\t\t\ti_mmap_unlock_write(mapping);\n\t}\n\n\tif (!folio_mapped(src))\n\t\trc = move_to_new_folio(dst, src, mode);\n\n\tif (page_was_mapped)\n\t\tremove_migration_ptes(src, !rc ? dst : src, 0);\n\nunlock_put_anon:\n\tfolio_unlock(dst);\n\nput_anon:\n\tif (anon_vma)\n\t\tput_anon_vma(anon_vma);\n\n\tif (!rc) {\n\t\tmove_hugetlb_state(src, dst, reason);\n\t\tput_new_folio = NULL;\n\t}\n\nout_unlock:\n\tfolio_unlock(src);\nout:\n\tif (!rc)\n\t\tfolio_putback_hugetlb(src);\n\telse if (rc != -EAGAIN)\n\t\tlist_move_tail(&src->lru, ret);\n\n\t/*\n\t * If migration was not successful and there's a freeing callback,\n\t * return the folio to that special allocator. Otherwise, simply drop\n\t * our additional reference.\n\t */\n\tif (put_new_folio)\n\t\tput_new_folio(dst, private);\n\telse\n\t\tfolio_put(dst);\n\n\treturn rc;\n}\n\nstatic inline int try_split_folio(struct folio *folio, struct list_head *split_folios,\n\t\t\t\t  enum migrate_mode mode)\n{\n\tint rc;\n\n\tif (mode == MIGRATE_ASYNC) {\n\t\tif (!folio_trylock(folio))\n\t\t\treturn -EAGAIN;\n\t} else {\n\t\tfolio_lock(folio);\n\t}\n\trc = split_folio_to_list(folio, split_folios);\n\tfolio_unlock(folio);\n\tif (!rc)\n\t\tlist_move_tail(&folio->lru, split_folios);\n\n\treturn rc;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define NR_MAX_BATCHED_MIGRATION\tHPAGE_PMD_NR\n#else\n#define NR_MAX_BATCHED_MIGRATION\t512\n#endif\n#define NR_MAX_MIGRATE_PAGES_RETRY\t10\n#define NR_MAX_MIGRATE_ASYNC_RETRY\t3\n#define NR_MAX_MIGRATE_SYNC_RETRY\t\t\t\t\t\\\n\t(NR_MAX_MIGRATE_PAGES_RETRY - NR_MAX_MIGRATE_ASYNC_RETRY)\n\nstruct migrate_pages_stats {\n\tint nr_succeeded;\t/* Normal and large folios migrated successfully, in\n\t\t\t\t   units of base pages */\n\tint nr_failed_pages;\t/* Normal and large folios failed to be migrated, in\n\t\t\t\t   units of base pages.  Untried folios aren't counted */\n\tint nr_thp_succeeded;\t/* THP migrated successfully */\n\tint nr_thp_failed;\t/* THP failed to be migrated */\n\tint nr_thp_split;\t/* THP split before migrating */\n\tint nr_split;\t/* Large folio (include THP) split before migrating */\n};\n\n/*\n * Returns the number of hugetlb folios that were not migrated, or an error code\n * after NR_MAX_MIGRATE_PAGES_RETRY attempts or if no hugetlb folios are movable\n * any more because the list has become empty or no retryable hugetlb folios\n * exist any more. It is caller's responsibility to call putback_movable_pages()\n * only if ret != 0.\n */\nstatic int migrate_hugetlbs(struct list_head *from, new_folio_t get_new_folio,\n\t\t\t    free_folio_t put_new_folio, unsigned long private,\n\t\t\t    enum migrate_mode mode, int reason,\n\t\t\t    struct migrate_pages_stats *stats,\n\t\t\t    struct list_head *ret_folios)\n{\n\tint retry = 1;\n\tint nr_failed = 0;\n\tint nr_retry_pages = 0;\n\tint pass = 0;\n\tstruct folio *folio, *folio2;\n\tint rc, nr_pages;\n\n\tfor (pass = 0; pass < NR_MAX_MIGRATE_PAGES_RETRY && retry; pass++) {\n\t\tretry = 0;\n\t\tnr_retry_pages = 0;\n\n\t\tlist_for_each_entry_safe(folio, folio2, from, lru) {\n\t\t\tif (!folio_test_hugetlb(folio))\n\t\t\t\tcontinue;\n\n\t\t\tnr_pages = folio_nr_pages(folio);\n\n\t\t\tcond_resched();\n\n\t\t\t/*\n\t\t\t * Migratability of hugepages depends on architectures and\n\t\t\t * their size.  This check is necessary because some callers\n\t\t\t * of hugepage migration like soft offline and memory\n\t\t\t * hotremove don't walk through page tables or check whether\n\t\t\t * the hugepage is pmd-based or not before kicking migration.\n\t\t\t */\n\t\t\tif (!hugepage_migration_supported(folio_hstate(folio))) {\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tlist_move_tail(&folio->lru, ret_folios);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\trc = unmap_and_move_huge_page(get_new_folio,\n\t\t\t\t\t\t      put_new_folio, private,\n\t\t\t\t\t\t      folio, pass > 2, mode,\n\t\t\t\t\t\t      reason, ret_folios);\n\t\t\t/*\n\t\t\t * The rules are:\n\t\t\t *\t0: hugetlb folio will be put back\n\t\t\t *\t-EAGAIN: stay on the from list\n\t\t\t *\t-ENOMEM: stay on the from list\n\t\t\t *\tOther errno: put on ret_folios list\n\t\t\t */\n\t\t\tswitch(rc) {\n\t\t\tcase -ENOMEM:\n\t\t\t\t/*\n\t\t\t\t * When memory is low, don't bother to try to migrate\n\t\t\t\t * other folios, just exit.\n\t\t\t\t */\n\t\t\t\tstats->nr_failed_pages += nr_pages + nr_retry_pages;\n\t\t\t\treturn -ENOMEM;\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tnr_retry_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\tcase 0:\n\t\t\t\tstats->nr_succeeded += nr_pages;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/*\n\t\t\t\t * Permanent failure (-EBUSY, etc.):\n\t\t\t\t * unlike -EAGAIN case, the failed folio is\n\t\t\t\t * removed from migration folio list and not\n\t\t\t\t * retried in the next outer loop.\n\t\t\t\t */\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\t/*\n\t * nr_failed is number of hugetlb folios failed to be migrated.  After\n\t * NR_MAX_MIGRATE_PAGES_RETRY attempts, give up and count retried hugetlb\n\t * folios as failed.\n\t */\n\tnr_failed += retry;\n\tstats->nr_failed_pages += nr_retry_pages;\n\n\treturn nr_failed;\n}\n\nstatic void migrate_folios_move(struct list_head *src_folios,\n\t\tstruct list_head *dst_folios,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tenum migrate_mode mode, int reason,\n\t\tstruct list_head *ret_folios,\n\t\tstruct migrate_pages_stats *stats,\n\t\tint *retry, int *thp_retry, int *nr_failed,\n\t\tint *nr_retry_pages)\n{\n\tstruct folio *folio, *folio2, *dst, *dst2;\n\tbool is_thp;\n\tint nr_pages;\n\tint rc;\n\n\tdst = list_first_entry(dst_folios, struct folio, lru);\n\tdst2 = list_next_entry(dst, lru);\n\tlist_for_each_entry_safe(folio, folio2, src_folios, lru) {\n\t\tis_thp = folio_test_large(folio) && folio_test_pmd_mappable(folio);\n\t\tnr_pages = folio_nr_pages(folio);\n\n\t\tcond_resched();\n\n\t\trc = migrate_folio_move(put_new_folio, private,\n\t\t\t\tfolio, dst, mode,\n\t\t\t\treason, ret_folios);\n\t\t/*\n\t\t * The rules are:\n\t\t *\t0: folio will be freed\n\t\t *\t-EAGAIN: stay on the unmap_folios list\n\t\t *\tOther errno: put on ret_folios list\n\t\t */\n\t\tswitch (rc) {\n\t\tcase -EAGAIN:\n\t\t\t*retry += 1;\n\t\t\t*thp_retry += is_thp;\n\t\t\t*nr_retry_pages += nr_pages;\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tstats->nr_succeeded += nr_pages;\n\t\t\tstats->nr_thp_succeeded += is_thp;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t*nr_failed += 1;\n\t\t\tstats->nr_thp_failed += is_thp;\n\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\tbreak;\n\t\t}\n\t\tdst = dst2;\n\t\tdst2 = list_next_entry(dst, lru);\n\t}\n}\n\nstatic void migrate_folios_undo(struct list_head *src_folios,\n\t\tstruct list_head *dst_folios,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tstruct list_head *ret_folios)\n{\n\tstruct folio *folio, *folio2, *dst, *dst2;\n\n\tdst = list_first_entry(dst_folios, struct folio, lru);\n\tdst2 = list_next_entry(dst, lru);\n\tlist_for_each_entry_safe(folio, folio2, src_folios, lru) {\n\t\tint old_page_state = 0;\n\t\tstruct anon_vma *anon_vma = NULL;\n\n\t\t__migrate_folio_extract(dst, &old_page_state, &anon_vma);\n\t\tmigrate_folio_undo_src(folio, old_page_state & PAGE_WAS_MAPPED,\n\t\t\t\tanon_vma, true, ret_folios);\n\t\tlist_del(&dst->lru);\n\t\tmigrate_folio_undo_dst(dst, true, put_new_folio, private);\n\t\tdst = dst2;\n\t\tdst2 = list_next_entry(dst, lru);\n\t}\n}\n\n/*\n * migrate_pages_batch() first unmaps folios in the from list as many as\n * possible, then move the unmapped folios.\n *\n * We only batch migration if mode == MIGRATE_ASYNC to avoid to wait a\n * lock or bit when we have locked more than one folio.  Which may cause\n * deadlock (e.g., for loop device).  So, if mode != MIGRATE_ASYNC, the\n * length of the from list must be <= 1.\n */\nstatic int migrate_pages_batch(struct list_head *from,\n\t\tnew_folio_t get_new_folio, free_folio_t put_new_folio,\n\t\tunsigned long private, enum migrate_mode mode, int reason,\n\t\tstruct list_head *ret_folios, struct list_head *split_folios,\n\t\tstruct migrate_pages_stats *stats, int nr_pass)\n{\n\tint retry = 1;\n\tint thp_retry = 1;\n\tint nr_failed = 0;\n\tint nr_retry_pages = 0;\n\tint pass = 0;\n\tbool is_thp = false;\n\tbool is_large = false;\n\tstruct folio *folio, *folio2, *dst = NULL;\n\tint rc, rc_saved = 0, nr_pages;\n\tLIST_HEAD(unmap_folios);\n\tLIST_HEAD(dst_folios);\n\tbool nosplit = (reason == MR_NUMA_MISPLACED);\n\n\tVM_WARN_ON_ONCE(mode != MIGRATE_ASYNC &&\n\t\t\t!list_empty(from) && !list_is_singular(from));\n\n\tfor (pass = 0; pass < nr_pass && retry; pass++) {\n\t\tretry = 0;\n\t\tthp_retry = 0;\n\t\tnr_retry_pages = 0;\n\n\t\tlist_for_each_entry_safe(folio, folio2, from, lru) {\n\t\t\tis_large = folio_test_large(folio);\n\t\t\tis_thp = folio_test_pmd_mappable(folio);\n\t\t\tnr_pages = folio_nr_pages(folio);\n\n\t\t\tcond_resched();\n\n\t\t\t/*\n\t\t\t * The rare folio on the deferred split list should\n\t\t\t * be split now. It should not count as a failure:\n\t\t\t * but increment nr_failed because, without doing so,\n\t\t\t * migrate_pages() may report success with (split but\n\t\t\t * unmigrated) pages still on its fromlist; whereas it\n\t\t\t * always reports success when its fromlist is empty.\n\t\t\t * stats->nr_thp_failed should be increased too,\n\t\t\t * otherwise stats inconsistency will happen when\n\t\t\t * migrate_pages_batch is called via migrate_pages()\n\t\t\t * with MIGRATE_SYNC and MIGRATE_ASYNC.\n\t\t\t *\n\t\t\t * Only check it without removing it from the list.\n\t\t\t * Since the folio can be on deferred_split_scan()\n\t\t\t * local list and removing it can cause the local list\n\t\t\t * corruption. Folio split process below can handle it\n\t\t\t * with the help of folio_ref_freeze().\n\t\t\t *\n\t\t\t * nr_pages > 2 is needed to avoid checking order-1\n\t\t\t * page cache folios. They exist, in contrast to\n\t\t\t * non-existent order-1 anonymous folios, and do not\n\t\t\t * use _deferred_list.\n\t\t\t */\n\t\t\tif (nr_pages > 2 &&\n\t\t\t   !list_empty(&folio->_deferred_list) &&\n\t\t\t   folio_test_partially_mapped(folio)) {\n\t\t\t\tif (!try_split_folio(folio, split_folios, mode)) {\n\t\t\t\t\tnr_failed++;\n\t\t\t\t\tstats->nr_thp_failed += is_thp;\n\t\t\t\t\tstats->nr_thp_split += is_thp;\n\t\t\t\t\tstats->nr_split++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Large folio migration might be unsupported or\n\t\t\t * the allocation might be failed so we should retry\n\t\t\t * on the same folio with the large folio split\n\t\t\t * to normal folios.\n\t\t\t *\n\t\t\t * Split folios are put in split_folios, and\n\t\t\t * we will migrate them after the rest of the\n\t\t\t * list is processed.\n\t\t\t */\n\t\t\tif (!thp_migration_supported() && is_thp) {\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_thp_failed++;\n\t\t\t\tif (!try_split_folio(folio, split_folios, mode)) {\n\t\t\t\t\tstats->nr_thp_split++;\n\t\t\t\t\tstats->nr_split++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tlist_move_tail(&folio->lru, ret_folios);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * If we are holding the last folio reference, the folio\n\t\t\t * was freed from under us, so just drop our reference.\n\t\t\t */\n\t\t\tif (likely(!page_has_movable_ops(&folio->page)) &&\n\t\t\t    folio_ref_count(folio) == 1) {\n\t\t\t\tfolio_clear_active(folio);\n\t\t\t\tfolio_clear_unevictable(folio);\n\t\t\t\tlist_del(&folio->lru);\n\t\t\t\tmigrate_folio_done(folio, reason);\n\t\t\t\tstats->nr_succeeded += nr_pages;\n\t\t\t\tstats->nr_thp_succeeded += is_thp;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\trc = migrate_folio_unmap(get_new_folio, put_new_folio,\n\t\t\t\t\tprivate, folio, &dst, mode, ret_folios);\n\t\t\t/*\n\t\t\t * The rules are:\n\t\t\t *\t0: folio will be put on unmap_folios list,\n\t\t\t *\t   dst folio put on dst_folios list\n\t\t\t *\t-EAGAIN: stay on the from list\n\t\t\t *\t-ENOMEM: stay on the from list\n\t\t\t *\tOther errno: put on ret_folios list\n\t\t\t */\n\t\t\tswitch(rc) {\n\t\t\tcase -ENOMEM:\n\t\t\t\t/*\n\t\t\t\t * When memory is low, don't bother to try to migrate\n\t\t\t\t * other folios, move unmapped folios, then exit.\n\t\t\t\t */\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_thp_failed += is_thp;\n\t\t\t\t/* Large folio NUMA faulting doesn't split to retry. */\n\t\t\t\tif (is_large && !nosplit) {\n\t\t\t\t\tint ret = try_split_folio(folio, split_folios, mode);\n\n\t\t\t\t\tif (!ret) {\n\t\t\t\t\t\tstats->nr_thp_split += is_thp;\n\t\t\t\t\t\tstats->nr_split++;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t} else if (reason == MR_LONGTERM_PIN &&\n\t\t\t\t\t\t   ret == -EAGAIN) {\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * Try again to split large folio to\n\t\t\t\t\t\t * mitigate the failure of longterm pinning.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tretry++;\n\t\t\t\t\t\tthp_retry += is_thp;\n\t\t\t\t\t\tnr_retry_pages += nr_pages;\n\t\t\t\t\t\t/* Undo duplicated failure counting. */\n\t\t\t\t\t\tnr_failed--;\n\t\t\t\t\t\tstats->nr_thp_failed -= is_thp;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tstats->nr_failed_pages += nr_pages + nr_retry_pages;\n\t\t\t\t/* nr_failed isn't updated for not used */\n\t\t\t\tstats->nr_thp_failed += thp_retry;\n\t\t\t\trc_saved = rc;\n\t\t\t\tif (list_empty(&unmap_folios))\n\t\t\t\t\tgoto out;\n\t\t\t\telse\n\t\t\t\t\tgoto move;\n\t\t\tcase -EAGAIN:\n\t\t\t\tretry++;\n\t\t\t\tthp_retry += is_thp;\n\t\t\t\tnr_retry_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\tcase 0:\n\t\t\t\tlist_move_tail(&folio->lru, &unmap_folios);\n\t\t\t\tlist_add_tail(&dst->lru, &dst_folios);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/*\n\t\t\t\t * Permanent failure (-EBUSY, etc.):\n\t\t\t\t * unlike -EAGAIN case, the failed folio is\n\t\t\t\t * removed from migration folio list and not\n\t\t\t\t * retried in the next outer loop.\n\t\t\t\t */\n\t\t\t\tnr_failed++;\n\t\t\t\tstats->nr_thp_failed += is_thp;\n\t\t\t\tstats->nr_failed_pages += nr_pages;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tnr_failed += retry;\n\tstats->nr_thp_failed += thp_retry;\n\tstats->nr_failed_pages += nr_retry_pages;\nmove:\n\t/* Flush TLBs for all unmapped folios */\n\ttry_to_unmap_flush();\n\n\tretry = 1;\n\tfor (pass = 0; pass < nr_pass && retry; pass++) {\n\t\tretry = 0;\n\t\tthp_retry = 0;\n\t\tnr_retry_pages = 0;\n\n\t\t/* Move the unmapped folios */\n\t\tmigrate_folios_move(&unmap_folios, &dst_folios,\n\t\t\t\tput_new_folio, private, mode, reason,\n\t\t\t\tret_folios, stats, &retry, &thp_retry,\n\t\t\t\t&nr_failed, &nr_retry_pages);\n\t}\n\tnr_failed += retry;\n\tstats->nr_thp_failed += thp_retry;\n\tstats->nr_failed_pages += nr_retry_pages;\n\n\trc = rc_saved ? : nr_failed;\nout:\n\t/* Cleanup remaining folios */\n\tmigrate_folios_undo(&unmap_folios, &dst_folios,\n\t\t\tput_new_folio, private, ret_folios);\n\n\treturn rc;\n}\n\nstatic int migrate_pages_sync(struct list_head *from, new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tenum migrate_mode mode, int reason,\n\t\tstruct list_head *ret_folios, struct list_head *split_folios,\n\t\tstruct migrate_pages_stats *stats)\n{\n\tint rc, nr_failed = 0;\n\tLIST_HEAD(folios);\n\tstruct migrate_pages_stats astats;\n\n\tmemset(&astats, 0, sizeof(astats));\n\t/* Try to migrate in batch with MIGRATE_ASYNC mode firstly */\n\trc = migrate_pages_batch(from, get_new_folio, put_new_folio, private, MIGRATE_ASYNC,\n\t\t\t\t reason, &folios, split_folios, &astats,\n\t\t\t\t NR_MAX_MIGRATE_ASYNC_RETRY);\n\tstats->nr_succeeded += astats.nr_succeeded;\n\tstats->nr_thp_succeeded += astats.nr_thp_succeeded;\n\tstats->nr_thp_split += astats.nr_thp_split;\n\tstats->nr_split += astats.nr_split;\n\tif (rc < 0) {\n\t\tstats->nr_failed_pages += astats.nr_failed_pages;\n\t\tstats->nr_thp_failed += astats.nr_thp_failed;\n\t\tlist_splice_tail(&folios, ret_folios);\n\t\treturn rc;\n\t}\n\tstats->nr_thp_failed += astats.nr_thp_split;\n\t/*\n\t * Do not count rc, as pages will be retried below.\n\t * Count nr_split only, since it includes nr_thp_split.\n\t */\n\tnr_failed += astats.nr_split;\n\t/*\n\t * Fall back to migrate all failed folios one by one synchronously. All\n\t * failed folios except split THPs will be retried, so their failure\n\t * isn't counted\n\t */\n\tlist_splice_tail_init(&folios, from);\n\twhile (!list_empty(from)) {\n\t\tlist_move(from->next, &folios);\n\t\trc = migrate_pages_batch(&folios, get_new_folio, put_new_folio,\n\t\t\t\t\t private, mode, reason, ret_folios,\n\t\t\t\t\t split_folios, stats, NR_MAX_MIGRATE_SYNC_RETRY);\n\t\tlist_splice_tail_init(&folios, ret_folios);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\tnr_failed += rc;\n\t}\n\n\treturn nr_failed;\n}\n\n/*\n * migrate_pages - migrate the folios specified in a list, to the free folios\n *\t\t   supplied as the target for the page migration\n *\n * @from:\t\tThe list of folios to be migrated.\n * @get_new_folio:\tThe function used to allocate free folios to be used\n *\t\t\tas the target of the folio migration.\n * @put_new_folio:\tThe function used to free target folios if migration\n *\t\t\tfails, or NULL if no special handling is necessary.\n * @private:\t\tPrivate data to be passed on to get_new_folio()\n * @mode:\t\tThe migration mode that specifies the constraints for\n *\t\t\tfolio migration, if any.\n * @reason:\t\tThe reason for folio migration.\n * @ret_succeeded:\tSet to the number of folios migrated successfully if\n *\t\t\tthe caller passes a non-NULL pointer.\n *\n * The function returns after NR_MAX_MIGRATE_PAGES_RETRY attempts or if no folios\n * are movable any more because the list has become empty or no retryable folios\n * exist any more. It is caller's responsibility to call putback_movable_pages()\n * only if ret != 0.\n *\n * Returns the number of {normal folio, large folio, hugetlb} that were not\n * migrated, or an error code. The number of large folio splits will be\n * considered as the number of non-migrated large folio, no matter how many\n * split folios of the large folio are migrated successfully.\n */\nint migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n\t\tfree_folio_t put_new_folio, unsigned long private,\n\t\tenum migrate_mode mode, int reason, unsigned int *ret_succeeded)\n{\n\tint rc, rc_gather;\n\tint nr_pages;\n\tstruct folio *folio, *folio2;\n\tLIST_HEAD(folios);\n\tLIST_HEAD(ret_folios);\n\tLIST_HEAD(split_folios);\n\tstruct migrate_pages_stats stats;\n\n\ttrace_mm_migrate_pages_start(mode, reason);\n\n\tmemset(&stats, 0, sizeof(stats));\n\n\trc_gather = migrate_hugetlbs(from, get_new_folio, put_new_folio, private,\n\t\t\t\t     mode, reason, &stats, &ret_folios);\n\tif (rc_gather < 0)\n\t\tgoto out;\n\nagain:\n\tnr_pages = 0;\n\tlist_for_each_entry_safe(folio, folio2, from, lru) {\n\t\t/* Retried hugetlb folios will be kept in list  */\n\t\tif (folio_test_hugetlb(folio)) {\n\t\t\tlist_move_tail(&folio->lru, &ret_folios);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnr_pages += folio_nr_pages(folio);\n\t\tif (nr_pages >= NR_MAX_BATCHED_MIGRATION)\n\t\t\tbreak;\n\t}\n\tif (nr_pages >= NR_MAX_BATCHED_MIGRATION)\n\t\tlist_cut_before(&folios, from, &folio2->lru);\n\telse\n\t\tlist_splice_init(from, &folios);\n\tif (mode == MIGRATE_ASYNC)\n\t\trc = migrate_pages_batch(&folios, get_new_folio, put_new_folio,\n\t\t\t\tprivate, mode, reason, &ret_folios,\n\t\t\t\t&split_folios, &stats,\n\t\t\t\tNR_MAX_MIGRATE_PAGES_RETRY);\n\telse\n\t\trc = migrate_pages_sync(&folios, get_new_folio, put_new_folio,\n\t\t\t\tprivate, mode, reason, &ret_folios,\n\t\t\t\t&split_folios, &stats);\n\tlist_splice_tail_init(&folios, &ret_folios);\n\tif (rc < 0) {\n\t\trc_gather = rc;\n\t\tlist_splice_tail(&split_folios, &ret_folios);\n\t\tgoto out;\n\t}\n\tif (!list_empty(&split_folios)) {\n\t\t/*\n\t\t * Failure isn't counted since all split folios of a large folio\n\t\t * is counted as 1 failure already.  And, we only try to migrate\n\t\t * with minimal effort, force MIGRATE_ASYNC mode and retry once.\n\t\t */\n\t\tmigrate_pages_batch(&split_folios, get_new_folio,\n\t\t\t\tput_new_folio, private, MIGRATE_ASYNC, reason,\n\t\t\t\t&ret_folios, NULL, &stats, 1);\n\t\tlist_splice_tail_init(&split_folios, &ret_folios);\n\t}\n\trc_gather += rc;\n\tif (!list_empty(from))\n\t\tgoto again;\nout:\n\t/*\n\t * Put the permanent failure folio back to migration list, they\n\t * will be put back to the right list by the caller.\n\t */\n\tlist_splice(&ret_folios, from);\n\n\t/*\n\t * Return 0 in case all split folios of fail-to-migrate large folios\n\t * are migrated successfully.\n\t */\n\tif (list_empty(from))\n\t\trc_gather = 0;\n\n\tcount_vm_events(PGMIGRATE_SUCCESS, stats.nr_succeeded);\n\tcount_vm_events(PGMIGRATE_FAIL, stats.nr_failed_pages);\n\tcount_vm_events(THP_MIGRATION_SUCCESS, stats.nr_thp_succeeded);\n\tcount_vm_events(THP_MIGRATION_FAIL, stats.nr_thp_failed);\n\tcount_vm_events(THP_MIGRATION_SPLIT, stats.nr_thp_split);\n\ttrace_mm_migrate_pages(stats.nr_succeeded, stats.nr_failed_pages,\n\t\t\t       stats.nr_thp_succeeded, stats.nr_thp_failed,\n\t\t\t       stats.nr_thp_split, stats.nr_split, mode,\n\t\t\t       reason);\n\n\tif (ret_succeeded)\n\t\t*ret_succeeded = stats.nr_succeeded;\n\n\treturn rc_gather;\n}\n\nstruct folio *alloc_migration_target(struct folio *src, unsigned long private)\n{\n\tstruct migration_target_control *mtc;\n\tgfp_t gfp_mask;\n\tunsigned int order = 0;\n\tint nid;\n\tint zidx;\n\n\tmtc = (struct migration_target_control *)private;\n\tgfp_mask = mtc->gfp_mask;\n\tnid = mtc->nid;\n\tif (nid == NUMA_NO_NODE)\n\t\tnid = folio_nid(src);\n\n\tif (folio_test_hugetlb(src)) {\n\t\tstruct hstate *h = folio_hstate(src);\n\n\t\tgfp_mask = htlb_modify_alloc_mask(h, gfp_mask);\n\t\treturn alloc_hugetlb_folio_nodemask(h, nid,\n\t\t\t\t\t\tmtc->nmask, gfp_mask,\n\t\t\t\t\t\thtlb_allow_alloc_fallback(mtc->reason));\n\t}\n\n\tif (folio_test_large(src)) {\n\t\t/*\n\t\t * clear __GFP_RECLAIM to make the migration callback\n\t\t * consistent with regular THP allocations.\n\t\t */\n\t\tgfp_mask &= ~__GFP_RECLAIM;\n\t\tgfp_mask |= GFP_TRANSHUGE;\n\t\torder = folio_order(src);\n\t}\n\tzidx = zone_idx(folio_zone(src));\n\tif (is_highmem_idx(zidx) || zidx == ZONE_MOVABLE)\n\t\tgfp_mask |= __GFP_HIGHMEM;\n\n\treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n}\n\n#ifdef CONFIG_NUMA\n\nstatic int store_status(int __user *status, int start, int value, int nr)\n{\n\twhile (nr-- > 0) {\n\t\tif (put_user(value, status + start))\n\t\t\treturn -EFAULT;\n\t\tstart++;\n\t}\n\n\treturn 0;\n}\n\nstatic int do_move_pages_to_node(struct list_head *pagelist, int node)\n{\n\tint err;\n\tstruct migration_target_control mtc = {\n\t\t.nid = node,\n\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n\t\t.reason = MR_SYSCALL,\n\t};\n\n\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n\tif (err)\n\t\tputback_movable_pages(pagelist);\n\treturn err;\n}\n\nstatic int __add_folio_for_migration(struct folio *folio, int node,\n\t\tstruct list_head *pagelist, bool migrate_all)\n{\n\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n\t\treturn -EFAULT;\n\n\tif (folio_is_zone_device(folio))\n\t\treturn -ENOENT;\n\n\tif (folio_nid(folio) == node)\n\t\treturn 0;\n\n\tif (folio_maybe_mapped_shared(folio) && !migrate_all)\n\t\treturn -EACCES;\n\n\tif (folio_test_hugetlb(folio)) {\n\t\tif (folio_isolate_hugetlb(folio, pagelist))\n\t\t\treturn 1;\n\t} else if (folio_isolate_lru(folio)) {\n\t\tlist_add_tail(&folio->lru, pagelist);\n\t\tnode_stat_mod_folio(folio,\n\t\t\tNR_ISOLATED_ANON + folio_is_file_lru(folio),\n\t\t\tfolio_nr_pages(folio));\n\t\treturn 1;\n\t}\n\treturn -EBUSY;\n}\n\n/*\n * Resolves the given address to a struct folio, isolates it from the LRU and\n * puts it to the given pagelist.\n * Returns:\n *     errno - if the folio cannot be found/isolated\n *     0 - when it doesn't have to be migrated because it is already on the\n *         target node\n *     1 - when it has been queued\n */\nstatic int add_folio_for_migration(struct mm_struct *mm, const void __user *p,\n\t\tint node, struct list_head *pagelist, bool migrate_all)\n{\n\tstruct vm_area_struct *vma;\n\tstruct folio_walk fw;\n\tstruct folio *folio;\n\tunsigned long addr;\n\tint err = -EFAULT;\n\n\tmmap_read_lock(mm);\n\taddr = (unsigned long)untagged_addr_remote(mm, p);\n\n\tvma = vma_lookup(mm, addr);\n\tif (vma && vma_migratable(vma)) {\n\t\tfolio = folio_walk_start(&fw, vma, addr, FW_ZEROPAGE);\n\t\tif (folio) {\n\t\t\terr = __add_folio_for_migration(folio, node, pagelist,\n\t\t\t\t\t\t\tmigrate_all);\n\t\t\tfolio_walk_end(&fw, vma);\n\t\t} else {\n\t\t\terr = -ENOENT;\n\t\t}\n\t}\n\tmmap_read_unlock(mm);\n\treturn err;\n}\n\nstatic int move_pages_and_store_status(int node,\n\t\tstruct list_head *pagelist, int __user *status,\n\t\tint start, int i, unsigned long nr_pages)\n{\n\tint err;\n\n\tif (list_empty(pagelist))\n\t\treturn 0;\n\n\terr = do_move_pages_to_node(pagelist, node);\n\tif (err) {\n\t\t/*\n\t\t * Positive err means the number of failed\n\t\t * pages to migrate.  Since we are going to\n\t\t * abort and return the number of non-migrated\n\t\t * pages, so need to include the rest of the\n\t\t * nr_pages that have not been attempted as\n\t\t * well.\n\t\t */\n\t\tif (err > 0)\n\t\t\terr += nr_pages - i;\n\t\treturn err;\n\t}\n\treturn store_status(status, start, node, i - start);\n}\n\n/*\n * Migrate an array of page address onto an array of nodes and fill\n * the corresponding array of status.\n */\nstatic int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n\t\t\t unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t const int __user *nodes,\n\t\t\t int __user *status, int flags)\n{\n\tcompat_uptr_t __user *compat_pages = (void __user *)pages;\n\tint current_node = NUMA_NO_NODE;\n\tLIST_HEAD(pagelist);\n\tint start, i;\n\tint err = 0, err1;\n\n\tlru_cache_disable();\n\n\tfor (i = start = 0; i < nr_pages; i++) {\n\t\tconst void __user *p;\n\t\tint node;\n\n\t\terr = -EFAULT;\n\t\tif (in_compat_syscall()) {\n\t\t\tcompat_uptr_t cp;\n\n\t\t\tif (get_user(cp, compat_pages + i))\n\t\t\t\tgoto out_flush;\n\n\t\t\tp = compat_ptr(cp);\n\t\t} else {\n\t\t\tif (get_user(p, pages + i))\n\t\t\t\tgoto out_flush;\n\t\t}\n\t\tif (get_user(node, nodes + i))\n\t\t\tgoto out_flush;\n\n\t\terr = -ENODEV;\n\t\tif (node < 0 || node >= MAX_NUMNODES)\n\t\t\tgoto out_flush;\n\t\tif (!node_state(node, N_MEMORY))\n\t\t\tgoto out_flush;\n\n\t\terr = -EACCES;\n\t\tif (!node_isset(node, task_nodes))\n\t\t\tgoto out_flush;\n\n\t\tif (current_node == NUMA_NO_NODE) {\n\t\t\tcurrent_node = node;\n\t\t\tstart = i;\n\t\t} else if (node != current_node) {\n\t\t\terr = move_pages_and_store_status(current_node,\n\t\t\t\t\t&pagelist, status, start, i, nr_pages);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tstart = i;\n\t\t\tcurrent_node = node;\n\t\t}\n\n\t\t/*\n\t\t * Errors in the page lookup or isolation are not fatal and we simply\n\t\t * report them via status\n\t\t */\n\t\terr = add_folio_for_migration(mm, p, current_node, &pagelist,\n\t\t\t\t\t      flags & MPOL_MF_MOVE_ALL);\n\n\t\tif (err > 0) {\n\t\t\t/* The page is successfully queued for migration */\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * If the page is already on the target node (!err), store the\n\t\t * node, otherwise, store the err.\n\t\t */\n\t\terr = store_status(status, i, err ? : current_node, 1);\n\t\tif (err)\n\t\t\tgoto out_flush;\n\n\t\terr = move_pages_and_store_status(current_node, &pagelist,\n\t\t\t\tstatus, start, i, nr_pages);\n\t\tif (err) {\n\t\t\t/* We have accounted for page i */\n\t\t\tif (err > 0)\n\t\t\t\terr--;\n\t\t\tgoto out;\n\t\t}\n\t\tcurrent_node = NUMA_NO_NODE;\n\t}\nout_flush:\n\t/* Make sure we do not overwrite the existing error */\n\terr1 = move_pages_and_store_status(current_node, &pagelist,\n\t\t\t\tstatus, start, i, nr_pages);\n\tif (err >= 0)\n\t\terr = err1;\nout:\n\tlru_cache_enable();\n\treturn err;\n}\n\n/*\n * Determine the nodes of an array of pages and store it in an array of status.\n */\nstatic void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t\tconst void __user **pages, int *status)\n{\n\tunsigned long i;\n\n\tmmap_read_lock(mm);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long addr = (unsigned long)(*pages);\n\t\tstruct vm_area_struct *vma;\n\t\tstruct folio_walk fw;\n\t\tstruct folio *folio;\n\t\tint err = -EFAULT;\n\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma)\n\t\t\tgoto set_status;\n\n\t\tfolio = folio_walk_start(&fw, vma, addr, FW_ZEROPAGE);\n\t\tif (folio) {\n\t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n\t\t\t\terr = -EFAULT;\n\t\t\telse if (folio_is_zone_device(folio))\n\t\t\t\terr = -ENOENT;\n\t\t\telse\n\t\t\t\terr = folio_nid(folio);\n\t\t\tfolio_walk_end(&fw, vma);\n\t\t} else {\n\t\t\terr = -ENOENT;\n\t\t}\nset_status:\n\t\t*status = err;\n\n\t\tpages++;\n\t\tstatus++;\n\t}\n\n\tmmap_read_unlock(mm);\n}\n\nstatic int get_compat_pages_array(const void __user *chunk_pages[],\n\t\t\t\t  const void __user * __user *pages,\n\t\t\t\t  unsigned long chunk_offset,\n\t\t\t\t  unsigned long chunk_nr)\n{\n\tcompat_uptr_t __user *pages32 = (compat_uptr_t __user *)pages;\n\tcompat_uptr_t p;\n\tint i;\n\n\tfor (i = 0; i < chunk_nr; i++) {\n\t\tif (get_user(p, pages32 + chunk_offset + i))\n\t\t\treturn -EFAULT;\n\t\tchunk_pages[i] = compat_ptr(p);\n\t}\n\n\treturn 0;\n}\n\n/*\n * Determine the nodes of a user array of pages and store it in\n * a user array of status.\n */\nstatic int do_pages_stat(struct mm_struct *mm, unsigned long nr_pages,\n\t\t\t const void __user * __user *pages,\n\t\t\t int __user *status)\n{\n#define DO_PAGES_STAT_CHUNK_NR 16UL\n\tconst void __user *chunk_pages[DO_PAGES_STAT_CHUNK_NR];\n\tint chunk_status[DO_PAGES_STAT_CHUNK_NR];\n\tunsigned long chunk_offset = 0;\n\n\twhile (nr_pages) {\n\t\tunsigned long chunk_nr = min(nr_pages, DO_PAGES_STAT_CHUNK_NR);\n\n\t\tif (in_compat_syscall()) {\n\t\t\tif (get_compat_pages_array(chunk_pages, pages,\n\t\t\t\t\t\t   chunk_offset, chunk_nr))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (copy_from_user(chunk_pages, pages + chunk_offset,\n\t\t\t\t      chunk_nr * sizeof(*chunk_pages)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tdo_pages_stat_array(mm, chunk_nr, chunk_pages, chunk_status);\n\n\t\tif (copy_to_user(status + chunk_offset, chunk_status,\n\t\t\t\t chunk_nr * sizeof(*status)))\n\t\t\tbreak;\n\n\t\tchunk_offset += chunk_nr;\n\t\tnr_pages -= chunk_nr;\n\t}\n\treturn nr_pages ? -EFAULT : 0;\n}\n\nstatic struct mm_struct *find_mm_struct(pid_t pid, nodemask_t *mem_nodes)\n{\n\tstruct task_struct *task;\n\tstruct mm_struct *mm;\n\n\t/*\n\t * There is no need to check if current process has the right to modify\n\t * the specified process when they are same.\n\t */\n\tif (!pid) {\n\t\tmmget(current->mm);\n\t\t*mem_nodes = cpuset_mems_allowed(current);\n\t\treturn current->mm;\n\t}\n\n\ttask = find_get_task_by_vpid(pid);\n\tif (!task) {\n\t\treturn ERR_PTR(-ESRCH);\n\t}\n\n\t/*\n\t * Check if this process has the right to modify the specified\n\t * process. Use the regular \"ptrace_may_access()\" checks.\n\t */\n\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {\n\t\tmm = ERR_PTR(-EPERM);\n\t\tgoto out;\n\t}\n\n\tmm = ERR_PTR(security_task_movememory(task));\n\tif (IS_ERR(mm))\n\t\tgoto out;\n\t*mem_nodes = cpuset_mems_allowed(task);\n\tmm = get_task_mm(task);\nout:\n\tput_task_struct(task);\n\tif (!mm)\n\t\tmm = ERR_PTR(-EINVAL);\n\treturn mm;\n}\n\n/*\n * Move a list of pages in the address space of the currently executing\n * process.\n */\nstatic int kernel_move_pages(pid_t pid, unsigned long nr_pages,\n\t\t\t     const void __user * __user *pages,\n\t\t\t     const int __user *nodes,\n\t\t\t     int __user *status, int flags)\n{\n\tstruct mm_struct *mm;\n\tint err;\n\tnodemask_t task_nodes;\n\n\t/* Check flags */\n\tif (flags & ~(MPOL_MF_MOVE|MPOL_MF_MOVE_ALL))\n\t\treturn -EINVAL;\n\n\tif ((flags & MPOL_MF_MOVE_ALL) && !capable(CAP_SYS_NICE))\n\t\treturn -EPERM;\n\n\tmm = find_mm_struct(pid, &task_nodes);\n\tif (IS_ERR(mm))\n\t\treturn PTR_ERR(mm);\n\n\tif (nodes)\n\t\terr = do_pages_move(mm, task_nodes, nr_pages, pages,\n\t\t\t\t    nodes, status, flags);\n\telse\n\t\terr = do_pages_stat(mm, nr_pages, pages, status);\n\n\tmmput(mm);\n\treturn err;\n}\n\nSYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,\n\t\tconst void __user * __user *, pages,\n\t\tconst int __user *, nodes,\n\t\tint __user *, status, int, flags)\n{\n\treturn kernel_move_pages(pid, nr_pages, pages, nodes, status, flags);\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n/*\n * Returns true if this is a safe migration target node for misplaced NUMA\n * pages. Currently it only checks the watermarks which is crude.\n */\nstatic bool migrate_balanced_pgdat(struct pglist_data *pgdat,\n\t\t\t\t   unsigned long nr_migrate_pages)\n{\n\tint z;\n\n\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\tstruct zone *zone = pgdat->node_zones + z;\n\n\t\tif (!managed_zone(zone))\n\t\t\tcontinue;\n\n\t\t/* Avoid waking kswapd by allocating pages_to_migrate pages. */\n\t\tif (!zone_watermark_ok(zone, 0,\n\t\t\t\t       high_wmark_pages(zone) +\n\t\t\t\t       nr_migrate_pages,\n\t\t\t\t       ZONE_MOVABLE, ALLOC_CMA))\n\t\t\tcontinue;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct folio *alloc_misplaced_dst_folio(struct folio *src,\n\t\t\t\t\t   unsigned long data)\n{\n\tint nid = (int) data;\n\tint order = folio_order(src);\n\tgfp_t gfp = __GFP_THISNODE;\n\n\tif (order > 0)\n\t\tgfp |= GFP_TRANSHUGE_LIGHT;\n\telse {\n\t\tgfp |= GFP_HIGHUSER_MOVABLE | __GFP_NOMEMALLOC | __GFP_NORETRY |\n\t\t\t__GFP_NOWARN;\n\t\tgfp &= ~__GFP_RECLAIM;\n\t}\n\treturn __folio_alloc_node(gfp, order, nid);\n}\n\n/*\n * Prepare for calling migrate_misplaced_folio() by isolating the folio if\n * permitted. Must be called with the PTL still held.\n */\nint migrate_misplaced_folio_prepare(struct folio *folio,\n\t\tstruct vm_area_struct *vma, int node)\n{\n\tint nr_pages = folio_nr_pages(folio);\n\tpg_data_t *pgdat = NODE_DATA(node);\n\n\tif (folio_is_file_lru(folio)) {\n\t\t/*\n\t\t * Do not migrate file folios that are mapped in multiple\n\t\t * processes with execute permissions as they are probably\n\t\t * shared libraries.\n\t\t *\n\t\t * See folio_maybe_mapped_shared() on possible imprecision\n\t\t * when we cannot easily detect if a folio is shared.\n\t\t */\n\t\tif ((vma->vm_flags & VM_EXEC) && folio_maybe_mapped_shared(folio))\n\t\t\treturn -EACCES;\n\n\t\t/*\n\t\t * Do not migrate dirty folios as not all filesystems can move\n\t\t * dirty folios in MIGRATE_ASYNC mode which is a waste of\n\t\t * cycles.\n\t\t */\n\t\tif (folio_test_dirty(folio))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/* Avoid migrating to a node that is nearly full */\n\tif (!migrate_balanced_pgdat(pgdat, nr_pages)) {\n\t\tint z;\n\n\t\tif (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING))\n\t\t\treturn -EAGAIN;\n\t\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n\t\t\tif (managed_zone(pgdat->node_zones + z))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If there are no managed zones, it should not proceed\n\t\t * further.\n\t\t */\n\t\tif (z < 0)\n\t\t\treturn -EAGAIN;\n\n\t\twakeup_kswapd(pgdat->node_zones + z, 0,\n\t\t\t      folio_order(folio), ZONE_MOVABLE);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!folio_isolate_lru(folio))\n\t\treturn -EAGAIN;\n\n\tnode_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),\n\t\t\t    nr_pages);\n\treturn 0;\n}\n\n/*\n * Attempt to migrate a misplaced folio to the specified destination\n * node. Caller is expected to have isolated the folio by calling\n * migrate_misplaced_folio_prepare(), which will result in an\n * elevated reference count on the folio. This function will un-isolate the\n * folio, dereferencing the folio before returning.\n */\nint migrate_misplaced_folio(struct folio *folio, int node)\n{\n\tpg_data_t *pgdat = NODE_DATA(node);\n\tint nr_remaining;\n\tunsigned int nr_succeeded;\n\tLIST_HEAD(migratepages);\n\tstruct mem_cgroup *memcg = get_mem_cgroup_from_folio(folio);\n\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n\n\tlist_add(&folio->lru, &migratepages);\n\tnr_remaining = migrate_pages(&migratepages, alloc_misplaced_dst_folio,\n\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n\t\t\t\t     MR_NUMA_MISPLACED, &nr_succeeded);\n\tif (nr_remaining && !list_empty(&migratepages))\n\t\tputback_movable_pages(&migratepages);\n\tif (nr_succeeded) {\n\t\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, nr_succeeded);\n\t\tcount_memcg_events(memcg, NUMA_PAGE_MIGRATE, nr_succeeded);\n\t\tif ((sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING)\n\t\t    && !node_is_toptier(folio_nid(folio))\n\t\t    && node_is_toptier(node))\n\t\t\tmod_lruvec_state(lruvec, PGPROMOTE_SUCCESS, nr_succeeded);\n\t}\n\tmem_cgroup_put(memcg);\n\tBUG_ON(!list_empty(&migratepages));\n\treturn nr_remaining ? -EAGAIN : 0;\n}\n#endif /* CONFIG_NUMA_BALANCING */\n#endif /* CONFIG_NUMA */\n", "patch": "@@ -30,6 +30,7 @@\n #include <linux/mempolicy.h>\n #include <linux/vmalloc.h>\n #include <linux/security.h>\n+#include <linux/backing-dev.h>\n #include <linux/syscalls.h>\n #include <linux/hugetlb.h>\n #include <linux/hugetlb_cgroup.h>\n@@ -313,6 +314,8 @@ int migrate_page_move_mapping(struct address_space *mapping,\n \t\tstruct buffer_head *head, enum migrate_mode mode,\n \t\tint extra_count)\n {\n+\tstruct zone *oldzone, *newzone;\n+\tint dirty;\n \tint expected_count = 1 + extra_count;\n \tvoid **pslot;\n \n@@ -331,6 +334,9 @@ int migrate_page_move_mapping(struct address_space *mapping,\n \t\treturn MIGRATEPAGE_SUCCESS;\n \t}\n \n+\toldzone = page_zone(page);\n+\tnewzone = page_zone(newpage);\n+\n \tspin_lock_irq(&mapping->tree_lock);\n \n \tpslot = radix_tree_lookup_slot(&mapping->page_tree,\n@@ -378,6 +384,13 @@ int migrate_page_move_mapping(struct address_space *mapping,\n \t\tset_page_private(newpage, page_private(page));\n \t}\n \n+\t/* Move dirty while page refs frozen and newpage not yet exposed */\n+\tdirty = PageDirty(page);\n+\tif (dirty) {\n+\t\tClearPageDirty(page);\n+\t\tSetPageDirty(newpage);\n+\t}\n+\n \tradix_tree_replace_slot(pslot, newpage);\n \n \t/*\n@@ -387,6 +400,9 @@ int migrate_page_move_mapping(struct address_space *mapping,\n \t */\n \tpage_unfreeze_refs(page, expected_count - 1);\n \n+\tspin_unlock(&mapping->tree_lock);\n+\t/* Leave irq disabled to prevent preemption while updating stats */\n+\n \t/*\n \t * If moved to a different zone then also account\n \t * the page for that zone. Other VM counters will be\n@@ -397,13 +413,19 @@ int migrate_page_move_mapping(struct address_space *mapping,\n \t * via NR_FILE_PAGES and NR_ANON_PAGES if they\n \t * are mapped to swap space.\n \t */\n-\t__dec_zone_page_state(page, NR_FILE_PAGES);\n-\t__inc_zone_page_state(newpage, NR_FILE_PAGES);\n-\tif (!PageSwapCache(page) && PageSwapBacked(page)) {\n-\t\t__dec_zone_page_state(page, NR_SHMEM);\n-\t\t__inc_zone_page_state(newpage, NR_SHMEM);\n+\tif (newzone != oldzone) {\n+\t\t__dec_zone_state(oldzone, NR_FILE_PAGES);\n+\t\t__inc_zone_state(newzone, NR_FILE_PAGES);\n+\t\tif (PageSwapBacked(page) && !PageSwapCache(page)) {\n+\t\t\t__dec_zone_state(oldzone, NR_SHMEM);\n+\t\t\t__inc_zone_state(newzone, NR_SHMEM);\n+\t\t}\n+\t\tif (dirty && mapping_cap_account_dirty(mapping)) {\n+\t\t\t__dec_zone_state(oldzone, NR_FILE_DIRTY);\n+\t\t\t__inc_zone_state(newzone, NR_FILE_DIRTY);\n+\t\t}\n \t}\n-\tspin_unlock_irq(&mapping->tree_lock);\n+\tlocal_irq_enable();\n \n \treturn MIGRATEPAGE_SUCCESS;\n }\n@@ -524,20 +546,9 @@ void migrate_page_copy(struct page *newpage, struct page *page)\n \tif (PageMappedToDisk(page))\n \t\tSetPageMappedToDisk(newpage);\n \n-\tif (PageDirty(page)) {\n-\t\tclear_page_dirty_for_io(page);\n-\t\t/*\n-\t\t * Want to mark the page and the radix tree as dirty, and\n-\t\t * redo the accounting that clear_page_dirty_for_io undid,\n-\t\t * but we can't use set_page_dirty because that function\n-\t\t * is actually a signal that all of the page has become dirty.\n-\t\t * Whereas only part of our page may be dirty.\n-\t\t */\n-\t\tif (PageSwapBacked(page))\n-\t\t\tSetPageDirty(newpage);\n-\t\telse\n-\t\t\t__set_page_dirty_nobuffers(newpage);\n- \t}\n+\t/* Move dirty on pages not done by migrate_page_move_mapping() */\n+\tif (PageDirty(page))\n+\t\tSetPageDirty(newpage);\n \n \tif (page_is_young(page))\n \t\tset_page_young(newpage);", "file_path": "files/2016_8\\106", "file_language": "c", "file_name": "mm/migrate.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 45, "cve_id": "CVE-2016-6136", "cwe_id": ["CWE-362"], "cve_language": "C", "cve_description": "Race condition in the audit_log_single_execve_arg function in kernel/auditsc.c in the Linux kernel through 4.7 allows local users to bypass intended character-set restrictions or disrupt system-call auditing by changing a certain string, aka a \"double fetch\" vulnerability.", "cvss": "4.7", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "HIGH", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "NONE", "I": "HIGH", "A": "NONE", "commit_id": "43761473c254b45883a64441dd0bc85a42f3645c", "commit_message": "audit: fix a double fetch in audit_log_single_execve_arg()\n\nThere is a double fetch problem in audit_log_single_execve_arg()\nwhere we first check the execve(2) argumnets for any \"bad\" characters\nwhich would require hex encoding and then re-fetch the arguments for\nlogging in the audit record[1].  Of course this leaves a window of\nopportunity for an unsavory application to munge with the data.\n\nThis patch reworks things by only fetching the argument data once[2]\ninto a buffer where it is scanned and logged into the audit\nrecords(s).  In addition to fixing the double fetch, this patch\nimproves on the original code in a few other ways: better handling\nof large arguments which require encoding, stricter record length\nchecking, and some performance improvements (completely unverified,\nbut we got rid of some strlen() calls, that's got to be a good\nthing).\n\nAs part of the development of this patch, I've also created a basic\nregression test for the audit-testsuite, the test can be tracked on\nGitHub at the following link:\n\n * https://github.com/linux-audit/audit-testsuite/issues/25\n\n[1] If you pay careful attention, there is actually a triple fetch\nproblem due to a strnlen_user() call at the top of the function.\n\n[2] This is a tiny white lie, we do make a call to strnlen_user()\nprior to fetching the argument data.  I don't like it, but due to the\nway the audit record is structured we really have no choice unless we\ncopy the entire argument at once (which would require a rather\nwasteful allocation).  The good news is that with this patch the\nkernel no longer relies on this strnlen_user() value for anything\nbeyond recording it in the log, we also update it with a trustworthy\nvalue whenever possible.\n\nReported-by: Pengfei Wang <wpengfeinudt@gmail.com>\nCc: <stable@vger.kernel.org>\nSigned-off-by: Paul Moore <paul@paul-moore.com>", "commit_date": "2016-07-20T18:15:46Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/43761473c254b45883a64441dd0bc85a42f3645c", "html_url": "https://github.com/torvalds/linux/commit/43761473c254b45883a64441dd0bc85a42f3645c", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "0b7a0fdb29715e38641beb39db4d01695b22b5aa", "url_before": "https://api.github.com/repos/torvalds/linux/commits/0b7a0fdb29715e38641beb39db4d01695b22b5aa", "html_url_before": "https://github.com/torvalds/linux/commit/0b7a0fdb29715e38641beb39db4d01695b22b5aa"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/43761473c254b45883a64441dd0bc85a42f3645c/kernel/auditsc.c", "code": "/* auditsc.c -- System-call auditing support\n * Handles all system-call specific auditing features.\n *\n * Copyright 2003-2004 Red Hat Inc., Durham, North Carolina.\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\n * Copyright (C) 2005, 2006 IBM Corporation\n * All Rights Reserved.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n *\n * Written by Rickard E. (Rik) Faith <faith@redhat.com>\n *\n * Many of the ideas implemented here are from Stephen C. Tweedie,\n * especially the idea of avoiding a copy by using getname.\n *\n * The method for actual interception of syscall entry and exit (not in\n * this file -- see entry.S) is based on a GPL'd patch written by\n * okir@suse.de and Copyright 2003 SuSE Linux AG.\n *\n * POSIX message queue support added by George Wilson <ltcgcw@us.ibm.com>,\n * 2006.\n *\n * The support of additional filter rules compares (>, <, >=, <=) was\n * added by Dustin Kirkland <dustin.kirkland@us.ibm.com>, 2005.\n *\n * Modified by Amy Griffis <amy.griffis@hp.com> to collect additional\n * filesystem information.\n *\n * Subject and object context labeling support added by <danjones@us.ibm.com>\n * and <dustin.kirkland@us.ibm.com> for LSPP certification compliance.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <asm/types.h>\n#include <linux/atomic.h>\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/mount.h>\n#include <linux/socket.h>\n#include <linux/mqueue.h>\n#include <linux/audit.h>\n#include <linux/personality.h>\n#include <linux/time.h>\n#include <linux/netlink.h>\n#include <linux/compiler.h>\n#include <asm/unistd.h>\n#include <linux/security.h>\n#include <linux/list.h>\n#include <linux/tty.h>\n#include <linux/binfmts.h>\n#include <linux/highmem.h>\n#include <linux/syscalls.h>\n#include <asm/syscall.h>\n#include <linux/capability.h>\n#include <linux/fs_struct.h>\n#include <linux/compat.h>\n#include <linux/ctype.h>\n#include <linux/string.h>\n#include <linux/uaccess.h>\n#include <uapi/linux/limits.h>\n\n#include \"audit.h\"\n\n/* flags stating the success for a syscall */\n#define AUDITSC_INVALID 0\n#define AUDITSC_SUCCESS 1\n#define AUDITSC_FAILURE 2\n\n/* no execve audit message should be longer than this (userspace limits),\n * see the note near the top of audit_log_execve_info() about this value */\n#define MAX_EXECVE_AUDIT_LEN 7500\n\n/* max length to print of cmdline/proctitle value during audit */\n#define MAX_PROCTITLE_AUDIT_LEN 128\n\n/* number of audit rules */\nint audit_n_rules;\n\n/* determines whether we collect data for signals sent */\nint audit_signals;\n\nstruct audit_aux_data {\n\tstruct audit_aux_data\t*next;\n\tint\t\t\ttype;\n};\n\n#define AUDIT_AUX_IPCPERM\t0\n\n/* Number of target pids per aux struct. */\n#define AUDIT_AUX_PIDS\t16\n\nstruct audit_aux_data_pids {\n\tstruct audit_aux_data\td;\n\tpid_t\t\t\ttarget_pid[AUDIT_AUX_PIDS];\n\tkuid_t\t\t\ttarget_auid[AUDIT_AUX_PIDS];\n\tkuid_t\t\t\ttarget_uid[AUDIT_AUX_PIDS];\n\tunsigned int\t\ttarget_sessionid[AUDIT_AUX_PIDS];\n\tu32\t\t\ttarget_sid[AUDIT_AUX_PIDS];\n\tchar \t\t\ttarget_comm[AUDIT_AUX_PIDS][TASK_COMM_LEN];\n\tint\t\t\tpid_count;\n};\n\nstruct audit_aux_data_bprm_fcaps {\n\tstruct audit_aux_data\td;\n\tstruct audit_cap_data\tfcap;\n\tunsigned int\t\tfcap_ver;\n\tstruct audit_cap_data\told_pcap;\n\tstruct audit_cap_data\tnew_pcap;\n};\n\nstruct audit_tree_refs {\n\tstruct audit_tree_refs *next;\n\tstruct audit_chunk *c[31];\n};\n\nstatic int audit_match_perm(struct audit_context *ctx, int mask)\n{\n\tunsigned n;\n\tif (unlikely(!ctx))\n\t\treturn 0;\n\tn = ctx->major;\n\n\tswitch (audit_classify_syscall(ctx->arch, n)) {\n\tcase 0:\t/* native */\n\t\tif ((mask & AUDIT_PERM_WRITE) &&\n\t\t     audit_match_class(AUDIT_CLASS_WRITE, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_READ) &&\n\t\t     audit_match_class(AUDIT_CLASS_READ, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_ATTR) &&\n\t\t     audit_match_class(AUDIT_CLASS_CHATTR, n))\n\t\t\treturn 1;\n\t\treturn 0;\n\tcase 1: /* 32bit on biarch */\n\t\tif ((mask & AUDIT_PERM_WRITE) &&\n\t\t     audit_match_class(AUDIT_CLASS_WRITE_32, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_READ) &&\n\t\t     audit_match_class(AUDIT_CLASS_READ_32, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_ATTR) &&\n\t\t     audit_match_class(AUDIT_CLASS_CHATTR_32, n))\n\t\t\treturn 1;\n\t\treturn 0;\n\tcase 2: /* open */\n\t\treturn mask & ACC_MODE(ctx->argv[1]);\n\tcase 3: /* openat */\n\t\treturn mask & ACC_MODE(ctx->argv[2]);\n\tcase 4: /* socketcall */\n\t\treturn ((mask & AUDIT_PERM_WRITE) && ctx->argv[0] == SYS_BIND);\n\tcase 5: /* execve */\n\t\treturn mask & AUDIT_PERM_EXEC;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int audit_match_filetype(struct audit_context *ctx, int val)\n{\n\tstruct audit_names *n;\n\tumode_t mode = (umode_t)val;\n\n\tif (unlikely(!ctx))\n\t\treturn 0;\n\n\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\tif ((n->ino != AUDIT_INO_UNSET) &&\n\t\t    ((n->mode & S_IFMT) == mode))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * We keep a linked list of fixed-sized (31 pointer) arrays of audit_chunk *;\n * ->first_trees points to its beginning, ->trees - to the current end of data.\n * ->tree_count is the number of free entries in array pointed to by ->trees.\n * Original condition is (NULL, NULL, 0); as soon as it grows we never revert to NULL,\n * \"empty\" becomes (p, p, 31) afterwards.  We don't shrink the list (and seriously,\n * it's going to remain 1-element for almost any setup) until we free context itself.\n * References in it _are_ dropped - at the same time we free/drop aux stuff.\n */\n\n#ifdef CONFIG_AUDIT_TREE\nstatic void audit_set_auditable(struct audit_context *ctx)\n{\n\tif (!ctx->prio) {\n\t\tctx->prio = 1;\n\t\tctx->current_state = AUDIT_RECORD_CONTEXT;\n\t}\n}\n\nstatic int put_tree_ref(struct audit_context *ctx, struct audit_chunk *chunk)\n{\n\tstruct audit_tree_refs *p = ctx->trees;\n\tint left = ctx->tree_count;\n\tif (likely(left)) {\n\t\tp->c[--left] = chunk;\n\t\tctx->tree_count = left;\n\t\treturn 1;\n\t}\n\tif (!p)\n\t\treturn 0;\n\tp = p->next;\n\tif (p) {\n\t\tp->c[30] = chunk;\n\t\tctx->trees = p;\n\t\tctx->tree_count = 30;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int grow_tree_refs(struct audit_context *ctx)\n{\n\tstruct audit_tree_refs *p = ctx->trees;\n\tctx->trees = kzalloc(sizeof(struct audit_tree_refs), GFP_KERNEL);\n\tif (!ctx->trees) {\n\t\tctx->trees = p;\n\t\treturn 0;\n\t}\n\tif (p)\n\t\tp->next = ctx->trees;\n\telse\n\t\tctx->first_trees = ctx->trees;\n\tctx->tree_count = 31;\n\treturn 1;\n}\n#endif\n\nstatic void unroll_tree_refs(struct audit_context *ctx,\n\t\t      struct audit_tree_refs *p, int count)\n{\n#ifdef CONFIG_AUDIT_TREE\n\tstruct audit_tree_refs *q;\n\tint n;\n\tif (!p) {\n\t\t/* we started with empty chain */\n\t\tp = ctx->first_trees;\n\t\tcount = 31;\n\t\t/* if the very first allocation has failed, nothing to do */\n\t\tif (!p)\n\t\t\treturn;\n\t}\n\tn = count;\n\tfor (q = p; q != ctx->trees; q = q->next, n = 31) {\n\t\twhile (n--) {\n\t\t\taudit_put_chunk(q->c[n]);\n\t\t\tq->c[n] = NULL;\n\t\t}\n\t}\n\twhile (n-- > ctx->tree_count) {\n\t\taudit_put_chunk(q->c[n]);\n\t\tq->c[n] = NULL;\n\t}\n\tctx->trees = p;\n\tctx->tree_count = count;\n#endif\n}\n\nstatic void free_tree_refs(struct audit_context *ctx)\n{\n\tstruct audit_tree_refs *p, *q;\n\tfor (p = ctx->first_trees; p; p = q) {\n\t\tq = p->next;\n\t\tkfree(p);\n\t}\n}\n\nstatic int match_tree_refs(struct audit_context *ctx, struct audit_tree *tree)\n{\n#ifdef CONFIG_AUDIT_TREE\n\tstruct audit_tree_refs *p;\n\tint n;\n\tif (!tree)\n\t\treturn 0;\n\t/* full ones */\n\tfor (p = ctx->first_trees; p != ctx->trees; p = p->next) {\n\t\tfor (n = 0; n < 31; n++)\n\t\t\tif (audit_tree_match(p->c[n], tree))\n\t\t\t\treturn 1;\n\t}\n\t/* partial */\n\tif (p) {\n\t\tfor (n = ctx->tree_count; n < 31; n++)\n\t\t\tif (audit_tree_match(p->c[n], tree))\n\t\t\t\treturn 1;\n\t}\n#endif\n\treturn 0;\n}\n\nstatic int audit_compare_uid(kuid_t uid,\n\t\t\t     struct audit_names *name,\n\t\t\t     struct audit_field *f,\n\t\t\t     struct audit_context *ctx)\n{\n\tstruct audit_names *n;\n\tint rc;\n \n\tif (name) {\n\t\trc = audit_uid_comparator(uid, f->op, name->uid);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n \n\tif (ctx) {\n\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\trc = audit_uid_comparator(uid, f->op, n->uid);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int audit_compare_gid(kgid_t gid,\n\t\t\t     struct audit_names *name,\n\t\t\t     struct audit_field *f,\n\t\t\t     struct audit_context *ctx)\n{\n\tstruct audit_names *n;\n\tint rc;\n \n\tif (name) {\n\t\trc = audit_gid_comparator(gid, f->op, name->gid);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n \n\tif (ctx) {\n\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\trc = audit_gid_comparator(gid, f->op, n->gid);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int audit_field_compare(struct task_struct *tsk,\n\t\t\t       const struct cred *cred,\n\t\t\t       struct audit_field *f,\n\t\t\t       struct audit_context *ctx,\n\t\t\t       struct audit_names *name)\n{\n\tswitch (f->val) {\n\t/* process to file object comparisons */\n\tcase AUDIT_COMPARE_UID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->uid, name, f, ctx);\n\tcase AUDIT_COMPARE_GID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->gid, name, f, ctx);\n\tcase AUDIT_COMPARE_EUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->euid, name, f, ctx);\n\tcase AUDIT_COMPARE_EGID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->egid, name, f, ctx);\n\tcase AUDIT_COMPARE_AUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(tsk->loginuid, name, f, ctx);\n\tcase AUDIT_COMPARE_SUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->suid, name, f, ctx);\n\tcase AUDIT_COMPARE_SGID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->sgid, name, f, ctx);\n\tcase AUDIT_COMPARE_FSUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->fsuid, name, f, ctx);\n\tcase AUDIT_COMPARE_FSGID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->fsgid, name, f, ctx);\n\t/* uid comparisons */\n\tcase AUDIT_COMPARE_UID_TO_AUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op, tsk->loginuid);\n\tcase AUDIT_COMPARE_UID_TO_EUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op, cred->euid);\n\tcase AUDIT_COMPARE_UID_TO_SUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op, cred->suid);\n\tcase AUDIT_COMPARE_UID_TO_FSUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op, cred->fsuid);\n\t/* auid comparisons */\n\tcase AUDIT_COMPARE_AUID_TO_EUID:\n\t\treturn audit_uid_comparator(tsk->loginuid, f->op, cred->euid);\n\tcase AUDIT_COMPARE_AUID_TO_SUID:\n\t\treturn audit_uid_comparator(tsk->loginuid, f->op, cred->suid);\n\tcase AUDIT_COMPARE_AUID_TO_FSUID:\n\t\treturn audit_uid_comparator(tsk->loginuid, f->op, cred->fsuid);\n\t/* euid comparisons */\n\tcase AUDIT_COMPARE_EUID_TO_SUID:\n\t\treturn audit_uid_comparator(cred->euid, f->op, cred->suid);\n\tcase AUDIT_COMPARE_EUID_TO_FSUID:\n\t\treturn audit_uid_comparator(cred->euid, f->op, cred->fsuid);\n\t/* suid comparisons */\n\tcase AUDIT_COMPARE_SUID_TO_FSUID:\n\t\treturn audit_uid_comparator(cred->suid, f->op, cred->fsuid);\n\t/* gid comparisons */\n\tcase AUDIT_COMPARE_GID_TO_EGID:\n\t\treturn audit_gid_comparator(cred->gid, f->op, cred->egid);\n\tcase AUDIT_COMPARE_GID_TO_SGID:\n\t\treturn audit_gid_comparator(cred->gid, f->op, cred->sgid);\n\tcase AUDIT_COMPARE_GID_TO_FSGID:\n\t\treturn audit_gid_comparator(cred->gid, f->op, cred->fsgid);\n\t/* egid comparisons */\n\tcase AUDIT_COMPARE_EGID_TO_SGID:\n\t\treturn audit_gid_comparator(cred->egid, f->op, cred->sgid);\n\tcase AUDIT_COMPARE_EGID_TO_FSGID:\n\t\treturn audit_gid_comparator(cred->egid, f->op, cred->fsgid);\n\t/* sgid comparison */\n\tcase AUDIT_COMPARE_SGID_TO_FSGID:\n\t\treturn audit_gid_comparator(cred->sgid, f->op, cred->fsgid);\n\tdefault:\n\t\tWARN(1, \"Missing AUDIT_COMPARE define.  Report as a bug\\n\");\n\t\treturn 0;\n\t}\n\treturn 0;\n}\n\n/* Determine if any context name data matches a rule's watch data */\n/* Compare a task_struct with an audit_rule.  Return 1 on match, 0\n * otherwise.\n *\n * If task_creation is true, this is an explicit indication that we are\n * filtering a task rule at task creation time.  This and tsk == current are\n * the only situations where tsk->cred may be accessed without an rcu read lock.\n */\nstatic int audit_filter_rules(struct task_struct *tsk,\n\t\t\t      struct audit_krule *rule,\n\t\t\t      struct audit_context *ctx,\n\t\t\t      struct audit_names *name,\n\t\t\t      enum audit_state *state,\n\t\t\t      bool task_creation)\n{\n\tconst struct cred *cred;\n\tint i, need_sid = 1;\n\tu32 sid;\n\n\tcred = rcu_dereference_check(tsk->cred, tsk == current || task_creation);\n\n\tfor (i = 0; i < rule->field_count; i++) {\n\t\tstruct audit_field *f = &rule->fields[i];\n\t\tstruct audit_names *n;\n\t\tint result = 0;\n\t\tpid_t pid;\n\n\t\tswitch (f->type) {\n\t\tcase AUDIT_PID:\n\t\t\tpid = task_pid_nr(tsk);\n\t\t\tresult = audit_comparator(pid, f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_PPID:\n\t\t\tif (ctx) {\n\t\t\t\tif (!ctx->ppid)\n\t\t\t\t\tctx->ppid = task_ppid_nr(tsk);\n\t\t\t\tresult = audit_comparator(ctx->ppid, f->op, f->val);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_EXE:\n\t\t\tresult = audit_exe_compare(tsk, rule->exe);\n\t\t\tbreak;\n\t\tcase AUDIT_UID:\n\t\t\tresult = audit_uid_comparator(cred->uid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_EUID:\n\t\t\tresult = audit_uid_comparator(cred->euid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_SUID:\n\t\t\tresult = audit_uid_comparator(cred->suid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_FSUID:\n\t\t\tresult = audit_uid_comparator(cred->fsuid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_GID:\n\t\t\tresult = audit_gid_comparator(cred->gid, f->op, f->gid);\n\t\t\tif (f->op == Audit_equal) {\n\t\t\t\tif (!result)\n\t\t\t\t\tresult = in_group_p(f->gid);\n\t\t\t} else if (f->op == Audit_not_equal) {\n\t\t\t\tif (result)\n\t\t\t\t\tresult = !in_group_p(f->gid);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_EGID:\n\t\t\tresult = audit_gid_comparator(cred->egid, f->op, f->gid);\n\t\t\tif (f->op == Audit_equal) {\n\t\t\t\tif (!result)\n\t\t\t\t\tresult = in_egroup_p(f->gid);\n\t\t\t} else if (f->op == Audit_not_equal) {\n\t\t\t\tif (result)\n\t\t\t\t\tresult = !in_egroup_p(f->gid);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_SGID:\n\t\t\tresult = audit_gid_comparator(cred->sgid, f->op, f->gid);\n\t\t\tbreak;\n\t\tcase AUDIT_FSGID:\n\t\t\tresult = audit_gid_comparator(cred->fsgid, f->op, f->gid);\n\t\t\tbreak;\n\t\tcase AUDIT_PERS:\n\t\t\tresult = audit_comparator(tsk->personality, f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_ARCH:\n\t\t\tif (ctx)\n\t\t\t\tresult = audit_comparator(ctx->arch, f->op, f->val);\n\t\t\tbreak;\n\n\t\tcase AUDIT_EXIT:\n\t\t\tif (ctx && ctx->return_valid)\n\t\t\t\tresult = audit_comparator(ctx->return_code, f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_SUCCESS:\n\t\t\tif (ctx && ctx->return_valid) {\n\t\t\t\tif (f->val)\n\t\t\t\t\tresult = audit_comparator(ctx->return_valid, f->op, AUDITSC_SUCCESS);\n\t\t\t\telse\n\t\t\t\t\tresult = audit_comparator(ctx->return_valid, f->op, AUDITSC_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_DEVMAJOR:\n\t\t\tif (name) {\n\t\t\t\tif (audit_comparator(MAJOR(name->dev), f->op, f->val) ||\n\t\t\t\t    audit_comparator(MAJOR(name->rdev), f->op, f->val))\n\t\t\t\t\t++result;\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_comparator(MAJOR(n->dev), f->op, f->val) ||\n\t\t\t\t\t    audit_comparator(MAJOR(n->rdev), f->op, f->val)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_DEVMINOR:\n\t\t\tif (name) {\n\t\t\t\tif (audit_comparator(MINOR(name->dev), f->op, f->val) ||\n\t\t\t\t    audit_comparator(MINOR(name->rdev), f->op, f->val))\n\t\t\t\t\t++result;\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_comparator(MINOR(n->dev), f->op, f->val) ||\n\t\t\t\t\t    audit_comparator(MINOR(n->rdev), f->op, f->val)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_INODE:\n\t\t\tif (name)\n\t\t\t\tresult = audit_comparator(name->ino, f->op, f->val);\n\t\t\telse if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_comparator(n->ino, f->op, f->val)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_OBJ_UID:\n\t\t\tif (name) {\n\t\t\t\tresult = audit_uid_comparator(name->uid, f->op, f->uid);\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_uid_comparator(n->uid, f->op, f->uid)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_OBJ_GID:\n\t\t\tif (name) {\n\t\t\t\tresult = audit_gid_comparator(name->gid, f->op, f->gid);\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_gid_comparator(n->gid, f->op, f->gid)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_WATCH:\n\t\t\tif (name)\n\t\t\t\tresult = audit_watch_compare(rule->watch, name->ino, name->dev);\n\t\t\tbreak;\n\t\tcase AUDIT_DIR:\n\t\t\tif (ctx)\n\t\t\t\tresult = match_tree_refs(ctx, rule->tree);\n\t\t\tbreak;\n\t\tcase AUDIT_LOGINUID:\n\t\t\tresult = audit_uid_comparator(tsk->loginuid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_LOGINUID_SET:\n\t\t\tresult = audit_comparator(audit_loginuid_set(tsk), f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_SUBJ_USER:\n\t\tcase AUDIT_SUBJ_ROLE:\n\t\tcase AUDIT_SUBJ_TYPE:\n\t\tcase AUDIT_SUBJ_SEN:\n\t\tcase AUDIT_SUBJ_CLR:\n\t\t\t/* NOTE: this may return negative values indicating\n\t\t\t   a temporary error.  We simply treat this as a\n\t\t\t   match for now to avoid losing information that\n\t\t\t   may be wanted.   An error message will also be\n\t\t\t   logged upon error */\n\t\t\tif (f->lsm_rule) {\n\t\t\t\tif (need_sid) {\n\t\t\t\t\tsecurity_task_getsecid(tsk, &sid);\n\t\t\t\t\tneed_sid = 0;\n\t\t\t\t}\n\t\t\t\tresult = security_audit_rule_match(sid, f->type,\n\t\t\t\t                                  f->op,\n\t\t\t\t                                  f->lsm_rule,\n\t\t\t\t                                  ctx);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_OBJ_USER:\n\t\tcase AUDIT_OBJ_ROLE:\n\t\tcase AUDIT_OBJ_TYPE:\n\t\tcase AUDIT_OBJ_LEV_LOW:\n\t\tcase AUDIT_OBJ_LEV_HIGH:\n\t\t\t/* The above note for AUDIT_SUBJ_USER...AUDIT_SUBJ_CLR\n\t\t\t   also applies here */\n\t\t\tif (f->lsm_rule) {\n\t\t\t\t/* Find files that match */\n\t\t\t\tif (name) {\n\t\t\t\t\tresult = security_audit_rule_match(\n\t\t\t\t\t           name->osid, f->type, f->op,\n\t\t\t\t\t           f->lsm_rule, ctx);\n\t\t\t\t} else if (ctx) {\n\t\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\t\tif (security_audit_rule_match(n->osid, f->type,\n\t\t\t\t\t\t\t\t\t      f->op, f->lsm_rule,\n\t\t\t\t\t\t\t\t\t      ctx)) {\n\t\t\t\t\t\t\t++result;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t/* Find ipc objects that match */\n\t\t\t\tif (!ctx || ctx->type != AUDIT_IPC)\n\t\t\t\t\tbreak;\n\t\t\t\tif (security_audit_rule_match(ctx->ipc.osid,\n\t\t\t\t\t\t\t      f->type, f->op,\n\t\t\t\t\t\t\t      f->lsm_rule, ctx))\n\t\t\t\t\t++result;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_ARG0:\n\t\tcase AUDIT_ARG1:\n\t\tcase AUDIT_ARG2:\n\t\tcase AUDIT_ARG3:\n\t\t\tif (ctx)\n\t\t\t\tresult = audit_comparator(ctx->argv[f->type-AUDIT_ARG0], f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_FILTERKEY:\n\t\t\t/* ignore this field for filtering */\n\t\t\tresult = 1;\n\t\t\tbreak;\n\t\tcase AUDIT_PERM:\n\t\t\tresult = audit_match_perm(ctx, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_FILETYPE:\n\t\t\tresult = audit_match_filetype(ctx, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_FIELD_COMPARE:\n\t\t\tresult = audit_field_compare(tsk, cred, f, ctx, name);\n\t\t\tbreak;\n\t\t}\n\t\tif (!result)\n\t\t\treturn 0;\n\t}\n\n\tif (ctx) {\n\t\tif (rule->prio <= ctx->prio)\n\t\t\treturn 0;\n\t\tif (rule->filterkey) {\n\t\t\tkfree(ctx->filterkey);\n\t\t\tctx->filterkey = kstrdup(rule->filterkey, GFP_ATOMIC);\n\t\t}\n\t\tctx->prio = rule->prio;\n\t}\n\tswitch (rule->action) {\n\tcase AUDIT_NEVER:\n\t\t*state = AUDIT_DISABLED;\n\t\tbreak;\n\tcase AUDIT_ALWAYS:\n\t\t*state = AUDIT_RECORD_CONTEXT;\n\t\tbreak;\n\t}\n\treturn 1;\n}\n\n/* At process creation time, we can determine if system-call auditing is\n * completely disabled for this task.  Since we only have the task\n * structure at this point, we can only check uid and gid.\n */\nstatic enum audit_state audit_filter_task(struct task_struct *tsk, char **key)\n{\n\tstruct audit_entry *e;\n\tenum audit_state   state;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(e, &audit_filter_list[AUDIT_FILTER_TASK], list) {\n\t\tif (audit_filter_rules(tsk, &e->rule, NULL, NULL,\n\t\t\t\t       &state, true)) {\n\t\t\tif (state == AUDIT_RECORD_CONTEXT)\n\t\t\t\t*key = kstrdup(e->rule.filterkey, GFP_ATOMIC);\n\t\t\trcu_read_unlock();\n\t\t\treturn state;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn AUDIT_BUILD_CONTEXT;\n}\n\nstatic int audit_in_mask(const struct audit_krule *rule, unsigned long val)\n{\n\tint word, bit;\n\n\tif (val > 0xffffffff)\n\t\treturn false;\n\n\tword = AUDIT_WORD(val);\n\tif (word >= AUDIT_BITMASK_SIZE)\n\t\treturn false;\n\n\tbit = AUDIT_BIT(val);\n\n\treturn rule->mask[word] & bit;\n}\n\n/* At syscall entry and exit time, this filter is called if the\n * audit_state is not low enough that auditing cannot take place, but is\n * also not high enough that we already know we have to write an audit\n * record (i.e., the state is AUDIT_SETUP_CONTEXT or AUDIT_BUILD_CONTEXT).\n */\nstatic enum audit_state audit_filter_syscall(struct task_struct *tsk,\n\t\t\t\t\t     struct audit_context *ctx,\n\t\t\t\t\t     struct list_head *list)\n{\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tif (audit_pid && tsk->tgid == audit_pid)\n\t\treturn AUDIT_DISABLED;\n\n\trcu_read_lock();\n\tif (!list_empty(list)) {\n\t\tlist_for_each_entry_rcu(e, list, list) {\n\t\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n\t\t\t    audit_filter_rules(tsk, &e->rule, ctx, NULL,\n\t\t\t\t\t       &state, false)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tctx->current_state = state;\n\t\t\t\treturn state;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn AUDIT_BUILD_CONTEXT;\n}\n\n/*\n * Given an audit_name check the inode hash table to see if they match.\n * Called holding the rcu read lock to protect the use of audit_inode_hash\n */\nstatic int audit_filter_inode_name(struct task_struct *tsk,\n\t\t\t\t   struct audit_names *n,\n\t\t\t\t   struct audit_context *ctx) {\n\tint h = audit_hash_ino((u32)n->ino);\n\tstruct list_head *list = &audit_inode_hash[h];\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tif (list_empty(list))\n\t\treturn 0;\n\n\tlist_for_each_entry_rcu(e, list, list) {\n\t\tif (audit_in_mask(&e->rule, ctx->major) &&\n\t\t    audit_filter_rules(tsk, &e->rule, ctx, n, &state, false)) {\n\t\t\tctx->current_state = state;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/* At syscall exit time, this filter is called if any audit_names have been\n * collected during syscall processing.  We only check rules in sublists at hash\n * buckets applicable to the inode numbers in audit_names.\n * Regarding audit_state, same rules apply as for audit_filter_syscall().\n */\nvoid audit_filter_inodes(struct task_struct *tsk, struct audit_context *ctx)\n{\n\tstruct audit_names *n;\n\n\tif (audit_pid && tsk->tgid == audit_pid)\n\t\treturn;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\tif (audit_filter_inode_name(tsk, n, ctx))\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n}\n\n/* Transfer the audit context pointer to the caller, clearing it in the tsk's struct */\nstatic inline struct audit_context *audit_take_context(struct task_struct *tsk,\n\t\t\t\t\t\t      int return_valid,\n\t\t\t\t\t\t      long return_code)\n{\n\tstruct audit_context *context = tsk->audit_context;\n\n\tif (!context)\n\t\treturn NULL;\n\tcontext->return_valid = return_valid;\n\n\t/*\n\t * we need to fix up the return code in the audit logs if the actual\n\t * return codes are later going to be fixed up by the arch specific\n\t * signal handlers\n\t *\n\t * This is actually a test for:\n\t * (rc == ERESTARTSYS ) || (rc == ERESTARTNOINTR) ||\n\t * (rc == ERESTARTNOHAND) || (rc == ERESTART_RESTARTBLOCK)\n\t *\n\t * but is faster than a bunch of ||\n\t */\n\tif (unlikely(return_code <= -ERESTARTSYS) &&\n\t    (return_code >= -ERESTART_RESTARTBLOCK) &&\n\t    (return_code != -ENOIOCTLCMD))\n\t\tcontext->return_code = -EINTR;\n\telse\n\t\tcontext->return_code  = return_code;\n\n\tif (context->in_syscall && !context->dummy) {\n\t\taudit_filter_syscall(tsk, context, &audit_filter_list[AUDIT_FILTER_EXIT]);\n\t\taudit_filter_inodes(tsk, context);\n\t}\n\n\ttsk->audit_context = NULL;\n\treturn context;\n}\n\nstatic inline void audit_proctitle_free(struct audit_context *context)\n{\n\tkfree(context->proctitle.value);\n\tcontext->proctitle.value = NULL;\n\tcontext->proctitle.len = 0;\n}\n\nstatic inline void audit_free_names(struct audit_context *context)\n{\n\tstruct audit_names *n, *next;\n\n\tlist_for_each_entry_safe(n, next, &context->names_list, list) {\n\t\tlist_del(&n->list);\n\t\tif (n->name)\n\t\t\tputname(n->name);\n\t\tif (n->should_free)\n\t\t\tkfree(n);\n\t}\n\tcontext->name_count = 0;\n\tpath_put(&context->pwd);\n\tcontext->pwd.dentry = NULL;\n\tcontext->pwd.mnt = NULL;\n}\n\nstatic inline void audit_free_aux(struct audit_context *context)\n{\n\tstruct audit_aux_data *aux;\n\n\twhile ((aux = context->aux)) {\n\t\tcontext->aux = aux->next;\n\t\tkfree(aux);\n\t}\n\twhile ((aux = context->aux_pids)) {\n\t\tcontext->aux_pids = aux->next;\n\t\tkfree(aux);\n\t}\n}\n\nstatic inline struct audit_context *audit_alloc_context(enum audit_state state)\n{\n\tstruct audit_context *context;\n\n\tcontext = kzalloc(sizeof(*context), GFP_KERNEL);\n\tif (!context)\n\t\treturn NULL;\n\tcontext->state = state;\n\tcontext->prio = state == AUDIT_RECORD_CONTEXT ? ~0ULL : 0;\n\tINIT_LIST_HEAD(&context->killed_trees);\n\tINIT_LIST_HEAD(&context->names_list);\n\treturn context;\n}\n\n/**\n * audit_alloc - allocate an audit context block for a task\n * @tsk: task\n *\n * Filter on the task information and allocate a per-task audit context\n * if necessary.  Doing so turns on system call auditing for the\n * specified task.  This is called from copy_process, so no lock is\n * needed.\n */\nint audit_alloc(struct task_struct *tsk)\n{\n\tstruct audit_context *context;\n\tenum audit_state     state;\n\tchar *key = NULL;\n\n\tif (likely(!audit_ever_enabled))\n\t\treturn 0; /* Return if not auditing. */\n\n\tstate = audit_filter_task(tsk, &key);\n\tif (state == AUDIT_DISABLED) {\n\t\tclear_tsk_thread_flag(tsk, TIF_SYSCALL_AUDIT);\n\t\treturn 0;\n\t}\n\n\tif (!(context = audit_alloc_context(state))) {\n\t\tkfree(key);\n\t\taudit_log_lost(\"out of memory in audit_alloc\");\n\t\treturn -ENOMEM;\n\t}\n\tcontext->filterkey = key;\n\n\ttsk->audit_context  = context;\n\tset_tsk_thread_flag(tsk, TIF_SYSCALL_AUDIT);\n\treturn 0;\n}\n\nstatic inline void audit_free_context(struct audit_context *context)\n{\n\taudit_free_names(context);\n\tunroll_tree_refs(context, NULL, 0);\n\tfree_tree_refs(context);\n\taudit_free_aux(context);\n\tkfree(context->filterkey);\n\tkfree(context->sockaddr);\n\taudit_proctitle_free(context);\n\tkfree(context);\n}\n\nstatic int audit_log_pid_context(struct audit_context *context, pid_t pid,\n\t\t\t\t kuid_t auid, kuid_t uid, unsigned int sessionid,\n\t\t\t\t u32 sid, char *comm)\n{\n\tstruct audit_buffer *ab;\n\tchar *ctx = NULL;\n\tu32 len;\n\tint rc = 0;\n\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_OBJ_PID);\n\tif (!ab)\n\t\treturn rc;\n\n\taudit_log_format(ab, \"opid=%d oauid=%d ouid=%d oses=%d\", pid,\n\t\t\t from_kuid(&init_user_ns, auid),\n\t\t\t from_kuid(&init_user_ns, uid), sessionid);\n\tif (sid) {\n\t\tif (security_secid_to_secctx(sid, &ctx, &len)) {\n\t\t\taudit_log_format(ab, \" obj=(none)\");\n\t\t\trc = 1;\n\t\t} else {\n\t\t\taudit_log_format(ab, \" obj=%s\", ctx);\n\t\t\tsecurity_release_secctx(ctx, len);\n\t\t}\n\t}\n\taudit_log_format(ab, \" ocomm=\");\n\taudit_log_untrustedstring(ab, comm);\n\taudit_log_end(ab);\n\n\treturn rc;\n}\n\nstatic void audit_log_execve_info(struct audit_context *context,\n\t\t\t\t  struct audit_buffer **ab)\n{\n\tlong len_max;\n\tlong len_rem;\n\tlong len_full;\n\tlong len_buf;\n\tlong len_abuf;\n\tlong len_tmp;\n\tbool require_data;\n\tbool encode;\n\tunsigned int iter;\n\tunsigned int arg;\n\tchar *buf_head;\n\tchar *buf;\n\tconst char __user *p = (const char __user *)current->mm->arg_start;\n\n\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n\t *       data we put in the audit record for this argument (see the\n\t *       code below) ... at this point in time 96 is plenty */\n\tchar abuf[96];\n\n\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n\t *       current value of 7500 is not as important as the fact that it\n\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n\t *       room if we go over a little bit in the logging below */\n\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n\tlen_max = MAX_EXECVE_AUDIT_LEN;\n\n\t/* scratch buffer to hold the userspace args */\n\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n\tif (!buf_head) {\n\t\taudit_panic(\"out of memory for argv string\");\n\t\treturn;\n\t}\n\tbuf = buf_head;\n\n\taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\n\n\tlen_rem = len_max;\n\tlen_buf = 0;\n\tlen_full = 0;\n\trequire_data = true;\n\tencode = false;\n\titer = 0;\n\targ = 0;\n\tdo {\n\t\t/* NOTE: we don't ever want to trust this value for anything\n\t\t *       serious, but the audit record format insists we\n\t\t *       provide an argument length for really long arguments,\n\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n\t\t *       to use strncpy_from_user() to obtain this value for\n\t\t *       recording in the log, although we don't use it\n\t\t *       anywhere here to avoid a double-fetch problem */\n\t\tif (len_full == 0)\n\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n\n\t\t/* read more data from userspace */\n\t\tif (require_data) {\n\t\t\t/* can we make more room in the buffer? */\n\t\t\tif (buf != buf_head) {\n\t\t\t\tmemmove(buf_head, buf, len_buf);\n\t\t\t\tbuf = buf_head;\n\t\t\t}\n\n\t\t\t/* fetch as much as we can of the argument */\n\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n\t\t\t\t\t\t    len_max - len_buf);\n\t\t\tif (len_tmp == -EFAULT) {\n\t\t\t\t/* unable to copy from userspace */\n\t\t\t\tsend_sig(SIGKILL, current, 0);\n\t\t\t\tgoto out;\n\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n\t\t\t\t/* buffer is not large enough */\n\t\t\t\trequire_data = true;\n\t\t\t\t/* NOTE: if we are going to span multiple\n\t\t\t\t *       buffers force the encoding so we stand\n\t\t\t\t *       a chance at a sane len_full value and\n\t\t\t\t *       consistent record encoding */\n\t\t\t\tencode = true;\n\t\t\t\tlen_full = len_full * 2;\n\t\t\t\tp += len_tmp;\n\t\t\t} else {\n\t\t\t\trequire_data = false;\n\t\t\t\tif (!encode)\n\t\t\t\t\tencode = audit_string_contains_control(\n\t\t\t\t\t\t\t\tbuf, len_tmp);\n\t\t\t\t/* try to use a trusted value for len_full */\n\t\t\t\tif (len_full < len_max)\n\t\t\t\t\tlen_full = (encode ?\n\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n\t\t\t\tp += len_tmp + 1;\n\t\t\t}\n\t\t\tlen_buf += len_tmp;\n\t\t\tbuf_head[len_buf] = '\\0';\n\n\t\t\t/* length of the buffer in the audit record? */\n\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n\t\t}\n\n\t\t/* write as much as we can to the audit log */\n\t\tif (len_buf > 0) {\n\t\t\t/* NOTE: some magic numbers here - basically if we\n\t\t\t *       can't fit a reasonable amount of data into the\n\t\t\t *       existing audit buffer, flush it and start with\n\t\t\t *       a new buffer */\n\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n\t\t\t\tlen_rem = len_max;\n\t\t\t\taudit_log_end(*ab);\n\t\t\t\t*ab = audit_log_start(context,\n\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n\t\t\t\tif (!*ab)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/* create the non-arg portion of the arg record */\n\t\t\tlen_tmp = 0;\n\t\t\tif (require_data || (iter > 0) ||\n\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n\t\t\t\tif (iter == 0) {\n\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n\t\t\t\t\t\t\t\" a%d_len=%lu\",\n\t\t\t\t\t\t\targ, len_full);\n\t\t\t\t}\n\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n\t\t\t\t\t\t    \" a%d[%d]=\", arg, iter++);\n\t\t\t} else\n\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n\t\t\t\t\t\t    \" a%d=\", arg);\n\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n\n\t\t\t/* log the arg in the audit record */\n\t\t\taudit_log_format(*ab, \"%s\", abuf);\n\t\t\tlen_rem -= len_tmp;\n\t\t\tlen_tmp = len_buf;\n\t\t\tif (encode) {\n\t\t\t\tif (len_abuf > len_rem)\n\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n\t\t\t\tlen_rem -= len_tmp * 2;\n\t\t\t\tlen_abuf -= len_tmp * 2;\n\t\t\t} else {\n\t\t\t\tif (len_abuf > len_rem)\n\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n\t\t\t\tlen_rem -= len_tmp + 2;\n\t\t\t\t/* don't subtract the \"2\" because we still need\n\t\t\t\t * to add quotes to the remaining string */\n\t\t\t\tlen_abuf -= len_tmp;\n\t\t\t}\n\t\t\tlen_buf -= len_tmp;\n\t\t\tbuf += len_tmp;\n\t\t}\n\n\t\t/* ready to move to the next argument? */\n\t\tif ((len_buf == 0) && !require_data) {\n\t\t\targ++;\n\t\t\titer = 0;\n\t\t\tlen_full = 0;\n\t\t\trequire_data = true;\n\t\t\tencode = false;\n\t\t}\n\t} while (arg < context->execve.argc);\n\n\t/* NOTE: the caller handles the final audit_log_end() call */\n\nout:\n\tkfree(buf_head);\n}\n\nstatic void show_special(struct audit_context *context, int *call_panic)\n{\n\tstruct audit_buffer *ab;\n\tint i;\n\n\tab = audit_log_start(context, GFP_KERNEL, context->type);\n\tif (!ab)\n\t\treturn;\n\n\tswitch (context->type) {\n\tcase AUDIT_SOCKETCALL: {\n\t\tint nargs = context->socketcall.nargs;\n\t\taudit_log_format(ab, \"nargs=%d\", nargs);\n\t\tfor (i = 0; i < nargs; i++)\n\t\t\taudit_log_format(ab, \" a%d=%lx\", i,\n\t\t\t\tcontext->socketcall.args[i]);\n\t\tbreak; }\n\tcase AUDIT_IPC: {\n\t\tu32 osid = context->ipc.osid;\n\n\t\taudit_log_format(ab, \"ouid=%u ogid=%u mode=%#ho\",\n\t\t\t\t from_kuid(&init_user_ns, context->ipc.uid),\n\t\t\t\t from_kgid(&init_user_ns, context->ipc.gid),\n\t\t\t\t context->ipc.mode);\n\t\tif (osid) {\n\t\t\tchar *ctx = NULL;\n\t\t\tu32 len;\n\t\t\tif (security_secid_to_secctx(osid, &ctx, &len)) {\n\t\t\t\taudit_log_format(ab, \" osid=%u\", osid);\n\t\t\t\t*call_panic = 1;\n\t\t\t} else {\n\t\t\t\taudit_log_format(ab, \" obj=%s\", ctx);\n\t\t\t\tsecurity_release_secctx(ctx, len);\n\t\t\t}\n\t\t}\n\t\tif (context->ipc.has_perm) {\n\t\t\taudit_log_end(ab);\n\t\t\tab = audit_log_start(context, GFP_KERNEL,\n\t\t\t\t\t     AUDIT_IPC_SET_PERM);\n\t\t\tif (unlikely(!ab))\n\t\t\t\treturn;\n\t\t\taudit_log_format(ab,\n\t\t\t\t\"qbytes=%lx ouid=%u ogid=%u mode=%#ho\",\n\t\t\t\tcontext->ipc.qbytes,\n\t\t\t\tcontext->ipc.perm_uid,\n\t\t\t\tcontext->ipc.perm_gid,\n\t\t\t\tcontext->ipc.perm_mode);\n\t\t}\n\t\tbreak; }\n\tcase AUDIT_MQ_OPEN: {\n\t\taudit_log_format(ab,\n\t\t\t\"oflag=0x%x mode=%#ho mq_flags=0x%lx mq_maxmsg=%ld \"\n\t\t\t\"mq_msgsize=%ld mq_curmsgs=%ld\",\n\t\t\tcontext->mq_open.oflag, context->mq_open.mode,\n\t\t\tcontext->mq_open.attr.mq_flags,\n\t\t\tcontext->mq_open.attr.mq_maxmsg,\n\t\t\tcontext->mq_open.attr.mq_msgsize,\n\t\t\tcontext->mq_open.attr.mq_curmsgs);\n\t\tbreak; }\n\tcase AUDIT_MQ_SENDRECV: {\n\t\taudit_log_format(ab,\n\t\t\t\"mqdes=%d msg_len=%zd msg_prio=%u \"\n\t\t\t\"abs_timeout_sec=%ld abs_timeout_nsec=%ld\",\n\t\t\tcontext->mq_sendrecv.mqdes,\n\t\t\tcontext->mq_sendrecv.msg_len,\n\t\t\tcontext->mq_sendrecv.msg_prio,\n\t\t\tcontext->mq_sendrecv.abs_timeout.tv_sec,\n\t\t\tcontext->mq_sendrecv.abs_timeout.tv_nsec);\n\t\tbreak; }\n\tcase AUDIT_MQ_NOTIFY: {\n\t\taudit_log_format(ab, \"mqdes=%d sigev_signo=%d\",\n\t\t\t\tcontext->mq_notify.mqdes,\n\t\t\t\tcontext->mq_notify.sigev_signo);\n\t\tbreak; }\n\tcase AUDIT_MQ_GETSETATTR: {\n\t\tstruct mq_attr *attr = &context->mq_getsetattr.mqstat;\n\t\taudit_log_format(ab,\n\t\t\t\"mqdes=%d mq_flags=0x%lx mq_maxmsg=%ld mq_msgsize=%ld \"\n\t\t\t\"mq_curmsgs=%ld \",\n\t\t\tcontext->mq_getsetattr.mqdes,\n\t\t\tattr->mq_flags, attr->mq_maxmsg,\n\t\t\tattr->mq_msgsize, attr->mq_curmsgs);\n\t\tbreak; }\n\tcase AUDIT_CAPSET: {\n\t\taudit_log_format(ab, \"pid=%d\", context->capset.pid);\n\t\taudit_log_cap(ab, \"cap_pi\", &context->capset.cap.inheritable);\n\t\taudit_log_cap(ab, \"cap_pp\", &context->capset.cap.permitted);\n\t\taudit_log_cap(ab, \"cap_pe\", &context->capset.cap.effective);\n\t\tbreak; }\n\tcase AUDIT_MMAP: {\n\t\taudit_log_format(ab, \"fd=%d flags=0x%x\", context->mmap.fd,\n\t\t\t\t context->mmap.flags);\n\t\tbreak; }\n\tcase AUDIT_EXECVE: {\n\t\taudit_log_execve_info(context, &ab);\n\t\tbreak; }\n\t}\n\taudit_log_end(ab);\n}\n\nstatic inline int audit_proctitle_rtrim(char *proctitle, int len)\n{\n\tchar *end = proctitle + len - 1;\n\twhile (end > proctitle && !isprint(*end))\n\t\tend--;\n\n\t/* catch the case where proctitle is only 1 non-print character */\n\tlen = end - proctitle + 1;\n\tlen -= isprint(proctitle[len-1]) == 0;\n\treturn len;\n}\n\nstatic void audit_log_proctitle(struct task_struct *tsk,\n\t\t\t struct audit_context *context)\n{\n\tint res;\n\tchar *buf;\n\tchar *msg = \"(null)\";\n\tint len = strlen(msg);\n\tstruct audit_buffer *ab;\n\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_PROCTITLE);\n\tif (!ab)\n\t\treturn;\t/* audit_panic or being filtered */\n\n\taudit_log_format(ab, \"proctitle=\");\n\n\t/* Not  cached */\n\tif (!context->proctitle.value) {\n\t\tbuf = kmalloc(MAX_PROCTITLE_AUDIT_LEN, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tgoto out;\n\t\t/* Historically called this from procfs naming */\n\t\tres = get_cmdline(tsk, buf, MAX_PROCTITLE_AUDIT_LEN);\n\t\tif (res == 0) {\n\t\t\tkfree(buf);\n\t\t\tgoto out;\n\t\t}\n\t\tres = audit_proctitle_rtrim(buf, res);\n\t\tif (res == 0) {\n\t\t\tkfree(buf);\n\t\t\tgoto out;\n\t\t}\n\t\tcontext->proctitle.value = buf;\n\t\tcontext->proctitle.len = res;\n\t}\n\tmsg = context->proctitle.value;\n\tlen = context->proctitle.len;\nout:\n\taudit_log_n_untrustedstring(ab, msg, len);\n\taudit_log_end(ab);\n}\n\nstatic void audit_log_exit(struct audit_context *context, struct task_struct *tsk)\n{\n\tint i, call_panic = 0;\n\tstruct audit_buffer *ab;\n\tstruct audit_aux_data *aux;\n\tstruct audit_names *n;\n\n\t/* tsk == current */\n\tcontext->personality = tsk->personality;\n\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_SYSCALL);\n\tif (!ab)\n\t\treturn;\t\t/* audit_panic has been called */\n\taudit_log_format(ab, \"arch=%x syscall=%d\",\n\t\t\t context->arch, context->major);\n\tif (context->personality != PER_LINUX)\n\t\taudit_log_format(ab, \" per=%lx\", context->personality);\n\tif (context->return_valid)\n\t\taudit_log_format(ab, \" success=%s exit=%ld\",\n\t\t\t\t (context->return_valid==AUDITSC_SUCCESS)?\"yes\":\"no\",\n\t\t\t\t context->return_code);\n\n\taudit_log_format(ab,\n\t\t\t \" a0=%lx a1=%lx a2=%lx a3=%lx items=%d\",\n\t\t\t context->argv[0],\n\t\t\t context->argv[1],\n\t\t\t context->argv[2],\n\t\t\t context->argv[3],\n\t\t\t context->name_count);\n\n\taudit_log_task_info(ab, tsk);\n\taudit_log_key(ab, context->filterkey);\n\taudit_log_end(ab);\n\n\tfor (aux = context->aux; aux; aux = aux->next) {\n\n\t\tab = audit_log_start(context, GFP_KERNEL, aux->type);\n\t\tif (!ab)\n\t\t\tcontinue; /* audit_panic has been called */\n\n\t\tswitch (aux->type) {\n\n\t\tcase AUDIT_BPRM_FCAPS: {\n\t\t\tstruct audit_aux_data_bprm_fcaps *axs = (void *)aux;\n\t\t\taudit_log_format(ab, \"fver=%x\", axs->fcap_ver);\n\t\t\taudit_log_cap(ab, \"fp\", &axs->fcap.permitted);\n\t\t\taudit_log_cap(ab, \"fi\", &axs->fcap.inheritable);\n\t\t\taudit_log_format(ab, \" fe=%d\", axs->fcap.fE);\n\t\t\taudit_log_cap(ab, \"old_pp\", &axs->old_pcap.permitted);\n\t\t\taudit_log_cap(ab, \"old_pi\", &axs->old_pcap.inheritable);\n\t\t\taudit_log_cap(ab, \"old_pe\", &axs->old_pcap.effective);\n\t\t\taudit_log_cap(ab, \"new_pp\", &axs->new_pcap.permitted);\n\t\t\taudit_log_cap(ab, \"new_pi\", &axs->new_pcap.inheritable);\n\t\t\taudit_log_cap(ab, \"new_pe\", &axs->new_pcap.effective);\n\t\t\tbreak; }\n\n\t\t}\n\t\taudit_log_end(ab);\n\t}\n\n\tif (context->type)\n\t\tshow_special(context, &call_panic);\n\n\tif (context->fds[0] >= 0) {\n\t\tab = audit_log_start(context, GFP_KERNEL, AUDIT_FD_PAIR);\n\t\tif (ab) {\n\t\t\taudit_log_format(ab, \"fd0=%d fd1=%d\",\n\t\t\t\t\tcontext->fds[0], context->fds[1]);\n\t\t\taudit_log_end(ab);\n\t\t}\n\t}\n\n\tif (context->sockaddr_len) {\n\t\tab = audit_log_start(context, GFP_KERNEL, AUDIT_SOCKADDR);\n\t\tif (ab) {\n\t\t\taudit_log_format(ab, \"saddr=\");\n\t\t\taudit_log_n_hex(ab, (void *)context->sockaddr,\n\t\t\t\t\tcontext->sockaddr_len);\n\t\t\taudit_log_end(ab);\n\t\t}\n\t}\n\n\tfor (aux = context->aux_pids; aux; aux = aux->next) {\n\t\tstruct audit_aux_data_pids *axs = (void *)aux;\n\n\t\tfor (i = 0; i < axs->pid_count; i++)\n\t\t\tif (audit_log_pid_context(context, axs->target_pid[i],\n\t\t\t\t\t\t  axs->target_auid[i],\n\t\t\t\t\t\t  axs->target_uid[i],\n\t\t\t\t\t\t  axs->target_sessionid[i],\n\t\t\t\t\t\t  axs->target_sid[i],\n\t\t\t\t\t\t  axs->target_comm[i]))\n\t\t\t\tcall_panic = 1;\n\t}\n\n\tif (context->target_pid &&\n\t    audit_log_pid_context(context, context->target_pid,\n\t\t\t\t  context->target_auid, context->target_uid,\n\t\t\t\t  context->target_sessionid,\n\t\t\t\t  context->target_sid, context->target_comm))\n\t\t\tcall_panic = 1;\n\n\tif (context->pwd.dentry && context->pwd.mnt) {\n\t\tab = audit_log_start(context, GFP_KERNEL, AUDIT_CWD);\n\t\tif (ab) {\n\t\t\taudit_log_d_path(ab, \"cwd=\", &context->pwd);\n\t\t\taudit_log_end(ab);\n\t\t}\n\t}\n\n\ti = 0;\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\tif (n->hidden)\n\t\t\tcontinue;\n\t\taudit_log_name(context, n, NULL, i++, &call_panic);\n\t}\n\n\taudit_log_proctitle(tsk, context);\n\n\t/* Send end of event record to help user space know we are finished */\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_EOE);\n\tif (ab)\n\t\taudit_log_end(ab);\n\tif (call_panic)\n\t\taudit_panic(\"error converting sid to string\");\n}\n\n/**\n * audit_free - free a per-task audit context\n * @tsk: task whose audit context block to free\n *\n * Called from copy_process and do_exit\n */\nvoid __audit_free(struct task_struct *tsk)\n{\n\tstruct audit_context *context;\n\n\tcontext = audit_take_context(tsk, 0, 0);\n\tif (!context)\n\t\treturn;\n\n\t/* Check for system calls that do not go through the exit\n\t * function (e.g., exit_group), then free context block.\n\t * We use GFP_ATOMIC here because we might be doing this\n\t * in the context of the idle thread */\n\t/* that can happen only if we are called from do_exit() */\n\tif (context->in_syscall && context->current_state == AUDIT_RECORD_CONTEXT)\n\t\taudit_log_exit(context, tsk);\n\tif (!list_empty(&context->killed_trees))\n\t\taudit_kill_trees(&context->killed_trees);\n\n\taudit_free_context(context);\n}\n\n/**\n * audit_syscall_entry - fill in an audit record at syscall entry\n * @major: major syscall type (function)\n * @a1: additional syscall register 1\n * @a2: additional syscall register 2\n * @a3: additional syscall register 3\n * @a4: additional syscall register 4\n *\n * Fill in audit context at syscall entry.  This only happens if the\n * audit context was created when the task was created and the state or\n * filters demand the audit context be built.  If the state from the\n * per-task filter or from the per-syscall filter is AUDIT_RECORD_CONTEXT,\n * then the record will be written at syscall exit time (otherwise, it\n * will only be written if another part of the kernel requests that it\n * be written).\n */\nvoid __audit_syscall_entry(int major, unsigned long a1, unsigned long a2,\n\t\t\t   unsigned long a3, unsigned long a4)\n{\n\tstruct task_struct *tsk = current;\n\tstruct audit_context *context = tsk->audit_context;\n\tenum audit_state     state;\n\n\tif (!context)\n\t\treturn;\n\n\tBUG_ON(context->in_syscall || context->name_count);\n\n\tif (!audit_enabled)\n\t\treturn;\n\n\tcontext->arch\t    = syscall_get_arch();\n\tcontext->major      = major;\n\tcontext->argv[0]    = a1;\n\tcontext->argv[1]    = a2;\n\tcontext->argv[2]    = a3;\n\tcontext->argv[3]    = a4;\n\n\tstate = context->state;\n\tcontext->dummy = !audit_n_rules;\n\tif (!context->dummy && state == AUDIT_BUILD_CONTEXT) {\n\t\tcontext->prio = 0;\n\t\tstate = audit_filter_syscall(tsk, context, &audit_filter_list[AUDIT_FILTER_ENTRY]);\n\t}\n\tif (state == AUDIT_DISABLED)\n\t\treturn;\n\n\tcontext->serial     = 0;\n\tcontext->ctime      = CURRENT_TIME;\n\tcontext->in_syscall = 1;\n\tcontext->current_state  = state;\n\tcontext->ppid       = 0;\n}\n\n/**\n * audit_syscall_exit - deallocate audit context after a system call\n * @success: success value of the syscall\n * @return_code: return value of the syscall\n *\n * Tear down after system call.  If the audit context has been marked as\n * auditable (either because of the AUDIT_RECORD_CONTEXT state from\n * filtering, or because some other part of the kernel wrote an audit\n * message), then write out the syscall information.  In call cases,\n * free the names stored from getname().\n */\nvoid __audit_syscall_exit(int success, long return_code)\n{\n\tstruct task_struct *tsk = current;\n\tstruct audit_context *context;\n\n\tif (success)\n\t\tsuccess = AUDITSC_SUCCESS;\n\telse\n\t\tsuccess = AUDITSC_FAILURE;\n\n\tcontext = audit_take_context(tsk, success, return_code);\n\tif (!context)\n\t\treturn;\n\n\tif (context->in_syscall && context->current_state == AUDIT_RECORD_CONTEXT)\n\t\taudit_log_exit(context, tsk);\n\n\tcontext->in_syscall = 0;\n\tcontext->prio = context->state == AUDIT_RECORD_CONTEXT ? ~0ULL : 0;\n\n\tif (!list_empty(&context->killed_trees))\n\t\taudit_kill_trees(&context->killed_trees);\n\n\taudit_free_names(context);\n\tunroll_tree_refs(context, NULL, 0);\n\taudit_free_aux(context);\n\tcontext->aux = NULL;\n\tcontext->aux_pids = NULL;\n\tcontext->target_pid = 0;\n\tcontext->target_sid = 0;\n\tcontext->sockaddr_len = 0;\n\tcontext->type = 0;\n\tcontext->fds[0] = -1;\n\tif (context->state != AUDIT_RECORD_CONTEXT) {\n\t\tkfree(context->filterkey);\n\t\tcontext->filterkey = NULL;\n\t}\n\ttsk->audit_context = context;\n}\n\nstatic inline void handle_one(const struct inode *inode)\n{\n#ifdef CONFIG_AUDIT_TREE\n\tstruct audit_context *context;\n\tstruct audit_tree_refs *p;\n\tstruct audit_chunk *chunk;\n\tint count;\n\tif (likely(hlist_empty(&inode->i_fsnotify_marks)))\n\t\treturn;\n\tcontext = current->audit_context;\n\tp = context->trees;\n\tcount = context->tree_count;\n\trcu_read_lock();\n\tchunk = audit_tree_lookup(inode);\n\trcu_read_unlock();\n\tif (!chunk)\n\t\treturn;\n\tif (likely(put_tree_ref(context, chunk)))\n\t\treturn;\n\tif (unlikely(!grow_tree_refs(context))) {\n\t\tpr_warn(\"out of memory, audit has lost a tree reference\\n\");\n\t\taudit_set_auditable(context);\n\t\taudit_put_chunk(chunk);\n\t\tunroll_tree_refs(context, p, count);\n\t\treturn;\n\t}\n\tput_tree_ref(context, chunk);\n#endif\n}\n\nstatic void handle_path(const struct dentry *dentry)\n{\n#ifdef CONFIG_AUDIT_TREE\n\tstruct audit_context *context;\n\tstruct audit_tree_refs *p;\n\tconst struct dentry *d, *parent;\n\tstruct audit_chunk *drop;\n\tunsigned long seq;\n\tint count;\n\n\tcontext = current->audit_context;\n\tp = context->trees;\n\tcount = context->tree_count;\nretry:\n\tdrop = NULL;\n\td = dentry;\n\trcu_read_lock();\n\tseq = read_seqbegin(&rename_lock);\n\tfor(;;) {\n\t\tstruct inode *inode = d_backing_inode(d);\n\t\tif (inode && unlikely(!hlist_empty(&inode->i_fsnotify_marks))) {\n\t\t\tstruct audit_chunk *chunk;\n\t\t\tchunk = audit_tree_lookup(inode);\n\t\t\tif (chunk) {\n\t\t\t\tif (unlikely(!put_tree_ref(context, chunk))) {\n\t\t\t\t\tdrop = chunk;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tparent = d->d_parent;\n\t\tif (parent == d)\n\t\t\tbreak;\n\t\td = parent;\n\t}\n\tif (unlikely(read_seqretry(&rename_lock, seq) || drop)) {  /* in this order */\n\t\trcu_read_unlock();\n\t\tif (!drop) {\n\t\t\t/* just a race with rename */\n\t\t\tunroll_tree_refs(context, p, count);\n\t\t\tgoto retry;\n\t\t}\n\t\taudit_put_chunk(drop);\n\t\tif (grow_tree_refs(context)) {\n\t\t\t/* OK, got more space */\n\t\t\tunroll_tree_refs(context, p, count);\n\t\t\tgoto retry;\n\t\t}\n\t\t/* too bad */\n\t\tpr_warn(\"out of memory, audit has lost a tree reference\\n\");\n\t\tunroll_tree_refs(context, p, count);\n\t\taudit_set_auditable(context);\n\t\treturn;\n\t}\n\trcu_read_unlock();\n#endif\n}\n\nstatic struct audit_names *audit_alloc_name(struct audit_context *context,\n\t\t\t\t\t\tunsigned char type)\n{\n\tstruct audit_names *aname;\n\n\tif (context->name_count < AUDIT_NAMES) {\n\t\taname = &context->preallocated_names[context->name_count];\n\t\tmemset(aname, 0, sizeof(*aname));\n\t} else {\n\t\taname = kzalloc(sizeof(*aname), GFP_NOFS);\n\t\tif (!aname)\n\t\t\treturn NULL;\n\t\taname->should_free = true;\n\t}\n\n\taname->ino = AUDIT_INO_UNSET;\n\taname->type = type;\n\tlist_add_tail(&aname->list, &context->names_list);\n\n\tcontext->name_count++;\n\treturn aname;\n}\n\n/**\n * audit_reusename - fill out filename with info from existing entry\n * @uptr: userland ptr to pathname\n *\n * Search the audit_names list for the current audit context. If there is an\n * existing entry with a matching \"uptr\" then return the filename\n * associated with that audit_name. If not, return NULL.\n */\nstruct filename *\n__audit_reusename(const __user char *uptr)\n{\n\tstruct audit_context *context = current->audit_context;\n\tstruct audit_names *n;\n\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\tif (!n->name)\n\t\t\tcontinue;\n\t\tif (n->name->uptr == uptr) {\n\t\t\tn->name->refcnt++;\n\t\t\treturn n->name;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/**\n * audit_getname - add a name to the list\n * @name: name to add\n *\n * Add a name to the list of audit names for this context.\n * Called from fs/namei.c:getname().\n */\nvoid __audit_getname(struct filename *name)\n{\n\tstruct audit_context *context = current->audit_context;\n\tstruct audit_names *n;\n\n\tif (!context->in_syscall)\n\t\treturn;\n\n\tn = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);\n\tif (!n)\n\t\treturn;\n\n\tn->name = name;\n\tn->name_len = AUDIT_NAME_FULL;\n\tname->aname = n;\n\tname->refcnt++;\n\n\tif (!context->pwd.dentry)\n\t\tget_fs_pwd(current->fs, &context->pwd);\n}\n\n/**\n * __audit_inode - store the inode and device from a lookup\n * @name: name being audited\n * @dentry: dentry being audited\n * @flags: attributes for this particular entry\n */\nvoid __audit_inode(struct filename *name, const struct dentry *dentry,\n\t\t   unsigned int flags)\n{\n\tstruct audit_context *context = current->audit_context;\n\tstruct inode *inode = d_backing_inode(dentry);\n\tstruct audit_names *n;\n\tbool parent = flags & AUDIT_INODE_PARENT;\n\n\tif (!context->in_syscall)\n\t\treturn;\n\n\tif (!name)\n\t\tgoto out_alloc;\n\n\t/*\n\t * If we have a pointer to an audit_names entry already, then we can\n\t * just use it directly if the type is correct.\n\t */\n\tn = name->aname;\n\tif (n) {\n\t\tif (parent) {\n\t\t\tif (n->type == AUDIT_TYPE_PARENT ||\n\t\t\t    n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\tif (n->type != AUDIT_TYPE_PARENT)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlist_for_each_entry_reverse(n, &context->names_list, list) {\n\t\tif (n->ino) {\n\t\t\t/* valid inode number, use that for the comparison */\n\t\t\tif (n->ino != inode->i_ino ||\n\t\t\t    n->dev != inode->i_sb->s_dev)\n\t\t\t\tcontinue;\n\t\t} else if (n->name) {\n\t\t\t/* inode number has not been set, check the name */\n\t\t\tif (strcmp(n->name->name, name->name))\n\t\t\t\tcontinue;\n\t\t} else\n\t\t\t/* no inode and no name (?!) ... this is odd ... */\n\t\t\tcontinue;\n\n\t\t/* match the correct record type */\n\t\tif (parent) {\n\t\t\tif (n->type == AUDIT_TYPE_PARENT ||\n\t\t\t    n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\tif (n->type != AUDIT_TYPE_PARENT)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\nout_alloc:\n\t/* unable to find an entry with both a matching name and type */\n\tn = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);\n\tif (!n)\n\t\treturn;\n\tif (name) {\n\t\tn->name = name;\n\t\tname->refcnt++;\n\t}\n\nout:\n\tif (parent) {\n\t\tn->name_len = n->name ? parent_len(n->name->name) : AUDIT_NAME_FULL;\n\t\tn->type = AUDIT_TYPE_PARENT;\n\t\tif (flags & AUDIT_INODE_HIDDEN)\n\t\t\tn->hidden = true;\n\t} else {\n\t\tn->name_len = AUDIT_NAME_FULL;\n\t\tn->type = AUDIT_TYPE_NORMAL;\n\t}\n\thandle_path(dentry);\n\taudit_copy_inode(n, dentry, inode);\n}\n\nvoid __audit_file(const struct file *file)\n{\n\t__audit_inode(NULL, file->f_path.dentry, 0);\n}\n\n/**\n * __audit_inode_child - collect inode info for created/removed objects\n * @parent: inode of dentry parent\n * @dentry: dentry being audited\n * @type:   AUDIT_TYPE_* value that we're looking for\n *\n * For syscalls that create or remove filesystem objects, audit_inode\n * can only collect information for the filesystem object's parent.\n * This call updates the audit context with the child's information.\n * Syscalls that create a new filesystem object must be hooked after\n * the object is created.  Syscalls that remove a filesystem object\n * must be hooked prior, in order to capture the target inode during\n * unsuccessful attempts.\n */\nvoid __audit_inode_child(struct inode *parent,\n\t\t\t const struct dentry *dentry,\n\t\t\t const unsigned char type)\n{\n\tstruct audit_context *context = current->audit_context;\n\tstruct inode *inode = d_backing_inode(dentry);\n\tconst char *dname = dentry->d_name.name;\n\tstruct audit_names *n, *found_parent = NULL, *found_child = NULL;\n\n\tif (!context->in_syscall)\n\t\treturn;\n\n\tif (inode)\n\t\thandle_one(inode);\n\n\t/* look for a parent entry first */\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\tif (!n->name ||\n\t\t    (n->type != AUDIT_TYPE_PARENT &&\n\t\t     n->type != AUDIT_TYPE_UNKNOWN))\n\t\t\tcontinue;\n\n\t\tif (n->ino == parent->i_ino && n->dev == parent->i_sb->s_dev &&\n\t\t    !audit_compare_dname_path(dname,\n\t\t\t\t\t      n->name->name, n->name_len)) {\n\t\t\tif (n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tn->type = AUDIT_TYPE_PARENT;\n\t\t\tfound_parent = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* is there a matching child entry? */\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\t/* can only match entries that have a name */\n\t\tif (!n->name ||\n\t\t    (n->type != type && n->type != AUDIT_TYPE_UNKNOWN))\n\t\t\tcontinue;\n\n\t\tif (!strcmp(dname, n->name->name) ||\n\t\t    !audit_compare_dname_path(dname, n->name->name,\n\t\t\t\t\t\tfound_parent ?\n\t\t\t\t\t\tfound_parent->name_len :\n\t\t\t\t\t\tAUDIT_NAME_FULL)) {\n\t\t\tif (n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tn->type = type;\n\t\t\tfound_child = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found_parent) {\n\t\t/* create a new, \"anonymous\" parent record */\n\t\tn = audit_alloc_name(context, AUDIT_TYPE_PARENT);\n\t\tif (!n)\n\t\t\treturn;\n\t\taudit_copy_inode(n, NULL, parent);\n\t}\n\n\tif (!found_child) {\n\t\tfound_child = audit_alloc_name(context, type);\n\t\tif (!found_child)\n\t\t\treturn;\n\n\t\t/* Re-use the name belonging to the slot for a matching parent\n\t\t * directory. All names for this context are relinquished in\n\t\t * audit_free_names() */\n\t\tif (found_parent) {\n\t\t\tfound_child->name = found_parent->name;\n\t\t\tfound_child->name_len = AUDIT_NAME_FULL;\n\t\t\tfound_child->name->refcnt++;\n\t\t}\n\t}\n\n\tif (inode)\n\t\taudit_copy_inode(found_child, dentry, inode);\n\telse\n\t\tfound_child->ino = AUDIT_INO_UNSET;\n}\nEXPORT_SYMBOL_GPL(__audit_inode_child);\n\n/**\n * auditsc_get_stamp - get local copies of audit_context values\n * @ctx: audit_context for the task\n * @t: timespec to store time recorded in the audit_context\n * @serial: serial value that is recorded in the audit_context\n *\n * Also sets the context as auditable.\n */\nint auditsc_get_stamp(struct audit_context *ctx,\n\t\t       struct timespec *t, unsigned int *serial)\n{\n\tif (!ctx->in_syscall)\n\t\treturn 0;\n\tif (!ctx->serial)\n\t\tctx->serial = audit_serial();\n\tt->tv_sec  = ctx->ctime.tv_sec;\n\tt->tv_nsec = ctx->ctime.tv_nsec;\n\t*serial    = ctx->serial;\n\tif (!ctx->prio) {\n\t\tctx->prio = 1;\n\t\tctx->current_state = AUDIT_RECORD_CONTEXT;\n\t}\n\treturn 1;\n}\n\n/* global counter which is incremented every time something logs in */\nstatic atomic_t session_id = ATOMIC_INIT(0);\n\nstatic int audit_set_loginuid_perm(kuid_t loginuid)\n{\n\t/* if we are unset, we don't need privs */\n\tif (!audit_loginuid_set(current))\n\t\treturn 0;\n\t/* if AUDIT_FEATURE_LOGINUID_IMMUTABLE means never ever allow a change*/\n\tif (is_audit_feature_set(AUDIT_FEATURE_LOGINUID_IMMUTABLE))\n\t\treturn -EPERM;\n\t/* it is set, you need permission */\n\tif (!capable(CAP_AUDIT_CONTROL))\n\t\treturn -EPERM;\n\t/* reject if this is not an unset and we don't allow that */\n\tif (is_audit_feature_set(AUDIT_FEATURE_ONLY_UNSET_LOGINUID) && uid_valid(loginuid))\n\t\treturn -EPERM;\n\treturn 0;\n}\n\nstatic void audit_log_set_loginuid(kuid_t koldloginuid, kuid_t kloginuid,\n\t\t\t\t   unsigned int oldsessionid, unsigned int sessionid,\n\t\t\t\t   int rc)\n{\n\tstruct audit_buffer *ab;\n\tuid_t uid, oldloginuid, loginuid;\n\n\tif (!audit_enabled)\n\t\treturn;\n\n\tuid = from_kuid(&init_user_ns, task_uid(current));\n\toldloginuid = from_kuid(&init_user_ns, koldloginuid);\n\tloginuid = from_kuid(&init_user_ns, kloginuid),\n\n\tab = audit_log_start(NULL, GFP_KERNEL, AUDIT_LOGIN);\n\tif (!ab)\n\t\treturn;\n\taudit_log_format(ab, \"pid=%d uid=%u\", task_pid_nr(current), uid);\n\taudit_log_task_context(ab);\n\taudit_log_format(ab, \" old-auid=%u auid=%u old-ses=%u ses=%u res=%d\",\n\t\t\t oldloginuid, loginuid, oldsessionid, sessionid, !rc);\n\taudit_log_end(ab);\n}\n\n/**\n * audit_set_loginuid - set current task's audit_context loginuid\n * @loginuid: loginuid value\n *\n * Returns 0.\n *\n * Called (set) from fs/proc/base.c::proc_loginuid_write().\n */\nint audit_set_loginuid(kuid_t loginuid)\n{\n\tstruct task_struct *task = current;\n\tunsigned int oldsessionid, sessionid = (unsigned int)-1;\n\tkuid_t oldloginuid;\n\tint rc;\n\n\toldloginuid = audit_get_loginuid(current);\n\toldsessionid = audit_get_sessionid(current);\n\n\trc = audit_set_loginuid_perm(loginuid);\n\tif (rc)\n\t\tgoto out;\n\n\t/* are we setting or clearing? */\n\tif (uid_valid(loginuid))\n\t\tsessionid = (unsigned int)atomic_inc_return(&session_id);\n\n\ttask->sessionid = sessionid;\n\ttask->loginuid = loginuid;\nout:\n\taudit_log_set_loginuid(oldloginuid, loginuid, oldsessionid, sessionid, rc);\n\treturn rc;\n}\n\n/**\n * __audit_mq_open - record audit data for a POSIX MQ open\n * @oflag: open flag\n * @mode: mode bits\n * @attr: queue attributes\n *\n */\nvoid __audit_mq_open(int oflag, umode_t mode, struct mq_attr *attr)\n{\n\tstruct audit_context *context = current->audit_context;\n\n\tif (attr)\n\t\tmemcpy(&context->mq_open.attr, attr, sizeof(struct mq_attr));\n\telse\n\t\tmemset(&context->mq_open.attr, 0, sizeof(struct mq_attr));\n\n\tcontext->mq_open.oflag = oflag;\n\tcontext->mq_open.mode = mode;\n\n\tcontext->type = AUDIT_MQ_OPEN;\n}\n\n/**\n * __audit_mq_sendrecv - record audit data for a POSIX MQ timed send/receive\n * @mqdes: MQ descriptor\n * @msg_len: Message length\n * @msg_prio: Message priority\n * @abs_timeout: Message timeout in absolute time\n *\n */\nvoid __audit_mq_sendrecv(mqd_t mqdes, size_t msg_len, unsigned int msg_prio,\n\t\t\tconst struct timespec *abs_timeout)\n{\n\tstruct audit_context *context = current->audit_context;\n\tstruct timespec *p = &context->mq_sendrecv.abs_timeout;\n\n\tif (abs_timeout)\n\t\tmemcpy(p, abs_timeout, sizeof(struct timespec));\n\telse\n\t\tmemset(p, 0, sizeof(struct timespec));\n\n\tcontext->mq_sendrecv.mqdes = mqdes;\n\tcontext->mq_sendrecv.msg_len = msg_len;\n\tcontext->mq_sendrecv.msg_prio = msg_prio;\n\n\tcontext->type = AUDIT_MQ_SENDRECV;\n}\n\n/**\n * __audit_mq_notify - record audit data for a POSIX MQ notify\n * @mqdes: MQ descriptor\n * @notification: Notification event\n *\n */\n\nvoid __audit_mq_notify(mqd_t mqdes, const struct sigevent *notification)\n{\n\tstruct audit_context *context = current->audit_context;\n\n\tif (notification)\n\t\tcontext->mq_notify.sigev_signo = notification->sigev_signo;\n\telse\n\t\tcontext->mq_notify.sigev_signo = 0;\n\n\tcontext->mq_notify.mqdes = mqdes;\n\tcontext->type = AUDIT_MQ_NOTIFY;\n}\n\n/**\n * __audit_mq_getsetattr - record audit data for a POSIX MQ get/set attribute\n * @mqdes: MQ descriptor\n * @mqstat: MQ flags\n *\n */\nvoid __audit_mq_getsetattr(mqd_t mqdes, struct mq_attr *mqstat)\n{\n\tstruct audit_context *context = current->audit_context;\n\tcontext->mq_getsetattr.mqdes = mqdes;\n\tcontext->mq_getsetattr.mqstat = *mqstat;\n\tcontext->type = AUDIT_MQ_GETSETATTR;\n}\n\n/**\n * audit_ipc_obj - record audit data for ipc object\n * @ipcp: ipc permissions\n *\n */\nvoid __audit_ipc_obj(struct kern_ipc_perm *ipcp)\n{\n\tstruct audit_context *context = current->audit_context;\n\tcontext->ipc.uid = ipcp->uid;\n\tcontext->ipc.gid = ipcp->gid;\n\tcontext->ipc.mode = ipcp->mode;\n\tcontext->ipc.has_perm = 0;\n\tsecurity_ipc_getsecid(ipcp, &context->ipc.osid);\n\tcontext->type = AUDIT_IPC;\n}\n\n/**\n * audit_ipc_set_perm - record audit data for new ipc permissions\n * @qbytes: msgq bytes\n * @uid: msgq user id\n * @gid: msgq group id\n * @mode: msgq mode (permissions)\n *\n * Called only after audit_ipc_obj().\n */\nvoid __audit_ipc_set_perm(unsigned long qbytes, uid_t uid, gid_t gid, umode_t mode)\n{\n\tstruct audit_context *context = current->audit_context;\n\n\tcontext->ipc.qbytes = qbytes;\n\tcontext->ipc.perm_uid = uid;\n\tcontext->ipc.perm_gid = gid;\n\tcontext->ipc.perm_mode = mode;\n\tcontext->ipc.has_perm = 1;\n}\n\nvoid __audit_bprm(struct linux_binprm *bprm)\n{\n\tstruct audit_context *context = current->audit_context;\n\n\tcontext->type = AUDIT_EXECVE;\n\tcontext->execve.argc = bprm->argc;\n}\n\n\n/**\n * audit_socketcall - record audit data for sys_socketcall\n * @nargs: number of args, which should not be more than AUDITSC_ARGS.\n * @args: args array\n *\n */\nint __audit_socketcall(int nargs, unsigned long *args)\n{\n\tstruct audit_context *context = current->audit_context;\n\n\tif (nargs <= 0 || nargs > AUDITSC_ARGS || !args)\n\t\treturn -EINVAL;\n\tcontext->type = AUDIT_SOCKETCALL;\n\tcontext->socketcall.nargs = nargs;\n\tmemcpy(context->socketcall.args, args, nargs * sizeof(unsigned long));\n\treturn 0;\n}\n\n/**\n * __audit_fd_pair - record audit data for pipe and socketpair\n * @fd1: the first file descriptor\n * @fd2: the second file descriptor\n *\n */\nvoid __audit_fd_pair(int fd1, int fd2)\n{\n\tstruct audit_context *context = current->audit_context;\n\tcontext->fds[0] = fd1;\n\tcontext->fds[1] = fd2;\n}\n\n/**\n * audit_sockaddr - record audit data for sys_bind, sys_connect, sys_sendto\n * @len: data length in user space\n * @a: data address in kernel space\n *\n * Returns 0 for success or NULL context or < 0 on error.\n */\nint __audit_sockaddr(int len, void *a)\n{\n\tstruct audit_context *context = current->audit_context;\n\n\tif (!context->sockaddr) {\n\t\tvoid *p = kmalloc(sizeof(struct sockaddr_storage), GFP_KERNEL);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\t\tcontext->sockaddr = p;\n\t}\n\n\tcontext->sockaddr_len = len;\n\tmemcpy(context->sockaddr, a, len);\n\treturn 0;\n}\n\nvoid __audit_ptrace(struct task_struct *t)\n{\n\tstruct audit_context *context = current->audit_context;\n\n\tcontext->target_pid = task_pid_nr(t);\n\tcontext->target_auid = audit_get_loginuid(t);\n\tcontext->target_uid = task_uid(t);\n\tcontext->target_sessionid = audit_get_sessionid(t);\n\tsecurity_task_getsecid(t, &context->target_sid);\n\tmemcpy(context->target_comm, t->comm, TASK_COMM_LEN);\n}\n\n/**\n * audit_signal_info - record signal info for shutting down audit subsystem\n * @sig: signal value\n * @t: task being signaled\n *\n * If the audit subsystem is being terminated, record the task (pid)\n * and uid that is doing that.\n */\nint __audit_signal_info(int sig, struct task_struct *t)\n{\n\tstruct audit_aux_data_pids *axp;\n\tstruct task_struct *tsk = current;\n\tstruct audit_context *ctx = tsk->audit_context;\n\tkuid_t uid = current_uid(), t_uid = task_uid(t);\n\n\tif (audit_pid && t->tgid == audit_pid) {\n\t\tif (sig == SIGTERM || sig == SIGHUP || sig == SIGUSR1 || sig == SIGUSR2) {\n\t\t\taudit_sig_pid = task_pid_nr(tsk);\n\t\t\tif (uid_valid(tsk->loginuid))\n\t\t\t\taudit_sig_uid = tsk->loginuid;\n\t\t\telse\n\t\t\t\taudit_sig_uid = uid;\n\t\t\tsecurity_task_getsecid(tsk, &audit_sig_sid);\n\t\t}\n\t\tif (!audit_signals || audit_dummy_context())\n\t\t\treturn 0;\n\t}\n\n\t/* optimize the common case by putting first signal recipient directly\n\t * in audit_context */\n\tif (!ctx->target_pid) {\n\t\tctx->target_pid = task_tgid_nr(t);\n\t\tctx->target_auid = audit_get_loginuid(t);\n\t\tctx->target_uid = t_uid;\n\t\tctx->target_sessionid = audit_get_sessionid(t);\n\t\tsecurity_task_getsecid(t, &ctx->target_sid);\n\t\tmemcpy(ctx->target_comm, t->comm, TASK_COMM_LEN);\n\t\treturn 0;\n\t}\n\n\taxp = (void *)ctx->aux_pids;\n\tif (!axp || axp->pid_count == AUDIT_AUX_PIDS) {\n\t\taxp = kzalloc(sizeof(*axp), GFP_ATOMIC);\n\t\tif (!axp)\n\t\t\treturn -ENOMEM;\n\n\t\taxp->d.type = AUDIT_OBJ_PID;\n\t\taxp->d.next = ctx->aux_pids;\n\t\tctx->aux_pids = (void *)axp;\n\t}\n\tBUG_ON(axp->pid_count >= AUDIT_AUX_PIDS);\n\n\taxp->target_pid[axp->pid_count] = task_tgid_nr(t);\n\taxp->target_auid[axp->pid_count] = audit_get_loginuid(t);\n\taxp->target_uid[axp->pid_count] = t_uid;\n\taxp->target_sessionid[axp->pid_count] = audit_get_sessionid(t);\n\tsecurity_task_getsecid(t, &axp->target_sid[axp->pid_count]);\n\tmemcpy(axp->target_comm[axp->pid_count], t->comm, TASK_COMM_LEN);\n\taxp->pid_count++;\n\n\treturn 0;\n}\n\n/**\n * __audit_log_bprm_fcaps - store information about a loading bprm and relevant fcaps\n * @bprm: pointer to the bprm being processed\n * @new: the proposed new credentials\n * @old: the old credentials\n *\n * Simply check if the proc already has the caps given by the file and if not\n * store the priv escalation info for later auditing at the end of the syscall\n *\n * -Eric\n */\nint __audit_log_bprm_fcaps(struct linux_binprm *bprm,\n\t\t\t   const struct cred *new, const struct cred *old)\n{\n\tstruct audit_aux_data_bprm_fcaps *ax;\n\tstruct audit_context *context = current->audit_context;\n\tstruct cpu_vfs_cap_data vcaps;\n\n\tax = kmalloc(sizeof(*ax), GFP_KERNEL);\n\tif (!ax)\n\t\treturn -ENOMEM;\n\n\tax->d.type = AUDIT_BPRM_FCAPS;\n\tax->d.next = context->aux;\n\tcontext->aux = (void *)ax;\n\n\tget_vfs_caps_from_disk(bprm->file->f_path.dentry, &vcaps);\n\n\tax->fcap.permitted = vcaps.permitted;\n\tax->fcap.inheritable = vcaps.inheritable;\n\tax->fcap.fE = !!(vcaps.magic_etc & VFS_CAP_FLAGS_EFFECTIVE);\n\tax->fcap_ver = (vcaps.magic_etc & VFS_CAP_REVISION_MASK) >> VFS_CAP_REVISION_SHIFT;\n\n\tax->old_pcap.permitted   = old->cap_permitted;\n\tax->old_pcap.inheritable = old->cap_inheritable;\n\tax->old_pcap.effective   = old->cap_effective;\n\n\tax->new_pcap.permitted   = new->cap_permitted;\n\tax->new_pcap.inheritable = new->cap_inheritable;\n\tax->new_pcap.effective   = new->cap_effective;\n\treturn 0;\n}\n\n/**\n * __audit_log_capset - store information about the arguments to the capset syscall\n * @new: the new credentials\n * @old: the old (current) credentials\n *\n * Record the arguments userspace sent to sys_capset for later printing by the\n * audit system if applicable\n */\nvoid __audit_log_capset(const struct cred *new, const struct cred *old)\n{\n\tstruct audit_context *context = current->audit_context;\n\tcontext->capset.pid = task_pid_nr(current);\n\tcontext->capset.cap.effective   = new->cap_effective;\n\tcontext->capset.cap.inheritable = new->cap_effective;\n\tcontext->capset.cap.permitted   = new->cap_permitted;\n\tcontext->type = AUDIT_CAPSET;\n}\n\nvoid __audit_mmap_fd(int fd, int flags)\n{\n\tstruct audit_context *context = current->audit_context;\n\tcontext->mmap.fd = fd;\n\tcontext->mmap.flags = flags;\n\tcontext->type = AUDIT_MMAP;\n}\n\nstatic void audit_log_task(struct audit_buffer *ab)\n{\n\tkuid_t auid, uid;\n\tkgid_t gid;\n\tunsigned int sessionid;\n\tchar comm[sizeof(current->comm)];\n\n\tauid = audit_get_loginuid(current);\n\tsessionid = audit_get_sessionid(current);\n\tcurrent_uid_gid(&uid, &gid);\n\n\taudit_log_format(ab, \"auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t from_kuid(&init_user_ns, auid),\n\t\t\t from_kuid(&init_user_ns, uid),\n\t\t\t from_kgid(&init_user_ns, gid),\n\t\t\t sessionid);\n\taudit_log_task_context(ab);\n\taudit_log_format(ab, \" pid=%d comm=\", task_pid_nr(current));\n\taudit_log_untrustedstring(ab, get_task_comm(comm, current));\n\taudit_log_d_path_exe(ab, current->mm);\n}\n\n/**\n * audit_core_dumps - record information about processes that end abnormally\n * @signr: signal value\n *\n * If a process ends with a core dump, something fishy is going on and we\n * should record the event for investigation.\n */\nvoid audit_core_dumps(long signr)\n{\n\tstruct audit_buffer *ab;\n\n\tif (!audit_enabled)\n\t\treturn;\n\n\tif (signr == SIGQUIT)\t/* don't care for those */\n\t\treturn;\n\n\tab = audit_log_start(NULL, GFP_KERNEL, AUDIT_ANOM_ABEND);\n\tif (unlikely(!ab))\n\t\treturn;\n\taudit_log_task(ab);\n\taudit_log_format(ab, \" sig=%ld\", signr);\n\taudit_log_end(ab);\n}\n\nvoid __audit_seccomp(unsigned long syscall, long signr, int code)\n{\n\tstruct audit_buffer *ab;\n\n\tab = audit_log_start(NULL, GFP_KERNEL, AUDIT_SECCOMP);\n\tif (unlikely(!ab))\n\t\treturn;\n\taudit_log_task(ab);\n\taudit_log_format(ab, \" sig=%ld arch=%x syscall=%ld compat=%d ip=0x%lx code=0x%x\",\n\t\t\t signr, syscall_get_arch(), syscall,\n\t\t\t in_compat_syscall(), KSTK_EIP(current), code);\n\taudit_log_end(ab);\n}\n\nstruct list_head *audit_killed_trees(void)\n{\n\tstruct audit_context *ctx = current->audit_context;\n\tif (likely(!ctx || !ctx->in_syscall))\n\t\treturn NULL;\n\treturn &ctx->killed_trees;\n}\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-or-later\n/* auditsc.c -- System-call auditing support\n * Handles all system-call specific auditing features.\n *\n * Copyright 2003-2004 Red Hat Inc., Durham, North Carolina.\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\n * Copyright (C) 2005, 2006 IBM Corporation\n * All Rights Reserved.\n *\n * Written by Rickard E. (Rik) Faith <faith@redhat.com>\n *\n * Many of the ideas implemented here are from Stephen C. Tweedie,\n * especially the idea of avoiding a copy by using getname.\n *\n * The method for actual interception of syscall entry and exit (not in\n * this file -- see entry.S) is based on a GPL'd patch written by\n * okir@suse.de and Copyright 2003 SuSE Linux AG.\n *\n * POSIX message queue support added by George Wilson <ltcgcw@us.ibm.com>,\n * 2006.\n *\n * The support of additional filter rules compares (>, <, >=, <=) was\n * added by Dustin Kirkland <dustin.kirkland@us.ibm.com>, 2005.\n *\n * Modified by Amy Griffis <amy.griffis@hp.com> to collect additional\n * filesystem information.\n *\n * Subject and object context labeling support added by <danjones@us.ibm.com>\n * and <dustin.kirkland@us.ibm.com> for LSPP certification compliance.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <asm/types.h>\n#include <linux/atomic.h>\n#include <linux/fs.h>\n#include <linux/namei.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/mount.h>\n#include <linux/socket.h>\n#include <linux/mqueue.h>\n#include <linux/audit.h>\n#include <linux/personality.h>\n#include <linux/time.h>\n#include <linux/netlink.h>\n#include <linux/compiler.h>\n#include <asm/unistd.h>\n#include <linux/security.h>\n#include <linux/list.h>\n#include <linux/binfmts.h>\n#include <linux/highmem.h>\n#include <linux/syscalls.h>\n#include <asm/syscall.h>\n#include <linux/capability.h>\n#include <linux/fs_struct.h>\n#include <linux/compat.h>\n#include <linux/ctype.h>\n#include <linux/string.h>\n#include <linux/uaccess.h>\n#include <linux/fsnotify_backend.h>\n#include <uapi/linux/limits.h>\n#include <uapi/linux/netfilter/nf_tables.h>\n#include <uapi/linux/openat2.h> // struct open_how\n#include <uapi/linux/fanotify.h>\n\n#include \"audit.h\"\n\n/* flags stating the success for a syscall */\n#define AUDITSC_INVALID 0\n#define AUDITSC_SUCCESS 1\n#define AUDITSC_FAILURE 2\n\n/* no execve audit message should be longer than this (userspace limits),\n * see the note near the top of audit_log_execve_info() about this value */\n#define MAX_EXECVE_AUDIT_LEN 7500\n\n/* max length to print of cmdline/proctitle value during audit */\n#define MAX_PROCTITLE_AUDIT_LEN 128\n\n/* number of audit rules */\nint audit_n_rules;\n\n/* determines whether we collect data for signals sent */\nint audit_signals;\n\nstruct audit_aux_data {\n\tstruct audit_aux_data\t*next;\n\tint\t\t\ttype;\n};\n\n/* Number of target pids per aux struct. */\n#define AUDIT_AUX_PIDS\t16\n\nstruct audit_aux_data_pids {\n\tstruct audit_aux_data\td;\n\tpid_t\t\t\ttarget_pid[AUDIT_AUX_PIDS];\n\tkuid_t\t\t\ttarget_auid[AUDIT_AUX_PIDS];\n\tkuid_t\t\t\ttarget_uid[AUDIT_AUX_PIDS];\n\tunsigned int\t\ttarget_sessionid[AUDIT_AUX_PIDS];\n\tstruct lsm_prop\t\ttarget_ref[AUDIT_AUX_PIDS];\n\tchar \t\t\ttarget_comm[AUDIT_AUX_PIDS][TASK_COMM_LEN];\n\tint\t\t\tpid_count;\n};\n\nstruct audit_aux_data_bprm_fcaps {\n\tstruct audit_aux_data\td;\n\tstruct audit_cap_data\tfcap;\n\tunsigned int\t\tfcap_ver;\n\tstruct audit_cap_data\told_pcap;\n\tstruct audit_cap_data\tnew_pcap;\n};\n\nstruct audit_tree_refs {\n\tstruct audit_tree_refs *next;\n\tstruct audit_chunk *c[31];\n};\n\nstruct audit_nfcfgop_tab {\n\tenum audit_nfcfgop\top;\n\tconst char\t\t*s;\n};\n\nstatic const struct audit_nfcfgop_tab audit_nfcfgs[] = {\n\t{ AUDIT_XT_OP_REGISTER,\t\t\t\"xt_register\"\t\t   },\n\t{ AUDIT_XT_OP_REPLACE,\t\t\t\"xt_replace\"\t\t   },\n\t{ AUDIT_XT_OP_UNREGISTER,\t\t\"xt_unregister\"\t\t   },\n\t{ AUDIT_NFT_OP_TABLE_REGISTER,\t\t\"nft_register_table\"\t   },\n\t{ AUDIT_NFT_OP_TABLE_UNREGISTER,\t\"nft_unregister_table\"\t   },\n\t{ AUDIT_NFT_OP_CHAIN_REGISTER,\t\t\"nft_register_chain\"\t   },\n\t{ AUDIT_NFT_OP_CHAIN_UNREGISTER,\t\"nft_unregister_chain\"\t   },\n\t{ AUDIT_NFT_OP_RULE_REGISTER,\t\t\"nft_register_rule\"\t   },\n\t{ AUDIT_NFT_OP_RULE_UNREGISTER,\t\t\"nft_unregister_rule\"\t   },\n\t{ AUDIT_NFT_OP_SET_REGISTER,\t\t\"nft_register_set\"\t   },\n\t{ AUDIT_NFT_OP_SET_UNREGISTER,\t\t\"nft_unregister_set\"\t   },\n\t{ AUDIT_NFT_OP_SETELEM_REGISTER,\t\"nft_register_setelem\"\t   },\n\t{ AUDIT_NFT_OP_SETELEM_UNREGISTER,\t\"nft_unregister_setelem\"   },\n\t{ AUDIT_NFT_OP_GEN_REGISTER,\t\t\"nft_register_gen\"\t   },\n\t{ AUDIT_NFT_OP_OBJ_REGISTER,\t\t\"nft_register_obj\"\t   },\n\t{ AUDIT_NFT_OP_OBJ_UNREGISTER,\t\t\"nft_unregister_obj\"\t   },\n\t{ AUDIT_NFT_OP_OBJ_RESET,\t\t\"nft_reset_obj\"\t\t   },\n\t{ AUDIT_NFT_OP_FLOWTABLE_REGISTER,\t\"nft_register_flowtable\"   },\n\t{ AUDIT_NFT_OP_FLOWTABLE_UNREGISTER,\t\"nft_unregister_flowtable\" },\n\t{ AUDIT_NFT_OP_SETELEM_RESET,\t\t\"nft_reset_setelem\"        },\n\t{ AUDIT_NFT_OP_RULE_RESET,\t\t\"nft_reset_rule\"           },\n\t{ AUDIT_NFT_OP_INVALID,\t\t\t\"nft_invalid\"\t\t   },\n};\n\nstatic int audit_match_perm(struct audit_context *ctx, int mask)\n{\n\tunsigned n;\n\n\tif (unlikely(!ctx))\n\t\treturn 0;\n\tn = ctx->major;\n\n\tswitch (audit_classify_syscall(ctx->arch, n)) {\n\tcase AUDITSC_NATIVE:\n\t\tif ((mask & AUDIT_PERM_WRITE) &&\n\t\t     audit_match_class(AUDIT_CLASS_WRITE, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_READ) &&\n\t\t     audit_match_class(AUDIT_CLASS_READ, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_ATTR) &&\n\t\t     audit_match_class(AUDIT_CLASS_CHATTR, n))\n\t\t\treturn 1;\n\t\treturn 0;\n\tcase AUDITSC_COMPAT: /* 32bit on biarch */\n\t\tif ((mask & AUDIT_PERM_WRITE) &&\n\t\t     audit_match_class(AUDIT_CLASS_WRITE_32, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_READ) &&\n\t\t     audit_match_class(AUDIT_CLASS_READ_32, n))\n\t\t\treturn 1;\n\t\tif ((mask & AUDIT_PERM_ATTR) &&\n\t\t     audit_match_class(AUDIT_CLASS_CHATTR_32, n))\n\t\t\treturn 1;\n\t\treturn 0;\n\tcase AUDITSC_OPEN:\n\t\treturn mask & ACC_MODE(ctx->argv[1]);\n\tcase AUDITSC_OPENAT:\n\t\treturn mask & ACC_MODE(ctx->argv[2]);\n\tcase AUDITSC_SOCKETCALL:\n\t\treturn ((mask & AUDIT_PERM_WRITE) && ctx->argv[0] == SYS_BIND);\n\tcase AUDITSC_EXECVE:\n\t\treturn mask & AUDIT_PERM_EXEC;\n\tcase AUDITSC_OPENAT2:\n\t\treturn mask & ACC_MODE((u32)ctx->openat2.flags);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int audit_match_filetype(struct audit_context *ctx, int val)\n{\n\tstruct audit_names *n;\n\tumode_t mode = (umode_t)val;\n\n\tif (unlikely(!ctx))\n\t\treturn 0;\n\n\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\tif ((n->ino != AUDIT_INO_UNSET) &&\n\t\t    ((n->mode & S_IFMT) == mode))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * We keep a linked list of fixed-sized (31 pointer) arrays of audit_chunk *;\n * ->first_trees points to its beginning, ->trees - to the current end of data.\n * ->tree_count is the number of free entries in array pointed to by ->trees.\n * Original condition is (NULL, NULL, 0); as soon as it grows we never revert to NULL,\n * \"empty\" becomes (p, p, 31) afterwards.  We don't shrink the list (and seriously,\n * it's going to remain 1-element for almost any setup) until we free context itself.\n * References in it _are_ dropped - at the same time we free/drop aux stuff.\n */\n\nstatic void audit_set_auditable(struct audit_context *ctx)\n{\n\tif (!ctx->prio) {\n\t\tctx->prio = 1;\n\t\tctx->current_state = AUDIT_STATE_RECORD;\n\t}\n}\n\nstatic int put_tree_ref(struct audit_context *ctx, struct audit_chunk *chunk)\n{\n\tstruct audit_tree_refs *p = ctx->trees;\n\tint left = ctx->tree_count;\n\n\tif (likely(left)) {\n\t\tp->c[--left] = chunk;\n\t\tctx->tree_count = left;\n\t\treturn 1;\n\t}\n\tif (!p)\n\t\treturn 0;\n\tp = p->next;\n\tif (p) {\n\t\tp->c[30] = chunk;\n\t\tctx->trees = p;\n\t\tctx->tree_count = 30;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int grow_tree_refs(struct audit_context *ctx)\n{\n\tstruct audit_tree_refs *p = ctx->trees;\n\n\tctx->trees = kzalloc(sizeof(struct audit_tree_refs), GFP_KERNEL);\n\tif (!ctx->trees) {\n\t\tctx->trees = p;\n\t\treturn 0;\n\t}\n\tif (p)\n\t\tp->next = ctx->trees;\n\telse\n\t\tctx->first_trees = ctx->trees;\n\tctx->tree_count = 31;\n\treturn 1;\n}\n\nstatic void unroll_tree_refs(struct audit_context *ctx,\n\t\t      struct audit_tree_refs *p, int count)\n{\n\tstruct audit_tree_refs *q;\n\tint n;\n\n\tif (!p) {\n\t\t/* we started with empty chain */\n\t\tp = ctx->first_trees;\n\t\tcount = 31;\n\t\t/* if the very first allocation has failed, nothing to do */\n\t\tif (!p)\n\t\t\treturn;\n\t}\n\tn = count;\n\tfor (q = p; q != ctx->trees; q = q->next, n = 31) {\n\t\twhile (n--) {\n\t\t\taudit_put_chunk(q->c[n]);\n\t\t\tq->c[n] = NULL;\n\t\t}\n\t}\n\twhile (n-- > ctx->tree_count) {\n\t\taudit_put_chunk(q->c[n]);\n\t\tq->c[n] = NULL;\n\t}\n\tctx->trees = p;\n\tctx->tree_count = count;\n}\n\nstatic void free_tree_refs(struct audit_context *ctx)\n{\n\tstruct audit_tree_refs *p, *q;\n\n\tfor (p = ctx->first_trees; p; p = q) {\n\t\tq = p->next;\n\t\tkfree(p);\n\t}\n}\n\nstatic int match_tree_refs(struct audit_context *ctx, struct audit_tree *tree)\n{\n\tstruct audit_tree_refs *p;\n\tint n;\n\n\tif (!tree)\n\t\treturn 0;\n\t/* full ones */\n\tfor (p = ctx->first_trees; p != ctx->trees; p = p->next) {\n\t\tfor (n = 0; n < 31; n++)\n\t\t\tif (audit_tree_match(p->c[n], tree))\n\t\t\t\treturn 1;\n\t}\n\t/* partial */\n\tif (p) {\n\t\tfor (n = ctx->tree_count; n < 31; n++)\n\t\t\tif (audit_tree_match(p->c[n], tree))\n\t\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int audit_compare_uid(kuid_t uid,\n\t\t\t     struct audit_names *name,\n\t\t\t     struct audit_field *f,\n\t\t\t     struct audit_context *ctx)\n{\n\tstruct audit_names *n;\n\tint rc;\n\n\tif (name) {\n\t\trc = audit_uid_comparator(uid, f->op, name->uid);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (ctx) {\n\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\trc = audit_uid_comparator(uid, f->op, n->uid);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int audit_compare_gid(kgid_t gid,\n\t\t\t     struct audit_names *name,\n\t\t\t     struct audit_field *f,\n\t\t\t     struct audit_context *ctx)\n{\n\tstruct audit_names *n;\n\tint rc;\n\n\tif (name) {\n\t\trc = audit_gid_comparator(gid, f->op, name->gid);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tif (ctx) {\n\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\trc = audit_gid_comparator(gid, f->op, n->gid);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int audit_field_compare(struct task_struct *tsk,\n\t\t\t       const struct cred *cred,\n\t\t\t       struct audit_field *f,\n\t\t\t       struct audit_context *ctx,\n\t\t\t       struct audit_names *name)\n{\n\tswitch (f->val) {\n\t/* process to file object comparisons */\n\tcase AUDIT_COMPARE_UID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->uid, name, f, ctx);\n\tcase AUDIT_COMPARE_GID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->gid, name, f, ctx);\n\tcase AUDIT_COMPARE_EUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->euid, name, f, ctx);\n\tcase AUDIT_COMPARE_EGID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->egid, name, f, ctx);\n\tcase AUDIT_COMPARE_AUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(audit_get_loginuid(tsk), name, f, ctx);\n\tcase AUDIT_COMPARE_SUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->suid, name, f, ctx);\n\tcase AUDIT_COMPARE_SGID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->sgid, name, f, ctx);\n\tcase AUDIT_COMPARE_FSUID_TO_OBJ_UID:\n\t\treturn audit_compare_uid(cred->fsuid, name, f, ctx);\n\tcase AUDIT_COMPARE_FSGID_TO_OBJ_GID:\n\t\treturn audit_compare_gid(cred->fsgid, name, f, ctx);\n\t/* uid comparisons */\n\tcase AUDIT_COMPARE_UID_TO_AUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op,\n\t\t\t\t\t    audit_get_loginuid(tsk));\n\tcase AUDIT_COMPARE_UID_TO_EUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op, cred->euid);\n\tcase AUDIT_COMPARE_UID_TO_SUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op, cred->suid);\n\tcase AUDIT_COMPARE_UID_TO_FSUID:\n\t\treturn audit_uid_comparator(cred->uid, f->op, cred->fsuid);\n\t/* auid comparisons */\n\tcase AUDIT_COMPARE_AUID_TO_EUID:\n\t\treturn audit_uid_comparator(audit_get_loginuid(tsk), f->op,\n\t\t\t\t\t    cred->euid);\n\tcase AUDIT_COMPARE_AUID_TO_SUID:\n\t\treturn audit_uid_comparator(audit_get_loginuid(tsk), f->op,\n\t\t\t\t\t    cred->suid);\n\tcase AUDIT_COMPARE_AUID_TO_FSUID:\n\t\treturn audit_uid_comparator(audit_get_loginuid(tsk), f->op,\n\t\t\t\t\t    cred->fsuid);\n\t/* euid comparisons */\n\tcase AUDIT_COMPARE_EUID_TO_SUID:\n\t\treturn audit_uid_comparator(cred->euid, f->op, cred->suid);\n\tcase AUDIT_COMPARE_EUID_TO_FSUID:\n\t\treturn audit_uid_comparator(cred->euid, f->op, cred->fsuid);\n\t/* suid comparisons */\n\tcase AUDIT_COMPARE_SUID_TO_FSUID:\n\t\treturn audit_uid_comparator(cred->suid, f->op, cred->fsuid);\n\t/* gid comparisons */\n\tcase AUDIT_COMPARE_GID_TO_EGID:\n\t\treturn audit_gid_comparator(cred->gid, f->op, cred->egid);\n\tcase AUDIT_COMPARE_GID_TO_SGID:\n\t\treturn audit_gid_comparator(cred->gid, f->op, cred->sgid);\n\tcase AUDIT_COMPARE_GID_TO_FSGID:\n\t\treturn audit_gid_comparator(cred->gid, f->op, cred->fsgid);\n\t/* egid comparisons */\n\tcase AUDIT_COMPARE_EGID_TO_SGID:\n\t\treturn audit_gid_comparator(cred->egid, f->op, cred->sgid);\n\tcase AUDIT_COMPARE_EGID_TO_FSGID:\n\t\treturn audit_gid_comparator(cred->egid, f->op, cred->fsgid);\n\t/* sgid comparison */\n\tcase AUDIT_COMPARE_SGID_TO_FSGID:\n\t\treturn audit_gid_comparator(cred->sgid, f->op, cred->fsgid);\n\tdefault:\n\t\tWARN(1, \"Missing AUDIT_COMPARE define.  Report as a bug\\n\");\n\t\treturn 0;\n\t}\n\treturn 0;\n}\n\n/* Determine if any context name data matches a rule's watch data */\n/* Compare a task_struct with an audit_rule.  Return 1 on match, 0\n * otherwise.\n *\n * If task_creation is true, this is an explicit indication that we are\n * filtering a task rule at task creation time.  This and tsk == current are\n * the only situations where tsk->cred may be accessed without an rcu read lock.\n */\nstatic int audit_filter_rules(struct task_struct *tsk,\n\t\t\t      struct audit_krule *rule,\n\t\t\t      struct audit_context *ctx,\n\t\t\t      struct audit_names *name,\n\t\t\t      enum audit_state *state,\n\t\t\t      bool task_creation)\n{\n\tconst struct cred *cred;\n\tint i, need_sid = 1;\n\tstruct lsm_prop prop = { };\n\tunsigned int sessionid;\n\n\tif (ctx && rule->prio <= ctx->prio)\n\t\treturn 0;\n\n\tcred = rcu_dereference_check(tsk->cred, tsk == current || task_creation);\n\n\tfor (i = 0; i < rule->field_count; i++) {\n\t\tstruct audit_field *f = &rule->fields[i];\n\t\tstruct audit_names *n;\n\t\tint result = 0;\n\t\tpid_t pid;\n\n\t\tswitch (f->type) {\n\t\tcase AUDIT_PID:\n\t\t\tpid = task_tgid_nr(tsk);\n\t\t\tresult = audit_comparator(pid, f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_PPID:\n\t\t\tif (ctx) {\n\t\t\t\tif (!ctx->ppid)\n\t\t\t\t\tctx->ppid = task_ppid_nr(tsk);\n\t\t\t\tresult = audit_comparator(ctx->ppid, f->op, f->val);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_EXE:\n\t\t\tresult = audit_exe_compare(tsk, rule->exe);\n\t\t\tif (f->op == Audit_not_equal)\n\t\t\t\tresult = !result;\n\t\t\tbreak;\n\t\tcase AUDIT_UID:\n\t\t\tresult = audit_uid_comparator(cred->uid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_EUID:\n\t\t\tresult = audit_uid_comparator(cred->euid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_SUID:\n\t\t\tresult = audit_uid_comparator(cred->suid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_FSUID:\n\t\t\tresult = audit_uid_comparator(cred->fsuid, f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_GID:\n\t\t\tresult = audit_gid_comparator(cred->gid, f->op, f->gid);\n\t\t\tif (f->op == Audit_equal) {\n\t\t\t\tif (!result)\n\t\t\t\t\tresult = groups_search(cred->group_info, f->gid);\n\t\t\t} else if (f->op == Audit_not_equal) {\n\t\t\t\tif (result)\n\t\t\t\t\tresult = !groups_search(cred->group_info, f->gid);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_EGID:\n\t\t\tresult = audit_gid_comparator(cred->egid, f->op, f->gid);\n\t\t\tif (f->op == Audit_equal) {\n\t\t\t\tif (!result)\n\t\t\t\t\tresult = groups_search(cred->group_info, f->gid);\n\t\t\t} else if (f->op == Audit_not_equal) {\n\t\t\t\tif (result)\n\t\t\t\t\tresult = !groups_search(cred->group_info, f->gid);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_SGID:\n\t\t\tresult = audit_gid_comparator(cred->sgid, f->op, f->gid);\n\t\t\tbreak;\n\t\tcase AUDIT_FSGID:\n\t\t\tresult = audit_gid_comparator(cred->fsgid, f->op, f->gid);\n\t\t\tbreak;\n\t\tcase AUDIT_SESSIONID:\n\t\t\tsessionid = audit_get_sessionid(tsk);\n\t\t\tresult = audit_comparator(sessionid, f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_PERS:\n\t\t\tresult = audit_comparator(tsk->personality, f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_ARCH:\n\t\t\tif (ctx)\n\t\t\t\tresult = audit_comparator(ctx->arch, f->op, f->val);\n\t\t\tbreak;\n\n\t\tcase AUDIT_EXIT:\n\t\t\tif (ctx && ctx->return_valid != AUDITSC_INVALID)\n\t\t\t\tresult = audit_comparator(ctx->return_code, f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_SUCCESS:\n\t\t\tif (ctx && ctx->return_valid != AUDITSC_INVALID) {\n\t\t\t\tif (f->val)\n\t\t\t\t\tresult = audit_comparator(ctx->return_valid, f->op, AUDITSC_SUCCESS);\n\t\t\t\telse\n\t\t\t\t\tresult = audit_comparator(ctx->return_valid, f->op, AUDITSC_FAILURE);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_DEVMAJOR:\n\t\t\tif (name) {\n\t\t\t\tif (audit_comparator(MAJOR(name->dev), f->op, f->val) ||\n\t\t\t\t    audit_comparator(MAJOR(name->rdev), f->op, f->val))\n\t\t\t\t\t++result;\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_comparator(MAJOR(n->dev), f->op, f->val) ||\n\t\t\t\t\t    audit_comparator(MAJOR(n->rdev), f->op, f->val)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_DEVMINOR:\n\t\t\tif (name) {\n\t\t\t\tif (audit_comparator(MINOR(name->dev), f->op, f->val) ||\n\t\t\t\t    audit_comparator(MINOR(name->rdev), f->op, f->val))\n\t\t\t\t\t++result;\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_comparator(MINOR(n->dev), f->op, f->val) ||\n\t\t\t\t\t    audit_comparator(MINOR(n->rdev), f->op, f->val)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_INODE:\n\t\t\tif (name)\n\t\t\t\tresult = audit_comparator(name->ino, f->op, f->val);\n\t\t\telse if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_comparator(n->ino, f->op, f->val)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_OBJ_UID:\n\t\t\tif (name) {\n\t\t\t\tresult = audit_uid_comparator(name->uid, f->op, f->uid);\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_uid_comparator(n->uid, f->op, f->uid)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_OBJ_GID:\n\t\t\tif (name) {\n\t\t\t\tresult = audit_gid_comparator(name->gid, f->op, f->gid);\n\t\t\t} else if (ctx) {\n\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\tif (audit_gid_comparator(n->gid, f->op, f->gid)) {\n\t\t\t\t\t\t++result;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_WATCH:\n\t\t\tif (name) {\n\t\t\t\tresult = audit_watch_compare(rule->watch,\n\t\t\t\t\t\t\t     name->ino,\n\t\t\t\t\t\t\t     name->dev);\n\t\t\t\tif (f->op == Audit_not_equal)\n\t\t\t\t\tresult = !result;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_DIR:\n\t\t\tif (ctx) {\n\t\t\t\tresult = match_tree_refs(ctx, rule->tree);\n\t\t\t\tif (f->op == Audit_not_equal)\n\t\t\t\t\tresult = !result;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_LOGINUID:\n\t\t\tresult = audit_uid_comparator(audit_get_loginuid(tsk),\n\t\t\t\t\t\t      f->op, f->uid);\n\t\t\tbreak;\n\t\tcase AUDIT_LOGINUID_SET:\n\t\t\tresult = audit_comparator(audit_loginuid_set(tsk), f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_SADDR_FAM:\n\t\t\tif (ctx && ctx->sockaddr)\n\t\t\t\tresult = audit_comparator(ctx->sockaddr->ss_family,\n\t\t\t\t\t\t\t  f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_SUBJ_USER:\n\t\tcase AUDIT_SUBJ_ROLE:\n\t\tcase AUDIT_SUBJ_TYPE:\n\t\tcase AUDIT_SUBJ_SEN:\n\t\tcase AUDIT_SUBJ_CLR:\n\t\t\t/* NOTE: this may return negative values indicating\n\t\t\t   a temporary error.  We simply treat this as a\n\t\t\t   match for now to avoid losing information that\n\t\t\t   may be wanted.   An error message will also be\n\t\t\t   logged upon error */\n\t\t\tif (f->lsm_rule) {\n\t\t\t\tif (need_sid) {\n\t\t\t\t\t/* @tsk should always be equal to\n\t\t\t\t\t * @current with the exception of\n\t\t\t\t\t * fork()/copy_process() in which case\n\t\t\t\t\t * the new @tsk creds are still a dup\n\t\t\t\t\t * of @current's creds so we can still\n\t\t\t\t\t * use\n\t\t\t\t\t * security_current_getlsmprop_subj()\n\t\t\t\t\t * here even though it always refs\n\t\t\t\t\t * @current's creds\n\t\t\t\t\t */\n\t\t\t\t\tsecurity_current_getlsmprop_subj(&prop);\n\t\t\t\t\tneed_sid = 0;\n\t\t\t\t}\n\t\t\t\tresult = security_audit_rule_match(&prop,\n\t\t\t\t\t\t\t\t   f->type,\n\t\t\t\t\t\t\t\t   f->op,\n\t\t\t\t\t\t\t\t   f->lsm_rule);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_OBJ_USER:\n\t\tcase AUDIT_OBJ_ROLE:\n\t\tcase AUDIT_OBJ_TYPE:\n\t\tcase AUDIT_OBJ_LEV_LOW:\n\t\tcase AUDIT_OBJ_LEV_HIGH:\n\t\t\t/* The above note for AUDIT_SUBJ_USER...AUDIT_SUBJ_CLR\n\t\t\t   also applies here */\n\t\t\tif (f->lsm_rule) {\n\t\t\t\t/* Find files that match */\n\t\t\t\tif (name) {\n\t\t\t\t\tresult = security_audit_rule_match(\n\t\t\t\t\t\t\t\t&name->oprop,\n\t\t\t\t\t\t\t\tf->type,\n\t\t\t\t\t\t\t\tf->op,\n\t\t\t\t\t\t\t\tf->lsm_rule);\n\t\t\t\t} else if (ctx) {\n\t\t\t\t\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\t\t\t\t\tif (security_audit_rule_match(\n\t\t\t\t\t\t\t\t&n->oprop,\n\t\t\t\t\t\t\t\tf->type,\n\t\t\t\t\t\t\t\tf->op,\n\t\t\t\t\t\t\t\tf->lsm_rule)) {\n\t\t\t\t\t\t\t++result;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t/* Find ipc objects that match */\n\t\t\t\tif (!ctx || ctx->type != AUDIT_IPC)\n\t\t\t\t\tbreak;\n\t\t\t\tif (security_audit_rule_match(&ctx->ipc.oprop,\n\t\t\t\t\t\t\t      f->type, f->op,\n\t\t\t\t\t\t\t      f->lsm_rule))\n\t\t\t\t\t++result;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase AUDIT_ARG0:\n\t\tcase AUDIT_ARG1:\n\t\tcase AUDIT_ARG2:\n\t\tcase AUDIT_ARG3:\n\t\t\tif (ctx)\n\t\t\t\tresult = audit_comparator(ctx->argv[f->type-AUDIT_ARG0], f->op, f->val);\n\t\t\tbreak;\n\t\tcase AUDIT_FILTERKEY:\n\t\t\t/* ignore this field for filtering */\n\t\t\tresult = 1;\n\t\t\tbreak;\n\t\tcase AUDIT_PERM:\n\t\t\tresult = audit_match_perm(ctx, f->val);\n\t\t\tif (f->op == Audit_not_equal)\n\t\t\t\tresult = !result;\n\t\t\tbreak;\n\t\tcase AUDIT_FILETYPE:\n\t\t\tresult = audit_match_filetype(ctx, f->val);\n\t\t\tif (f->op == Audit_not_equal)\n\t\t\t\tresult = !result;\n\t\t\tbreak;\n\t\tcase AUDIT_FIELD_COMPARE:\n\t\t\tresult = audit_field_compare(tsk, cred, f, ctx, name);\n\t\t\tbreak;\n\t\t}\n\t\tif (!result)\n\t\t\treturn 0;\n\t}\n\n\tif (ctx) {\n\t\tif (rule->filterkey) {\n\t\t\tkfree(ctx->filterkey);\n\t\t\tctx->filterkey = kstrdup(rule->filterkey, GFP_ATOMIC);\n\t\t}\n\t\tctx->prio = rule->prio;\n\t}\n\tswitch (rule->action) {\n\tcase AUDIT_NEVER:\n\t\t*state = AUDIT_STATE_DISABLED;\n\t\tbreak;\n\tcase AUDIT_ALWAYS:\n\t\t*state = AUDIT_STATE_RECORD;\n\t\tbreak;\n\t}\n\treturn 1;\n}\n\n/* At process creation time, we can determine if system-call auditing is\n * completely disabled for this task.  Since we only have the task\n * structure at this point, we can only check uid and gid.\n */\nstatic enum audit_state audit_filter_task(struct task_struct *tsk, char **key)\n{\n\tstruct audit_entry *e;\n\tenum audit_state   state;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(e, &audit_filter_list[AUDIT_FILTER_TASK], list) {\n\t\tif (audit_filter_rules(tsk, &e->rule, NULL, NULL,\n\t\t\t\t       &state, true)) {\n\t\t\tif (state == AUDIT_STATE_RECORD)\n\t\t\t\t*key = kstrdup(e->rule.filterkey, GFP_ATOMIC);\n\t\t\trcu_read_unlock();\n\t\t\treturn state;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn AUDIT_STATE_BUILD;\n}\n\nstatic int audit_in_mask(const struct audit_krule *rule, unsigned long val)\n{\n\tint word, bit;\n\n\tif (val > 0xffffffff)\n\t\treturn false;\n\n\tword = AUDIT_WORD(val);\n\tif (word >= AUDIT_BITMASK_SIZE)\n\t\treturn false;\n\n\tbit = AUDIT_BIT(val);\n\n\treturn rule->mask[word] & bit;\n}\n\n/**\n * __audit_filter_op - common filter helper for operations (syscall/uring/etc)\n * @tsk: associated task\n * @ctx: audit context\n * @list: audit filter list\n * @name: audit_name (can be NULL)\n * @op: current syscall/uring_op\n *\n * Run the udit filters specified in @list against @tsk using @ctx,\n * @name, and @op, as necessary; the caller is responsible for ensuring\n * that the call is made while the RCU read lock is held. The @name\n * parameter can be NULL, but all others must be specified.\n * Returns 1/true if the filter finds a match, 0/false if none are found.\n */\nstatic int __audit_filter_op(struct task_struct *tsk,\n\t\t\t   struct audit_context *ctx,\n\t\t\t   struct list_head *list,\n\t\t\t   struct audit_names *name,\n\t\t\t   unsigned long op)\n{\n\tstruct audit_entry *e;\n\tenum audit_state state;\n\n\tlist_for_each_entry_rcu(e, list, list) {\n\t\tif (audit_in_mask(&e->rule, op) &&\n\t\t    audit_filter_rules(tsk, &e->rule, ctx, name,\n\t\t\t\t       &state, false)) {\n\t\t\tctx->current_state = state;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n * audit_filter_uring - apply filters to an io_uring operation\n * @tsk: associated task\n * @ctx: audit context\n */\nstatic void audit_filter_uring(struct task_struct *tsk,\n\t\t\t       struct audit_context *ctx)\n{\n\tif (auditd_test_task(tsk))\n\t\treturn;\n\n\trcu_read_lock();\n\t__audit_filter_op(tsk, ctx, &audit_filter_list[AUDIT_FILTER_URING_EXIT],\n\t\t\tNULL, ctx->uring_op);\n\trcu_read_unlock();\n}\n\n/* At syscall exit time, this filter is called if the audit_state is\n * not low enough that auditing cannot take place, but is also not\n * high enough that we already know we have to write an audit record\n * (i.e., the state is AUDIT_STATE_BUILD).\n */\nstatic void audit_filter_syscall(struct task_struct *tsk,\n\t\t\t\t struct audit_context *ctx)\n{\n\tif (auditd_test_task(tsk))\n\t\treturn;\n\n\trcu_read_lock();\n\t__audit_filter_op(tsk, ctx, &audit_filter_list[AUDIT_FILTER_EXIT],\n\t\t\tNULL, ctx->major);\n\trcu_read_unlock();\n}\n\n/*\n * Given an audit_name check the inode hash table to see if they match.\n * Called holding the rcu read lock to protect the use of audit_inode_hash\n */\nstatic int audit_filter_inode_name(struct task_struct *tsk,\n\t\t\t\t   struct audit_names *n,\n\t\t\t\t   struct audit_context *ctx)\n{\n\tint h = audit_hash_ino((u32)n->ino);\n\tstruct list_head *list = &audit_inode_hash[h];\n\n\treturn __audit_filter_op(tsk, ctx, list, n, ctx->major);\n}\n\n/* At syscall exit time, this filter is called if any audit_names have been\n * collected during syscall processing.  We only check rules in sublists at hash\n * buckets applicable to the inode numbers in audit_names.\n * Regarding audit_state, same rules apply as for audit_filter_syscall().\n */\nvoid audit_filter_inodes(struct task_struct *tsk, struct audit_context *ctx)\n{\n\tstruct audit_names *n;\n\n\tif (auditd_test_task(tsk))\n\t\treturn;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry(n, &ctx->names_list, list) {\n\t\tif (audit_filter_inode_name(tsk, n, ctx))\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n}\n\nstatic inline void audit_proctitle_free(struct audit_context *context)\n{\n\tkfree(context->proctitle.value);\n\tcontext->proctitle.value = NULL;\n\tcontext->proctitle.len = 0;\n}\n\nstatic inline void audit_free_module(struct audit_context *context)\n{\n\tif (context->type == AUDIT_KERN_MODULE) {\n\t\tkfree(context->module.name);\n\t\tcontext->module.name = NULL;\n\t}\n}\nstatic inline void audit_free_names(struct audit_context *context)\n{\n\tstruct audit_names *n, *next;\n\n\tlist_for_each_entry_safe(n, next, &context->names_list, list) {\n\t\tlist_del(&n->list);\n\t\tif (n->name)\n\t\t\tputname(n->name);\n\t\tif (n->should_free)\n\t\t\tkfree(n);\n\t}\n\tcontext->name_count = 0;\n\tpath_put(&context->pwd);\n\tcontext->pwd.dentry = NULL;\n\tcontext->pwd.mnt = NULL;\n}\n\nstatic inline void audit_free_aux(struct audit_context *context)\n{\n\tstruct audit_aux_data *aux;\n\n\twhile ((aux = context->aux)) {\n\t\tcontext->aux = aux->next;\n\t\tkfree(aux);\n\t}\n\tcontext->aux = NULL;\n\twhile ((aux = context->aux_pids)) {\n\t\tcontext->aux_pids = aux->next;\n\t\tkfree(aux);\n\t}\n\tcontext->aux_pids = NULL;\n}\n\n/**\n * audit_reset_context - reset a audit_context structure\n * @ctx: the audit_context to reset\n *\n * All fields in the audit_context will be reset to an initial state, all\n * references held by fields will be dropped, and private memory will be\n * released.  When this function returns the audit_context will be suitable\n * for reuse, so long as the passed context is not NULL or a dummy context.\n */\nstatic void audit_reset_context(struct audit_context *ctx)\n{\n\tif (!ctx)\n\t\treturn;\n\n\t/* if ctx is non-null, reset the \"ctx->context\" regardless */\n\tctx->context = AUDIT_CTX_UNUSED;\n\tif (ctx->dummy)\n\t\treturn;\n\n\t/*\n\t * NOTE: It shouldn't matter in what order we release the fields, so\n\t *       release them in the order in which they appear in the struct;\n\t *       this gives us some hope of quickly making sure we are\n\t *       resetting the audit_context properly.\n\t *\n\t *       Other things worth mentioning:\n\t *       - we don't reset \"dummy\"\n\t *       - we don't reset \"state\", we do reset \"current_state\"\n\t *       - we preserve \"filterkey\" if \"state\" is AUDIT_STATE_RECORD\n\t *       - much of this is likely overkill, but play it safe for now\n\t *       - we really need to work on improving the audit_context struct\n\t */\n\n\tctx->current_state = ctx->state;\n\tctx->stamp.serial = 0;\n\tctx->stamp.ctime = (struct timespec64){ .tv_sec = 0, .tv_nsec = 0 };\n\tctx->major = 0;\n\tctx->uring_op = 0;\n\tmemset(ctx->argv, 0, sizeof(ctx->argv));\n\tctx->return_code = 0;\n\tctx->prio = (ctx->state == AUDIT_STATE_RECORD ? ~0ULL : 0);\n\tctx->return_valid = AUDITSC_INVALID;\n\taudit_free_names(ctx);\n\tif (ctx->state != AUDIT_STATE_RECORD) {\n\t\tkfree(ctx->filterkey);\n\t\tctx->filterkey = NULL;\n\t}\n\taudit_free_aux(ctx);\n\tkfree(ctx->sockaddr);\n\tctx->sockaddr = NULL;\n\tctx->sockaddr_len = 0;\n\tctx->ppid = 0;\n\tctx->uid = ctx->euid = ctx->suid = ctx->fsuid = KUIDT_INIT(0);\n\tctx->gid = ctx->egid = ctx->sgid = ctx->fsgid = KGIDT_INIT(0);\n\tctx->personality = 0;\n\tctx->arch = 0;\n\tctx->target_pid = 0;\n\tctx->target_auid = ctx->target_uid = KUIDT_INIT(0);\n\tctx->target_sessionid = 0;\n\tlsmprop_init(&ctx->target_ref);\n\tctx->target_comm[0] = '\\0';\n\tunroll_tree_refs(ctx, NULL, 0);\n\tWARN_ON(!list_empty(&ctx->killed_trees));\n\taudit_free_module(ctx);\n\tctx->fds[0] = -1;\n\tctx->type = 0; /* reset last for audit_free_*() */\n}\n\nstatic inline struct audit_context *audit_alloc_context(enum audit_state state)\n{\n\tstruct audit_context *context;\n\n\tcontext = kzalloc(sizeof(*context), GFP_KERNEL);\n\tif (!context)\n\t\treturn NULL;\n\tcontext->context = AUDIT_CTX_UNUSED;\n\tcontext->state = state;\n\tcontext->prio = state == AUDIT_STATE_RECORD ? ~0ULL : 0;\n\tINIT_LIST_HEAD(&context->killed_trees);\n\tINIT_LIST_HEAD(&context->names_list);\n\tcontext->fds[0] = -1;\n\tcontext->return_valid = AUDITSC_INVALID;\n\treturn context;\n}\n\n/**\n * audit_alloc - allocate an audit context block for a task\n * @tsk: task\n *\n * Filter on the task information and allocate a per-task audit context\n * if necessary.  Doing so turns on system call auditing for the\n * specified task.  This is called from copy_process, so no lock is\n * needed.\n */\nint audit_alloc(struct task_struct *tsk)\n{\n\tstruct audit_context *context;\n\tenum audit_state     state;\n\tchar *key = NULL;\n\n\tif (likely(!audit_ever_enabled))\n\t\treturn 0;\n\n\tstate = audit_filter_task(tsk, &key);\n\tif (state == AUDIT_STATE_DISABLED) {\n\t\tclear_task_syscall_work(tsk, SYSCALL_AUDIT);\n\t\treturn 0;\n\t}\n\n\tcontext = audit_alloc_context(state);\n\tif (!context) {\n\t\tkfree(key);\n\t\taudit_log_lost(\"out of memory in audit_alloc\");\n\t\treturn -ENOMEM;\n\t}\n\tcontext->filterkey = key;\n\n\taudit_set_context(tsk, context);\n\tset_task_syscall_work(tsk, SYSCALL_AUDIT);\n\treturn 0;\n}\n\nstatic inline void audit_free_context(struct audit_context *context)\n{\n\t/* resetting is extra work, but it is likely just noise */\n\taudit_reset_context(context);\n\taudit_proctitle_free(context);\n\tfree_tree_refs(context);\n\tkfree(context->filterkey);\n\tkfree(context);\n}\n\nstatic int audit_log_pid_context(struct audit_context *context, pid_t pid,\n\t\t\t\t kuid_t auid, kuid_t uid,\n\t\t\t\t unsigned int sessionid, struct lsm_prop *prop,\n\t\t\t\t char *comm)\n{\n\tstruct audit_buffer *ab;\n\tstruct lsm_context ctx;\n\tint rc = 0;\n\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_OBJ_PID);\n\tif (!ab)\n\t\treturn rc;\n\n\taudit_log_format(ab, \"opid=%d oauid=%d ouid=%d oses=%d\", pid,\n\t\t\t from_kuid(&init_user_ns, auid),\n\t\t\t from_kuid(&init_user_ns, uid), sessionid);\n\tif (lsmprop_is_set(prop)) {\n\t\tif (security_lsmprop_to_secctx(prop, &ctx, LSM_ID_UNDEF) < 0) {\n\t\t\taudit_log_format(ab, \" obj=(none)\");\n\t\t\trc = 1;\n\t\t} else {\n\t\t\taudit_log_format(ab, \" obj=%s\", ctx.context);\n\t\t\tsecurity_release_secctx(&ctx);\n\t\t}\n\t}\n\taudit_log_format(ab, \" ocomm=\");\n\taudit_log_untrustedstring(ab, comm);\n\taudit_log_end(ab);\n\n\treturn rc;\n}\n\nstatic void audit_log_execve_info(struct audit_context *context,\n\t\t\t\t  struct audit_buffer **ab)\n{\n\tlong len_max;\n\tlong len_rem;\n\tlong len_full;\n\tlong len_buf;\n\tlong len_abuf = 0;\n\tlong len_tmp;\n\tbool require_data;\n\tbool encode;\n\tunsigned int iter;\n\tunsigned int arg;\n\tchar *buf_head;\n\tchar *buf;\n\tconst char __user *p = (const char __user *)current->mm->arg_start;\n\n\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n\t *       data we put in the audit record for this argument (see the\n\t *       code below) ... at this point in time 96 is plenty */\n\tchar abuf[96];\n\n\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n\t *       current value of 7500 is not as important as the fact that it\n\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n\t *       room if we go over a little bit in the logging below */\n\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n\tlen_max = MAX_EXECVE_AUDIT_LEN;\n\n\t/* scratch buffer to hold the userspace args */\n\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n\tif (!buf_head) {\n\t\taudit_panic(\"out of memory for argv string\");\n\t\treturn;\n\t}\n\tbuf = buf_head;\n\n\taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\n\n\tlen_rem = len_max;\n\tlen_buf = 0;\n\tlen_full = 0;\n\trequire_data = true;\n\tencode = false;\n\titer = 0;\n\targ = 0;\n\tdo {\n\t\t/* NOTE: we don't ever want to trust this value for anything\n\t\t *       serious, but the audit record format insists we\n\t\t *       provide an argument length for really long arguments,\n\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n\t\t *       to use strncpy_from_user() to obtain this value for\n\t\t *       recording in the log, although we don't use it\n\t\t *       anywhere here to avoid a double-fetch problem */\n\t\tif (len_full == 0)\n\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n\n\t\t/* read more data from userspace */\n\t\tif (require_data) {\n\t\t\t/* can we make more room in the buffer? */\n\t\t\tif (buf != buf_head) {\n\t\t\t\tmemmove(buf_head, buf, len_buf);\n\t\t\t\tbuf = buf_head;\n\t\t\t}\n\n\t\t\t/* fetch as much as we can of the argument */\n\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n\t\t\t\t\t\t    len_max - len_buf);\n\t\t\tif (len_tmp == -EFAULT) {\n\t\t\t\t/* unable to copy from userspace */\n\t\t\t\tsend_sig(SIGKILL, current, 0);\n\t\t\t\tgoto out;\n\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n\t\t\t\t/* buffer is not large enough */\n\t\t\t\trequire_data = true;\n\t\t\t\t/* NOTE: if we are going to span multiple\n\t\t\t\t *       buffers force the encoding so we stand\n\t\t\t\t *       a chance at a sane len_full value and\n\t\t\t\t *       consistent record encoding */\n\t\t\t\tencode = true;\n\t\t\t\tlen_full = len_full * 2;\n\t\t\t\tp += len_tmp;\n\t\t\t} else {\n\t\t\t\trequire_data = false;\n\t\t\t\tif (!encode)\n\t\t\t\t\tencode = audit_string_contains_control(\n\t\t\t\t\t\t\t\tbuf, len_tmp);\n\t\t\t\t/* try to use a trusted value for len_full */\n\t\t\t\tif (len_full < len_max)\n\t\t\t\t\tlen_full = (encode ?\n\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n\t\t\t\tp += len_tmp + 1;\n\t\t\t}\n\t\t\tlen_buf += len_tmp;\n\t\t\tbuf_head[len_buf] = '\\0';\n\n\t\t\t/* length of the buffer in the audit record? */\n\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n\t\t}\n\n\t\t/* write as much as we can to the audit log */\n\t\tif (len_buf >= 0) {\n\t\t\t/* NOTE: some magic numbers here - basically if we\n\t\t\t *       can't fit a reasonable amount of data into the\n\t\t\t *       existing audit buffer, flush it and start with\n\t\t\t *       a new buffer */\n\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n\t\t\t\tlen_rem = len_max;\n\t\t\t\taudit_log_end(*ab);\n\t\t\t\t*ab = audit_log_start(context,\n\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n\t\t\t\tif (!*ab)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t/* create the non-arg portion of the arg record */\n\t\t\tlen_tmp = 0;\n\t\t\tif (require_data || (iter > 0) ||\n\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n\t\t\t\tif (iter == 0) {\n\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n\t\t\t\t\t\t\t\" a%d_len=%lu\",\n\t\t\t\t\t\t\targ, len_full);\n\t\t\t\t}\n\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n\t\t\t\t\t\t    \" a%d[%d]=\", arg, iter++);\n\t\t\t} else\n\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n\t\t\t\t\t\t    \" a%d=\", arg);\n\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n\n\t\t\t/* log the arg in the audit record */\n\t\t\taudit_log_format(*ab, \"%s\", abuf);\n\t\t\tlen_rem -= len_tmp;\n\t\t\tlen_tmp = len_buf;\n\t\t\tif (encode) {\n\t\t\t\tif (len_abuf > len_rem)\n\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n\t\t\t\tlen_rem -= len_tmp * 2;\n\t\t\t\tlen_abuf -= len_tmp * 2;\n\t\t\t} else {\n\t\t\t\tif (len_abuf > len_rem)\n\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n\t\t\t\tlen_rem -= len_tmp + 2;\n\t\t\t\t/* don't subtract the \"2\" because we still need\n\t\t\t\t * to add quotes to the remaining string */\n\t\t\t\tlen_abuf -= len_tmp;\n\t\t\t}\n\t\t\tlen_buf -= len_tmp;\n\t\t\tbuf += len_tmp;\n\t\t}\n\n\t\t/* ready to move to the next argument? */\n\t\tif ((len_buf == 0) && !require_data) {\n\t\t\targ++;\n\t\t\titer = 0;\n\t\t\tlen_full = 0;\n\t\t\trequire_data = true;\n\t\t\tencode = false;\n\t\t}\n\t} while (arg < context->execve.argc);\n\n\t/* NOTE: the caller handles the final audit_log_end() call */\n\nout:\n\tkfree(buf_head);\n}\n\nstatic void audit_log_cap(struct audit_buffer *ab, char *prefix,\n\t\t\t  kernel_cap_t *cap)\n{\n\tif (cap_isclear(*cap)) {\n\t\taudit_log_format(ab, \" %s=0\", prefix);\n\t\treturn;\n\t}\n\taudit_log_format(ab, \" %s=%016llx\", prefix, cap->val);\n}\n\nstatic void audit_log_fcaps(struct audit_buffer *ab, struct audit_names *name)\n{\n\tif (name->fcap_ver == -1) {\n\t\taudit_log_format(ab, \" cap_fe=? cap_fver=? cap_fp=? cap_fi=?\");\n\t\treturn;\n\t}\n\taudit_log_cap(ab, \"cap_fp\", &name->fcap.permitted);\n\taudit_log_cap(ab, \"cap_fi\", &name->fcap.inheritable);\n\taudit_log_format(ab, \" cap_fe=%d cap_fver=%x cap_frootid=%d\",\n\t\t\t name->fcap.fE, name->fcap_ver,\n\t\t\t from_kuid(&init_user_ns, name->fcap.rootid));\n}\n\nstatic void audit_log_time(struct audit_context *context, struct audit_buffer **ab)\n{\n\tconst struct audit_ntp_data *ntp = &context->time.ntp_data;\n\tconst struct timespec64 *tk = &context->time.tk_injoffset;\n\tstatic const char * const ntp_name[] = {\n\t\t\"offset\",\n\t\t\"freq\",\n\t\t\"status\",\n\t\t\"tai\",\n\t\t\"tick\",\n\t\t\"adjust\",\n\t};\n\tint type;\n\n\tif (context->type == AUDIT_TIME_ADJNTPVAL) {\n\t\tfor (type = 0; type < AUDIT_NTP_NVALS; type++) {\n\t\t\tif (ntp->vals[type].newval != ntp->vals[type].oldval) {\n\t\t\t\tif (!*ab) {\n\t\t\t\t\t*ab = audit_log_start(context,\n\t\t\t\t\t\t\tGFP_KERNEL,\n\t\t\t\t\t\t\tAUDIT_TIME_ADJNTPVAL);\n\t\t\t\t\tif (!*ab)\n\t\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\taudit_log_format(*ab, \"op=%s old=%lli new=%lli\",\n\t\t\t\t\t\t ntp_name[type],\n\t\t\t\t\t\t ntp->vals[type].oldval,\n\t\t\t\t\t\t ntp->vals[type].newval);\n\t\t\t\taudit_log_end(*ab);\n\t\t\t\t*ab = NULL;\n\t\t\t}\n\t\t}\n\t}\n\tif (tk->tv_sec != 0 || tk->tv_nsec != 0) {\n\t\tif (!*ab) {\n\t\t\t*ab = audit_log_start(context, GFP_KERNEL,\n\t\t\t\t\t      AUDIT_TIME_INJOFFSET);\n\t\t\tif (!*ab)\n\t\t\t\treturn;\n\t\t}\n\t\taudit_log_format(*ab, \"sec=%lli nsec=%li\",\n\t\t\t\t (long long)tk->tv_sec, tk->tv_nsec);\n\t\taudit_log_end(*ab);\n\t\t*ab = NULL;\n\t}\n}\n\nstatic void show_special(struct audit_context *context, int *call_panic)\n{\n\tstruct audit_buffer *ab;\n\tint i;\n\n\tab = audit_log_start(context, GFP_KERNEL, context->type);\n\tif (!ab)\n\t\treturn;\n\n\tswitch (context->type) {\n\tcase AUDIT_SOCKETCALL: {\n\t\tint nargs = context->socketcall.nargs;\n\n\t\taudit_log_format(ab, \"nargs=%d\", nargs);\n\t\tfor (i = 0; i < nargs; i++)\n\t\t\taudit_log_format(ab, \" a%d=%lx\", i,\n\t\t\t\tcontext->socketcall.args[i]);\n\t\tbreak; }\n\tcase AUDIT_IPC:\n\t\taudit_log_format(ab, \"ouid=%u ogid=%u mode=%#ho\",\n\t\t\t\t from_kuid(&init_user_ns, context->ipc.uid),\n\t\t\t\t from_kgid(&init_user_ns, context->ipc.gid),\n\t\t\t\t context->ipc.mode);\n\t\tif (lsmprop_is_set(&context->ipc.oprop)) {\n\t\t\tstruct lsm_context lsmctx;\n\n\t\t\tif (security_lsmprop_to_secctx(&context->ipc.oprop,\n\t\t\t\t\t\t       &lsmctx,\n\t\t\t\t\t\t       LSM_ID_UNDEF) < 0) {\n\t\t\t\t*call_panic = 1;\n\t\t\t} else {\n\t\t\t\taudit_log_format(ab, \" obj=%s\", lsmctx.context);\n\t\t\t\tsecurity_release_secctx(&lsmctx);\n\t\t\t}\n\t\t}\n\t\tif (context->ipc.has_perm) {\n\t\t\taudit_log_end(ab);\n\t\t\tab = audit_log_start(context, GFP_KERNEL,\n\t\t\t\t\t     AUDIT_IPC_SET_PERM);\n\t\t\tif (unlikely(!ab))\n\t\t\t\treturn;\n\t\t\taudit_log_format(ab,\n\t\t\t\t\"qbytes=%lx ouid=%u ogid=%u mode=%#ho\",\n\t\t\t\tcontext->ipc.qbytes,\n\t\t\t\tcontext->ipc.perm_uid,\n\t\t\t\tcontext->ipc.perm_gid,\n\t\t\t\tcontext->ipc.perm_mode);\n\t\t}\n\t\tbreak;\n\tcase AUDIT_MQ_OPEN:\n\t\taudit_log_format(ab,\n\t\t\t\"oflag=0x%x mode=%#ho mq_flags=0x%lx mq_maxmsg=%ld \"\n\t\t\t\"mq_msgsize=%ld mq_curmsgs=%ld\",\n\t\t\tcontext->mq_open.oflag, context->mq_open.mode,\n\t\t\tcontext->mq_open.attr.mq_flags,\n\t\t\tcontext->mq_open.attr.mq_maxmsg,\n\t\t\tcontext->mq_open.attr.mq_msgsize,\n\t\t\tcontext->mq_open.attr.mq_curmsgs);\n\t\tbreak;\n\tcase AUDIT_MQ_SENDRECV:\n\t\taudit_log_format(ab,\n\t\t\t\"mqdes=%d msg_len=%zd msg_prio=%u \"\n\t\t\t\"abs_timeout_sec=%lld abs_timeout_nsec=%ld\",\n\t\t\tcontext->mq_sendrecv.mqdes,\n\t\t\tcontext->mq_sendrecv.msg_len,\n\t\t\tcontext->mq_sendrecv.msg_prio,\n\t\t\t(long long) context->mq_sendrecv.abs_timeout.tv_sec,\n\t\t\tcontext->mq_sendrecv.abs_timeout.tv_nsec);\n\t\tbreak;\n\tcase AUDIT_MQ_NOTIFY:\n\t\taudit_log_format(ab, \"mqdes=%d sigev_signo=%d\",\n\t\t\t\tcontext->mq_notify.mqdes,\n\t\t\t\tcontext->mq_notify.sigev_signo);\n\t\tbreak;\n\tcase AUDIT_MQ_GETSETATTR: {\n\t\tstruct mq_attr *attr = &context->mq_getsetattr.mqstat;\n\n\t\taudit_log_format(ab,\n\t\t\t\"mqdes=%d mq_flags=0x%lx mq_maxmsg=%ld mq_msgsize=%ld \"\n\t\t\t\"mq_curmsgs=%ld \",\n\t\t\tcontext->mq_getsetattr.mqdes,\n\t\t\tattr->mq_flags, attr->mq_maxmsg,\n\t\t\tattr->mq_msgsize, attr->mq_curmsgs);\n\t\tbreak; }\n\tcase AUDIT_CAPSET:\n\t\taudit_log_format(ab, \"pid=%d\", context->capset.pid);\n\t\taudit_log_cap(ab, \"cap_pi\", &context->capset.cap.inheritable);\n\t\taudit_log_cap(ab, \"cap_pp\", &context->capset.cap.permitted);\n\t\taudit_log_cap(ab, \"cap_pe\", &context->capset.cap.effective);\n\t\taudit_log_cap(ab, \"cap_pa\", &context->capset.cap.ambient);\n\t\tbreak;\n\tcase AUDIT_MMAP:\n\t\taudit_log_format(ab, \"fd=%d flags=0x%x\", context->mmap.fd,\n\t\t\t\t context->mmap.flags);\n\t\tbreak;\n\tcase AUDIT_OPENAT2:\n\t\taudit_log_format(ab, \"oflag=0%llo mode=0%llo resolve=0x%llx\",\n\t\t\t\t context->openat2.flags,\n\t\t\t\t context->openat2.mode,\n\t\t\t\t context->openat2.resolve);\n\t\tbreak;\n\tcase AUDIT_EXECVE:\n\t\taudit_log_execve_info(context, &ab);\n\t\tbreak;\n\tcase AUDIT_KERN_MODULE:\n\t\taudit_log_format(ab, \"name=\");\n\t\tif (context->module.name) {\n\t\t\taudit_log_untrustedstring(ab, context->module.name);\n\t\t} else\n\t\t\taudit_log_format(ab, \"(null)\");\n\n\t\tbreak;\n\tcase AUDIT_TIME_ADJNTPVAL:\n\tcase AUDIT_TIME_INJOFFSET:\n\t\t/* this call deviates from the rest, eating the buffer */\n\t\taudit_log_time(context, &ab);\n\t\tbreak;\n\t}\n\taudit_log_end(ab);\n}\n\nstatic inline int audit_proctitle_rtrim(char *proctitle, int len)\n{\n\tchar *end = proctitle + len - 1;\n\n\twhile (end > proctitle && !isprint(*end))\n\t\tend--;\n\n\t/* catch the case where proctitle is only 1 non-print character */\n\tlen = end - proctitle + 1;\n\tlen -= isprint(proctitle[len-1]) == 0;\n\treturn len;\n}\n\n/*\n * audit_log_name - produce AUDIT_PATH record from struct audit_names\n * @context: audit_context for the task\n * @n: audit_names structure with reportable details\n * @path: optional path to report instead of audit_names->name\n * @record_num: record number to report when handling a list of names\n * @call_panic: optional pointer to int that will be updated if secid fails\n */\nstatic void audit_log_name(struct audit_context *context, struct audit_names *n,\n\t\t    const struct path *path, int record_num, int *call_panic)\n{\n\tstruct audit_buffer *ab;\n\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_PATH);\n\tif (!ab)\n\t\treturn;\n\n\taudit_log_format(ab, \"item=%d\", record_num);\n\n\tif (path)\n\t\taudit_log_d_path(ab, \" name=\", path);\n\telse if (n->name) {\n\t\tswitch (n->name_len) {\n\t\tcase AUDIT_NAME_FULL:\n\t\t\t/* log the full path */\n\t\t\taudit_log_format(ab, \" name=\");\n\t\t\taudit_log_untrustedstring(ab, n->name->name);\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\t/* name was specified as a relative path and the\n\t\t\t * directory component is the cwd\n\t\t\t */\n\t\t\tif (context->pwd.dentry && context->pwd.mnt)\n\t\t\t\taudit_log_d_path(ab, \" name=\", &context->pwd);\n\t\t\telse\n\t\t\t\taudit_log_format(ab, \" name=(null)\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/* log the name's directory component */\n\t\t\taudit_log_format(ab, \" name=\");\n\t\t\taudit_log_n_untrustedstring(ab, n->name->name,\n\t\t\t\t\t\t    n->name_len);\n\t\t}\n\t} else\n\t\taudit_log_format(ab, \" name=(null)\");\n\n\tif (n->ino != AUDIT_INO_UNSET)\n\t\taudit_log_format(ab, \" inode=%lu dev=%02x:%02x mode=%#ho ouid=%u ogid=%u rdev=%02x:%02x\",\n\t\t\t\t n->ino,\n\t\t\t\t MAJOR(n->dev),\n\t\t\t\t MINOR(n->dev),\n\t\t\t\t n->mode,\n\t\t\t\t from_kuid(&init_user_ns, n->uid),\n\t\t\t\t from_kgid(&init_user_ns, n->gid),\n\t\t\t\t MAJOR(n->rdev),\n\t\t\t\t MINOR(n->rdev));\n\tif (lsmprop_is_set(&n->oprop)) {\n\t\tstruct lsm_context ctx;\n\n\t\tif (security_lsmprop_to_secctx(&n->oprop, &ctx,\n\t\t\t\t\t       LSM_ID_UNDEF) < 0) {\n\t\t\tif (call_panic)\n\t\t\t\t*call_panic = 2;\n\t\t} else {\n\t\t\taudit_log_format(ab, \" obj=%s\", ctx.context);\n\t\t\tsecurity_release_secctx(&ctx);\n\t\t}\n\t}\n\n\t/* log the audit_names record type */\n\tswitch (n->type) {\n\tcase AUDIT_TYPE_NORMAL:\n\t\taudit_log_format(ab, \" nametype=NORMAL\");\n\t\tbreak;\n\tcase AUDIT_TYPE_PARENT:\n\t\taudit_log_format(ab, \" nametype=PARENT\");\n\t\tbreak;\n\tcase AUDIT_TYPE_CHILD_DELETE:\n\t\taudit_log_format(ab, \" nametype=DELETE\");\n\t\tbreak;\n\tcase AUDIT_TYPE_CHILD_CREATE:\n\t\taudit_log_format(ab, \" nametype=CREATE\");\n\t\tbreak;\n\tdefault:\n\t\taudit_log_format(ab, \" nametype=UNKNOWN\");\n\t\tbreak;\n\t}\n\n\taudit_log_fcaps(ab, n);\n\taudit_log_end(ab);\n}\n\nstatic void audit_log_proctitle(void)\n{\n\tint res;\n\tchar *buf;\n\tchar *msg = \"(null)\";\n\tint len = strlen(msg);\n\tstruct audit_context *context = audit_context();\n\tstruct audit_buffer *ab;\n\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_PROCTITLE);\n\tif (!ab)\n\t\treturn;\t/* audit_panic or being filtered */\n\n\taudit_log_format(ab, \"proctitle=\");\n\n\t/* Not  cached */\n\tif (!context->proctitle.value) {\n\t\tbuf = kmalloc(MAX_PROCTITLE_AUDIT_LEN, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tgoto out;\n\t\t/* Historically called this from procfs naming */\n\t\tres = get_cmdline(current, buf, MAX_PROCTITLE_AUDIT_LEN);\n\t\tif (res == 0) {\n\t\t\tkfree(buf);\n\t\t\tgoto out;\n\t\t}\n\t\tres = audit_proctitle_rtrim(buf, res);\n\t\tif (res == 0) {\n\t\t\tkfree(buf);\n\t\t\tgoto out;\n\t\t}\n\t\tcontext->proctitle.value = buf;\n\t\tcontext->proctitle.len = res;\n\t}\n\tmsg = context->proctitle.value;\n\tlen = context->proctitle.len;\nout:\n\taudit_log_n_untrustedstring(ab, msg, len);\n\taudit_log_end(ab);\n}\n\n/**\n * audit_log_uring - generate a AUDIT_URINGOP record\n * @ctx: the audit context\n */\nstatic void audit_log_uring(struct audit_context *ctx)\n{\n\tstruct audit_buffer *ab;\n\tconst struct cred *cred;\n\n\tab = audit_log_start(ctx, GFP_ATOMIC, AUDIT_URINGOP);\n\tif (!ab)\n\t\treturn;\n\tcred = current_cred();\n\taudit_log_format(ab, \"uring_op=%d\", ctx->uring_op);\n\tif (ctx->return_valid != AUDITSC_INVALID)\n\t\taudit_log_format(ab, \" success=%s exit=%ld\",\n\t\t\t\t str_yes_no(ctx->return_valid ==\n\t\t\t\t\t    AUDITSC_SUCCESS),\n\t\t\t\t ctx->return_code);\n\taudit_log_format(ab,\n\t\t\t \" items=%d\"\n\t\t\t \" ppid=%d pid=%d uid=%u gid=%u euid=%u suid=%u\"\n\t\t\t \" fsuid=%u egid=%u sgid=%u fsgid=%u\",\n\t\t\t ctx->name_count,\n\t\t\t task_ppid_nr(current), task_tgid_nr(current),\n\t\t\t from_kuid(&init_user_ns, cred->uid),\n\t\t\t from_kgid(&init_user_ns, cred->gid),\n\t\t\t from_kuid(&init_user_ns, cred->euid),\n\t\t\t from_kuid(&init_user_ns, cred->suid),\n\t\t\t from_kuid(&init_user_ns, cred->fsuid),\n\t\t\t from_kgid(&init_user_ns, cred->egid),\n\t\t\t from_kgid(&init_user_ns, cred->sgid),\n\t\t\t from_kgid(&init_user_ns, cred->fsgid));\n\taudit_log_task_context(ab);\n\taudit_log_key(ab, ctx->filterkey);\n\taudit_log_end(ab);\n}\n\nstatic void audit_log_exit(void)\n{\n\tint i, call_panic = 0;\n\tstruct audit_context *context = audit_context();\n\tstruct audit_buffer *ab;\n\tstruct audit_aux_data *aux;\n\tstruct audit_names *n;\n\n\tcontext->personality = current->personality;\n\n\tswitch (context->context) {\n\tcase AUDIT_CTX_SYSCALL:\n\t\tab = audit_log_start(context, GFP_KERNEL, AUDIT_SYSCALL);\n\t\tif (!ab)\n\t\t\treturn;\n\t\taudit_log_format(ab, \"arch=%x syscall=%d\",\n\t\t\t\t context->arch, context->major);\n\t\tif (context->personality != PER_LINUX)\n\t\t\taudit_log_format(ab, \" per=%lx\", context->personality);\n\t\tif (context->return_valid != AUDITSC_INVALID)\n\t\t\taudit_log_format(ab, \" success=%s exit=%ld\",\n\t\t\t\t\t str_yes_no(context->return_valid ==\n\t\t\t\t\t\t    AUDITSC_SUCCESS),\n\t\t\t\t\t context->return_code);\n\t\taudit_log_format(ab,\n\t\t\t\t \" a0=%lx a1=%lx a2=%lx a3=%lx items=%d\",\n\t\t\t\t context->argv[0],\n\t\t\t\t context->argv[1],\n\t\t\t\t context->argv[2],\n\t\t\t\t context->argv[3],\n\t\t\t\t context->name_count);\n\t\taudit_log_task_info(ab);\n\t\taudit_log_key(ab, context->filterkey);\n\t\taudit_log_end(ab);\n\t\tbreak;\n\tcase AUDIT_CTX_URING:\n\t\taudit_log_uring(context);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tfor (aux = context->aux; aux; aux = aux->next) {\n\n\t\tab = audit_log_start(context, GFP_KERNEL, aux->type);\n\t\tif (!ab)\n\t\t\tcontinue; /* audit_panic has been called */\n\n\t\tswitch (aux->type) {\n\n\t\tcase AUDIT_BPRM_FCAPS: {\n\t\t\tstruct audit_aux_data_bprm_fcaps *axs = (void *)aux;\n\n\t\t\taudit_log_format(ab, \"fver=%x\", axs->fcap_ver);\n\t\t\taudit_log_cap(ab, \"fp\", &axs->fcap.permitted);\n\t\t\taudit_log_cap(ab, \"fi\", &axs->fcap.inheritable);\n\t\t\taudit_log_format(ab, \" fe=%d\", axs->fcap.fE);\n\t\t\taudit_log_cap(ab, \"old_pp\", &axs->old_pcap.permitted);\n\t\t\taudit_log_cap(ab, \"old_pi\", &axs->old_pcap.inheritable);\n\t\t\taudit_log_cap(ab, \"old_pe\", &axs->old_pcap.effective);\n\t\t\taudit_log_cap(ab, \"old_pa\", &axs->old_pcap.ambient);\n\t\t\taudit_log_cap(ab, \"pp\", &axs->new_pcap.permitted);\n\t\t\taudit_log_cap(ab, \"pi\", &axs->new_pcap.inheritable);\n\t\t\taudit_log_cap(ab, \"pe\", &axs->new_pcap.effective);\n\t\t\taudit_log_cap(ab, \"pa\", &axs->new_pcap.ambient);\n\t\t\taudit_log_format(ab, \" frootid=%d\",\n\t\t\t\t\t from_kuid(&init_user_ns,\n\t\t\t\t\t\t   axs->fcap.rootid));\n\t\t\tbreak; }\n\n\t\t}\n\t\taudit_log_end(ab);\n\t}\n\n\tif (context->type)\n\t\tshow_special(context, &call_panic);\n\n\tif (context->fds[0] >= 0) {\n\t\tab = audit_log_start(context, GFP_KERNEL, AUDIT_FD_PAIR);\n\t\tif (ab) {\n\t\t\taudit_log_format(ab, \"fd0=%d fd1=%d\",\n\t\t\t\t\tcontext->fds[0], context->fds[1]);\n\t\t\taudit_log_end(ab);\n\t\t}\n\t}\n\n\tif (context->sockaddr_len) {\n\t\tab = audit_log_start(context, GFP_KERNEL, AUDIT_SOCKADDR);\n\t\tif (ab) {\n\t\t\taudit_log_format(ab, \"saddr=\");\n\t\t\taudit_log_n_hex(ab, (void *)context->sockaddr,\n\t\t\t\t\tcontext->sockaddr_len);\n\t\t\taudit_log_end(ab);\n\t\t}\n\t}\n\n\tfor (aux = context->aux_pids; aux; aux = aux->next) {\n\t\tstruct audit_aux_data_pids *axs = (void *)aux;\n\n\t\tfor (i = 0; i < axs->pid_count; i++)\n\t\t\tif (audit_log_pid_context(context, axs->target_pid[i],\n\t\t\t\t\t\t  axs->target_auid[i],\n\t\t\t\t\t\t  axs->target_uid[i],\n\t\t\t\t\t\t  axs->target_sessionid[i],\n\t\t\t\t\t\t  &axs->target_ref[i],\n\t\t\t\t\t\t  axs->target_comm[i]))\n\t\t\t\tcall_panic = 1;\n\t}\n\n\tif (context->target_pid &&\n\t    audit_log_pid_context(context, context->target_pid,\n\t\t\t\t  context->target_auid, context->target_uid,\n\t\t\t\t  context->target_sessionid,\n\t\t\t\t  &context->target_ref,\n\t\t\t\t  context->target_comm))\n\t\tcall_panic = 1;\n\n\tif (context->pwd.dentry && context->pwd.mnt) {\n\t\tab = audit_log_start(context, GFP_KERNEL, AUDIT_CWD);\n\t\tif (ab) {\n\t\t\taudit_log_d_path(ab, \"cwd=\", &context->pwd);\n\t\t\taudit_log_end(ab);\n\t\t}\n\t}\n\n\ti = 0;\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\tif (n->hidden)\n\t\t\tcontinue;\n\t\taudit_log_name(context, n, NULL, i++, &call_panic);\n\t}\n\n\tif (context->context == AUDIT_CTX_SYSCALL)\n\t\taudit_log_proctitle();\n\n\t/* Send end of event record to help user space know we are finished */\n\tab = audit_log_start(context, GFP_KERNEL, AUDIT_EOE);\n\tif (ab)\n\t\taudit_log_end(ab);\n\tif (call_panic)\n\t\taudit_panic(\"error in audit_log_exit()\");\n}\n\n/**\n * __audit_free - free a per-task audit context\n * @tsk: task whose audit context block to free\n *\n * Called from copy_process, do_exit, and the io_uring code\n */\nvoid __audit_free(struct task_struct *tsk)\n{\n\tstruct audit_context *context = tsk->audit_context;\n\n\tif (!context)\n\t\treturn;\n\n\t/* this may generate CONFIG_CHANGE records */\n\tif (!list_empty(&context->killed_trees))\n\t\taudit_kill_trees(context);\n\n\t/* We are called either by do_exit() or the fork() error handling code;\n\t * in the former case tsk == current and in the latter tsk is a\n\t * random task_struct that doesn't have any meaningful data we\n\t * need to log via audit_log_exit().\n\t */\n\tif (tsk == current && !context->dummy) {\n\t\tcontext->return_valid = AUDITSC_INVALID;\n\t\tcontext->return_code = 0;\n\t\tif (context->context == AUDIT_CTX_SYSCALL) {\n\t\t\taudit_filter_syscall(tsk, context);\n\t\t\taudit_filter_inodes(tsk, context);\n\t\t\tif (context->current_state == AUDIT_STATE_RECORD)\n\t\t\t\taudit_log_exit();\n\t\t} else if (context->context == AUDIT_CTX_URING) {\n\t\t\t/* TODO: verify this case is real and valid */\n\t\t\taudit_filter_uring(tsk, context);\n\t\t\taudit_filter_inodes(tsk, context);\n\t\t\tif (context->current_state == AUDIT_STATE_RECORD)\n\t\t\t\taudit_log_uring(context);\n\t\t}\n\t}\n\n\taudit_set_context(tsk, NULL);\n\taudit_free_context(context);\n}\n\n/**\n * audit_return_fixup - fixup the return codes in the audit_context\n * @ctx: the audit_context\n * @success: true/false value to indicate if the operation succeeded or not\n * @code: operation return code\n *\n * We need to fixup the return code in the audit logs if the actual return\n * codes are later going to be fixed by the arch specific signal handlers.\n */\nstatic void audit_return_fixup(struct audit_context *ctx,\n\t\t\t       int success, long code)\n{\n\t/*\n\t * This is actually a test for:\n\t * (rc == ERESTARTSYS ) || (rc == ERESTARTNOINTR) ||\n\t * (rc == ERESTARTNOHAND) || (rc == ERESTART_RESTARTBLOCK)\n\t *\n\t * but is faster than a bunch of ||\n\t */\n\tif (unlikely(code <= -ERESTARTSYS) &&\n\t    (code >= -ERESTART_RESTARTBLOCK) &&\n\t    (code != -ENOIOCTLCMD))\n\t\tctx->return_code = -EINTR;\n\telse\n\t\tctx->return_code  = code;\n\tctx->return_valid = (success ? AUDITSC_SUCCESS : AUDITSC_FAILURE);\n}\n\n/**\n * __audit_uring_entry - prepare the kernel task's audit context for io_uring\n * @op: the io_uring opcode\n *\n * This is similar to audit_syscall_entry() but is intended for use by io_uring\n * operations.  This function should only ever be called from\n * audit_uring_entry() as we rely on the audit context checking present in that\n * function.\n */\nvoid __audit_uring_entry(u8 op)\n{\n\tstruct audit_context *ctx = audit_context();\n\n\tif (ctx->state == AUDIT_STATE_DISABLED)\n\t\treturn;\n\n\t/*\n\t * NOTE: It's possible that we can be called from the process' context\n\t *       before it returns to userspace, and before audit_syscall_exit()\n\t *       is called.  In this case there is not much to do, just record\n\t *       the io_uring details and return.\n\t */\n\tctx->uring_op = op;\n\tif (ctx->context == AUDIT_CTX_SYSCALL)\n\t\treturn;\n\n\tctx->dummy = !audit_n_rules;\n\tif (!ctx->dummy && ctx->state == AUDIT_STATE_BUILD)\n\t\tctx->prio = 0;\n\n\tctx->context = AUDIT_CTX_URING;\n\tctx->current_state = ctx->state;\n\tktime_get_coarse_real_ts64(&ctx->stamp.ctime);\n}\n\n/**\n * __audit_uring_exit - wrap up the kernel task's audit context after io_uring\n * @success: true/false value to indicate if the operation succeeded or not\n * @code: operation return code\n *\n * This is similar to audit_syscall_exit() but is intended for use by io_uring\n * operations.  This function should only ever be called from\n * audit_uring_exit() as we rely on the audit context checking present in that\n * function.\n */\nvoid __audit_uring_exit(int success, long code)\n{\n\tstruct audit_context *ctx = audit_context();\n\n\tif (ctx->dummy) {\n\t\tif (ctx->context != AUDIT_CTX_URING)\n\t\t\treturn;\n\t\tgoto out;\n\t}\n\n\taudit_return_fixup(ctx, success, code);\n\tif (ctx->context == AUDIT_CTX_SYSCALL) {\n\t\t/*\n\t\t * NOTE: See the note in __audit_uring_entry() about the case\n\t\t *       where we may be called from process context before we\n\t\t *       return to userspace via audit_syscall_exit().  In this\n\t\t *       case we simply emit a URINGOP record and bail, the\n\t\t *       normal syscall exit handling will take care of\n\t\t *       everything else.\n\t\t *       It is also worth mentioning that when we are called,\n\t\t *       the current process creds may differ from the creds\n\t\t *       used during the normal syscall processing; keep that\n\t\t *       in mind if/when we move the record generation code.\n\t\t */\n\n\t\t/*\n\t\t * We need to filter on the syscall info here to decide if we\n\t\t * should emit a URINGOP record.  I know it seems odd but this\n\t\t * solves the problem where users have a filter to block *all*\n\t\t * syscall records in the \"exit\" filter; we want to preserve\n\t\t * the behavior here.\n\t\t */\n\t\taudit_filter_syscall(current, ctx);\n\t\tif (ctx->current_state != AUDIT_STATE_RECORD)\n\t\t\taudit_filter_uring(current, ctx);\n\t\taudit_filter_inodes(current, ctx);\n\t\tif (ctx->current_state != AUDIT_STATE_RECORD)\n\t\t\treturn;\n\n\t\taudit_log_uring(ctx);\n\t\treturn;\n\t}\n\n\t/* this may generate CONFIG_CHANGE records */\n\tif (!list_empty(&ctx->killed_trees))\n\t\taudit_kill_trees(ctx);\n\n\t/* run through both filters to ensure we set the filterkey properly */\n\taudit_filter_uring(current, ctx);\n\taudit_filter_inodes(current, ctx);\n\tif (ctx->current_state != AUDIT_STATE_RECORD)\n\t\tgoto out;\n\taudit_log_exit();\n\nout:\n\taudit_reset_context(ctx);\n}\n\n/**\n * __audit_syscall_entry - fill in an audit record at syscall entry\n * @major: major syscall type (function)\n * @a1: additional syscall register 1\n * @a2: additional syscall register 2\n * @a3: additional syscall register 3\n * @a4: additional syscall register 4\n *\n * Fill in audit context at syscall entry.  This only happens if the\n * audit context was created when the task was created and the state or\n * filters demand the audit context be built.  If the state from the\n * per-task filter or from the per-syscall filter is AUDIT_STATE_RECORD,\n * then the record will be written at syscall exit time (otherwise, it\n * will only be written if another part of the kernel requests that it\n * be written).\n */\nvoid __audit_syscall_entry(int major, unsigned long a1, unsigned long a2,\n\t\t\t   unsigned long a3, unsigned long a4)\n{\n\tstruct audit_context *context = audit_context();\n\tenum audit_state     state;\n\n\tif (!audit_enabled || !context)\n\t\treturn;\n\n\tWARN_ON(context->context != AUDIT_CTX_UNUSED);\n\tWARN_ON(context->name_count);\n\tif (context->context != AUDIT_CTX_UNUSED || context->name_count) {\n\t\taudit_panic(\"unrecoverable error in audit_syscall_entry()\");\n\t\treturn;\n\t}\n\n\tstate = context->state;\n\tif (state == AUDIT_STATE_DISABLED)\n\t\treturn;\n\n\tcontext->dummy = !audit_n_rules;\n\tif (!context->dummy && state == AUDIT_STATE_BUILD) {\n\t\tcontext->prio = 0;\n\t\tif (auditd_test_task(current))\n\t\t\treturn;\n\t}\n\n\tcontext->arch\t    = syscall_get_arch(current);\n\tcontext->major      = major;\n\tcontext->argv[0]    = a1;\n\tcontext->argv[1]    = a2;\n\tcontext->argv[2]    = a3;\n\tcontext->argv[3]    = a4;\n\tcontext->context = AUDIT_CTX_SYSCALL;\n\tcontext->current_state  = state;\n\tktime_get_coarse_real_ts64(&context->stamp.ctime);\n}\n\n/**\n * __audit_syscall_exit - deallocate audit context after a system call\n * @success: success value of the syscall\n * @return_code: return value of the syscall\n *\n * Tear down after system call.  If the audit context has been marked as\n * auditable (either because of the AUDIT_STATE_RECORD state from\n * filtering, or because some other part of the kernel wrote an audit\n * message), then write out the syscall information.  In call cases,\n * free the names stored from getname().\n */\nvoid __audit_syscall_exit(int success, long return_code)\n{\n\tstruct audit_context *context = audit_context();\n\n\tif (!context || context->dummy ||\n\t    context->context != AUDIT_CTX_SYSCALL)\n\t\tgoto out;\n\n\t/* this may generate CONFIG_CHANGE records */\n\tif (!list_empty(&context->killed_trees))\n\t\taudit_kill_trees(context);\n\n\taudit_return_fixup(context, success, return_code);\n\t/* run through both filters to ensure we set the filterkey properly */\n\taudit_filter_syscall(current, context);\n\taudit_filter_inodes(current, context);\n\tif (context->current_state != AUDIT_STATE_RECORD)\n\t\tgoto out;\n\n\taudit_log_exit();\n\nout:\n\taudit_reset_context(context);\n}\n\nstatic inline void handle_one(const struct inode *inode)\n{\n\tstruct audit_context *context;\n\tstruct audit_tree_refs *p;\n\tstruct audit_chunk *chunk;\n\tint count;\n\n\tif (likely(!inode->i_fsnotify_marks))\n\t\treturn;\n\tcontext = audit_context();\n\tp = context->trees;\n\tcount = context->tree_count;\n\trcu_read_lock();\n\tchunk = audit_tree_lookup(inode);\n\trcu_read_unlock();\n\tif (!chunk)\n\t\treturn;\n\tif (likely(put_tree_ref(context, chunk)))\n\t\treturn;\n\tif (unlikely(!grow_tree_refs(context))) {\n\t\tpr_warn(\"out of memory, audit has lost a tree reference\\n\");\n\t\taudit_set_auditable(context);\n\t\taudit_put_chunk(chunk);\n\t\tunroll_tree_refs(context, p, count);\n\t\treturn;\n\t}\n\tput_tree_ref(context, chunk);\n}\n\nstatic void handle_path(const struct dentry *dentry)\n{\n\tstruct audit_context *context;\n\tstruct audit_tree_refs *p;\n\tconst struct dentry *d, *parent;\n\tstruct audit_chunk *drop;\n\tunsigned long seq;\n\tint count;\n\n\tcontext = audit_context();\n\tp = context->trees;\n\tcount = context->tree_count;\nretry:\n\tdrop = NULL;\n\td = dentry;\n\trcu_read_lock();\n\tseq = read_seqbegin(&rename_lock);\n\tfor (;;) {\n\t\tstruct inode *inode = d_backing_inode(d);\n\n\t\tif (inode && unlikely(inode->i_fsnotify_marks)) {\n\t\t\tstruct audit_chunk *chunk;\n\n\t\t\tchunk = audit_tree_lookup(inode);\n\t\t\tif (chunk) {\n\t\t\t\tif (unlikely(!put_tree_ref(context, chunk))) {\n\t\t\t\t\tdrop = chunk;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tparent = d->d_parent;\n\t\tif (parent == d)\n\t\t\tbreak;\n\t\td = parent;\n\t}\n\tif (unlikely(read_seqretry(&rename_lock, seq) || drop)) {  /* in this order */\n\t\trcu_read_unlock();\n\t\tif (!drop) {\n\t\t\t/* just a race with rename */\n\t\t\tunroll_tree_refs(context, p, count);\n\t\t\tgoto retry;\n\t\t}\n\t\taudit_put_chunk(drop);\n\t\tif (grow_tree_refs(context)) {\n\t\t\t/* OK, got more space */\n\t\t\tunroll_tree_refs(context, p, count);\n\t\t\tgoto retry;\n\t\t}\n\t\t/* too bad */\n\t\tpr_warn(\"out of memory, audit has lost a tree reference\\n\");\n\t\tunroll_tree_refs(context, p, count);\n\t\taudit_set_auditable(context);\n\t\treturn;\n\t}\n\trcu_read_unlock();\n}\n\nstatic struct audit_names *audit_alloc_name(struct audit_context *context,\n\t\t\t\t\t\tunsigned char type)\n{\n\tstruct audit_names *aname;\n\n\tif (context->name_count < AUDIT_NAMES) {\n\t\taname = &context->preallocated_names[context->name_count];\n\t\tmemset(aname, 0, sizeof(*aname));\n\t} else {\n\t\taname = kzalloc(sizeof(*aname), GFP_NOFS);\n\t\tif (!aname)\n\t\t\treturn NULL;\n\t\taname->should_free = true;\n\t}\n\n\taname->ino = AUDIT_INO_UNSET;\n\taname->type = type;\n\tlist_add_tail(&aname->list, &context->names_list);\n\n\tcontext->name_count++;\n\tif (!context->pwd.dentry)\n\t\tget_fs_pwd(current->fs, &context->pwd);\n\treturn aname;\n}\n\n/**\n * __audit_reusename - fill out filename with info from existing entry\n * @uptr: userland ptr to pathname\n *\n * Search the audit_names list for the current audit context. If there is an\n * existing entry with a matching \"uptr\" then return the filename\n * associated with that audit_name. If not, return NULL.\n */\nstruct filename *\n__audit_reusename(const __user char *uptr)\n{\n\tstruct audit_context *context = audit_context();\n\tstruct audit_names *n;\n\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\tif (!n->name)\n\t\t\tcontinue;\n\t\tif (n->name->uptr == uptr)\n\t\t\treturn refname(n->name);\n\t}\n\treturn NULL;\n}\n\n/**\n * __audit_getname - add a name to the list\n * @name: name to add\n *\n * Add a name to the list of audit names for this context.\n * Called from fs/namei.c:getname().\n */\nvoid __audit_getname(struct filename *name)\n{\n\tstruct audit_context *context = audit_context();\n\tstruct audit_names *n;\n\n\tif (context->context == AUDIT_CTX_UNUSED)\n\t\treturn;\n\n\tn = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);\n\tif (!n)\n\t\treturn;\n\n\tn->name = name;\n\tn->name_len = AUDIT_NAME_FULL;\n\tname->aname = n;\n\trefname(name);\n}\n\nstatic inline int audit_copy_fcaps(struct audit_names *name,\n\t\t\t\t   const struct dentry *dentry)\n{\n\tstruct cpu_vfs_cap_data caps;\n\tint rc;\n\n\tif (!dentry)\n\t\treturn 0;\n\n\trc = get_vfs_caps_from_disk(&nop_mnt_idmap, dentry, &caps);\n\tif (rc)\n\t\treturn rc;\n\n\tname->fcap.permitted = caps.permitted;\n\tname->fcap.inheritable = caps.inheritable;\n\tname->fcap.fE = !!(caps.magic_etc & VFS_CAP_FLAGS_EFFECTIVE);\n\tname->fcap.rootid = caps.rootid;\n\tname->fcap_ver = (caps.magic_etc & VFS_CAP_REVISION_MASK) >>\n\t\t\t\tVFS_CAP_REVISION_SHIFT;\n\n\treturn 0;\n}\n\n/* Copy inode data into an audit_names. */\nstatic void audit_copy_inode(struct audit_names *name,\n\t\t\t     const struct dentry *dentry,\n\t\t\t     struct inode *inode, unsigned int flags)\n{\n\tname->ino   = inode->i_ino;\n\tname->dev   = inode->i_sb->s_dev;\n\tname->mode  = inode->i_mode;\n\tname->uid   = inode->i_uid;\n\tname->gid   = inode->i_gid;\n\tname->rdev  = inode->i_rdev;\n\tsecurity_inode_getlsmprop(inode, &name->oprop);\n\tif (flags & AUDIT_INODE_NOEVAL) {\n\t\tname->fcap_ver = -1;\n\t\treturn;\n\t}\n\taudit_copy_fcaps(name, dentry);\n}\n\n/**\n * __audit_inode - store the inode and device from a lookup\n * @name: name being audited\n * @dentry: dentry being audited\n * @flags: attributes for this particular entry\n */\nvoid __audit_inode(struct filename *name, const struct dentry *dentry,\n\t\t   unsigned int flags)\n{\n\tstruct audit_context *context = audit_context();\n\tstruct inode *inode = d_backing_inode(dentry);\n\tstruct audit_names *n;\n\tbool parent = flags & AUDIT_INODE_PARENT;\n\tstruct audit_entry *e;\n\tstruct list_head *list = &audit_filter_list[AUDIT_FILTER_FS];\n\tint i;\n\n\tif (context->context == AUDIT_CTX_UNUSED)\n\t\treturn;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(e, list, list) {\n\t\tfor (i = 0; i < e->rule.field_count; i++) {\n\t\t\tstruct audit_field *f = &e->rule.fields[i];\n\n\t\t\tif (f->type == AUDIT_FSTYPE\n\t\t\t    && audit_comparator(inode->i_sb->s_magic,\n\t\t\t\t\t\tf->op, f->val)\n\t\t\t    && e->rule.action == AUDIT_NEVER) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (!name)\n\t\tgoto out_alloc;\n\n\t/*\n\t * If we have a pointer to an audit_names entry already, then we can\n\t * just use it directly if the type is correct.\n\t */\n\tn = name->aname;\n\tif (n) {\n\t\tif (parent) {\n\t\t\tif (n->type == AUDIT_TYPE_PARENT ||\n\t\t\t    n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\tif (n->type != AUDIT_TYPE_PARENT)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlist_for_each_entry_reverse(n, &context->names_list, list) {\n\t\tif (n->ino) {\n\t\t\t/* valid inode number, use that for the comparison */\n\t\t\tif (n->ino != inode->i_ino ||\n\t\t\t    n->dev != inode->i_sb->s_dev)\n\t\t\t\tcontinue;\n\t\t} else if (n->name) {\n\t\t\t/* inode number has not been set, check the name */\n\t\t\tif (strcmp(n->name->name, name->name))\n\t\t\t\tcontinue;\n\t\t} else\n\t\t\t/* no inode and no name (?!) ... this is odd ... */\n\t\t\tcontinue;\n\n\t\t/* match the correct record type */\n\t\tif (parent) {\n\t\t\tif (n->type == AUDIT_TYPE_PARENT ||\n\t\t\t    n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\tif (n->type != AUDIT_TYPE_PARENT)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\nout_alloc:\n\t/* unable to find an entry with both a matching name and type */\n\tn = audit_alloc_name(context, AUDIT_TYPE_UNKNOWN);\n\tif (!n)\n\t\treturn;\n\tif (name) {\n\t\tn->name = name;\n\t\trefname(name);\n\t}\n\nout:\n\tif (parent) {\n\t\tn->name_len = n->name ? parent_len(n->name->name) : AUDIT_NAME_FULL;\n\t\tn->type = AUDIT_TYPE_PARENT;\n\t\tif (flags & AUDIT_INODE_HIDDEN)\n\t\t\tn->hidden = true;\n\t} else {\n\t\tn->name_len = AUDIT_NAME_FULL;\n\t\tn->type = AUDIT_TYPE_NORMAL;\n\t}\n\thandle_path(dentry);\n\taudit_copy_inode(n, dentry, inode, flags & AUDIT_INODE_NOEVAL);\n}\n\nvoid __audit_file(const struct file *file)\n{\n\t__audit_inode(NULL, file->f_path.dentry, 0);\n}\n\n/**\n * __audit_inode_child - collect inode info for created/removed objects\n * @parent: inode of dentry parent\n * @dentry: dentry being audited\n * @type:   AUDIT_TYPE_* value that we're looking for\n *\n * For syscalls that create or remove filesystem objects, audit_inode\n * can only collect information for the filesystem object's parent.\n * This call updates the audit context with the child's information.\n * Syscalls that create a new filesystem object must be hooked after\n * the object is created.  Syscalls that remove a filesystem object\n * must be hooked prior, in order to capture the target inode during\n * unsuccessful attempts.\n */\nvoid __audit_inode_child(struct inode *parent,\n\t\t\t const struct dentry *dentry,\n\t\t\t const unsigned char type)\n{\n\tstruct audit_context *context = audit_context();\n\tstruct inode *inode = d_backing_inode(dentry);\n\tconst struct qstr *dname = &dentry->d_name;\n\tstruct audit_names *n, *found_parent = NULL, *found_child = NULL;\n\tstruct audit_entry *e;\n\tstruct list_head *list = &audit_filter_list[AUDIT_FILTER_FS];\n\tint i;\n\n\tif (context->context == AUDIT_CTX_UNUSED)\n\t\treturn;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(e, list, list) {\n\t\tfor (i = 0; i < e->rule.field_count; i++) {\n\t\t\tstruct audit_field *f = &e->rule.fields[i];\n\n\t\t\tif (f->type == AUDIT_FSTYPE\n\t\t\t    && audit_comparator(parent->i_sb->s_magic,\n\t\t\t\t\t\tf->op, f->val)\n\t\t\t    && e->rule.action == AUDIT_NEVER) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (inode)\n\t\thandle_one(inode);\n\n\t/* look for a parent entry first */\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\tif (!n->name ||\n\t\t    (n->type != AUDIT_TYPE_PARENT &&\n\t\t     n->type != AUDIT_TYPE_UNKNOWN))\n\t\t\tcontinue;\n\n\t\tif (n->ino == parent->i_ino && n->dev == parent->i_sb->s_dev &&\n\t\t    !audit_compare_dname_path(dname,\n\t\t\t\t\t      n->name->name, n->name_len)) {\n\t\t\tif (n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tn->type = AUDIT_TYPE_PARENT;\n\t\t\tfound_parent = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tcond_resched();\n\n\t/* is there a matching child entry? */\n\tlist_for_each_entry(n, &context->names_list, list) {\n\t\t/* can only match entries that have a name */\n\t\tif (!n->name ||\n\t\t    (n->type != type && n->type != AUDIT_TYPE_UNKNOWN))\n\t\t\tcontinue;\n\n\t\tif (!strcmp(dname->name, n->name->name) ||\n\t\t    !audit_compare_dname_path(dname, n->name->name,\n\t\t\t\t\t\tfound_parent ?\n\t\t\t\t\t\tfound_parent->name_len :\n\t\t\t\t\t\tAUDIT_NAME_FULL)) {\n\t\t\tif (n->type == AUDIT_TYPE_UNKNOWN)\n\t\t\t\tn->type = type;\n\t\t\tfound_child = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found_parent) {\n\t\t/* create a new, \"anonymous\" parent record */\n\t\tn = audit_alloc_name(context, AUDIT_TYPE_PARENT);\n\t\tif (!n)\n\t\t\treturn;\n\t\taudit_copy_inode(n, NULL, parent, 0);\n\t}\n\n\tif (!found_child) {\n\t\tfound_child = audit_alloc_name(context, type);\n\t\tif (!found_child)\n\t\t\treturn;\n\n\t\t/* Re-use the name belonging to the slot for a matching parent\n\t\t * directory. All names for this context are relinquished in\n\t\t * audit_free_names() */\n\t\tif (found_parent) {\n\t\t\tfound_child->name = found_parent->name;\n\t\t\tfound_child->name_len = AUDIT_NAME_FULL;\n\t\t\trefname(found_child->name);\n\t\t}\n\t}\n\n\tif (inode)\n\t\taudit_copy_inode(found_child, dentry, inode, 0);\n\telse\n\t\tfound_child->ino = AUDIT_INO_UNSET;\n}\nEXPORT_SYMBOL_GPL(__audit_inode_child);\n\n/**\n * auditsc_get_stamp - get local copies of audit_context values\n * @ctx: audit_context for the task\n * @stamp: timestamp to record\n *\n * Also sets the context as auditable.\n */\nint auditsc_get_stamp(struct audit_context *ctx, struct audit_stamp *stamp)\n{\n\tif (ctx->context == AUDIT_CTX_UNUSED)\n\t\treturn 0;\n\tif (!ctx->stamp.serial)\n\t\tctx->stamp.serial = audit_serial();\n\t*stamp = ctx->stamp;\n\tif (!ctx->prio) {\n\t\tctx->prio = 1;\n\t\tctx->current_state = AUDIT_STATE_RECORD;\n\t}\n\treturn 1;\n}\n\n/**\n * __audit_mq_open - record audit data for a POSIX MQ open\n * @oflag: open flag\n * @mode: mode bits\n * @attr: queue attributes\n *\n */\nvoid __audit_mq_open(int oflag, umode_t mode, struct mq_attr *attr)\n{\n\tstruct audit_context *context = audit_context();\n\n\tif (attr)\n\t\tmemcpy(&context->mq_open.attr, attr, sizeof(struct mq_attr));\n\telse\n\t\tmemset(&context->mq_open.attr, 0, sizeof(struct mq_attr));\n\n\tcontext->mq_open.oflag = oflag;\n\tcontext->mq_open.mode = mode;\n\n\tcontext->type = AUDIT_MQ_OPEN;\n}\n\n/**\n * __audit_mq_sendrecv - record audit data for a POSIX MQ timed send/receive\n * @mqdes: MQ descriptor\n * @msg_len: Message length\n * @msg_prio: Message priority\n * @abs_timeout: Message timeout in absolute time\n *\n */\nvoid __audit_mq_sendrecv(mqd_t mqdes, size_t msg_len, unsigned int msg_prio,\n\t\t\tconst struct timespec64 *abs_timeout)\n{\n\tstruct audit_context *context = audit_context();\n\tstruct timespec64 *p = &context->mq_sendrecv.abs_timeout;\n\n\tif (abs_timeout)\n\t\tmemcpy(p, abs_timeout, sizeof(*p));\n\telse\n\t\tmemset(p, 0, sizeof(*p));\n\n\tcontext->mq_sendrecv.mqdes = mqdes;\n\tcontext->mq_sendrecv.msg_len = msg_len;\n\tcontext->mq_sendrecv.msg_prio = msg_prio;\n\n\tcontext->type = AUDIT_MQ_SENDRECV;\n}\n\n/**\n * __audit_mq_notify - record audit data for a POSIX MQ notify\n * @mqdes: MQ descriptor\n * @notification: Notification event\n *\n */\n\nvoid __audit_mq_notify(mqd_t mqdes, const struct sigevent *notification)\n{\n\tstruct audit_context *context = audit_context();\n\n\tif (notification)\n\t\tcontext->mq_notify.sigev_signo = notification->sigev_signo;\n\telse\n\t\tcontext->mq_notify.sigev_signo = 0;\n\n\tcontext->mq_notify.mqdes = mqdes;\n\tcontext->type = AUDIT_MQ_NOTIFY;\n}\n\n/**\n * __audit_mq_getsetattr - record audit data for a POSIX MQ get/set attribute\n * @mqdes: MQ descriptor\n * @mqstat: MQ flags\n *\n */\nvoid __audit_mq_getsetattr(mqd_t mqdes, struct mq_attr *mqstat)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->mq_getsetattr.mqdes = mqdes;\n\tcontext->mq_getsetattr.mqstat = *mqstat;\n\tcontext->type = AUDIT_MQ_GETSETATTR;\n}\n\n/**\n * __audit_ipc_obj - record audit data for ipc object\n * @ipcp: ipc permissions\n *\n */\nvoid __audit_ipc_obj(struct kern_ipc_perm *ipcp)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->ipc.uid = ipcp->uid;\n\tcontext->ipc.gid = ipcp->gid;\n\tcontext->ipc.mode = ipcp->mode;\n\tcontext->ipc.has_perm = 0;\n\tsecurity_ipc_getlsmprop(ipcp, &context->ipc.oprop);\n\tcontext->type = AUDIT_IPC;\n}\n\n/**\n * __audit_ipc_set_perm - record audit data for new ipc permissions\n * @qbytes: msgq bytes\n * @uid: msgq user id\n * @gid: msgq group id\n * @mode: msgq mode (permissions)\n *\n * Called only after audit_ipc_obj().\n */\nvoid __audit_ipc_set_perm(unsigned long qbytes, uid_t uid, gid_t gid, umode_t mode)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->ipc.qbytes = qbytes;\n\tcontext->ipc.perm_uid = uid;\n\tcontext->ipc.perm_gid = gid;\n\tcontext->ipc.perm_mode = mode;\n\tcontext->ipc.has_perm = 1;\n}\n\nvoid __audit_bprm(struct linux_binprm *bprm)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->type = AUDIT_EXECVE;\n\tcontext->execve.argc = bprm->argc;\n}\n\n\n/**\n * __audit_socketcall - record audit data for sys_socketcall\n * @nargs: number of args, which should not be more than AUDITSC_ARGS.\n * @args: args array\n *\n */\nint __audit_socketcall(int nargs, unsigned long *args)\n{\n\tstruct audit_context *context = audit_context();\n\n\tif (nargs <= 0 || nargs > AUDITSC_ARGS || !args)\n\t\treturn -EINVAL;\n\tcontext->type = AUDIT_SOCKETCALL;\n\tcontext->socketcall.nargs = nargs;\n\tmemcpy(context->socketcall.args, args, nargs * sizeof(unsigned long));\n\treturn 0;\n}\n\n/**\n * __audit_fd_pair - record audit data for pipe and socketpair\n * @fd1: the first file descriptor\n * @fd2: the second file descriptor\n *\n */\nvoid __audit_fd_pair(int fd1, int fd2)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->fds[0] = fd1;\n\tcontext->fds[1] = fd2;\n}\n\n/**\n * __audit_sockaddr - record audit data for sys_bind, sys_connect, sys_sendto\n * @len: data length in user space\n * @a: data address in kernel space\n *\n * Returns 0 for success or NULL context or < 0 on error.\n */\nint __audit_sockaddr(int len, void *a)\n{\n\tstruct audit_context *context = audit_context();\n\n\tif (!context->sockaddr) {\n\t\tvoid *p = kmalloc(sizeof(struct sockaddr_storage), GFP_KERNEL);\n\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\t\tcontext->sockaddr = p;\n\t}\n\n\tcontext->sockaddr_len = len;\n\tmemcpy(context->sockaddr, a, len);\n\treturn 0;\n}\n\nvoid __audit_ptrace(struct task_struct *t)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->target_pid = task_tgid_nr(t);\n\tcontext->target_auid = audit_get_loginuid(t);\n\tcontext->target_uid = task_uid(t);\n\tcontext->target_sessionid = audit_get_sessionid(t);\n\tstrscpy(context->target_comm, t->comm);\n\tsecurity_task_getlsmprop_obj(t, &context->target_ref);\n}\n\n/**\n * audit_signal_info_syscall - record signal info for syscalls\n * @t: task being signaled\n *\n * If the audit subsystem is being terminated, record the task (pid)\n * and uid that is doing that.\n */\nint audit_signal_info_syscall(struct task_struct *t)\n{\n\tstruct audit_aux_data_pids *axp;\n\tstruct audit_context *ctx = audit_context();\n\tkuid_t t_uid = task_uid(t);\n\n\tif (!audit_signals || audit_dummy_context())\n\t\treturn 0;\n\n\t/* optimize the common case by putting first signal recipient directly\n\t * in audit_context */\n\tif (!ctx->target_pid) {\n\t\tctx->target_pid = task_tgid_nr(t);\n\t\tctx->target_auid = audit_get_loginuid(t);\n\t\tctx->target_uid = t_uid;\n\t\tctx->target_sessionid = audit_get_sessionid(t);\n\t\tstrscpy(ctx->target_comm, t->comm);\n\t\tsecurity_task_getlsmprop_obj(t, &ctx->target_ref);\n\t\treturn 0;\n\t}\n\n\taxp = (void *)ctx->aux_pids;\n\tif (!axp || axp->pid_count == AUDIT_AUX_PIDS) {\n\t\taxp = kzalloc(sizeof(*axp), GFP_ATOMIC);\n\t\tif (!axp)\n\t\t\treturn -ENOMEM;\n\n\t\taxp->d.type = AUDIT_OBJ_PID;\n\t\taxp->d.next = ctx->aux_pids;\n\t\tctx->aux_pids = (void *)axp;\n\t}\n\tBUG_ON(axp->pid_count >= AUDIT_AUX_PIDS);\n\n\taxp->target_pid[axp->pid_count] = task_tgid_nr(t);\n\taxp->target_auid[axp->pid_count] = audit_get_loginuid(t);\n\taxp->target_uid[axp->pid_count] = t_uid;\n\taxp->target_sessionid[axp->pid_count] = audit_get_sessionid(t);\n\tsecurity_task_getlsmprop_obj(t, &axp->target_ref[axp->pid_count]);\n\tstrscpy(axp->target_comm[axp->pid_count], t->comm);\n\taxp->pid_count++;\n\n\treturn 0;\n}\n\n/**\n * __audit_log_bprm_fcaps - store information about a loading bprm and relevant fcaps\n * @bprm: pointer to the bprm being processed\n * @new: the proposed new credentials\n * @old: the old credentials\n *\n * Simply check if the proc already has the caps given by the file and if not\n * store the priv escalation info for later auditing at the end of the syscall\n *\n * -Eric\n */\nint __audit_log_bprm_fcaps(struct linux_binprm *bprm,\n\t\t\t   const struct cred *new, const struct cred *old)\n{\n\tstruct audit_aux_data_bprm_fcaps *ax;\n\tstruct audit_context *context = audit_context();\n\tstruct cpu_vfs_cap_data vcaps;\n\n\tax = kmalloc(sizeof(*ax), GFP_KERNEL);\n\tif (!ax)\n\t\treturn -ENOMEM;\n\n\tax->d.type = AUDIT_BPRM_FCAPS;\n\tax->d.next = context->aux;\n\tcontext->aux = (void *)ax;\n\n\tget_vfs_caps_from_disk(&nop_mnt_idmap,\n\t\t\t       bprm->file->f_path.dentry, &vcaps);\n\n\tax->fcap.permitted = vcaps.permitted;\n\tax->fcap.inheritable = vcaps.inheritable;\n\tax->fcap.fE = !!(vcaps.magic_etc & VFS_CAP_FLAGS_EFFECTIVE);\n\tax->fcap.rootid = vcaps.rootid;\n\tax->fcap_ver = (vcaps.magic_etc & VFS_CAP_REVISION_MASK) >> VFS_CAP_REVISION_SHIFT;\n\n\tax->old_pcap.permitted   = old->cap_permitted;\n\tax->old_pcap.inheritable = old->cap_inheritable;\n\tax->old_pcap.effective   = old->cap_effective;\n\tax->old_pcap.ambient     = old->cap_ambient;\n\n\tax->new_pcap.permitted   = new->cap_permitted;\n\tax->new_pcap.inheritable = new->cap_inheritable;\n\tax->new_pcap.effective   = new->cap_effective;\n\tax->new_pcap.ambient     = new->cap_ambient;\n\treturn 0;\n}\n\n/**\n * __audit_log_capset - store information about the arguments to the capset syscall\n * @new: the new credentials\n * @old: the old (current) credentials\n *\n * Record the arguments userspace sent to sys_capset for later printing by the\n * audit system if applicable\n */\nvoid __audit_log_capset(const struct cred *new, const struct cred *old)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->capset.pid = task_tgid_nr(current);\n\tcontext->capset.cap.effective   = new->cap_effective;\n\tcontext->capset.cap.inheritable = new->cap_effective;\n\tcontext->capset.cap.permitted   = new->cap_permitted;\n\tcontext->capset.cap.ambient     = new->cap_ambient;\n\tcontext->type = AUDIT_CAPSET;\n}\n\nvoid __audit_mmap_fd(int fd, int flags)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->mmap.fd = fd;\n\tcontext->mmap.flags = flags;\n\tcontext->type = AUDIT_MMAP;\n}\n\nvoid __audit_openat2_how(struct open_how *how)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->openat2.flags = how->flags;\n\tcontext->openat2.mode = how->mode;\n\tcontext->openat2.resolve = how->resolve;\n\tcontext->type = AUDIT_OPENAT2;\n}\n\nvoid __audit_log_kern_module(const char *name)\n{\n\tstruct audit_context *context = audit_context();\n\n\tcontext->module.name = kstrdup(name, GFP_KERNEL);\n\tif (!context->module.name)\n\t\taudit_log_lost(\"out of memory in __audit_log_kern_module\");\n\tcontext->type = AUDIT_KERN_MODULE;\n}\n\nvoid __audit_fanotify(u32 response, struct fanotify_response_info_audit_rule *friar)\n{\n\t/* {subj,obj}_trust values are {0,1,2}: no,yes,unknown */\n\tswitch (friar->hdr.type) {\n\tcase FAN_RESPONSE_INFO_NONE:\n\t\taudit_log(audit_context(), GFP_KERNEL, AUDIT_FANOTIFY,\n\t\t\t  \"resp=%u fan_type=%u fan_info=0 subj_trust=2 obj_trust=2\",\n\t\t\t  response, FAN_RESPONSE_INFO_NONE);\n\t\tbreak;\n\tcase FAN_RESPONSE_INFO_AUDIT_RULE:\n\t\taudit_log(audit_context(), GFP_KERNEL, AUDIT_FANOTIFY,\n\t\t\t  \"resp=%u fan_type=%u fan_info=%X subj_trust=%u obj_trust=%u\",\n\t\t\t  response, friar->hdr.type, friar->rule_number,\n\t\t\t  friar->subj_trust, friar->obj_trust);\n\t}\n}\n\nvoid __audit_tk_injoffset(struct timespec64 offset)\n{\n\tstruct audit_context *context = audit_context();\n\n\t/* only set type if not already set by NTP */\n\tif (!context->type)\n\t\tcontext->type = AUDIT_TIME_INJOFFSET;\n\tmemcpy(&context->time.tk_injoffset, &offset, sizeof(offset));\n}\n\nvoid __audit_ntp_log(const struct audit_ntp_data *ad)\n{\n\tstruct audit_context *context = audit_context();\n\tint type;\n\n\tfor (type = 0; type < AUDIT_NTP_NVALS; type++)\n\t\tif (ad->vals[type].newval != ad->vals[type].oldval) {\n\t\t\t/* unconditionally set type, overwriting TK */\n\t\t\tcontext->type = AUDIT_TIME_ADJNTPVAL;\n\t\t\tmemcpy(&context->time.ntp_data, ad, sizeof(*ad));\n\t\t\tbreak;\n\t\t}\n}\n\nvoid __audit_log_nfcfg(const char *name, u8 af, unsigned int nentries,\n\t\t       enum audit_nfcfgop op, gfp_t gfp)\n{\n\tstruct audit_buffer *ab;\n\tchar comm[sizeof(current->comm)];\n\n\tab = audit_log_start(audit_context(), gfp, AUDIT_NETFILTER_CFG);\n\tif (!ab)\n\t\treturn;\n\taudit_log_format(ab, \"table=%s family=%u entries=%u op=%s\",\n\t\t\t name, af, nentries, audit_nfcfgs[op].s);\n\n\taudit_log_format(ab, \" pid=%u\", task_tgid_nr(current));\n\taudit_log_task_context(ab); /* subj= */\n\taudit_log_format(ab, \" comm=\");\n\taudit_log_untrustedstring(ab, get_task_comm(comm, current));\n\taudit_log_end(ab);\n}\nEXPORT_SYMBOL_GPL(__audit_log_nfcfg);\n\nstatic void audit_log_task(struct audit_buffer *ab)\n{\n\tkuid_t auid, uid;\n\tkgid_t gid;\n\tunsigned int sessionid;\n\tchar comm[sizeof(current->comm)];\n\n\tauid = audit_get_loginuid(current);\n\tsessionid = audit_get_sessionid(current);\n\tcurrent_uid_gid(&uid, &gid);\n\n\taudit_log_format(ab, \"auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t from_kuid(&init_user_ns, auid),\n\t\t\t from_kuid(&init_user_ns, uid),\n\t\t\t from_kgid(&init_user_ns, gid),\n\t\t\t sessionid);\n\taudit_log_task_context(ab);\n\taudit_log_format(ab, \" pid=%d comm=\", task_tgid_nr(current));\n\taudit_log_untrustedstring(ab, get_task_comm(comm, current));\n\taudit_log_d_path_exe(ab, current->mm);\n}\n\n/**\n * audit_core_dumps - record information about processes that end abnormally\n * @signr: signal value\n *\n * If a process ends with a core dump, something fishy is going on and we\n * should record the event for investigation.\n */\nvoid audit_core_dumps(long signr)\n{\n\tstruct audit_buffer *ab;\n\n\tif (!audit_enabled)\n\t\treturn;\n\n\tif (signr == SIGQUIT)\t/* don't care for those */\n\t\treturn;\n\n\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_ANOM_ABEND);\n\tif (unlikely(!ab))\n\t\treturn;\n\taudit_log_task(ab);\n\taudit_log_format(ab, \" sig=%ld res=1\", signr);\n\taudit_log_end(ab);\n}\n\n/**\n * audit_seccomp - record information about a seccomp action\n * @syscall: syscall number\n * @signr: signal value\n * @code: the seccomp action\n *\n * Record the information associated with a seccomp action. Event filtering for\n * seccomp actions that are not to be logged is done in seccomp_log().\n * Therefore, this function forces auditing independent of the audit_enabled\n * and dummy context state because seccomp actions should be logged even when\n * audit is not in use.\n */\nvoid audit_seccomp(unsigned long syscall, long signr, int code)\n{\n\tstruct audit_buffer *ab;\n\n\tab = audit_log_start(audit_context(), GFP_KERNEL, AUDIT_SECCOMP);\n\tif (unlikely(!ab))\n\t\treturn;\n\taudit_log_task(ab);\n\taudit_log_format(ab, \" sig=%ld arch=%x syscall=%ld compat=%d ip=0x%lx code=0x%x\",\n\t\t\t signr, syscall_get_arch(current), syscall,\n\t\t\t in_compat_syscall(), KSTK_EIP(current), code);\n\taudit_log_end(ab);\n}\n\nvoid audit_seccomp_actions_logged(const char *names, const char *old_names,\n\t\t\t\t  int res)\n{\n\tstruct audit_buffer *ab;\n\n\tif (!audit_enabled)\n\t\treturn;\n\n\tab = audit_log_start(audit_context(), GFP_KERNEL,\n\t\t\t     AUDIT_CONFIG_CHANGE);\n\tif (unlikely(!ab))\n\t\treturn;\n\n\taudit_log_format(ab,\n\t\t\t \"op=seccomp-logging actions=%s old-actions=%s res=%d\",\n\t\t\t names, old_names, res);\n\taudit_log_end(ab);\n}\n\nstruct list_head *audit_killed_trees(void)\n{\n\tstruct audit_context *ctx = audit_context();\n\tif (likely(!ctx || ctx->context == AUDIT_CTX_UNUSED))\n\t\treturn NULL;\n\treturn &ctx->killed_trees;\n}\n", "patch": "@@ -73,6 +73,7 @@\n #include <linux/compat.h>\n #include <linux/ctype.h>\n #include <linux/string.h>\n+#include <linux/uaccess.h>\n #include <uapi/linux/limits.h>\n \n #include \"audit.h\"\n@@ -82,7 +83,8 @@\n #define AUDITSC_SUCCESS 1\n #define AUDITSC_FAILURE 2\n \n-/* no execve audit message should be longer than this (userspace limits) */\n+/* no execve audit message should be longer than this (userspace limits),\n+ * see the note near the top of audit_log_execve_info() about this value */\n #define MAX_EXECVE_AUDIT_LEN 7500\n \n /* max length to print of cmdline/proctitle value during audit */\n@@ -992,184 +994,178 @@ static int audit_log_pid_context(struct audit_context *context, pid_t pid,\n \treturn rc;\n }\n \n-/*\n- * to_send and len_sent accounting are very loose estimates.  We aren't\n- * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being\n- * within about 500 bytes (next page boundary)\n- *\n- * why snprintf?  an int is up to 12 digits long.  if we just assumed when\n- * logging that a[%d]= was going to be 16 characters long we would be wasting\n- * space in every audit message.  In one 7500 byte message we can log up to\n- * about 1000 min size arguments.  That comes down to about 50% waste of space\n- * if we didn't do the snprintf to find out how long arg_num_len was.\n- */\n-static int audit_log_single_execve_arg(struct audit_context *context,\n-\t\t\t\t\tstruct audit_buffer **ab,\n-\t\t\t\t\tint arg_num,\n-\t\t\t\t\tsize_t *len_sent,\n-\t\t\t\t\tconst char __user *p,\n-\t\t\t\t\tchar *buf)\n+static void audit_log_execve_info(struct audit_context *context,\n+\t\t\t\t  struct audit_buffer **ab)\n {\n-\tchar arg_num_len_buf[12];\n-\tconst char __user *tmp_p = p;\n-\t/* how many digits are in arg_num? 5 is the length of ' a=\"\"' */\n-\tsize_t arg_num_len = snprintf(arg_num_len_buf, 12, \"%d\", arg_num) + 5;\n-\tsize_t len, len_left, to_send;\n-\tsize_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;\n-\tunsigned int i, has_cntl = 0, too_long = 0;\n-\tint ret;\n-\n-\t/* strnlen_user includes the null we don't want to send */\n-\tlen_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n-\n-\t/*\n-\t * We just created this mm, if we can't find the strings\n-\t * we just copied into it something is _very_ wrong. Similar\n-\t * for strings that are too long, we should not have created\n-\t * any.\n-\t */\n-\tif (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {\n-\t\tsend_sig(SIGKILL, current, 0);\n-\t\treturn -1;\n+\tlong len_max;\n+\tlong len_rem;\n+\tlong len_full;\n+\tlong len_buf;\n+\tlong len_abuf;\n+\tlong len_tmp;\n+\tbool require_data;\n+\tbool encode;\n+\tunsigned int iter;\n+\tunsigned int arg;\n+\tchar *buf_head;\n+\tchar *buf;\n+\tconst char __user *p = (const char __user *)current->mm->arg_start;\n+\n+\t/* NOTE: this buffer needs to be large enough to hold all the non-arg\n+\t *       data we put in the audit record for this argument (see the\n+\t *       code below) ... at this point in time 96 is plenty */\n+\tchar abuf[96];\n+\n+\t/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the\n+\t *       current value of 7500 is not as important as the fact that it\n+\t *       is less than 8k, a setting of 7500 gives us plenty of wiggle\n+\t *       room if we go over a little bit in the logging below */\n+\tWARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);\n+\tlen_max = MAX_EXECVE_AUDIT_LEN;\n+\n+\t/* scratch buffer to hold the userspace args */\n+\tbuf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n+\tif (!buf_head) {\n+\t\taudit_panic(\"out of memory for argv string\");\n+\t\treturn;\n \t}\n+\tbuf = buf_head;\n \n-\t/* walk the whole argument looking for non-ascii chars */\n+\taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\n+\n+\tlen_rem = len_max;\n+\tlen_buf = 0;\n+\tlen_full = 0;\n+\trequire_data = true;\n+\tencode = false;\n+\titer = 0;\n+\targ = 0;\n \tdo {\n-\t\tif (len_left > MAX_EXECVE_AUDIT_LEN)\n-\t\t\tto_send = MAX_EXECVE_AUDIT_LEN;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\t\tret = copy_from_user(buf, tmp_p, to_send);\n-\t\t/*\n-\t\t * There is no reason for this copy to be short. We just\n-\t\t * copied them here, and the mm hasn't been exposed to user-\n-\t\t * space yet.\n-\t\t */\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n-\t\t}\n-\t\tbuf[to_send] = '\\0';\n-\t\thas_cntl = audit_string_contains_control(buf, to_send);\n-\t\tif (has_cntl) {\n-\t\t\t/*\n-\t\t\t * hex messages get logged as 2 bytes, so we can only\n-\t\t\t * send half as much in each message\n-\t\t\t */\n-\t\t\tmax_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;\n-\t\t\tbreak;\n-\t\t}\n-\t\tlen_left -= to_send;\n-\t\ttmp_p += to_send;\n-\t} while (len_left > 0);\n-\n-\tlen_left = len;\n-\n-\tif (len > max_execve_audit_len)\n-\t\ttoo_long = 1;\n-\n-\t/* rewalk the argument actually logging the message */\n-\tfor (i = 0; len_left > 0; i++) {\n-\t\tint room_left;\n-\n-\t\tif (len_left > max_execve_audit_len)\n-\t\t\tto_send = max_execve_audit_len;\n-\t\telse\n-\t\t\tto_send = len_left;\n-\n-\t\t/* do we have space left to send this argument in this ab? */\n-\t\troom_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;\n-\t\tif (has_cntl)\n-\t\t\troom_left -= (to_send * 2);\n-\t\telse\n-\t\t\troom_left -= to_send;\n-\t\tif (room_left < 0) {\n-\t\t\t*len_sent = 0;\n-\t\t\taudit_log_end(*ab);\n-\t\t\t*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);\n-\t\t\tif (!*ab)\n-\t\t\t\treturn 0;\n-\t\t}\n+\t\t/* NOTE: we don't ever want to trust this value for anything\n+\t\t *       serious, but the audit record format insists we\n+\t\t *       provide an argument length for really long arguments,\n+\t\t *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but\n+\t\t *       to use strncpy_from_user() to obtain this value for\n+\t\t *       recording in the log, although we don't use it\n+\t\t *       anywhere here to avoid a double-fetch problem */\n+\t\tif (len_full == 0)\n+\t\t\tlen_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;\n+\n+\t\t/* read more data from userspace */\n+\t\tif (require_data) {\n+\t\t\t/* can we make more room in the buffer? */\n+\t\t\tif (buf != buf_head) {\n+\t\t\t\tmemmove(buf_head, buf, len_buf);\n+\t\t\t\tbuf = buf_head;\n+\t\t\t}\n+\n+\t\t\t/* fetch as much as we can of the argument */\n+\t\t\tlen_tmp = strncpy_from_user(&buf_head[len_buf], p,\n+\t\t\t\t\t\t    len_max - len_buf);\n+\t\t\tif (len_tmp == -EFAULT) {\n+\t\t\t\t/* unable to copy from userspace */\n+\t\t\t\tsend_sig(SIGKILL, current, 0);\n+\t\t\t\tgoto out;\n+\t\t\t} else if (len_tmp == (len_max - len_buf)) {\n+\t\t\t\t/* buffer is not large enough */\n+\t\t\t\trequire_data = true;\n+\t\t\t\t/* NOTE: if we are going to span multiple\n+\t\t\t\t *       buffers force the encoding so we stand\n+\t\t\t\t *       a chance at a sane len_full value and\n+\t\t\t\t *       consistent record encoding */\n+\t\t\t\tencode = true;\n+\t\t\t\tlen_full = len_full * 2;\n+\t\t\t\tp += len_tmp;\n+\t\t\t} else {\n+\t\t\t\trequire_data = false;\n+\t\t\t\tif (!encode)\n+\t\t\t\t\tencode = audit_string_contains_control(\n+\t\t\t\t\t\t\t\tbuf, len_tmp);\n+\t\t\t\t/* try to use a trusted value for len_full */\n+\t\t\t\tif (len_full < len_max)\n+\t\t\t\t\tlen_full = (encode ?\n+\t\t\t\t\t\t    len_tmp * 2 : len_tmp);\n+\t\t\t\tp += len_tmp + 1;\n+\t\t\t}\n+\t\t\tlen_buf += len_tmp;\n+\t\t\tbuf_head[len_buf] = '\\0';\n \n-\t\t/*\n-\t\t * first record needs to say how long the original string was\n-\t\t * so we can be sure nothing was lost.\n-\t\t */\n-\t\tif ((i == 0) && (too_long))\n-\t\t\taudit_log_format(*ab, \" a%d_len=%zu\", arg_num,\n-\t\t\t\t\t has_cntl ? 2*len : len);\n-\n-\t\t/*\n-\t\t * normally arguments are small enough to fit and we already\n-\t\t * filled buf above when we checked for control characters\n-\t\t * so don't bother with another copy_from_user\n-\t\t */\n-\t\tif (len >= max_execve_audit_len)\n-\t\t\tret = copy_from_user(buf, p, to_send);\n-\t\telse\n-\t\t\tret = 0;\n-\t\tif (ret) {\n-\t\t\tWARN_ON(1);\n-\t\t\tsend_sig(SIGKILL, current, 0);\n-\t\t\treturn -1;\n+\t\t\t/* length of the buffer in the audit record? */\n+\t\t\tlen_abuf = (encode ? len_buf * 2 : len_buf + 2);\n \t\t}\n-\t\tbuf[to_send] = '\\0';\n-\n-\t\t/* actually log it */\n-\t\taudit_log_format(*ab, \" a%d\", arg_num);\n-\t\tif (too_long)\n-\t\t\taudit_log_format(*ab, \"[%d]\", i);\n-\t\taudit_log_format(*ab, \"=\");\n-\t\tif (has_cntl)\n-\t\t\taudit_log_n_hex(*ab, buf, to_send);\n-\t\telse\n-\t\t\taudit_log_string(*ab, buf);\n-\n-\t\tp += to_send;\n-\t\tlen_left -= to_send;\n-\t\t*len_sent += arg_num_len;\n-\t\tif (has_cntl)\n-\t\t\t*len_sent += to_send * 2;\n-\t\telse\n-\t\t\t*len_sent += to_send;\n-\t}\n-\t/* include the null we didn't log */\n-\treturn len + 1;\n-}\n \n-static void audit_log_execve_info(struct audit_context *context,\n-\t\t\t\t  struct audit_buffer **ab)\n-{\n-\tint i, len;\n-\tsize_t len_sent = 0;\n-\tconst char __user *p;\n-\tchar *buf;\n+\t\t/* write as much as we can to the audit log */\n+\t\tif (len_buf > 0) {\n+\t\t\t/* NOTE: some magic numbers here - basically if we\n+\t\t\t *       can't fit a reasonable amount of data into the\n+\t\t\t *       existing audit buffer, flush it and start with\n+\t\t\t *       a new buffer */\n+\t\t\tif ((sizeof(abuf) + 8) > len_rem) {\n+\t\t\t\tlen_rem = len_max;\n+\t\t\t\taudit_log_end(*ab);\n+\t\t\t\t*ab = audit_log_start(context,\n+\t\t\t\t\t\t      GFP_KERNEL, AUDIT_EXECVE);\n+\t\t\t\tif (!*ab)\n+\t\t\t\t\tgoto out;\n+\t\t\t}\n \n-\tp = (const char __user *)current->mm->arg_start;\n+\t\t\t/* create the non-arg portion of the arg record */\n+\t\t\tlen_tmp = 0;\n+\t\t\tif (require_data || (iter > 0) ||\n+\t\t\t    ((len_abuf + sizeof(abuf)) > len_rem)) {\n+\t\t\t\tif (iter == 0) {\n+\t\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t\tsizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t\t\" a%d_len=%lu\",\n+\t\t\t\t\t\t\targ, len_full);\n+\t\t\t\t}\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \" a%d[%d]=\", arg, iter++);\n+\t\t\t} else\n+\t\t\t\tlen_tmp += snprintf(&abuf[len_tmp],\n+\t\t\t\t\t\t    sizeof(abuf) - len_tmp,\n+\t\t\t\t\t\t    \" a%d=\", arg);\n+\t\t\tWARN_ON(len_tmp >= sizeof(abuf));\n+\t\t\tabuf[sizeof(abuf) - 1] = '\\0';\n+\n+\t\t\t/* log the arg in the audit record */\n+\t\t\taudit_log_format(*ab, \"%s\", abuf);\n+\t\t\tlen_rem -= len_tmp;\n+\t\t\tlen_tmp = len_buf;\n+\t\t\tif (encode) {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem / 2; /* encoding */\n+\t\t\t\taudit_log_n_hex(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp * 2;\n+\t\t\t\tlen_abuf -= len_tmp * 2;\n+\t\t\t} else {\n+\t\t\t\tif (len_abuf > len_rem)\n+\t\t\t\t\tlen_tmp = len_rem - 2; /* quotes */\n+\t\t\t\taudit_log_n_string(*ab, buf, len_tmp);\n+\t\t\t\tlen_rem -= len_tmp + 2;\n+\t\t\t\t/* don't subtract the \"2\" because we still need\n+\t\t\t\t * to add quotes to the remaining string */\n+\t\t\t\tlen_abuf -= len_tmp;\n+\t\t\t}\n+\t\t\tlen_buf -= len_tmp;\n+\t\t\tbuf += len_tmp;\n+\t\t}\n \n-\taudit_log_format(*ab, \"argc=%d\", context->execve.argc);\n+\t\t/* ready to move to the next argument? */\n+\t\tif ((len_buf == 0) && !require_data) {\n+\t\t\targ++;\n+\t\t\titer = 0;\n+\t\t\tlen_full = 0;\n+\t\t\trequire_data = true;\n+\t\t\tencode = false;\n+\t\t}\n+\t} while (arg < context->execve.argc);\n \n-\t/*\n-\t * we need some kernel buffer to hold the userspace args.  Just\n-\t * allocate one big one rather than allocating one of the right size\n-\t * for every single argument inside audit_log_single_execve_arg()\n-\t * should be <8k allocation so should be pretty safe.\n-\t */\n-\tbuf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);\n-\tif (!buf) {\n-\t\taudit_panic(\"out of memory for argv string\");\n-\t\treturn;\n-\t}\n+\t/* NOTE: the caller handles the final audit_log_end() call */\n \n-\tfor (i = 0; i < context->execve.argc; i++) {\n-\t\tlen = audit_log_single_execve_arg(context, ab, i,\n-\t\t\t\t\t\t  &len_sent, p, buf);\n-\t\tif (len <= 0)\n-\t\t\tbreak;\n-\t\tp += len;\n-\t}\n-\tkfree(buf);\n+out:\n+\tkfree(buf_head);\n }\n \n static void show_special(struct audit_context *context, int *call_panic)", "file_path": "files/2016_8\\107", "file_language": "c", "file_name": "kernel/auditsc.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 46, "cve_id": "CVE-2014-9888", "cwe_id": ["CWE-264"], "cve_language": "C", "cve_description": "arch/arm/mm/dma-mapping.c in the Linux kernel before 3.13 on ARM platforms, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not prevent executable DMA mappings, which might allow local users to gain privileges via a crafted application, aka Android internal bug 28803642 and Qualcomm internal bug CR642735.", "cvss": "7.8", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "LOW", "UI": "NONE", "S": "UNCHANGED", "C": "HIGH", "I": "HIGH", "A": "HIGH", "commit_id": "0ea1ec713f04bdfac343c9702b21cd3a7c711826", "commit_message": "ARM: dma-mapping: don't allow DMA mappings to be marked executable\n\nDMA mapping permissions were being derived from pgprot_kernel directly\nwithout using PAGE_KERNEL.  This causes them to be marked with executable\npermission, which is not what we want.  Fix this.\n\nSigned-off-by: Russell King <rmk+kernel@arm.linux.org.uk>", "commit_date": "2013-10-24T10:17:27Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/0ea1ec713f04bdfac343c9702b21cd3a7c711826", "html_url": "https://github.com/torvalds/linux/commit/0ea1ec713f04bdfac343c9702b21cd3a7c711826", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "59fd3033b55642da97b4ecde0c85de78d7229675", "url_before": "https://api.github.com/repos/torvalds/linux/commits/59fd3033b55642da97b4ecde0c85de78d7229675", "html_url_before": "https://github.com/torvalds/linux/commit/59fd3033b55642da97b4ecde0c85de78d7229675"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/0ea1ec713f04bdfac343c9702b21cd3a7c711826/arch/arm/mm/dma-mapping.c", "code": "/*\n *  linux/arch/arm/mm/dma-mapping.c\n *\n *  Copyright (C) 2000-2004 Russell King\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n *  DMA uncached mapping support.\n */\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/gfp.h>\n#include <linux/errno.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/dma-contiguous.h>\n#include <linux/highmem.h>\n#include <linux/memblock.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/io.h>\n#include <linux/vmalloc.h>\n#include <linux/sizes.h>\n\n#include <asm/memory.h>\n#include <asm/highmem.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n#include <asm/mach/arch.h>\n#include <asm/dma-iommu.h>\n#include <asm/mach/map.h>\n#include <asm/system_info.h>\n#include <asm/dma-contiguous.h>\n\n#include \"mm.h\"\n\n/*\n * The DMA API is built upon the notion of \"buffer ownership\".  A buffer\n * is either exclusively owned by the CPU (and therefore may be accessed\n * by it) or exclusively owned by the DMA device.  These helper functions\n * represent the transitions between these two ownership states.\n *\n * Note, however, that on later ARMs, this notion does not work due to\n * speculative prefetches.  We model our approach on the assumption that\n * the CPU does do speculative prefetches, which means we clean caches\n * before transfers and delay cache invalidation until transfer completion.\n *\n */\nstatic void __dma_page_cpu_to_dev(struct page *, unsigned long,\n\t\tsize_t, enum dma_data_direction);\nstatic void __dma_page_dev_to_cpu(struct page *, unsigned long,\n\t\tsize_t, enum dma_data_direction);\n\n/**\n * arm_dma_map_page - map a portion of a page for streaming DMA\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * Ensure that any data held in the cache is appropriately discarded\n * or written back.\n *\n * The device owns this memory once this call has completed.  The CPU\n * can regain ownership by calling dma_unmap_page().\n */\nstatic dma_addr_t arm_dma_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_cpu_to_dev(page, offset, size, dir);\n\treturn pfn_to_dma(dev, page_to_pfn(page)) + offset;\n}\n\nstatic dma_addr_t arm_coherent_dma_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\treturn pfn_to_dma(dev, page_to_pfn(page)) + offset;\n}\n\n/**\n * arm_dma_unmap_page - unmap a buffer previously mapped through dma_map_page()\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * Unmap a page streaming mode DMA translation.  The handle and size\n * must match what was provided in the previous dma_map_page() call.\n * All other usages are undefined.\n *\n * After this call, reads by the CPU to the buffer are guaranteed to see\n * whatever the device wrote there.\n */\nstatic void arm_dma_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),\n\t\t\t\t      handle & ~PAGE_MASK, size, dir);\n}\n\nstatic void arm_dma_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tunsigned int offset = handle & (PAGE_SIZE - 1);\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));\n\t__dma_page_dev_to_cpu(page, offset, size, dir);\n}\n\nstatic void arm_dma_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tunsigned int offset = handle & (PAGE_SIZE - 1);\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle-offset));\n\t__dma_page_cpu_to_dev(page, offset, size, dir);\n}\n\nstruct dma_map_ops arm_dma_ops = {\n\t.alloc\t\t\t= arm_dma_alloc,\n\t.free\t\t\t= arm_dma_free,\n\t.mmap\t\t\t= arm_dma_mmap,\n\t.get_sgtable\t\t= arm_dma_get_sgtable,\n\t.map_page\t\t= arm_dma_map_page,\n\t.unmap_page\t\t= arm_dma_unmap_page,\n\t.map_sg\t\t\t= arm_dma_map_sg,\n\t.unmap_sg\t\t= arm_dma_unmap_sg,\n\t.sync_single_for_cpu\t= arm_dma_sync_single_for_cpu,\n\t.sync_single_for_device\t= arm_dma_sync_single_for_device,\n\t.sync_sg_for_cpu\t= arm_dma_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= arm_dma_sync_sg_for_device,\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\nEXPORT_SYMBOL(arm_dma_ops);\n\nstatic void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs);\nstatic void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t\t  dma_addr_t handle, struct dma_attrs *attrs);\n\nstruct dma_map_ops arm_coherent_dma_ops = {\n\t.alloc\t\t\t= arm_coherent_dma_alloc,\n\t.free\t\t\t= arm_coherent_dma_free,\n\t.mmap\t\t\t= arm_dma_mmap,\n\t.get_sgtable\t\t= arm_dma_get_sgtable,\n\t.map_page\t\t= arm_coherent_dma_map_page,\n\t.map_sg\t\t\t= arm_dma_map_sg,\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\nEXPORT_SYMBOL(arm_coherent_dma_ops);\n\nstatic u64 get_coherent_dma_mask(struct device *dev)\n{\n\tu64 mask = (u64)arm_dma_limit;\n\n\tif (dev) {\n\t\tmask = dev->coherent_dma_mask;\n\n\t\t/*\n\t\t * Sanity check the DMA mask - it must be non-zero, and\n\t\t * must be able to be satisfied by a DMA allocation.\n\t\t */\n\t\tif (mask == 0) {\n\t\t\tdev_warn(dev, \"coherent DMA mask is unset\\n\");\n\t\t\treturn 0;\n\t\t}\n\n\t\tif ((~mask) & (u64)arm_dma_limit) {\n\t\t\tdev_warn(dev, \"coherent DMA mask %#llx is smaller \"\n\t\t\t\t \"than system GFP_DMA mask %#llx\\n\",\n\t\t\t\t mask, (u64)arm_dma_limit);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn mask;\n}\n\nstatic void __dma_clear_buffer(struct page *page, size_t size)\n{\n\t/*\n\t * Ensure that the allocated pages are zeroed, and that any data\n\t * lurking in the kernel direct-mapped region is invalidated.\n\t */\n\tif (PageHighMem(page)) {\n\t\tphys_addr_t base = __pfn_to_phys(page_to_pfn(page));\n\t\tphys_addr_t end = base + size;\n\t\twhile (size > 0) {\n\t\t\tvoid *ptr = kmap_atomic(page);\n\t\t\tmemset(ptr, 0, PAGE_SIZE);\n\t\t\tdmac_flush_range(ptr, ptr + PAGE_SIZE);\n\t\t\tkunmap_atomic(ptr);\n\t\t\tpage++;\n\t\t\tsize -= PAGE_SIZE;\n\t\t}\n\t\touter_flush_range(base, end);\n\t} else {\n\t\tvoid *ptr = page_address(page);\n\t\tmemset(ptr, 0, size);\n\t\tdmac_flush_range(ptr, ptr + size);\n\t\touter_flush_range(__pa(ptr), __pa(ptr) + size);\n\t}\n}\n\n/*\n * Allocate a DMA buffer for 'dev' of size 'size' using the\n * specified gfp mask.  Note that 'size' must be page aligned.\n */\nstatic struct page *__dma_alloc_buffer(struct device *dev, size_t size, gfp_t gfp)\n{\n\tunsigned long order = get_order(size);\n\tstruct page *page, *p, *e;\n\n\tpage = alloc_pages(gfp, order);\n\tif (!page)\n\t\treturn NULL;\n\n\t/*\n\t * Now split the huge page and free the excess pages\n\t */\n\tsplit_page(page, order);\n\tfor (p = page + (size >> PAGE_SHIFT), e = page + (1 << order); p < e; p++)\n\t\t__free_page(p);\n\n\t__dma_clear_buffer(page, size);\n\n\treturn page;\n}\n\n/*\n * Free a DMA buffer.  'size' must be page aligned.\n */\nstatic void __dma_free_buffer(struct page *page, size_t size)\n{\n\tstruct page *e = page + (size >> PAGE_SHIFT);\n\n\twhile (page < e) {\n\t\t__free_page(page);\n\t\tpage++;\n\t}\n}\n\n#ifdef CONFIG_MMU\n#ifdef CONFIG_HUGETLB_PAGE\n#warning ARM Coherent DMA allocator does not (yet) support huge TLB\n#endif\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller);\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller);\n\nstatic void *\n__dma_alloc_remap(struct page *page, size_t size, gfp_t gfp, pgprot_t prot,\n\tconst void *caller)\n{\n\tstruct vm_struct *area;\n\tunsigned long addr;\n\n\t/*\n\t * DMA allocation can be mapped to user space, so lets\n\t * set VM_USERMAP flags too.\n\t */\n\tarea = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,\n\t\t\t\t  caller);\n\tif (!area)\n\t\treturn NULL;\n\taddr = (unsigned long)area->addr;\n\tarea->phys_addr = __pfn_to_phys(page_to_pfn(page));\n\n\tif (ioremap_page_range(addr, addr + size, area->phys_addr, prot)) {\n\t\tvunmap((void *)addr);\n\t\treturn NULL;\n\t}\n\treturn (void *)addr;\n}\n\nstatic void __dma_free_remap(void *cpu_addr, size_t size)\n{\n\tunsigned int flags = VM_ARM_DMA_CONSISTENT | VM_USERMAP;\n\tstruct vm_struct *area = find_vm_area(cpu_addr);\n\tif (!area || (area->flags & flags) != flags) {\n\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);\n\t\treturn;\n\t}\n\tunmap_kernel_range((unsigned long)cpu_addr, size);\n\tvunmap(cpu_addr);\n}\n\n#define DEFAULT_DMA_COHERENT_POOL_SIZE\tSZ_256K\n\nstruct dma_pool {\n\tsize_t size;\n\tspinlock_t lock;\n\tunsigned long *bitmap;\n\tunsigned long nr_pages;\n\tvoid *vaddr;\n\tstruct page **pages;\n};\n\nstatic struct dma_pool atomic_pool = {\n\t.size = DEFAULT_DMA_COHERENT_POOL_SIZE,\n};\n\nstatic int __init early_coherent_pool(char *p)\n{\n\tatomic_pool.size = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"coherent_pool\", early_coherent_pool);\n\nvoid __init init_dma_coherent_pool_size(unsigned long size)\n{\n\t/*\n\t * Catch any attempt to set the pool size too late.\n\t */\n\tBUG_ON(atomic_pool.vaddr);\n\n\t/*\n\t * Set architecture specific coherent pool size only if\n\t * it has not been changed by kernel command line parameter.\n\t */\n\tif (atomic_pool.size == DEFAULT_DMA_COHERENT_POOL_SIZE)\n\t\tatomic_pool.size = size;\n}\n\n/*\n * Initialise the coherent pool for atomic allocations.\n */\nstatic int __init atomic_pool_init(void)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tpgprot_t prot = pgprot_dmacoherent(pgprot_kernel);\n\tgfp_t gfp = GFP_KERNEL | GFP_DMA;\n\tunsigned long nr_pages = pool->size >> PAGE_SHIFT;\n\tunsigned long *bitmap;\n\tstruct page *page;\n\tstruct page **pages;\n\tvoid *ptr;\n\tint bitmap_size = BITS_TO_LONGS(nr_pages) * sizeof(long);\n\n\tbitmap = kzalloc(bitmap_size, GFP_KERNEL);\n\tif (!bitmap)\n\t\tgoto no_bitmap;\n\n\tpages = kzalloc(nr_pages * sizeof(struct page *), GFP_KERNEL);\n\tif (!pages)\n\t\tgoto no_pages;\n\n\tif (IS_ENABLED(CONFIG_DMA_CMA))\n\t\tptr = __alloc_from_contiguous(NULL, pool->size, prot, &page,\n\t\t\t\t\t      atomic_pool_init);\n\telse\n\t\tptr = __alloc_remap_buffer(NULL, pool->size, gfp, prot, &page,\n\t\t\t\t\t   atomic_pool_init);\n\tif (ptr) {\n\t\tint i;\n\n\t\tfor (i = 0; i < nr_pages; i++)\n\t\t\tpages[i] = page + i;\n\n\t\tspin_lock_init(&pool->lock);\n\t\tpool->vaddr = ptr;\n\t\tpool->pages = pages;\n\t\tpool->bitmap = bitmap;\n\t\tpool->nr_pages = nr_pages;\n\t\tpr_info(\"DMA: preallocated %u KiB pool for atomic coherent allocations\\n\",\n\t\t       (unsigned)pool->size / 1024);\n\t\treturn 0;\n\t}\n\n\tkfree(pages);\nno_pages:\n\tkfree(bitmap);\nno_bitmap:\n\tpr_err(\"DMA: failed to allocate %u KiB pool for atomic coherent allocation\\n\",\n\t       (unsigned)pool->size / 1024);\n\treturn -ENOMEM;\n}\n/*\n * CMA is activated by core_initcall, so we must be called after it.\n */\npostcore_initcall(atomic_pool_init);\n\nstruct dma_contig_early_reserve {\n\tphys_addr_t base;\n\tunsigned long size;\n};\n\nstatic struct dma_contig_early_reserve dma_mmu_remap[MAX_CMA_AREAS] __initdata;\n\nstatic int dma_mmu_remap_num __initdata;\n\nvoid __init dma_contiguous_early_fixup(phys_addr_t base, unsigned long size)\n{\n\tdma_mmu_remap[dma_mmu_remap_num].base = base;\n\tdma_mmu_remap[dma_mmu_remap_num].size = size;\n\tdma_mmu_remap_num++;\n}\n\nvoid __init dma_contiguous_remap(void)\n{\n\tint i;\n\tfor (i = 0; i < dma_mmu_remap_num; i++) {\n\t\tphys_addr_t start = dma_mmu_remap[i].base;\n\t\tphys_addr_t end = start + dma_mmu_remap[i].size;\n\t\tstruct map_desc map;\n\t\tunsigned long addr;\n\n\t\tif (end > arm_lowmem_limit)\n\t\t\tend = arm_lowmem_limit;\n\t\tif (start >= end)\n\t\t\tcontinue;\n\n\t\tmap.pfn = __phys_to_pfn(start);\n\t\tmap.virtual = __phys_to_virt(start);\n\t\tmap.length = end - start;\n\t\tmap.type = MT_MEMORY_DMA_READY;\n\n\t\t/*\n\t\t * Clear previous low-memory mapping\n\t\t */\n\t\tfor (addr = __phys_to_virt(start); addr < __phys_to_virt(end);\n\t\t     addr += PMD_SIZE)\n\t\t\tpmd_clear(pmd_off_k(addr));\n\n\t\tiotable_init(&map, 1);\n\t}\n}\n\nstatic int __dma_update_pte(pte_t *pte, pgtable_t token, unsigned long addr,\n\t\t\t    void *data)\n{\n\tstruct page *page = virt_to_page(addr);\n\tpgprot_t prot = *(pgprot_t *)data;\n\n\tset_pte_ext(pte, mk_pte(page, prot), 0);\n\treturn 0;\n}\n\nstatic void __dma_remap(struct page *page, size_t size, pgprot_t prot)\n{\n\tunsigned long start = (unsigned long) page_address(page);\n\tunsigned end = start + size;\n\n\tapply_to_page_range(&init_mm, start, size, __dma_update_pte, &prot);\n\tflush_tlb_kernel_range(start, end);\n}\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller)\n{\n\tstruct page *page;\n\tvoid *ptr;\n\tpage = __dma_alloc_buffer(dev, size, gfp);\n\tif (!page)\n\t\treturn NULL;\n\n\tptr = __dma_alloc_remap(page, size, gfp, prot, caller);\n\tif (!ptr) {\n\t\t__dma_free_buffer(page, size);\n\t\treturn NULL;\n\t}\n\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void *__alloc_from_pool(size_t size, struct page **ret_page)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned int pageno;\n\tunsigned long flags;\n\tvoid *ptr = NULL;\n\tunsigned long align_mask;\n\n\tif (!pool->vaddr) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * Align the region allocation - allocations from pool are rather\n\t * small, so align them to their order in pages, minimum is a page\n\t * size. This helps reduce fragmentation of the DMA space.\n\t */\n\talign_mask = (1 << get_order(size)) - 1;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tpageno = bitmap_find_next_zero_area(pool->bitmap, pool->nr_pages,\n\t\t\t\t\t    0, count, align_mask);\n\tif (pageno < pool->nr_pages) {\n\t\tbitmap_set(pool->bitmap, pageno, count);\n\t\tptr = pool->vaddr + PAGE_SIZE * pageno;\n\t\t*ret_page = pool->pages[pageno];\n\t} else {\n\t\tpr_err_once(\"ERROR: %u KiB atomic DMA coherent pool is too small!\\n\"\n\t\t\t    \"Please increase it with coherent_pool= kernel parameter!\\n\",\n\t\t\t    (unsigned)pool->size / 1024);\n\t}\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\treturn ptr;\n}\n\nstatic bool __in_atomic_pool(void *start, size_t size)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tvoid *end = start + size;\n\tvoid *pool_start = pool->vaddr;\n\tvoid *pool_end = pool->vaddr + pool->size;\n\n\tif (start < pool_start || start >= pool_end)\n\t\treturn false;\n\n\tif (end <= pool_end)\n\t\treturn true;\n\n\tWARN(1, \"Wrong coherent size(%p-%p) from atomic pool(%p-%p)\\n\",\n\t     start, end - 1, pool_start, pool_end - 1);\n\n\treturn false;\n}\n\nstatic int __free_from_pool(void *start, size_t size)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tunsigned long pageno, count;\n\tunsigned long flags;\n\n\tif (!__in_atomic_pool(start, size))\n\t\treturn 0;\n\n\tpageno = (start - pool->vaddr) >> PAGE_SHIFT;\n\tcount = size >> PAGE_SHIFT;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tbitmap_clear(pool->bitmap, pageno, count);\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\n\treturn 1;\n}\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller)\n{\n\tunsigned long order = get_order(size);\n\tsize_t count = size >> PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tpage = dma_alloc_from_contiguous(dev, count, order);\n\tif (!page)\n\t\treturn NULL;\n\n\t__dma_clear_buffer(page, size);\n\n\tif (PageHighMem(page)) {\n\t\tptr = __dma_alloc_remap(page, size, GFP_KERNEL, prot, caller);\n\t\tif (!ptr) {\n\t\t\tdma_release_from_contiguous(dev, page, count);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\t__dma_remap(page, size, prot);\n\t\tptr = page_address(page);\n\t}\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void __free_from_contiguous(struct device *dev, struct page *page,\n\t\t\t\t   void *cpu_addr, size_t size)\n{\n\tif (PageHighMem(page))\n\t\t__dma_free_remap(cpu_addr, size);\n\telse\n\t\t__dma_remap(page, size, pgprot_kernel);\n\tdma_release_from_contiguous(dev, page, size >> PAGE_SHIFT);\n}\n\nstatic inline pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot)\n{\n\tprot = dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs) ?\n\t\t\t    pgprot_writecombine(prot) :\n\t\t\t    pgprot_dmacoherent(prot);\n\treturn prot;\n}\n\n#define nommu() 0\n\n#else\t/* !CONFIG_MMU */\n\n#define nommu() 1\n\n#define __get_dma_pgprot(attrs, prot)\t__pgprot(0)\n#define __alloc_remap_buffer(dev, size, gfp, prot, ret, c)\tNULL\n#define __alloc_from_pool(size, ret_page)\t\t\tNULL\n#define __alloc_from_contiguous(dev, size, prot, ret, c)\tNULL\n#define __free_from_pool(cpu_addr, size)\t\t\t0\n#define __free_from_contiguous(dev, page, cpu_addr, size)\tdo { } while (0)\n#define __dma_free_remap(cpu_addr, size)\t\t\tdo { } while (0)\n\n#endif\t/* CONFIG_MMU */\n\nstatic void *__alloc_simple_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t   struct page **ret_page)\n{\n\tstruct page *page;\n\tpage = __dma_alloc_buffer(dev, size, gfp);\n\tif (!page)\n\t\treturn NULL;\n\n\t*ret_page = page;\n\treturn page_address(page);\n}\n\n\n\nstatic void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t\t gfp_t gfp, pgprot_t prot, bool is_coherent, const void *caller)\n{\n\tu64 mask = get_coherent_dma_mask(dev);\n\tstruct page *page = NULL;\n\tvoid *addr;\n\n#ifdef CONFIG_DMA_API_DEBUG\n\tu64 limit = (mask + 1) & ~mask;\n\tif (limit && size >= limit) {\n\t\tdev_warn(dev, \"coherent allocation too big (requested %#x mask %#llx)\\n\",\n\t\t\tsize, mask);\n\t\treturn NULL;\n\t}\n#endif\n\n\tif (!mask)\n\t\treturn NULL;\n\n\tif (mask < 0xffffffffULL)\n\t\tgfp |= GFP_DMA;\n\n\t/*\n\t * Following is a work-around (a.k.a. hack) to prevent pages\n\t * with __GFP_COMP being passed to split_page() which cannot\n\t * handle them.  The real problem is that this flag probably\n\t * should be 0 on ARM as it is not supported on this\n\t * platform; see CONFIG_HUGETLBFS.\n\t */\n\tgfp &= ~(__GFP_COMP);\n\n\t*handle = DMA_ERROR_CODE;\n\tsize = PAGE_ALIGN(size);\n\n\tif (is_coherent || nommu())\n\t\taddr = __alloc_simple_buffer(dev, size, gfp, &page);\n\telse if (!(gfp & __GFP_WAIT))\n\t\taddr = __alloc_from_pool(size, &page);\n\telse if (!IS_ENABLED(CONFIG_DMA_CMA))\n\t\taddr = __alloc_remap_buffer(dev, size, gfp, prot, &page, caller);\n\telse\n\t\taddr = __alloc_from_contiguous(dev, size, prot, &page, caller);\n\n\tif (addr)\n\t\t*handle = pfn_to_dma(dev, page_to_pfn(page));\n\n\treturn addr;\n}\n\n/*\n * Allocate DMA-coherent memory space and return both the kernel remapped\n * virtual and bus address for that space.\n */\nvoid *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t    gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, false,\n\t\t\t   __builtin_return_address(0));\n}\n\nstatic void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n\tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tvoid *memory;\n\n\tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n\t\treturn memory;\n\n\treturn __dma_alloc(dev, size, handle, gfp, prot, true,\n\t\t\t   __builtin_return_address(0));\n}\n\n/*\n * Create userspace mapping for the DMA-coherent memory.\n */\nint arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,\n\t\t void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t struct dma_attrs *attrs)\n{\n\tint ret = -ENXIO;\n#ifdef CONFIG_MMU\n\tunsigned long nr_vma_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n\tunsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tunsigned long pfn = dma_to_pfn(dev, dma_addr);\n\tunsigned long off = vma->vm_pgoff;\n\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);\n\n\tif (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))\n\t\treturn ret;\n\n\tif (off < nr_pages && nr_vma_pages <= (nr_pages - off)) {\n\t\tret = remap_pfn_range(vma, vma->vm_start,\n\t\t\t\t      pfn + off,\n\t\t\t\t      vma->vm_end - vma->vm_start,\n\t\t\t\t      vma->vm_page_prot);\n\t}\n#endif\t/* CONFIG_MMU */\n\n\treturn ret;\n}\n\n/*\n * Free a buffer as defined by the above mapping.\n */\nstatic void __arm_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t   dma_addr_t handle, struct dma_attrs *attrs,\n\t\t\t   bool is_coherent)\n{\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle));\n\n\tif (dma_release_from_coherent(dev, get_order(size), cpu_addr))\n\t\treturn;\n\n\tsize = PAGE_ALIGN(size);\n\n\tif (is_coherent || nommu()) {\n\t\t__dma_free_buffer(page, size);\n\t} else if (__free_from_pool(cpu_addr, size)) {\n\t\treturn;\n\t} else if (!IS_ENABLED(CONFIG_DMA_CMA)) {\n\t\t__dma_free_remap(cpu_addr, size);\n\t\t__dma_free_buffer(page, size);\n\t} else {\n\t\t/*\n\t\t * Non-atomic allocations cannot be freed with IRQs disabled\n\t\t */\n\t\tWARN_ON(irqs_disabled());\n\t\t__free_from_contiguous(dev, page, cpu_addr, size);\n\t}\n}\n\nvoid arm_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\t__arm_dma_free(dev, size, cpu_addr, handle, attrs, false);\n}\n\nstatic void arm_coherent_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\t__arm_dma_free(dev, size, cpu_addr, handle, attrs, true);\n}\n\nint arm_dma_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t void *cpu_addr, dma_addr_t handle, size_t size,\n\t\t struct dma_attrs *attrs)\n{\n\tstruct page *page = pfn_to_page(dma_to_pfn(dev, handle));\n\tint ret;\n\n\tret = sg_alloc_table(sgt, 1, GFP_KERNEL);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);\n\treturn 0;\n}\n\nstatic void dma_cache_maint_page(struct page *page, unsigned long offset,\n\tsize_t size, enum dma_data_direction dir,\n\tvoid (*op)(const void *, size_t, int))\n{\n\tunsigned long pfn;\n\tsize_t left = size;\n\n\tpfn = page_to_pfn(page) + offset / PAGE_SIZE;\n\toffset %= PAGE_SIZE;\n\n\t/*\n\t * A single sg entry may refer to multiple physically contiguous\n\t * pages.  But we still need to process highmem pages individually.\n\t * If highmem is not configured then the bulk of this loop gets\n\t * optimized out.\n\t */\n\tdo {\n\t\tsize_t len = left;\n\t\tvoid *vaddr;\n\n\t\tpage = pfn_to_page(pfn);\n\n\t\tif (PageHighMem(page)) {\n\t\t\tif (len + offset > PAGE_SIZE)\n\t\t\t\tlen = PAGE_SIZE - offset;\n\n\t\t\tif (cache_is_vipt_nonaliasing()) {\n\t\t\t\tvaddr = kmap_atomic(page);\n\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t} else {\n\t\t\t\tvaddr = kmap_high_get(page);\n\t\t\t\tif (vaddr) {\n\t\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\t\tkunmap_high(page);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tvaddr = page_address(page) + offset;\n\t\t\top(vaddr, len, dir);\n\t\t}\n\t\toffset = 0;\n\t\tpfn++;\n\t\tleft -= len;\n\t} while (left);\n}\n\n/*\n * Make an area consistent for devices.\n * Note: Drivers should NOT use this function directly, as it will break\n * platforms with CONFIG_DMABOUNCE.\n * Use the driver DMA support - see dma-mapping.h (dma_sync_*)\n */\nstatic void __dma_page_cpu_to_dev(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tunsigned long paddr;\n\n\tdma_cache_maint_page(page, off, size, dir, dmac_map_area);\n\n\tpaddr = page_to_phys(page) + off;\n\tif (dir == DMA_FROM_DEVICE) {\n\t\touter_inv_range(paddr, paddr + size);\n\t} else {\n\t\touter_clean_range(paddr, paddr + size);\n\t}\n\t/* FIXME: non-speculating: flush on bidirectional mappings? */\n}\n\nstatic void __dma_page_dev_to_cpu(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tunsigned long paddr = page_to_phys(page) + off;\n\n\t/* FIXME: non-speculating: not required */\n\t/* don't bother invalidating if DMA to device */\n\tif (dir != DMA_TO_DEVICE)\n\t\touter_inv_range(paddr, paddr + size);\n\n\tdma_cache_maint_page(page, off, size, dir, dmac_unmap_area);\n\n\t/*\n\t * Mark the D-cache clean for these pages to avoid extra flushing.\n\t */\n\tif (dir != DMA_TO_DEVICE && size >= PAGE_SIZE) {\n\t\tunsigned long pfn;\n\t\tsize_t left = size;\n\n\t\tpfn = page_to_pfn(page) + off / PAGE_SIZE;\n\t\toff %= PAGE_SIZE;\n\t\tif (off) {\n\t\t\tpfn++;\n\t\t\tleft -= PAGE_SIZE - off;\n\t\t}\n\t\twhile (left >= PAGE_SIZE) {\n\t\t\tpage = pfn_to_page(pfn++);\n\t\t\tset_bit(PG_dcache_clean, &page->flags);\n\t\t\tleft -= PAGE_SIZE;\n\t\t}\n\t}\n}\n\n/**\n * arm_dma_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of buffers described by scatterlist in streaming mode for DMA.\n * This is the scatter-gather version of the dma_map_single interface.\n * Here the scatter gather list elements are each tagged with the\n * appropriate dma address and length.  They are obtained via\n * sg_dma_{address,length}.\n *\n * Device ownership issues as mentioned for dma_map_single are the same\n * here.\n */\nint arm_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i, j;\n\n\tfor_each_sg(sg, s, nents, i) {\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\t\ts->dma_length = s->length;\n#endif\n\t\ts->dma_address = ops->map_page(dev, sg_page(s), s->offset,\n\t\t\t\t\t\ts->length, dir, attrs);\n\t\tif (dma_mapping_error(dev, s->dma_address))\n\t\t\tgoto bad_mapping;\n\t}\n\treturn nents;\n\n bad_mapping:\n\tfor_each_sg(sg, s, i, j)\n\t\tops->unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir, attrs);\n\treturn 0;\n}\n\n/**\n * arm_dma_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->unmap_page(dev, sg_dma_address(s), sg_dma_len(s), dir, attrs);\n}\n\n/**\n * arm_dma_sync_sg_for_cpu\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->sync_single_for_cpu(dev, sg_dma_address(s), s->length,\n\t\t\t\t\t dir);\n}\n\n/**\n * arm_dma_sync_sg_for_device\n * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\tops->sync_single_for_device(dev, sg_dma_address(s), s->length,\n\t\t\t\t\t    dir);\n}\n\n/*\n * Return whether the given device DMA address mask can be supported\n * properly.  For example, if your device can only drive the low 24-bits\n * during bus mastering, then you would pass 0x00ffffff as the mask\n * to this function.\n */\nint dma_supported(struct device *dev, u64 mask)\n{\n\tif (mask < (u64)arm_dma_limit)\n\t\treturn 0;\n\treturn 1;\n}\nEXPORT_SYMBOL(dma_supported);\n\nint arm_dma_set_mask(struct device *dev, u64 dma_mask)\n{\n\tif (!dev->dma_mask || !dma_supported(dev, dma_mask))\n\t\treturn -EIO;\n\n\t*dev->dma_mask = dma_mask;\n\n\treturn 0;\n}\n\n#define PREALLOC_DMA_DEBUG_ENTRIES\t4096\n\nstatic int __init dma_debug_do_init(void)\n{\n\tdma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);\n\treturn 0;\n}\nfs_initcall(dma_debug_do_init);\n\n#ifdef CONFIG_ARM_DMA_USE_IOMMU\n\n/* IOMMU */\n\nstatic inline dma_addr_t __alloc_iova(struct dma_iommu_mapping *mapping,\n\t\t\t\t      size_t size)\n{\n\tunsigned int order = get_order(size);\n\tunsigned int align = 0;\n\tunsigned int count, start;\n\tunsigned long flags;\n\n\tif (order > CONFIG_ARM_DMA_IOMMU_ALIGNMENT)\n\t\torder = CONFIG_ARM_DMA_IOMMU_ALIGNMENT;\n\n\tcount = ((PAGE_ALIGN(size) >> PAGE_SHIFT) +\n\t\t (1 << mapping->order) - 1) >> mapping->order;\n\n\tif (order > mapping->order)\n\t\talign = (1 << (order - mapping->order)) - 1;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tstart = bitmap_find_next_zero_area(mapping->bitmap, mapping->bits, 0,\n\t\t\t\t\t   count, align);\n\tif (start > mapping->bits) {\n\t\tspin_unlock_irqrestore(&mapping->lock, flags);\n\t\treturn DMA_ERROR_CODE;\n\t}\n\n\tbitmap_set(mapping->bitmap, start, count);\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n\n\treturn mapping->base + (start << (mapping->order + PAGE_SHIFT));\n}\n\nstatic inline void __free_iova(struct dma_iommu_mapping *mapping,\n\t\t\t       dma_addr_t addr, size_t size)\n{\n\tunsigned int start = (addr - mapping->base) >>\n\t\t\t     (mapping->order + PAGE_SHIFT);\n\tunsigned int count = ((size >> PAGE_SHIFT) +\n\t\t\t      (1 << mapping->order) - 1) >> mapping->order;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tbitmap_clear(mapping->bitmap, start, count);\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n}\n\nstatic struct page **__iommu_alloc_buffer(struct device *dev, size_t size,\n\t\t\t\t\t  gfp_t gfp, struct dma_attrs *attrs)\n{\n\tstruct page **pages;\n\tint count = size >> PAGE_SHIFT;\n\tint array_size = count * sizeof(struct page *);\n\tint i = 0;\n\n\tif (array_size <= PAGE_SIZE)\n\t\tpages = kzalloc(array_size, gfp);\n\telse\n\t\tpages = vzalloc(array_size);\n\tif (!pages)\n\t\treturn NULL;\n\n\tif (dma_get_attr(DMA_ATTR_FORCE_CONTIGUOUS, attrs))\n\t{\n\t\tunsigned long order = get_order(size);\n\t\tstruct page *page;\n\n\t\tpage = dma_alloc_from_contiguous(dev, count, order);\n\t\tif (!page)\n\t\t\tgoto error;\n\n\t\t__dma_clear_buffer(page, size);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tpages[i] = page + i;\n\n\t\treturn pages;\n\t}\n\n\t/*\n\t * IOMMU can map any pages, so himem can also be used here\n\t */\n\tgfp |= __GFP_NOWARN | __GFP_HIGHMEM;\n\n\twhile (count) {\n\t\tint j, order = __fls(count);\n\n\t\tpages[i] = alloc_pages(gfp, order);\n\t\twhile (!pages[i] && order)\n\t\t\tpages[i] = alloc_pages(gfp, --order);\n\t\tif (!pages[i])\n\t\t\tgoto error;\n\n\t\tif (order) {\n\t\t\tsplit_page(pages[i], order);\n\t\t\tj = 1 << order;\n\t\t\twhile (--j)\n\t\t\t\tpages[i + j] = pages[i] + j;\n\t\t}\n\n\t\t__dma_clear_buffer(pages[i], PAGE_SIZE << order);\n\t\ti += 1 << order;\n\t\tcount -= 1 << order;\n\t}\n\n\treturn pages;\nerror:\n\twhile (i--)\n\t\tif (pages[i])\n\t\t\t__free_pages(pages[i], 0);\n\tif (array_size <= PAGE_SIZE)\n\t\tkfree(pages);\n\telse\n\t\tvfree(pages);\n\treturn NULL;\n}\n\nstatic int __iommu_free_buffer(struct device *dev, struct page **pages,\n\t\t\t       size_t size, struct dma_attrs *attrs)\n{\n\tint count = size >> PAGE_SHIFT;\n\tint array_size = count * sizeof(struct page *);\n\tint i;\n\n\tif (dma_get_attr(DMA_ATTR_FORCE_CONTIGUOUS, attrs)) {\n\t\tdma_release_from_contiguous(dev, pages[0], count);\n\t} else {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (pages[i])\n\t\t\t\t__free_pages(pages[i], 0);\n\t}\n\n\tif (array_size <= PAGE_SIZE)\n\t\tkfree(pages);\n\telse\n\t\tvfree(pages);\n\treturn 0;\n}\n\n/*\n * Create a CPU mapping for a specified pages\n */\nstatic void *\n__iommu_alloc_remap(struct page **pages, size_t size, gfp_t gfp, pgprot_t prot,\n\t\t    const void *caller)\n{\n\tunsigned int i, nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tstruct vm_struct *area;\n\tunsigned long p;\n\n\tarea = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,\n\t\t\t\t  caller);\n\tif (!area)\n\t\treturn NULL;\n\n\tarea->pages = pages;\n\tarea->nr_pages = nr_pages;\n\tp = (unsigned long)area->addr;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tphys_addr_t phys = __pfn_to_phys(page_to_pfn(pages[i]));\n\t\tif (ioremap_page_range(p, p + PAGE_SIZE, phys, prot))\n\t\t\tgoto err;\n\t\tp += PAGE_SIZE;\n\t}\n\treturn area->addr;\nerr:\n\tunmap_kernel_range((unsigned long)area->addr, size);\n\tvunmap(area->addr);\n\treturn NULL;\n}\n\n/*\n * Create a mapping in device IO address space for specified pages\n */\nstatic dma_addr_t\n__iommu_create_mapping(struct device *dev, struct page **pages, size_t size)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tdma_addr_t dma_addr, iova;\n\tint i, ret = DMA_ERROR_CODE;\n\n\tdma_addr = __alloc_iova(mapping, size);\n\tif (dma_addr == DMA_ERROR_CODE)\n\t\treturn dma_addr;\n\n\tiova = dma_addr;\n\tfor (i = 0; i < count; ) {\n\t\tunsigned int next_pfn = page_to_pfn(pages[i]) + 1;\n\t\tphys_addr_t phys = page_to_phys(pages[i]);\n\t\tunsigned int len, j;\n\n\t\tfor (j = i + 1; j < count; j++, next_pfn++)\n\t\t\tif (page_to_pfn(pages[j]) != next_pfn)\n\t\t\t\tbreak;\n\n\t\tlen = (j - i) << PAGE_SHIFT;\n\t\tret = iommu_map(mapping->domain, iova, phys, len, 0);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tiova += len;\n\t\ti = j;\n\t}\n\treturn dma_addr;\nfail:\n\tiommu_unmap(mapping->domain, dma_addr, iova-dma_addr);\n\t__free_iova(mapping, dma_addr, size);\n\treturn DMA_ERROR_CODE;\n}\n\nstatic int __iommu_remove_mapping(struct device *dev, dma_addr_t iova, size_t size)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\n\t/*\n\t * add optional in-page offset from iova to size and align\n\t * result to page size\n\t */\n\tsize = PAGE_ALIGN((iova & ~PAGE_MASK) + size);\n\tiova &= PAGE_MASK;\n\n\tiommu_unmap(mapping->domain, iova, size);\n\t__free_iova(mapping, iova, size);\n\treturn 0;\n}\n\nstatic struct page **__atomic_get_pages(void *addr)\n{\n\tstruct dma_pool *pool = &atomic_pool;\n\tstruct page **pages = pool->pages;\n\tint offs = (addr - pool->vaddr) >> PAGE_SHIFT;\n\n\treturn pages + offs;\n}\n\nstatic struct page **__iommu_get_pages(void *cpu_addr, struct dma_attrs *attrs)\n{\n\tstruct vm_struct *area;\n\n\tif (__in_atomic_pool(cpu_addr, PAGE_SIZE))\n\t\treturn __atomic_get_pages(cpu_addr);\n\n\tif (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))\n\t\treturn cpu_addr;\n\n\tarea = find_vm_area(cpu_addr);\n\tif (area && (area->flags & VM_ARM_DMA_CONSISTENT))\n\t\treturn area->pages;\n\treturn NULL;\n}\n\nstatic void *__iommu_alloc_atomic(struct device *dev, size_t size,\n\t\t\t\t  dma_addr_t *handle)\n{\n\tstruct page *page;\n\tvoid *addr;\n\n\taddr = __alloc_from_pool(size, &page);\n\tif (!addr)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, &page, size);\n\tif (*handle == DMA_ERROR_CODE)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__free_from_pool(addr, size);\n\treturn NULL;\n}\n\nstatic void __iommu_free_atomic(struct device *dev, void *cpu_addr,\n\t\t\t\tdma_addr_t handle, size_t size)\n{\n\t__iommu_remove_mapping(dev, handle, size);\n\t__free_from_pool(cpu_addr, size);\n}\n\nstatic void *arm_iommu_alloc_attrs(struct device *dev, size_t size,\n\t    dma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n\tstruct page **pages;\n\tvoid *addr = NULL;\n\n\t*handle = DMA_ERROR_CODE;\n\tsize = PAGE_ALIGN(size);\n\n\tif (gfp & GFP_ATOMIC)\n\t\treturn __iommu_alloc_atomic(dev, size, handle);\n\n\t/*\n\t * Following is a work-around (a.k.a. hack) to prevent pages\n\t * with __GFP_COMP being passed to split_page() which cannot\n\t * handle them.  The real problem is that this flag probably\n\t * should be 0 on ARM as it is not supported on this\n\t * platform; see CONFIG_HUGETLBFS.\n\t */\n\tgfp &= ~(__GFP_COMP);\n\n\tpages = __iommu_alloc_buffer(dev, size, gfp, attrs);\n\tif (!pages)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, pages, size);\n\tif (*handle == DMA_ERROR_CODE)\n\t\tgoto err_buffer;\n\n\tif (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))\n\t\treturn pages;\n\n\taddr = __iommu_alloc_remap(pages, size, gfp, prot,\n\t\t\t\t   __builtin_return_address(0));\n\tif (!addr)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__iommu_remove_mapping(dev, *handle, size);\nerr_buffer:\n\t__iommu_free_buffer(dev, pages, size, attrs);\n\treturn NULL;\n}\n\nstatic int arm_iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,\n\t\t    void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t    struct dma_attrs *attrs)\n{\n\tunsigned long uaddr = vma->vm_start;\n\tunsigned long usize = vma->vm_end - vma->vm_start;\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\n\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\tdo {\n\t\tint ret = vm_insert_page(vma, uaddr, *pages++);\n\t\tif (ret) {\n\t\t\tpr_err(\"Remapping memory failed: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t\tuaddr += PAGE_SIZE;\n\t\tusize -= PAGE_SIZE;\n\t} while (usize > 0);\n\n\treturn 0;\n}\n\n/*\n * free a page as defined by the above mapping.\n * Must not be called with IRQs disabled.\n */\nvoid arm_iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t  dma_addr_t handle, struct dma_attrs *attrs)\n{\n\tstruct page **pages;\n\tsize = PAGE_ALIGN(size);\n\n\tif (__in_atomic_pool(cpu_addr, size)) {\n\t\t__iommu_free_atomic(dev, cpu_addr, handle, size);\n\t\treturn;\n\t}\n\n\tpages = __iommu_get_pages(cpu_addr, attrs);\n\tif (!pages) {\n\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);\n\t\treturn;\n\t}\n\n\tif (!dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs)) {\n\t\tunmap_kernel_range((unsigned long)cpu_addr, size);\n\t\tvunmap(cpu_addr);\n\t}\n\n\t__iommu_remove_mapping(dev, handle, size);\n\t__iommu_free_buffer(dev, pages, size, attrs);\n}\n\nstatic int arm_iommu_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t\t\t void *cpu_addr, dma_addr_t dma_addr,\n\t\t\t\t size_t size, struct dma_attrs *attrs)\n{\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\treturn sg_alloc_table_from_pages(sgt, pages, count, 0, size,\n\t\t\t\t\t GFP_KERNEL);\n}\n\n/*\n * Map a part of the scatter-gather list into contiguous io address space\n */\nstatic int __map_sg_chunk(struct device *dev, struct scatterlist *sg,\n\t\t\t  size_t size, dma_addr_t *handle,\n\t\t\t  enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\t\t  bool is_coherent)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova, iova_base;\n\tint ret = 0;\n\tunsigned int count;\n\tstruct scatterlist *s;\n\n\tsize = PAGE_ALIGN(size);\n\t*handle = DMA_ERROR_CODE;\n\n\tiova_base = iova = __alloc_iova(mapping, size);\n\tif (iova == DMA_ERROR_CODE)\n\t\treturn -ENOMEM;\n\n\tfor (count = 0, s = sg; count < (size >> PAGE_SHIFT); s = sg_next(s)) {\n\t\tphys_addr_t phys = page_to_phys(sg_page(s));\n\t\tunsigned int len = PAGE_ALIGN(s->offset + s->length);\n\n\t\tif (!is_coherent &&\n\t\t\t!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n\n\t\tret = iommu_map(mapping->domain, iova, phys, len, 0);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tcount += len >> PAGE_SHIFT;\n\t\tiova += len;\n\t}\n\t*handle = iova_base;\n\n\treturn 0;\nfail:\n\tiommu_unmap(mapping->domain, iova_base, count * PAGE_SIZE);\n\t__free_iova(mapping, iova_base, size);\n\treturn ret;\n}\n\nstatic int __iommu_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t     enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\t     bool is_coherent)\n{\n\tstruct scatterlist *s = sg, *dma = sg, *start = sg;\n\tint i, count = 0;\n\tunsigned int offset = s->offset;\n\tunsigned int size = s->offset + s->length;\n\tunsigned int max = dma_get_max_seg_size(dev);\n\n\tfor (i = 1; i < nents; i++) {\n\t\ts = sg_next(s);\n\n\t\ts->dma_address = DMA_ERROR_CODE;\n\t\ts->dma_length = 0;\n\n\t\tif (s->offset || (size & ~PAGE_MASK) || size + s->length > max) {\n\t\t\tif (__map_sg_chunk(dev, start, size, &dma->dma_address,\n\t\t\t    dir, attrs, is_coherent) < 0)\n\t\t\t\tgoto bad_mapping;\n\n\t\t\tdma->dma_address += offset;\n\t\t\tdma->dma_length = size - offset;\n\n\t\t\tsize = offset = s->offset;\n\t\t\tstart = s;\n\t\t\tdma = sg_next(dma);\n\t\t\tcount += 1;\n\t\t}\n\t\tsize += s->length;\n\t}\n\tif (__map_sg_chunk(dev, start, size, &dma->dma_address, dir, attrs,\n\t\tis_coherent) < 0)\n\t\tgoto bad_mapping;\n\n\tdma->dma_address += offset;\n\tdma->dma_length = size - offset;\n\n\treturn count+1;\n\nbad_mapping:\n\tfor_each_sg(sg, s, count, i)\n\t\t__iommu_remove_mapping(dev, sg_dma_address(s), sg_dma_len(s));\n\treturn 0;\n}\n\n/**\n * arm_coherent_iommu_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of i/o coherent buffers described by scatterlist in streaming\n * mode for DMA. The scatter gather list elements are merged together (if\n * possible) and tagged with the appropriate dma address and length. They are\n * obtained via sg_dma_{address,length}.\n */\nint arm_coherent_iommu_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\treturn __iommu_map_sg(dev, sg, nents, dir, attrs, true);\n}\n\n/**\n * arm_iommu_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of buffers described by scatterlist in streaming mode for DMA.\n * The scatter gather list elements are merged together (if possible) and\n * tagged with the appropriate dma address and length. They are obtained via\n * sg_dma_{address,length}.\n */\nint arm_iommu_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\treturn __iommu_map_sg(dev, sg, nents, dir, attrs, false);\n}\n\nstatic void __iommu_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs,\n\t\tbool is_coherent)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tif (sg_dma_len(s))\n\t\t\t__iommu_remove_mapping(dev, sg_dma_address(s),\n\t\t\t\t\t       sg_dma_len(s));\n\t\tif (!is_coherent &&\n\t\t    !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset,\n\t\t\t\t\t      s->length, dir);\n\t}\n}\n\n/**\n * arm_coherent_iommu_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_coherent_iommu_unmap_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\t__iommu_unmap_sg(dev, sg, nents, dir, attrs, true);\n}\n\n/**\n * arm_iommu_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nvoid arm_iommu_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t\tenum dma_data_direction dir, struct dma_attrs *attrs)\n{\n\t__iommu_unmap_sg(dev, sg, nents, dir, attrs, false);\n}\n\n/**\n * arm_iommu_sync_sg_for_cpu\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_iommu_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset, s->length, dir);\n\n}\n\n/**\n * arm_iommu_sync_sg_for_device\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nvoid arm_iommu_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n}\n\n\n/**\n * arm_coherent_iommu_map_page\n * @dev: valid struct device pointer\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * Coherent IOMMU aware version of arm_dma_map_page()\n */\nstatic dma_addr_t arm_coherent_iommu_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t dma_addr;\n\tint ret, prot, len = PAGE_ALIGN(size + offset);\n\n\tdma_addr = __alloc_iova(mapping, len);\n\tif (dma_addr == DMA_ERROR_CODE)\n\t\treturn dma_addr;\n\n\tswitch (dir) {\n\tcase DMA_BIDIRECTIONAL:\n\t\tprot = IOMMU_READ | IOMMU_WRITE;\n\t\tbreak;\n\tcase DMA_TO_DEVICE:\n\t\tprot = IOMMU_READ;\n\t\tbreak;\n\tcase DMA_FROM_DEVICE:\n\t\tprot = IOMMU_WRITE;\n\t\tbreak;\n\tdefault:\n\t\tprot = 0;\n\t}\n\n\tret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len, prot);\n\tif (ret < 0)\n\t\tgoto fail;\n\n\treturn dma_addr + offset;\nfail:\n\t__free_iova(mapping, dma_addr, len);\n\treturn DMA_ERROR_CODE;\n}\n\n/**\n * arm_iommu_map_page\n * @dev: valid struct device pointer\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * IOMMU aware version of arm_dma_map_page()\n */\nstatic dma_addr_t arm_iommu_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     struct dma_attrs *attrs)\n{\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_cpu_to_dev(page, offset, size, dir);\n\n\treturn arm_coherent_iommu_map_page(dev, page, offset, size, dir, attrs);\n}\n\n/**\n * arm_coherent_iommu_unmap_page\n * @dev: valid struct device pointer\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * Coherent IOMMU aware version of arm_dma_unmap_page()\n */\nstatic void arm_coherent_iommu_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tint offset = handle & ~PAGE_MASK;\n\tint len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\n/**\n * arm_iommu_unmap_page\n * @dev: valid struct device pointer\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * IOMMU aware version of arm_dma_unmap_page()\n */\nstatic void arm_iommu_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tstruct dma_attrs *attrs)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tint offset = handle & ~PAGE_MASK;\n\tint len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tif (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))\n\t\t__dma_page_dev_to_cpu(page, offset, size, dir);\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\nstatic void arm_iommu_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (!iova)\n\t\treturn;\n\n\t__dma_page_dev_to_cpu(page, offset, size, dir);\n}\n\nstatic void arm_iommu_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = dev->archdata.mapping;\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (!iova)\n\t\treturn;\n\n\t__dma_page_cpu_to_dev(page, offset, size, dir);\n}\n\nstruct dma_map_ops iommu_ops = {\n\t.alloc\t\t= arm_iommu_alloc_attrs,\n\t.free\t\t= arm_iommu_free_attrs,\n\t.mmap\t\t= arm_iommu_mmap_attrs,\n\t.get_sgtable\t= arm_iommu_get_sgtable,\n\n\t.map_page\t\t= arm_iommu_map_page,\n\t.unmap_page\t\t= arm_iommu_unmap_page,\n\t.sync_single_for_cpu\t= arm_iommu_sync_single_for_cpu,\n\t.sync_single_for_device\t= arm_iommu_sync_single_for_device,\n\n\t.map_sg\t\t\t= arm_iommu_map_sg,\n\t.unmap_sg\t\t= arm_iommu_unmap_sg,\n\t.sync_sg_for_cpu\t= arm_iommu_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= arm_iommu_sync_sg_for_device,\n\n\t.set_dma_mask\t\t= arm_dma_set_mask,\n};\n\nstruct dma_map_ops iommu_coherent_ops = {\n\t.alloc\t\t= arm_iommu_alloc_attrs,\n\t.free\t\t= arm_iommu_free_attrs,\n\t.mmap\t\t= arm_iommu_mmap_attrs,\n\t.get_sgtable\t= arm_iommu_get_sgtable,\n\n\t.map_page\t= arm_coherent_iommu_map_page,\n\t.unmap_page\t= arm_coherent_iommu_unmap_page,\n\n\t.map_sg\t\t= arm_coherent_iommu_map_sg,\n\t.unmap_sg\t= arm_coherent_iommu_unmap_sg,\n\n\t.set_dma_mask\t= arm_dma_set_mask,\n};\n\n/**\n * arm_iommu_create_mapping\n * @bus: pointer to the bus holding the client device (for IOMMU calls)\n * @base: start address of the valid IO address space\n * @size: size of the valid IO address space\n * @order: accuracy of the IO addresses allocations\n *\n * Creates a mapping structure which holds information about used/unused\n * IO address ranges, which is required to perform memory allocation and\n * mapping with IOMMU aware functions.\n *\n * The client device need to be attached to the mapping with\n * arm_iommu_attach_device function.\n */\nstruct dma_iommu_mapping *\narm_iommu_create_mapping(struct bus_type *bus, dma_addr_t base, size_t size,\n\t\t\t int order)\n{\n\tunsigned int count = size >> (PAGE_SHIFT + order);\n\tunsigned int bitmap_size = BITS_TO_LONGS(count) * sizeof(long);\n\tstruct dma_iommu_mapping *mapping;\n\tint err = -ENOMEM;\n\n\tif (!count)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmapping = kzalloc(sizeof(struct dma_iommu_mapping), GFP_KERNEL);\n\tif (!mapping)\n\t\tgoto err;\n\n\tmapping->bitmap = kzalloc(bitmap_size, GFP_KERNEL);\n\tif (!mapping->bitmap)\n\t\tgoto err2;\n\n\tmapping->base = base;\n\tmapping->bits = BITS_PER_BYTE * bitmap_size;\n\tmapping->order = order;\n\tspin_lock_init(&mapping->lock);\n\n\tmapping->domain = iommu_domain_alloc(bus);\n\tif (!mapping->domain)\n\t\tgoto err3;\n\n\tkref_init(&mapping->kref);\n\treturn mapping;\nerr3:\n\tkfree(mapping->bitmap);\nerr2:\n\tkfree(mapping);\nerr:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_create_mapping);\n\nstatic void release_iommu_mapping(struct kref *kref)\n{\n\tstruct dma_iommu_mapping *mapping =\n\t\tcontainer_of(kref, struct dma_iommu_mapping, kref);\n\n\tiommu_domain_free(mapping->domain);\n\tkfree(mapping->bitmap);\n\tkfree(mapping);\n}\n\nvoid arm_iommu_release_mapping(struct dma_iommu_mapping *mapping)\n{\n\tif (mapping)\n\t\tkref_put(&mapping->kref, release_iommu_mapping);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_release_mapping);\n\n/**\n * arm_iommu_attach_device\n * @dev: valid struct device pointer\n * @mapping: io address space mapping structure (returned from\n *\tarm_iommu_create_mapping)\n *\n * Attaches specified io address space mapping to the provided device,\n * this replaces the dma operations (dma_map_ops pointer) with the\n * IOMMU aware version. More than one client might be attached to\n * the same io address space mapping.\n */\nint arm_iommu_attach_device(struct device *dev,\n\t\t\t    struct dma_iommu_mapping *mapping)\n{\n\tint err;\n\n\terr = iommu_attach_device(mapping->domain, dev);\n\tif (err)\n\t\treturn err;\n\n\tkref_get(&mapping->kref);\n\tdev->archdata.mapping = mapping;\n\tset_dma_ops(dev, &iommu_ops);\n\n\tpr_debug(\"Attached IOMMU controller to %s device.\\n\", dev_name(dev));\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(arm_iommu_attach_device);\n\n/**\n * arm_iommu_detach_device\n * @dev: valid struct device pointer\n *\n * Detaches the provided device from a previously attached map.\n * This voids the dma operations (dma_map_ops pointer)\n */\nvoid arm_iommu_detach_device(struct device *dev)\n{\n\tstruct dma_iommu_mapping *mapping;\n\n\tmapping = to_dma_iommu_mapping(dev);\n\tif (!mapping) {\n\t\tdev_warn(dev, \"Not attached\\n\");\n\t\treturn;\n\t}\n\n\tiommu_detach_device(mapping->domain, dev);\n\tkref_put(&mapping->kref, release_iommu_mapping);\n\tdev->archdata.mapping = NULL;\n\tset_dma_ops(dev, NULL);\n\n\tpr_debug(\"Detached IOMMU controller from %s device.\\n\", dev_name(dev));\n}\nEXPORT_SYMBOL_GPL(arm_iommu_detach_device);\n\n#endif\n", "code_before": "// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/arch/arm/mm/dma-mapping.c\n *\n *  Copyright (C) 2000-2004 Russell King\n *\n *  DMA uncached mapping support.\n */\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/genalloc.h>\n#include <linux/gfp.h>\n#include <linux/errno.h>\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/device.h>\n#include <linux/dma-direct.h>\n#include <linux/dma-map-ops.h>\n#include <linux/highmem.h>\n#include <linux/memblock.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/io.h>\n#include <linux/vmalloc.h>\n#include <linux/sizes.h>\n#include <linux/cma.h>\n\n#include <asm/page.h>\n#include <asm/highmem.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n#include <asm/mach/arch.h>\n#include <asm/dma-iommu.h>\n#include <asm/mach/map.h>\n#include <asm/system_info.h>\n#include <asm/xen/xen-ops.h>\n\n#include \"dma.h\"\n#include \"mm.h\"\n\nstruct arm_dma_alloc_args {\n\tstruct device *dev;\n\tsize_t size;\n\tgfp_t gfp;\n\tpgprot_t prot;\n\tconst void *caller;\n\tbool want_vaddr;\n\tint coherent_flag;\n};\n\nstruct arm_dma_free_args {\n\tstruct device *dev;\n\tsize_t size;\n\tvoid *cpu_addr;\n\tstruct page *page;\n\tbool want_vaddr;\n};\n\n#define NORMAL\t    0\n#define COHERENT    1\n\nstruct arm_dma_allocator {\n\tvoid *(*alloc)(struct arm_dma_alloc_args *args,\n\t\t       struct page **ret_page);\n\tvoid (*free)(struct arm_dma_free_args *args);\n};\n\nstruct arm_dma_buffer {\n\tstruct list_head list;\n\tvoid *virt;\n\tstruct arm_dma_allocator *allocator;\n};\n\nstatic LIST_HEAD(arm_dma_bufs);\nstatic DEFINE_SPINLOCK(arm_dma_bufs_lock);\n\nstatic struct arm_dma_buffer *arm_dma_buffer_find(void *virt)\n{\n\tstruct arm_dma_buffer *buf, *found = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&arm_dma_bufs_lock, flags);\n\tlist_for_each_entry(buf, &arm_dma_bufs, list) {\n\t\tif (buf->virt == virt) {\n\t\t\tlist_del(&buf->list);\n\t\t\tfound = buf;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&arm_dma_bufs_lock, flags);\n\treturn found;\n}\n\n/*\n * The DMA API is built upon the notion of \"buffer ownership\".  A buffer\n * is either exclusively owned by the CPU (and therefore may be accessed\n * by it) or exclusively owned by the DMA device.  These helper functions\n * represent the transitions between these two ownership states.\n *\n * Note, however, that on later ARMs, this notion does not work due to\n * speculative prefetches.  We model our approach on the assumption that\n * the CPU does do speculative prefetches, which means we clean caches\n * before transfers and delay cache invalidation until transfer completion.\n *\n */\n\nstatic void __dma_clear_buffer(struct page *page, size_t size, int coherent_flag)\n{\n\t/*\n\t * Ensure that the allocated pages are zeroed, and that any data\n\t * lurking in the kernel direct-mapped region is invalidated.\n\t */\n\tif (PageHighMem(page)) {\n\t\tphys_addr_t base = __pfn_to_phys(page_to_pfn(page));\n\t\tphys_addr_t end = base + size;\n\t\twhile (size > 0) {\n\t\t\tvoid *ptr = kmap_atomic(page);\n\t\t\tmemset(ptr, 0, PAGE_SIZE);\n\t\t\tif (coherent_flag != COHERENT)\n\t\t\t\tdmac_flush_range(ptr, ptr + PAGE_SIZE);\n\t\t\tkunmap_atomic(ptr);\n\t\t\tpage++;\n\t\t\tsize -= PAGE_SIZE;\n\t\t}\n\t\tif (coherent_flag != COHERENT)\n\t\t\touter_flush_range(base, end);\n\t} else {\n\t\tvoid *ptr = page_address(page);\n\t\tmemset(ptr, 0, size);\n\t\tif (coherent_flag != COHERENT) {\n\t\t\tdmac_flush_range(ptr, ptr + size);\n\t\t\touter_flush_range(__pa(ptr), __pa(ptr) + size);\n\t\t}\n\t}\n}\n\n/*\n * Allocate a DMA buffer for 'dev' of size 'size' using the\n * specified gfp mask.  Note that 'size' must be page aligned.\n */\nstatic struct page *__dma_alloc_buffer(struct device *dev, size_t size,\n\t\t\t\t       gfp_t gfp, int coherent_flag)\n{\n\tunsigned long order = get_order(size);\n\tstruct page *page, *p, *e;\n\n\tpage = alloc_pages(gfp, order);\n\tif (!page)\n\t\treturn NULL;\n\n\t/*\n\t * Now split the huge page and free the excess pages\n\t */\n\tsplit_page(page, order);\n\tfor (p = page + (size >> PAGE_SHIFT), e = page + (1 << order); p < e; p++)\n\t\t__free_page(p);\n\n\t__dma_clear_buffer(page, size, coherent_flag);\n\n\treturn page;\n}\n\n/*\n * Free a DMA buffer.  'size' must be page aligned.\n */\nstatic void __dma_free_buffer(struct page *page, size_t size)\n{\n\tstruct page *e = page + (size >> PAGE_SHIFT);\n\n\twhile (page < e) {\n\t\t__free_page(page);\n\t\tpage++;\n\t}\n}\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller, bool want_vaddr,\n\t\t\t\t     int coherent_flag, gfp_t gfp);\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller, bool want_vaddr);\n\n#define DEFAULT_DMA_COHERENT_POOL_SIZE\tSZ_256K\nstatic struct gen_pool *atomic_pool __ro_after_init;\n\nstatic size_t atomic_pool_size __initdata = DEFAULT_DMA_COHERENT_POOL_SIZE;\n\nstatic int __init early_coherent_pool(char *p)\n{\n\tatomic_pool_size = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"coherent_pool\", early_coherent_pool);\n\n/*\n * Initialise the coherent pool for atomic allocations.\n */\nstatic int __init atomic_pool_init(void)\n{\n\tpgprot_t prot = pgprot_dmacoherent(PAGE_KERNEL);\n\tgfp_t gfp = GFP_KERNEL | GFP_DMA;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tatomic_pool = gen_pool_create(PAGE_SHIFT, -1);\n\tif (!atomic_pool)\n\t\tgoto out;\n\t/*\n\t * The atomic pool is only used for non-coherent allocations\n\t * so we must pass NORMAL for coherent_flag.\n\t */\n\tif (dev_get_cma_area(NULL))\n\t\tptr = __alloc_from_contiguous(NULL, atomic_pool_size, prot,\n\t\t\t\t      &page, atomic_pool_init, true, NORMAL,\n\t\t\t\t      GFP_KERNEL);\n\telse\n\t\tptr = __alloc_remap_buffer(NULL, atomic_pool_size, gfp, prot,\n\t\t\t\t\t   &page, atomic_pool_init, true);\n\tif (ptr) {\n\t\tint ret;\n\n\t\tret = gen_pool_add_virt(atomic_pool, (unsigned long)ptr,\n\t\t\t\t\tpage_to_phys(page),\n\t\t\t\t\tatomic_pool_size, -1);\n\t\tif (ret)\n\t\t\tgoto destroy_genpool;\n\n\t\tgen_pool_set_algo(atomic_pool,\n\t\t\t\tgen_pool_first_fit_order_align,\n\t\t\t\tNULL);\n\t\tpr_info(\"DMA: preallocated %zu KiB pool for atomic coherent allocations\\n\",\n\t\t       atomic_pool_size / 1024);\n\t\treturn 0;\n\t}\n\ndestroy_genpool:\n\tgen_pool_destroy(atomic_pool);\n\tatomic_pool = NULL;\nout:\n\tpr_err(\"DMA: failed to allocate %zu KiB pool for atomic coherent allocation\\n\",\n\t       atomic_pool_size / 1024);\n\treturn -ENOMEM;\n}\n/*\n * CMA is activated by core_initcall, so we must be called after it.\n */\npostcore_initcall(atomic_pool_init);\n\n#ifdef CONFIG_CMA_AREAS\nstruct dma_contig_early_reserve {\n\tphys_addr_t base;\n\tunsigned long size;\n};\n\nstatic struct dma_contig_early_reserve dma_mmu_remap[MAX_CMA_AREAS] __initdata;\n\nstatic int dma_mmu_remap_num __initdata;\n\n#ifdef CONFIG_DMA_CMA\nvoid __init dma_contiguous_early_fixup(phys_addr_t base, unsigned long size)\n{\n\tdma_mmu_remap[dma_mmu_remap_num].base = base;\n\tdma_mmu_remap[dma_mmu_remap_num].size = size;\n\tdma_mmu_remap_num++;\n}\n#endif\n\nvoid __init dma_contiguous_remap(void)\n{\n\tint i;\n\tfor (i = 0; i < dma_mmu_remap_num; i++) {\n\t\tphys_addr_t start = dma_mmu_remap[i].base;\n\t\tphys_addr_t end = start + dma_mmu_remap[i].size;\n\t\tstruct map_desc map;\n\t\tunsigned long addr;\n\n\t\tif (end > arm_lowmem_limit)\n\t\t\tend = arm_lowmem_limit;\n\t\tif (start >= end)\n\t\t\tcontinue;\n\n\t\tmap.pfn = __phys_to_pfn(start);\n\t\tmap.virtual = __phys_to_virt(start);\n\t\tmap.length = end - start;\n\t\tmap.type = MT_MEMORY_DMA_READY;\n\n\t\t/*\n\t\t * Clear previous low-memory mapping to ensure that the\n\t\t * TLB does not see any conflicting entries, then flush\n\t\t * the TLB of the old entries before creating new mappings.\n\t\t *\n\t\t * This ensures that any speculatively loaded TLB entries\n\t\t * (even though they may be rare) can not cause any problems,\n\t\t * and ensures that this code is architecturally compliant.\n\t\t */\n\t\tfor (addr = __phys_to_virt(start); addr < __phys_to_virt(end);\n\t\t     addr += PMD_SIZE)\n\t\t\tpmd_clear(pmd_off_k(addr));\n\n\t\tflush_tlb_kernel_range(__phys_to_virt(start),\n\t\t\t\t       __phys_to_virt(end));\n\n\t\tiotable_init(&map, 1);\n\t}\n}\n#endif\n\nstatic int __dma_update_pte(pte_t *pte, unsigned long addr, void *data)\n{\n\tstruct page *page = virt_to_page((void *)addr);\n\tpgprot_t prot = *(pgprot_t *)data;\n\n\tset_pte_ext(pte, mk_pte(page, prot), 0);\n\treturn 0;\n}\n\nstatic void __dma_remap(struct page *page, size_t size, pgprot_t prot)\n{\n\tunsigned long start = (unsigned long) page_address(page);\n\tunsigned end = start + size;\n\n\tapply_to_page_range(&init_mm, start, size, __dma_update_pte, &prot);\n\tflush_tlb_kernel_range(start, end);\n}\n\nstatic void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t pgprot_t prot, struct page **ret_page,\n\t\t\t\t const void *caller, bool want_vaddr)\n{\n\tstruct page *page;\n\tvoid *ptr = NULL;\n\t/*\n\t * __alloc_remap_buffer is only called when the device is\n\t * non-coherent\n\t */\n\tpage = __dma_alloc_buffer(dev, size, gfp, NORMAL);\n\tif (!page)\n\t\treturn NULL;\n\tif (!want_vaddr)\n\t\tgoto out;\n\n\tptr = dma_common_contiguous_remap(page, size, prot, caller);\n\tif (!ptr) {\n\t\t__dma_free_buffer(page, size);\n\t\treturn NULL;\n\t}\n\n out:\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void *__alloc_from_pool(size_t size, struct page **ret_page)\n{\n\tunsigned long val;\n\tvoid *ptr = NULL;\n\n\tif (!atomic_pool) {\n\t\tWARN(1, \"coherent pool not initialised!\\n\");\n\t\treturn NULL;\n\t}\n\n\tval = gen_pool_alloc(atomic_pool, size);\n\tif (val) {\n\t\tphys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);\n\n\t\t*ret_page = phys_to_page(phys);\n\t\tptr = (void *)val;\n\t}\n\n\treturn ptr;\n}\n\nstatic bool __in_atomic_pool(void *start, size_t size)\n{\n\treturn gen_pool_has_addr(atomic_pool, (unsigned long)start, size);\n}\n\nstatic int __free_from_pool(void *start, size_t size)\n{\n\tif (!__in_atomic_pool(start, size))\n\t\treturn 0;\n\n\tgen_pool_free(atomic_pool, (unsigned long)start, size);\n\n\treturn 1;\n}\n\nstatic void *__alloc_from_contiguous(struct device *dev, size_t size,\n\t\t\t\t     pgprot_t prot, struct page **ret_page,\n\t\t\t\t     const void *caller, bool want_vaddr,\n\t\t\t\t     int coherent_flag, gfp_t gfp)\n{\n\tunsigned long order = get_order(size);\n\tsize_t count = size >> PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr = NULL;\n\n\tpage = dma_alloc_from_contiguous(dev, count, order, gfp & __GFP_NOWARN);\n\tif (!page)\n\t\treturn NULL;\n\n\t__dma_clear_buffer(page, size, coherent_flag);\n\n\tif (!want_vaddr)\n\t\tgoto out;\n\n\tif (PageHighMem(page)) {\n\t\tptr = dma_common_contiguous_remap(page, size, prot, caller);\n\t\tif (!ptr) {\n\t\t\tdma_release_from_contiguous(dev, page, count);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\t__dma_remap(page, size, prot);\n\t\tptr = page_address(page);\n\t}\n\n out:\n\t*ret_page = page;\n\treturn ptr;\n}\n\nstatic void __free_from_contiguous(struct device *dev, struct page *page,\n\t\t\t\t   void *cpu_addr, size_t size, bool want_vaddr)\n{\n\tif (want_vaddr) {\n\t\tif (PageHighMem(page))\n\t\t\tdma_common_free_remap(cpu_addr, size);\n\t\telse\n\t\t\t__dma_remap(page, size, PAGE_KERNEL);\n\t}\n\tdma_release_from_contiguous(dev, page, size >> PAGE_SHIFT);\n}\n\nstatic inline pgprot_t __get_dma_pgprot(unsigned long attrs, pgprot_t prot)\n{\n\tprot = (attrs & DMA_ATTR_WRITE_COMBINE) ?\n\t\t\tpgprot_writecombine(prot) :\n\t\t\tpgprot_dmacoherent(prot);\n\treturn prot;\n}\n\nstatic void *__alloc_simple_buffer(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t   struct page **ret_page)\n{\n\tstruct page *page;\n\t/* __alloc_simple_buffer is only called when the device is coherent */\n\tpage = __dma_alloc_buffer(dev, size, gfp, COHERENT);\n\tif (!page)\n\t\treturn NULL;\n\n\t*ret_page = page;\n\treturn page_address(page);\n}\n\nstatic void *simple_allocator_alloc(struct arm_dma_alloc_args *args,\n\t\t\t\t    struct page **ret_page)\n{\n\treturn __alloc_simple_buffer(args->dev, args->size, args->gfp,\n\t\t\t\t     ret_page);\n}\n\nstatic void simple_allocator_free(struct arm_dma_free_args *args)\n{\n\t__dma_free_buffer(args->page, args->size);\n}\n\nstatic struct arm_dma_allocator simple_allocator = {\n\t.alloc = simple_allocator_alloc,\n\t.free = simple_allocator_free,\n};\n\nstatic void *cma_allocator_alloc(struct arm_dma_alloc_args *args,\n\t\t\t\t struct page **ret_page)\n{\n\treturn __alloc_from_contiguous(args->dev, args->size, args->prot,\n\t\t\t\t       ret_page, args->caller,\n\t\t\t\t       args->want_vaddr, args->coherent_flag,\n\t\t\t\t       args->gfp);\n}\n\nstatic void cma_allocator_free(struct arm_dma_free_args *args)\n{\n\t__free_from_contiguous(args->dev, args->page, args->cpu_addr,\n\t\t\t       args->size, args->want_vaddr);\n}\n\nstatic struct arm_dma_allocator cma_allocator = {\n\t.alloc = cma_allocator_alloc,\n\t.free = cma_allocator_free,\n};\n\nstatic void *pool_allocator_alloc(struct arm_dma_alloc_args *args,\n\t\t\t\t  struct page **ret_page)\n{\n\treturn __alloc_from_pool(args->size, ret_page);\n}\n\nstatic void pool_allocator_free(struct arm_dma_free_args *args)\n{\n\t__free_from_pool(args->cpu_addr, args->size);\n}\n\nstatic struct arm_dma_allocator pool_allocator = {\n\t.alloc = pool_allocator_alloc,\n\t.free = pool_allocator_free,\n};\n\nstatic void *remap_allocator_alloc(struct arm_dma_alloc_args *args,\n\t\t\t\t   struct page **ret_page)\n{\n\treturn __alloc_remap_buffer(args->dev, args->size, args->gfp,\n\t\t\t\t    args->prot, ret_page, args->caller,\n\t\t\t\t    args->want_vaddr);\n}\n\nstatic void remap_allocator_free(struct arm_dma_free_args *args)\n{\n\tif (args->want_vaddr)\n\t\tdma_common_free_remap(args->cpu_addr, args->size);\n\n\t__dma_free_buffer(args->page, args->size);\n}\n\nstatic struct arm_dma_allocator remap_allocator = {\n\t.alloc = remap_allocator_alloc,\n\t.free = remap_allocator_free,\n};\n\nstatic void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n\t\t\t gfp_t gfp, pgprot_t prot, bool is_coherent,\n\t\t\t unsigned long attrs, const void *caller)\n{\n\tu64 mask = min_not_zero(dev->coherent_dma_mask, dev->bus_dma_limit);\n\tstruct page *page = NULL;\n\tvoid *addr;\n\tbool allowblock, cma;\n\tstruct arm_dma_buffer *buf;\n\tstruct arm_dma_alloc_args args = {\n\t\t.dev = dev,\n\t\t.size = PAGE_ALIGN(size),\n\t\t.gfp = gfp,\n\t\t.prot = prot,\n\t\t.caller = caller,\n\t\t.want_vaddr = ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) == 0),\n\t\t.coherent_flag = is_coherent ? COHERENT : NORMAL,\n\t};\n\n#ifdef CONFIG_DMA_API_DEBUG\n\tu64 limit = (mask + 1) & ~mask;\n\tif (limit && size >= limit) {\n\t\tdev_warn(dev, \"coherent allocation too big (requested %#x mask %#llx)\\n\",\n\t\t\tsize, mask);\n\t\treturn NULL;\n\t}\n#endif\n\n\tbuf = kzalloc(sizeof(*buf),\n\t\t      gfp & ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM));\n\tif (!buf)\n\t\treturn NULL;\n\n\tif (mask < 0xffffffffULL)\n\t\tgfp |= GFP_DMA;\n\n\targs.gfp = gfp;\n\n\t*handle = DMA_MAPPING_ERROR;\n\tallowblock = gfpflags_allow_blocking(gfp);\n\tcma = allowblock ? dev_get_cma_area(dev) : NULL;\n\n\tif (cma)\n\t\tbuf->allocator = &cma_allocator;\n\telse if (is_coherent)\n\t\tbuf->allocator = &simple_allocator;\n\telse if (allowblock)\n\t\tbuf->allocator = &remap_allocator;\n\telse\n\t\tbuf->allocator = &pool_allocator;\n\n\taddr = buf->allocator->alloc(&args, &page);\n\n\tif (page) {\n\t\tunsigned long flags;\n\n\t\t*handle = phys_to_dma(dev, page_to_phys(page));\n\t\tbuf->virt = args.want_vaddr ? addr : page;\n\n\t\tspin_lock_irqsave(&arm_dma_bufs_lock, flags);\n\t\tlist_add(&buf->list, &arm_dma_bufs);\n\t\tspin_unlock_irqrestore(&arm_dma_bufs_lock, flags);\n\t} else {\n\t\tkfree(buf);\n\t}\n\n\treturn args.want_vaddr ? addr : page;\n}\n\n/*\n * Free a buffer as defined by the above mapping.\n */\nstatic void __arm_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\t\t   dma_addr_t handle, unsigned long attrs,\n\t\t\t   bool is_coherent)\n{\n\tstruct page *page = phys_to_page(dma_to_phys(dev, handle));\n\tstruct arm_dma_buffer *buf;\n\tstruct arm_dma_free_args args = {\n\t\t.dev = dev,\n\t\t.size = PAGE_ALIGN(size),\n\t\t.cpu_addr = cpu_addr,\n\t\t.page = page,\n\t\t.want_vaddr = ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) == 0),\n\t};\n\n\tbuf = arm_dma_buffer_find(cpu_addr);\n\tif (WARN(!buf, \"Freeing invalid buffer %p\\n\", cpu_addr))\n\t\treturn;\n\n\tbuf->allocator->free(&args);\n\tkfree(buf);\n}\n\nstatic void dma_cache_maint_page(struct page *page, unsigned long offset,\n\tsize_t size, enum dma_data_direction dir,\n\tvoid (*op)(const void *, size_t, int))\n{\n\tunsigned long pfn;\n\tsize_t left = size;\n\n\tpfn = page_to_pfn(page) + offset / PAGE_SIZE;\n\toffset %= PAGE_SIZE;\n\n\t/*\n\t * A single sg entry may refer to multiple physically contiguous\n\t * pages.  But we still need to process highmem pages individually.\n\t * If highmem is not configured then the bulk of this loop gets\n\t * optimized out.\n\t */\n\tdo {\n\t\tsize_t len = left;\n\t\tvoid *vaddr;\n\n\t\tpage = pfn_to_page(pfn);\n\n\t\tif (PageHighMem(page)) {\n\t\t\tif (len + offset > PAGE_SIZE)\n\t\t\t\tlen = PAGE_SIZE - offset;\n\n\t\t\tif (cache_is_vipt_nonaliasing()) {\n\t\t\t\tvaddr = kmap_atomic(page);\n\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\tkunmap_atomic(vaddr);\n\t\t\t} else {\n\t\t\t\tvaddr = kmap_high_get(page);\n\t\t\t\tif (vaddr) {\n\t\t\t\t\top(vaddr + offset, len, dir);\n\t\t\t\t\tkunmap_high(page);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tvaddr = page_address(page) + offset;\n\t\t\top(vaddr, len, dir);\n\t\t}\n\t\toffset = 0;\n\t\tpfn++;\n\t\tleft -= len;\n\t} while (left);\n}\n\n/*\n * Make an area consistent for devices.\n * Note: Drivers should NOT use this function directly.\n * Use the driver DMA support - see dma-mapping.h (dma_sync_*)\n */\nstatic void __dma_page_cpu_to_dev(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tphys_addr_t paddr;\n\n\tdma_cache_maint_page(page, off, size, dir, dmac_map_area);\n\n\tpaddr = page_to_phys(page) + off;\n\tif (dir == DMA_FROM_DEVICE) {\n\t\touter_inv_range(paddr, paddr + size);\n\t} else {\n\t\touter_clean_range(paddr, paddr + size);\n\t}\n\t/* FIXME: non-speculating: flush on bidirectional mappings? */\n}\n\nstatic void __dma_page_dev_to_cpu(struct page *page, unsigned long off,\n\tsize_t size, enum dma_data_direction dir)\n{\n\tphys_addr_t paddr = page_to_phys(page) + off;\n\n\t/* FIXME: non-speculating: not required */\n\t/* in any case, don't bother invalidating if DMA to device */\n\tif (dir != DMA_TO_DEVICE) {\n\t\touter_inv_range(paddr, paddr + size);\n\n\t\tdma_cache_maint_page(page, off, size, dir, dmac_unmap_area);\n\t}\n\n\t/*\n\t * Mark the D-cache clean for these pages to avoid extra flushing.\n\t */\n\tif (dir != DMA_TO_DEVICE && size >= PAGE_SIZE) {\n\t\tstruct folio *folio = pfn_folio(paddr / PAGE_SIZE);\n\t\tsize_t offset = offset_in_folio(folio, paddr);\n\n\t\tfor (;;) {\n\t\t\tsize_t sz = folio_size(folio) - offset;\n\n\t\t\tif (size < sz)\n\t\t\t\tbreak;\n\t\t\tif (!offset)\n\t\t\t\tset_bit(PG_dcache_clean, &folio->flags);\n\t\t\toffset = 0;\n\t\t\tsize -= sz;\n\t\t\tif (!size)\n\t\t\t\tbreak;\n\t\t\tfolio = folio_next(folio);\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_ARM_DMA_USE_IOMMU\n\nstatic int __dma_info_to_prot(enum dma_data_direction dir, unsigned long attrs)\n{\n\tint prot = 0;\n\n\tif (attrs & DMA_ATTR_PRIVILEGED)\n\t\tprot |= IOMMU_PRIV;\n\n\tswitch (dir) {\n\tcase DMA_BIDIRECTIONAL:\n\t\treturn prot | IOMMU_READ | IOMMU_WRITE;\n\tcase DMA_TO_DEVICE:\n\t\treturn prot | IOMMU_READ;\n\tcase DMA_FROM_DEVICE:\n\t\treturn prot | IOMMU_WRITE;\n\tdefault:\n\t\treturn prot;\n\t}\n}\n\n/* IOMMU */\n\nstatic int extend_iommu_mapping(struct dma_iommu_mapping *mapping);\n\nstatic inline dma_addr_t __alloc_iova(struct dma_iommu_mapping *mapping,\n\t\t\t\t      size_t size)\n{\n\tunsigned int order = get_order(size);\n\tunsigned int align = 0;\n\tunsigned int count, start;\n\tsize_t mapping_size = mapping->bits << PAGE_SHIFT;\n\tunsigned long flags;\n\tdma_addr_t iova;\n\tint i;\n\n\tif (order > CONFIG_ARM_DMA_IOMMU_ALIGNMENT)\n\t\torder = CONFIG_ARM_DMA_IOMMU_ALIGNMENT;\n\n\tcount = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\talign = (1 << order) - 1;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tfor (i = 0; i < mapping->nr_bitmaps; i++) {\n\t\tstart = bitmap_find_next_zero_area(mapping->bitmaps[i],\n\t\t\t\tmapping->bits, 0, count, align);\n\n\t\tif (start > mapping->bits)\n\t\t\tcontinue;\n\n\t\tbitmap_set(mapping->bitmaps[i], start, count);\n\t\tbreak;\n\t}\n\n\t/*\n\t * No unused range found. Try to extend the existing mapping\n\t * and perform a second attempt to reserve an IO virtual\n\t * address range of size bytes.\n\t */\n\tif (i == mapping->nr_bitmaps) {\n\t\tif (extend_iommu_mapping(mapping)) {\n\t\t\tspin_unlock_irqrestore(&mapping->lock, flags);\n\t\t\treturn DMA_MAPPING_ERROR;\n\t\t}\n\n\t\tstart = bitmap_find_next_zero_area(mapping->bitmaps[i],\n\t\t\t\tmapping->bits, 0, count, align);\n\n\t\tif (start > mapping->bits) {\n\t\t\tspin_unlock_irqrestore(&mapping->lock, flags);\n\t\t\treturn DMA_MAPPING_ERROR;\n\t\t}\n\n\t\tbitmap_set(mapping->bitmaps[i], start, count);\n\t}\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n\n\tiova = mapping->base + (mapping_size * i);\n\tiova += start << PAGE_SHIFT;\n\n\treturn iova;\n}\n\nstatic inline void __free_iova(struct dma_iommu_mapping *mapping,\n\t\t\t       dma_addr_t addr, size_t size)\n{\n\tunsigned int start, count;\n\tsize_t mapping_size = mapping->bits << PAGE_SHIFT;\n\tunsigned long flags;\n\tdma_addr_t bitmap_base;\n\tu32 bitmap_index;\n\n\tif (!size)\n\t\treturn;\n\n\tbitmap_index = (u32) (addr - mapping->base) / (u32) mapping_size;\n\tBUG_ON(addr < mapping->base || bitmap_index > mapping->extensions);\n\n\tbitmap_base = mapping->base + mapping_size * bitmap_index;\n\n\tstart = (addr - bitmap_base) >>\tPAGE_SHIFT;\n\n\tif (addr + size > bitmap_base + mapping_size) {\n\t\t/*\n\t\t * The address range to be freed reaches into the iova\n\t\t * range of the next bitmap. This should not happen as\n\t\t * we don't allow this in __alloc_iova (at the\n\t\t * moment).\n\t\t */\n\t\tBUG();\n\t} else\n\t\tcount = size >> PAGE_SHIFT;\n\n\tspin_lock_irqsave(&mapping->lock, flags);\n\tbitmap_clear(mapping->bitmaps[bitmap_index], start, count);\n\tspin_unlock_irqrestore(&mapping->lock, flags);\n}\n\n/* We'll try 2M, 1M, 64K, and finally 4K; array must end with 0! */\nstatic const int iommu_order_array[] = { 9, 8, 4, 0 };\n\nstatic struct page **__iommu_alloc_buffer(struct device *dev, size_t size,\n\t\t\t\t\t  gfp_t gfp, unsigned long attrs,\n\t\t\t\t\t  int coherent_flag)\n{\n\tstruct page **pages;\n\tint count = size >> PAGE_SHIFT;\n\tint array_size = count * sizeof(struct page *);\n\tint i = 0;\n\tint order_idx = 0;\n\n\tpages = kvzalloc(array_size, GFP_KERNEL);\n\tif (!pages)\n\t\treturn NULL;\n\n\tif (attrs & DMA_ATTR_FORCE_CONTIGUOUS)\n\t{\n\t\tunsigned long order = get_order(size);\n\t\tstruct page *page;\n\n\t\tpage = dma_alloc_from_contiguous(dev, count, order,\n\t\t\t\t\t\t gfp & __GFP_NOWARN);\n\t\tif (!page)\n\t\t\tgoto error;\n\n\t\t__dma_clear_buffer(page, size, coherent_flag);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tpages[i] = page + i;\n\n\t\treturn pages;\n\t}\n\n\t/* Go straight to 4K chunks if caller says it's OK. */\n\tif (attrs & DMA_ATTR_ALLOC_SINGLE_PAGES)\n\t\torder_idx = ARRAY_SIZE(iommu_order_array) - 1;\n\n\t/*\n\t * IOMMU can map any pages, so himem can also be used here\n\t */\n\tgfp |= __GFP_NOWARN | __GFP_HIGHMEM;\n\n\twhile (count) {\n\t\tint j, order;\n\n\t\torder = iommu_order_array[order_idx];\n\n\t\t/* Drop down when we get small */\n\t\tif (__fls(count) < order) {\n\t\t\torder_idx++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (order) {\n\t\t\t/* See if it's easy to allocate a high-order chunk */\n\t\t\tpages[i] = alloc_pages(gfp | __GFP_NORETRY, order);\n\n\t\t\t/* Go down a notch at first sign of pressure */\n\t\t\tif (!pages[i]) {\n\t\t\t\torder_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else {\n\t\t\tpages[i] = alloc_pages(gfp, 0);\n\t\t\tif (!pages[i])\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tif (order) {\n\t\t\tsplit_page(pages[i], order);\n\t\t\tj = 1 << order;\n\t\t\twhile (--j)\n\t\t\t\tpages[i + j] = pages[i] + j;\n\t\t}\n\n\t\t__dma_clear_buffer(pages[i], PAGE_SIZE << order, coherent_flag);\n\t\ti += 1 << order;\n\t\tcount -= 1 << order;\n\t}\n\n\treturn pages;\nerror:\n\twhile (i--)\n\t\tif (pages[i])\n\t\t\t__free_pages(pages[i], 0);\n\tkvfree(pages);\n\treturn NULL;\n}\n\nstatic int __iommu_free_buffer(struct device *dev, struct page **pages,\n\t\t\t       size_t size, unsigned long attrs)\n{\n\tint count = size >> PAGE_SHIFT;\n\tint i;\n\n\tif (attrs & DMA_ATTR_FORCE_CONTIGUOUS) {\n\t\tdma_release_from_contiguous(dev, pages[0], count);\n\t} else {\n\t\tfor (i = 0; i < count; i++)\n\t\t\tif (pages[i])\n\t\t\t\t__free_pages(pages[i], 0);\n\t}\n\n\tkvfree(pages);\n\treturn 0;\n}\n\n/*\n * Create a mapping in device IO address space for specified pages\n */\nstatic dma_addr_t\n__iommu_create_mapping(struct device *dev, struct page **pages, size_t size,\n\t\t       unsigned long attrs)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tdma_addr_t dma_addr, iova;\n\tint i;\n\n\tdma_addr = __alloc_iova(mapping, size);\n\tif (dma_addr == DMA_MAPPING_ERROR)\n\t\treturn dma_addr;\n\n\tiova = dma_addr;\n\tfor (i = 0; i < count; ) {\n\t\tint ret;\n\n\t\tunsigned int next_pfn = page_to_pfn(pages[i]) + 1;\n\t\tphys_addr_t phys = page_to_phys(pages[i]);\n\t\tunsigned int len, j;\n\n\t\tfor (j = i + 1; j < count; j++, next_pfn++)\n\t\t\tif (page_to_pfn(pages[j]) != next_pfn)\n\t\t\t\tbreak;\n\n\t\tlen = (j - i) << PAGE_SHIFT;\n\t\tret = iommu_map(mapping->domain, iova, phys, len,\n\t\t\t\t__dma_info_to_prot(DMA_BIDIRECTIONAL, attrs),\n\t\t\t\tGFP_KERNEL);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tiova += len;\n\t\ti = j;\n\t}\n\treturn dma_addr;\nfail:\n\tiommu_unmap(mapping->domain, dma_addr, iova-dma_addr);\n\t__free_iova(mapping, dma_addr, size);\n\treturn DMA_MAPPING_ERROR;\n}\n\nstatic int __iommu_remove_mapping(struct device *dev, dma_addr_t iova, size_t size)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\n\t/*\n\t * add optional in-page offset from iova to size and align\n\t * result to page size\n\t */\n\tsize = PAGE_ALIGN((iova & ~PAGE_MASK) + size);\n\tiova &= PAGE_MASK;\n\n\tiommu_unmap(mapping->domain, iova, size);\n\t__free_iova(mapping, iova, size);\n\treturn 0;\n}\n\nstatic struct page **__atomic_get_pages(void *addr)\n{\n\tstruct page *page;\n\tphys_addr_t phys;\n\n\tphys = gen_pool_virt_to_phys(atomic_pool, (unsigned long)addr);\n\tpage = phys_to_page(phys);\n\n\treturn (struct page **)page;\n}\n\nstatic struct page **__iommu_get_pages(void *cpu_addr, unsigned long attrs)\n{\n\tif (__in_atomic_pool(cpu_addr, PAGE_SIZE))\n\t\treturn __atomic_get_pages(cpu_addr);\n\n\tif (attrs & DMA_ATTR_NO_KERNEL_MAPPING)\n\t\treturn cpu_addr;\n\n\treturn dma_common_find_pages(cpu_addr);\n}\n\nstatic void *__iommu_alloc_simple(struct device *dev, size_t size, gfp_t gfp,\n\t\t\t\t  dma_addr_t *handle, int coherent_flag,\n\t\t\t\t  unsigned long attrs)\n{\n\tstruct page *page;\n\tvoid *addr;\n\n\tif (coherent_flag  == COHERENT)\n\t\taddr = __alloc_simple_buffer(dev, size, gfp, &page);\n\telse\n\t\taddr = __alloc_from_pool(size, &page);\n\tif (!addr)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, &page, size, attrs);\n\tif (*handle == DMA_MAPPING_ERROR)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__free_from_pool(addr, size);\n\treturn NULL;\n}\n\nstatic void __iommu_free_atomic(struct device *dev, void *cpu_addr,\n\t\t\tdma_addr_t handle, size_t size, int coherent_flag)\n{\n\t__iommu_remove_mapping(dev, handle, size);\n\tif (coherent_flag == COHERENT)\n\t\t__dma_free_buffer(virt_to_page(cpu_addr), size);\n\telse\n\t\t__free_from_pool(cpu_addr, size);\n}\n\nstatic void *arm_iommu_alloc_attrs(struct device *dev, size_t size,\n\t    dma_addr_t *handle, gfp_t gfp, unsigned long attrs)\n{\n\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n\tstruct page **pages;\n\tvoid *addr = NULL;\n\tint coherent_flag = dev->dma_coherent ? COHERENT : NORMAL;\n\n\t*handle = DMA_MAPPING_ERROR;\n\tsize = PAGE_ALIGN(size);\n\n\tif (coherent_flag  == COHERENT || !gfpflags_allow_blocking(gfp))\n\t\treturn __iommu_alloc_simple(dev, size, gfp, handle,\n\t\t\t\t\t    coherent_flag, attrs);\n\n\tpages = __iommu_alloc_buffer(dev, size, gfp, attrs, coherent_flag);\n\tif (!pages)\n\t\treturn NULL;\n\n\t*handle = __iommu_create_mapping(dev, pages, size, attrs);\n\tif (*handle == DMA_MAPPING_ERROR)\n\t\tgoto err_buffer;\n\n\tif (attrs & DMA_ATTR_NO_KERNEL_MAPPING)\n\t\treturn pages;\n\n\taddr = dma_common_pages_remap(pages, size, prot,\n\t\t\t\t   __builtin_return_address(0));\n\tif (!addr)\n\t\tgoto err_mapping;\n\n\treturn addr;\n\nerr_mapping:\n\t__iommu_remove_mapping(dev, *handle, size);\nerr_buffer:\n\t__iommu_free_buffer(dev, pages, size, attrs);\n\treturn NULL;\n}\n\nstatic int arm_iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,\n\t\t    void *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\t    unsigned long attrs)\n{\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\tunsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tint err;\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\tif (vma->vm_pgoff >= nr_pages)\n\t\treturn -ENXIO;\n\n\tif (!dev->dma_coherent)\n\t\tvma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);\n\n\terr = vm_map_pages(vma, pages, nr_pages);\n\tif (err)\n\t\tpr_err(\"Remapping memory failed: %d\\n\", err);\n\n\treturn err;\n}\n\n/*\n * free a page as defined by the above mapping.\n * Must not be called with IRQs disabled.\n */\nstatic void arm_iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,\n\tdma_addr_t handle, unsigned long attrs)\n{\n\tint coherent_flag = dev->dma_coherent ? COHERENT : NORMAL;\n\tstruct page **pages;\n\tsize = PAGE_ALIGN(size);\n\n\tif (coherent_flag == COHERENT || __in_atomic_pool(cpu_addr, size)) {\n\t\t__iommu_free_atomic(dev, cpu_addr, handle, size, coherent_flag);\n\t\treturn;\n\t}\n\n\tpages = __iommu_get_pages(cpu_addr, attrs);\n\tif (!pages) {\n\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);\n\t\treturn;\n\t}\n\n\tif ((attrs & DMA_ATTR_NO_KERNEL_MAPPING) == 0)\n\t\tdma_common_free_remap(cpu_addr, size);\n\n\t__iommu_remove_mapping(dev, handle, size);\n\t__iommu_free_buffer(dev, pages, size, attrs);\n}\n\nstatic int arm_iommu_get_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t\t\t void *cpu_addr, dma_addr_t dma_addr,\n\t\t\t\t size_t size, unsigned long attrs)\n{\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\tstruct page **pages = __iommu_get_pages(cpu_addr, attrs);\n\n\tif (!pages)\n\t\treturn -ENXIO;\n\n\treturn sg_alloc_table_from_pages(sgt, pages, count, 0, size,\n\t\t\t\t\t GFP_KERNEL);\n}\n\n/*\n * Map a part of the scatter-gather list into contiguous io address space\n */\nstatic int __map_sg_chunk(struct device *dev, struct scatterlist *sg,\n\t\t\t  size_t size, dma_addr_t *handle,\n\t\t\t  enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tdma_addr_t iova, iova_base;\n\tint ret = 0;\n\tunsigned int count;\n\tstruct scatterlist *s;\n\tint prot;\n\n\tsize = PAGE_ALIGN(size);\n\t*handle = DMA_MAPPING_ERROR;\n\n\tiova_base = iova = __alloc_iova(mapping, size);\n\tif (iova == DMA_MAPPING_ERROR)\n\t\treturn -ENOMEM;\n\n\tfor (count = 0, s = sg; count < (size >> PAGE_SHIFT); s = sg_next(s)) {\n\t\tphys_addr_t phys = page_to_phys(sg_page(s));\n\t\tunsigned int len = PAGE_ALIGN(s->offset + s->length);\n\n\t\tif (!dev->dma_coherent && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))\n\t\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n\n\t\tprot = __dma_info_to_prot(dir, attrs);\n\n\t\tret = iommu_map(mapping->domain, iova, phys, len, prot,\n\t\t\t\tGFP_KERNEL);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\t\tcount += len >> PAGE_SHIFT;\n\t\tiova += len;\n\t}\n\t*handle = iova_base;\n\n\treturn 0;\nfail:\n\tiommu_unmap(mapping->domain, iova_base, count * PAGE_SIZE);\n\t__free_iova(mapping, iova_base, size);\n\treturn ret;\n}\n\n/**\n * arm_iommu_map_sg - map a set of SG buffers for streaming mode DMA\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map\n * @dir: DMA transfer direction\n *\n * Map a set of buffers described by scatterlist in streaming mode for DMA.\n * The scatter gather list elements are merged together (if possible) and\n * tagged with the appropriate dma address and length. They are obtained via\n * sg_dma_{address,length}.\n */\nstatic int arm_iommu_map_sg(struct device *dev, struct scatterlist *sg,\n\t\tint nents, enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct scatterlist *s = sg, *dma = sg, *start = sg;\n\tint i, count = 0, ret;\n\tunsigned int offset = s->offset;\n\tunsigned int size = s->offset + s->length;\n\tunsigned int max = dma_get_max_seg_size(dev);\n\n\tfor (i = 1; i < nents; i++) {\n\t\ts = sg_next(s);\n\n\t\ts->dma_length = 0;\n\n\t\tif (s->offset || (size & ~PAGE_MASK) || size + s->length > max) {\n\t\t\tret = __map_sg_chunk(dev, start, size,\n\t\t\t\t\t     &dma->dma_address, dir, attrs);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto bad_mapping;\n\n\t\t\tdma->dma_address += offset;\n\t\t\tdma->dma_length = size - offset;\n\n\t\t\tsize = offset = s->offset;\n\t\t\tstart = s;\n\t\t\tdma = sg_next(dma);\n\t\t\tcount += 1;\n\t\t}\n\t\tsize += s->length;\n\t}\n\tret = __map_sg_chunk(dev, start, size, &dma->dma_address, dir, attrs);\n\tif (ret < 0)\n\t\tgoto bad_mapping;\n\n\tdma->dma_address += offset;\n\tdma->dma_length = size - offset;\n\n\treturn count+1;\n\nbad_mapping:\n\tfor_each_sg(sg, s, count, i)\n\t\t__iommu_remove_mapping(dev, sg_dma_address(s), sg_dma_len(s));\n\tif (ret == -ENOMEM)\n\t\treturn ret;\n\treturn -EINVAL;\n}\n\n/**\n * arm_iommu_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to unmap (same as was passed to dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n *\n * Unmap a set of streaming mode DMA translations.  Again, CPU access\n * rules concerning calls here are the same as for dma_unmap_single().\n */\nstatic void arm_iommu_unmap_sg(struct device *dev,\n\t\t\t       struct scatterlist *sg, int nents,\n\t\t\t       enum dma_data_direction dir,\n\t\t\t       unsigned long attrs)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tif (sg_dma_len(s))\n\t\t\t__iommu_remove_mapping(dev, sg_dma_address(s),\n\t\t\t\t\t       sg_dma_len(s));\n\t\tif (!dev->dma_coherent && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))\n\t\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset,\n\t\t\t\t\t      s->length, dir);\n\t}\n}\n\n/**\n * arm_iommu_sync_sg_for_cpu\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nstatic void arm_iommu_sync_sg_for_cpu(struct device *dev,\n\t\t\tstruct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tif (dev->dma_coherent)\n\t\treturn;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_dev_to_cpu(sg_page(s), s->offset, s->length, dir);\n\n}\n\n/**\n * arm_iommu_sync_sg_for_device\n * @dev: valid struct device pointer\n * @sg: list of buffers\n * @nents: number of buffers to map (returned from dma_map_sg)\n * @dir: DMA transfer direction (same as was passed to dma_map_sg)\n */\nstatic void arm_iommu_sync_sg_for_device(struct device *dev,\n\t\t\tstruct scatterlist *sg,\n\t\t\tint nents, enum dma_data_direction dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tif (dev->dma_coherent)\n\t\treturn;\n\n\tfor_each_sg(sg, s, nents, i)\n\t\t__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);\n}\n\n/**\n * arm_iommu_map_page\n * @dev: valid struct device pointer\n * @page: page that buffer resides in\n * @offset: offset into page for start of buffer\n * @size: size of buffer to map\n * @dir: DMA transfer direction\n *\n * IOMMU aware version of arm_dma_map_page()\n */\nstatic dma_addr_t arm_iommu_map_page(struct device *dev, struct page *page,\n\t     unsigned long offset, size_t size, enum dma_data_direction dir,\n\t     unsigned long attrs)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tdma_addr_t dma_addr;\n\tint ret, prot, len = PAGE_ALIGN(size + offset);\n\n\tif (!dev->dma_coherent && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))\n\t\t__dma_page_cpu_to_dev(page, offset, size, dir);\n\n\tdma_addr = __alloc_iova(mapping, len);\n\tif (dma_addr == DMA_MAPPING_ERROR)\n\t\treturn dma_addr;\n\n\tprot = __dma_info_to_prot(dir, attrs);\n\n\tret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len,\n\t\t\tprot, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto fail;\n\n\treturn dma_addr + offset;\nfail:\n\t__free_iova(mapping, dma_addr, len);\n\treturn DMA_MAPPING_ERROR;\n}\n\n/**\n * arm_iommu_unmap_page\n * @dev: valid struct device pointer\n * @handle: DMA address of buffer\n * @size: size of buffer (same as passed to dma_map_page)\n * @dir: DMA transfer direction (same as passed to dma_map_page)\n *\n * IOMMU aware version of arm_dma_unmap_page()\n */\nstatic void arm_iommu_unmap_page(struct device *dev, dma_addr_t handle,\n\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page;\n\tint offset = handle & ~PAGE_MASK;\n\tint len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tif (!dev->dma_coherent && !(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {\n\t\tpage = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\t\t__dma_page_dev_to_cpu(page, offset, size, dir);\n\t}\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\n/**\n * arm_iommu_map_resource - map a device resource for DMA\n * @dev: valid struct device pointer\n * @phys_addr: physical address of resource\n * @size: size of resource to map\n * @dir: DMA transfer direction\n */\nstatic dma_addr_t arm_iommu_map_resource(struct device *dev,\n\t\tphys_addr_t phys_addr, size_t size,\n\t\tenum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tdma_addr_t dma_addr;\n\tint ret, prot;\n\tphys_addr_t addr = phys_addr & PAGE_MASK;\n\tunsigned int offset = phys_addr & ~PAGE_MASK;\n\tsize_t len = PAGE_ALIGN(size + offset);\n\n\tdma_addr = __alloc_iova(mapping, len);\n\tif (dma_addr == DMA_MAPPING_ERROR)\n\t\treturn dma_addr;\n\n\tprot = __dma_info_to_prot(dir, attrs) | IOMMU_MMIO;\n\n\tret = iommu_map(mapping->domain, dma_addr, addr, len, prot, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto fail;\n\n\treturn dma_addr + offset;\nfail:\n\t__free_iova(mapping, dma_addr, len);\n\treturn DMA_MAPPING_ERROR;\n}\n\n/**\n * arm_iommu_unmap_resource - unmap a device DMA resource\n * @dev: valid struct device pointer\n * @dma_handle: DMA address to resource\n * @size: size of resource to map\n * @dir: DMA transfer direction\n */\nstatic void arm_iommu_unmap_resource(struct device *dev, dma_addr_t dma_handle,\n\t\tsize_t size, enum dma_data_direction dir,\n\t\tunsigned long attrs)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tdma_addr_t iova = dma_handle & PAGE_MASK;\n\tunsigned int offset = dma_handle & ~PAGE_MASK;\n\tsize_t len = PAGE_ALIGN(size + offset);\n\n\tif (!iova)\n\t\treturn;\n\n\tiommu_unmap(mapping->domain, iova, len);\n\t__free_iova(mapping, iova, len);\n}\n\nstatic void arm_iommu_sync_single_for_cpu(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page;\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (dev->dma_coherent || !iova)\n\t\treturn;\n\n\tpage = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\t__dma_page_dev_to_cpu(page, offset, size, dir);\n}\n\nstatic void arm_iommu_sync_single_for_device(struct device *dev,\n\t\tdma_addr_t handle, size_t size, enum dma_data_direction dir)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\tdma_addr_t iova = handle & PAGE_MASK;\n\tstruct page *page;\n\tunsigned int offset = handle & ~PAGE_MASK;\n\n\tif (dev->dma_coherent || !iova)\n\t\treturn;\n\n\tpage = phys_to_page(iommu_iova_to_phys(mapping->domain, iova));\n\t__dma_page_cpu_to_dev(page, offset, size, dir);\n}\n\nstatic const struct dma_map_ops iommu_ops = {\n\t.alloc\t\t= arm_iommu_alloc_attrs,\n\t.free\t\t= arm_iommu_free_attrs,\n\t.mmap\t\t= arm_iommu_mmap_attrs,\n\t.get_sgtable\t= arm_iommu_get_sgtable,\n\n\t.map_page\t\t= arm_iommu_map_page,\n\t.unmap_page\t\t= arm_iommu_unmap_page,\n\t.sync_single_for_cpu\t= arm_iommu_sync_single_for_cpu,\n\t.sync_single_for_device\t= arm_iommu_sync_single_for_device,\n\n\t.map_sg\t\t\t= arm_iommu_map_sg,\n\t.unmap_sg\t\t= arm_iommu_unmap_sg,\n\t.sync_sg_for_cpu\t= arm_iommu_sync_sg_for_cpu,\n\t.sync_sg_for_device\t= arm_iommu_sync_sg_for_device,\n\n\t.map_resource\t\t= arm_iommu_map_resource,\n\t.unmap_resource\t\t= arm_iommu_unmap_resource,\n};\n\n/**\n * arm_iommu_create_mapping\n * @dev: pointer to the client device (for IOMMU calls)\n * @base: start address of the valid IO address space\n * @size: maximum size of the valid IO address space\n *\n * Creates a mapping structure which holds information about used/unused\n * IO address ranges, which is required to perform memory allocation and\n * mapping with IOMMU aware functions.\n *\n * The client device need to be attached to the mapping with\n * arm_iommu_attach_device function.\n */\nstruct dma_iommu_mapping *\narm_iommu_create_mapping(struct device *dev, dma_addr_t base, u64 size)\n{\n\tunsigned int bits = size >> PAGE_SHIFT;\n\tunsigned int bitmap_size = BITS_TO_LONGS(bits) * sizeof(long);\n\tstruct dma_iommu_mapping *mapping;\n\tint extensions = 1;\n\tint err = -ENOMEM;\n\n\t/* currently only 32-bit DMA address space is supported */\n\tif (size > DMA_BIT_MASK(32) + 1)\n\t\treturn ERR_PTR(-ERANGE);\n\n\tif (!bitmap_size)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (bitmap_size > PAGE_SIZE) {\n\t\textensions = bitmap_size / PAGE_SIZE;\n\t\tbitmap_size = PAGE_SIZE;\n\t}\n\n\tmapping = kzalloc(sizeof(struct dma_iommu_mapping), GFP_KERNEL);\n\tif (!mapping)\n\t\tgoto err;\n\n\tmapping->bitmap_size = bitmap_size;\n\tmapping->bitmaps = kcalloc(extensions, sizeof(unsigned long *),\n\t\t\t\t   GFP_KERNEL);\n\tif (!mapping->bitmaps)\n\t\tgoto err2;\n\n\tmapping->bitmaps[0] = kzalloc(bitmap_size, GFP_KERNEL);\n\tif (!mapping->bitmaps[0])\n\t\tgoto err3;\n\n\tmapping->nr_bitmaps = 1;\n\tmapping->extensions = extensions;\n\tmapping->base = base;\n\tmapping->bits = BITS_PER_BYTE * bitmap_size;\n\n\tspin_lock_init(&mapping->lock);\n\n\tmapping->domain = iommu_paging_domain_alloc(dev);\n\tif (IS_ERR(mapping->domain)) {\n\t\terr = PTR_ERR(mapping->domain);\n\t\tgoto err4;\n\t}\n\n\tkref_init(&mapping->kref);\n\treturn mapping;\nerr4:\n\tkfree(mapping->bitmaps[0]);\nerr3:\n\tkfree(mapping->bitmaps);\nerr2:\n\tkfree(mapping);\nerr:\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_create_mapping);\n\nstatic void release_iommu_mapping(struct kref *kref)\n{\n\tint i;\n\tstruct dma_iommu_mapping *mapping =\n\t\tcontainer_of(kref, struct dma_iommu_mapping, kref);\n\n\tiommu_domain_free(mapping->domain);\n\tfor (i = 0; i < mapping->nr_bitmaps; i++)\n\t\tkfree(mapping->bitmaps[i]);\n\tkfree(mapping->bitmaps);\n\tkfree(mapping);\n}\n\nstatic int extend_iommu_mapping(struct dma_iommu_mapping *mapping)\n{\n\tint next_bitmap;\n\n\tif (mapping->nr_bitmaps >= mapping->extensions)\n\t\treturn -EINVAL;\n\n\tnext_bitmap = mapping->nr_bitmaps;\n\tmapping->bitmaps[next_bitmap] = kzalloc(mapping->bitmap_size,\n\t\t\t\t\t\tGFP_ATOMIC);\n\tif (!mapping->bitmaps[next_bitmap])\n\t\treturn -ENOMEM;\n\n\tmapping->nr_bitmaps++;\n\n\treturn 0;\n}\n\nvoid arm_iommu_release_mapping(struct dma_iommu_mapping *mapping)\n{\n\tif (mapping)\n\t\tkref_put(&mapping->kref, release_iommu_mapping);\n}\nEXPORT_SYMBOL_GPL(arm_iommu_release_mapping);\n\nstatic int __arm_iommu_attach_device(struct device *dev,\n\t\t\t\t     struct dma_iommu_mapping *mapping)\n{\n\tint err;\n\n\terr = iommu_attach_device(mapping->domain, dev);\n\tif (err)\n\t\treturn err;\n\n\tkref_get(&mapping->kref);\n\tto_dma_iommu_mapping(dev) = mapping;\n\n\tpr_debug(\"Attached IOMMU controller to %s device.\\n\", dev_name(dev));\n\treturn 0;\n}\n\n/**\n * arm_iommu_attach_device\n * @dev: valid struct device pointer\n * @mapping: io address space mapping structure (returned from\n *\tarm_iommu_create_mapping)\n *\n * Attaches specified io address space mapping to the provided device.\n * This replaces the dma operations (dma_map_ops pointer) with the\n * IOMMU aware version.\n *\n * More than one client might be attached to the same io address space\n * mapping.\n */\nint arm_iommu_attach_device(struct device *dev,\n\t\t\t    struct dma_iommu_mapping *mapping)\n{\n\tint err;\n\n\terr = __arm_iommu_attach_device(dev, mapping);\n\tif (err)\n\t\treturn err;\n\n\tset_dma_ops(dev, &iommu_ops);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(arm_iommu_attach_device);\n\n/**\n * arm_iommu_detach_device\n * @dev: valid struct device pointer\n *\n * Detaches the provided device from a previously attached map.\n * This overwrites the dma_ops pointer with appropriate non-IOMMU ops.\n */\nvoid arm_iommu_detach_device(struct device *dev)\n{\n\tstruct dma_iommu_mapping *mapping;\n\n\tmapping = to_dma_iommu_mapping(dev);\n\tif (!mapping) {\n\t\tdev_warn(dev, \"Not attached\\n\");\n\t\treturn;\n\t}\n\n\tiommu_detach_device(mapping->domain, dev);\n\tkref_put(&mapping->kref, release_iommu_mapping);\n\tto_dma_iommu_mapping(dev) = NULL;\n\tset_dma_ops(dev, NULL);\n\n\tpr_debug(\"Detached IOMMU controller from %s device.\\n\", dev_name(dev));\n}\nEXPORT_SYMBOL_GPL(arm_iommu_detach_device);\n\nstatic void arm_setup_iommu_dma_ops(struct device *dev)\n{\n\tstruct dma_iommu_mapping *mapping;\n\tu64 dma_base = 0, size = 1ULL << 32;\n\n\tif (dev->dma_range_map) {\n\t\tdma_base = dma_range_map_min(dev->dma_range_map);\n\t\tsize = dma_range_map_max(dev->dma_range_map) - dma_base;\n\t}\n\tmapping = arm_iommu_create_mapping(dev, dma_base, size);\n\tif (IS_ERR(mapping)) {\n\t\tpr_warn(\"Failed to create %llu-byte IOMMU mapping for device %s\\n\",\n\t\t\t\tsize, dev_name(dev));\n\t\treturn;\n\t}\n\n\tif (__arm_iommu_attach_device(dev, mapping)) {\n\t\tpr_warn(\"Failed to attached device %s to IOMMU_mapping\\n\",\n\t\t\t\tdev_name(dev));\n\t\tarm_iommu_release_mapping(mapping);\n\t\treturn;\n\t}\n\n\tset_dma_ops(dev, &iommu_ops);\n}\n\nstatic void arm_teardown_iommu_dma_ops(struct device *dev)\n{\n\tstruct dma_iommu_mapping *mapping = to_dma_iommu_mapping(dev);\n\n\tif (!mapping)\n\t\treturn;\n\n\tarm_iommu_detach_device(dev);\n\tarm_iommu_release_mapping(mapping);\n}\n\n#else\n\nstatic void arm_setup_iommu_dma_ops(struct device *dev)\n{\n}\n\nstatic void arm_teardown_iommu_dma_ops(struct device *dev) { }\n\n#endif\t/* CONFIG_ARM_DMA_USE_IOMMU */\n\nvoid arch_setup_dma_ops(struct device *dev, bool coherent)\n{\n\t/*\n\t * Due to legacy code that sets the ->dma_coherent flag from a bus\n\t * notifier we can't just assign coherent to the ->dma_coherent flag\n\t * here, but instead have to make sure we only set but never clear it\n\t * for now.\n\t */\n\tif (coherent)\n\t\tdev->dma_coherent = true;\n\n\t/*\n\t * Don't override the dma_ops if they have already been set. Ideally\n\t * this should be the only location where dma_ops are set, remove this\n\t * check when all other callers of set_dma_ops will have disappeared.\n\t */\n\tif (dev->dma_ops)\n\t\treturn;\n\n\tif (device_iommu_mapped(dev))\n\t\tarm_setup_iommu_dma_ops(dev);\n\n\txen_setup_dma_ops(dev);\n\tdev->archdata.dma_ops_setup = true;\n}\n\nvoid arch_teardown_dma_ops(struct device *dev)\n{\n\tif (!dev->archdata.dma_ops_setup)\n\t\treturn;\n\n\tarm_teardown_iommu_dma_ops(dev);\n\t/* Let arch_setup_dma_ops() start again from scratch upon re-probe */\n\tset_dma_ops(dev, NULL);\n}\n\nvoid arch_sync_dma_for_device(phys_addr_t paddr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\t__dma_page_cpu_to_dev(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),\n\t\t\t      size, dir);\n}\n\nvoid arch_sync_dma_for_cpu(phys_addr_t paddr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\t__dma_page_dev_to_cpu(phys_to_page(paddr), paddr & (PAGE_SIZE - 1),\n\t\t\t      size, dir);\n}\n\nvoid *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,\n\t\tgfp_t gfp, unsigned long attrs)\n{\n\treturn __dma_alloc(dev, size, dma_handle, gfp,\n\t\t\t   __get_dma_pgprot(attrs, PAGE_KERNEL), false,\n\t\t\t   attrs, __builtin_return_address(0));\n}\n\nvoid arch_dma_free(struct device *dev, size_t size, void *cpu_addr,\n\t\tdma_addr_t dma_handle, unsigned long attrs)\n{\n\t__arm_dma_free(dev, size, cpu_addr, dma_handle, attrs, false);\n}\n", "patch": "@@ -687,7 +687,7 @@ static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n \t\t    gfp_t gfp, struct dma_attrs *attrs)\n {\n-\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n+\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n \tvoid *memory;\n \n \tif (dma_alloc_from_coherent(dev, size, handle, &memory))\n@@ -700,7 +700,7 @@ void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,\n static void *arm_coherent_dma_alloc(struct device *dev, size_t size,\n \tdma_addr_t *handle, gfp_t gfp, struct dma_attrs *attrs)\n {\n-\tpgprot_t prot = __get_dma_pgprot(attrs, pgprot_kernel);\n+\tpgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL);\n \tvoid *memory;\n \n \tif (dma_alloc_from_coherent(dev, size, handle, &memory))", "file_path": "files/2016_8\\108", "file_language": "c", "file_name": "arch/arm/mm/dma-mapping.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
{"index": 47, "cve_id": "CVE-2014-9895", "cwe_id": ["CWE-200"], "cve_language": "C", "cve_description": "drivers/media/media-device.c in the Linux kernel before 3.11, as used in Android before 2016-08-05 on Nexus 5 and 7 (2013) devices, does not properly initialize certain data structures, which allows local users to obtain sensitive information via a crafted application, aka Android internal bug 28750150 and Qualcomm internal bug CR570757, a different vulnerability than CVE-2014-1739.", "cvss": "5.5", "publish_date": "August 6, 2016", "AV": "LOCAL", "AC": "LOW", "PR": "NONE", "UI": "REQUIRED", "S": "UNCHANGED", "C": "HIGH", "I": "NONE", "A": "NONE", "commit_id": "c88e739b1fad662240e99ecbd0bdaac871717987", "commit_message": "[media] media: info leak in __media_device_enum_links()\n\nThese structs have holes and reserved struct members which aren't\ncleared.  I've added a memset() so we don't leak stack information.\n\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>\nSigned-off-by: Mauro Carvalho Chehab <mchehab@redhat.com>", "commit_date": "2013-06-12T21:52:45Z", "project": "torvalds/linux", "url": "https://api.github.com/repos/torvalds/linux/commits/c88e739b1fad662240e99ecbd0bdaac871717987", "html_url": "https://github.com/torvalds/linux/commit/c88e739b1fad662240e99ecbd0bdaac871717987", "windows_before": "", "windows_after": "", "parents": [{"commit_id_before": "fe905b8cc992edd3d1f729f4a88c4701e7c3b774", "url_before": "https://api.github.com/repos/torvalds/linux/commits/fe905b8cc992edd3d1f729f4a88c4701e7c3b774", "html_url_before": "https://github.com/torvalds/linux/commit/fe905b8cc992edd3d1f729f4a88c4701e7c3b774"}], "details": [{"raw_url": "https://github.com/torvalds/linux/raw/c88e739b1fad662240e99ecbd0bdaac871717987/drivers/media/media-device.c", "code": "/*\n * Media device\n *\n * Copyright (C) 2010 Nokia Corporation\n *\n * Contacts: Laurent Pinchart <laurent.pinchart@ideasonboard.com>\n *\t     Sakari Ailus <sakari.ailus@iki.fi>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n */\n\n#include <linux/compat.h>\n#include <linux/export.h>\n#include <linux/ioctl.h>\n#include <linux/media.h>\n#include <linux/types.h>\n\n#include <media/media-device.h>\n#include <media/media-devnode.h>\n#include <media/media-entity.h>\n\n/* -----------------------------------------------------------------------------\n * Userspace API\n */\n\nstatic int media_device_open(struct file *filp)\n{\n\treturn 0;\n}\n\nstatic int media_device_close(struct file *filp)\n{\n\treturn 0;\n}\n\nstatic int media_device_get_info(struct media_device *dev,\n\t\t\t\t struct media_device_info __user *__info)\n{\n\tstruct media_device_info info;\n\n\tmemset(&info, 0, sizeof(info));\n\n\tstrlcpy(info.driver, dev->dev->driver->name, sizeof(info.driver));\n\tstrlcpy(info.model, dev->model, sizeof(info.model));\n\tstrlcpy(info.serial, dev->serial, sizeof(info.serial));\n\tstrlcpy(info.bus_info, dev->bus_info, sizeof(info.bus_info));\n\n\tinfo.media_version = MEDIA_API_VERSION;\n\tinfo.hw_revision = dev->hw_revision;\n\tinfo.driver_version = dev->driver_version;\n\n\tif (copy_to_user(__info, &info, sizeof(*__info)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic struct media_entity *find_entity(struct media_device *mdev, u32 id)\n{\n\tstruct media_entity *entity;\n\tint next = id & MEDIA_ENT_ID_FLAG_NEXT;\n\n\tid &= ~MEDIA_ENT_ID_FLAG_NEXT;\n\n\tspin_lock(&mdev->lock);\n\n\tmedia_device_for_each_entity(entity, mdev) {\n\t\tif ((entity->id == id && !next) ||\n\t\t    (entity->id > id && next)) {\n\t\t\tspin_unlock(&mdev->lock);\n\t\t\treturn entity;\n\t\t}\n\t}\n\n\tspin_unlock(&mdev->lock);\n\n\treturn NULL;\n}\n\nstatic long media_device_enum_entities(struct media_device *mdev,\n\t\t\t\t       struct media_entity_desc __user *uent)\n{\n\tstruct media_entity *ent;\n\tstruct media_entity_desc u_ent;\n\n\tif (copy_from_user(&u_ent.id, &uent->id, sizeof(u_ent.id)))\n\t\treturn -EFAULT;\n\n\tent = find_entity(mdev, u_ent.id);\n\n\tif (ent == NULL)\n\t\treturn -EINVAL;\n\n\tu_ent.id = ent->id;\n\tif (ent->name) {\n\t\tstrncpy(u_ent.name, ent->name, sizeof(u_ent.name));\n\t\tu_ent.name[sizeof(u_ent.name) - 1] = '\\0';\n\t} else {\n\t\tmemset(u_ent.name, 0, sizeof(u_ent.name));\n\t}\n\tu_ent.type = ent->type;\n\tu_ent.revision = ent->revision;\n\tu_ent.flags = ent->flags;\n\tu_ent.group_id = ent->group_id;\n\tu_ent.pads = ent->num_pads;\n\tu_ent.links = ent->num_links - ent->num_backlinks;\n\tmemcpy(&u_ent.raw, &ent->info, sizeof(ent->info));\n\tif (copy_to_user(uent, &u_ent, sizeof(u_ent)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void media_device_kpad_to_upad(const struct media_pad *kpad,\n\t\t\t\t      struct media_pad_desc *upad)\n{\n\tupad->entity = kpad->entity->id;\n\tupad->index = kpad->index;\n\tupad->flags = kpad->flags;\n}\n\nstatic long __media_device_enum_links(struct media_device *mdev,\n\t\t\t\t      struct media_links_enum *links)\n{\n\tstruct media_entity *entity;\n\n\tentity = find_entity(mdev, links->entity);\n\tif (entity == NULL)\n\t\treturn -EINVAL;\n\n\tif (links->pads) {\n\t\tunsigned int p;\n\n\t\tfor (p = 0; p < entity->num_pads; p++) {\n\t\t\tstruct media_pad_desc pad;\n\n\t\t\tmemset(&pad, 0, sizeof(pad));\n\t\t\tmedia_device_kpad_to_upad(&entity->pads[p], &pad);\n\t\t\tif (copy_to_user(&links->pads[p], &pad, sizeof(pad)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (links->links) {\n\t\tstruct media_link_desc __user *ulink;\n\t\tunsigned int l;\n\n\t\tfor (l = 0, ulink = links->links; l < entity->num_links; l++) {\n\t\t\tstruct media_link_desc link;\n\n\t\t\t/* Ignore backlinks. */\n\t\t\tif (entity->links[l].source->entity != entity)\n\t\t\t\tcontinue;\n\n\t\t\tmemset(&link, 0, sizeof(link));\n\t\t\tmedia_device_kpad_to_upad(entity->links[l].source,\n\t\t\t\t\t\t  &link.source);\n\t\t\tmedia_device_kpad_to_upad(entity->links[l].sink,\n\t\t\t\t\t\t  &link.sink);\n\t\t\tlink.flags = entity->links[l].flags;\n\t\t\tif (copy_to_user(ulink, &link, sizeof(*ulink)))\n\t\t\t\treturn -EFAULT;\n\t\t\tulink++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic long media_device_enum_links(struct media_device *mdev,\n\t\t\t\t    struct media_links_enum __user *ulinks)\n{\n\tstruct media_links_enum links;\n\tint rval;\n\n\tif (copy_from_user(&links, ulinks, sizeof(links)))\n\t\treturn -EFAULT;\n\n\trval = __media_device_enum_links(mdev, &links);\n\tif (rval < 0)\n\t\treturn rval;\n\n\tif (copy_to_user(ulinks, &links, sizeof(*ulinks)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long media_device_setup_link(struct media_device *mdev,\n\t\t\t\t    struct media_link_desc __user *_ulink)\n{\n\tstruct media_link *link = NULL;\n\tstruct media_link_desc ulink;\n\tstruct media_entity *source;\n\tstruct media_entity *sink;\n\tint ret;\n\n\tif (copy_from_user(&ulink, _ulink, sizeof(ulink)))\n\t\treturn -EFAULT;\n\n\t/* Find the source and sink entities and link.\n\t */\n\tsource = find_entity(mdev, ulink.source.entity);\n\tsink = find_entity(mdev, ulink.sink.entity);\n\n\tif (source == NULL || sink == NULL)\n\t\treturn -EINVAL;\n\n\tif (ulink.source.index >= source->num_pads ||\n\t    ulink.sink.index >= sink->num_pads)\n\t\treturn -EINVAL;\n\n\tlink = media_entity_find_link(&source->pads[ulink.source.index],\n\t\t\t\t      &sink->pads[ulink.sink.index]);\n\tif (link == NULL)\n\t\treturn -EINVAL;\n\n\t/* Setup the link on both entities. */\n\tret = __media_entity_setup_link(link, ulink.flags);\n\n\tif (copy_to_user(_ulink, &ulink, sizeof(ulink)))\n\t\treturn -EFAULT;\n\n\treturn ret;\n}\n\nstatic long media_device_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t       unsigned long arg)\n{\n\tstruct media_devnode *devnode = media_devnode_data(filp);\n\tstruct media_device *dev = to_media_device(devnode);\n\tlong ret;\n\n\tswitch (cmd) {\n\tcase MEDIA_IOC_DEVICE_INFO:\n\t\tret = media_device_get_info(dev,\n\t\t\t\t(struct media_device_info __user *)arg);\n\t\tbreak;\n\n\tcase MEDIA_IOC_ENUM_ENTITIES:\n\t\tret = media_device_enum_entities(dev,\n\t\t\t\t(struct media_entity_desc __user *)arg);\n\t\tbreak;\n\n\tcase MEDIA_IOC_ENUM_LINKS:\n\t\tmutex_lock(&dev->graph_mutex);\n\t\tret = media_device_enum_links(dev,\n\t\t\t\t(struct media_links_enum __user *)arg);\n\t\tmutex_unlock(&dev->graph_mutex);\n\t\tbreak;\n\n\tcase MEDIA_IOC_SETUP_LINK:\n\t\tmutex_lock(&dev->graph_mutex);\n\t\tret = media_device_setup_link(dev,\n\t\t\t\t(struct media_link_desc __user *)arg);\n\t\tmutex_unlock(&dev->graph_mutex);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\n\nstruct media_links_enum32 {\n\t__u32 entity;\n\tcompat_uptr_t pads; /* struct media_pad_desc * */\n\tcompat_uptr_t links; /* struct media_link_desc * */\n\t__u32 reserved[4];\n};\n\nstatic long media_device_enum_links32(struct media_device *mdev,\n\t\t\t\t      struct media_links_enum32 __user *ulinks)\n{\n\tstruct media_links_enum links;\n\tcompat_uptr_t pads_ptr, links_ptr;\n\n\tmemset(&links, 0, sizeof(links));\n\n\tif (get_user(links.entity, &ulinks->entity)\n\t    || get_user(pads_ptr, &ulinks->pads)\n\t    || get_user(links_ptr, &ulinks->links))\n\t\treturn -EFAULT;\n\n\tlinks.pads = compat_ptr(pads_ptr);\n\tlinks.links = compat_ptr(links_ptr);\n\n\treturn __media_device_enum_links(mdev, &links);\n}\n\n#define MEDIA_IOC_ENUM_LINKS32\t\t_IOWR('|', 0x02, struct media_links_enum32)\n\nstatic long media_device_compat_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t\t      unsigned long arg)\n{\n\tstruct media_devnode *devnode = media_devnode_data(filp);\n\tstruct media_device *dev = to_media_device(devnode);\n\tlong ret;\n\n\tswitch (cmd) {\n\tcase MEDIA_IOC_DEVICE_INFO:\n\tcase MEDIA_IOC_ENUM_ENTITIES:\n\tcase MEDIA_IOC_SETUP_LINK:\n\t\treturn media_device_ioctl(filp, cmd, arg);\n\n\tcase MEDIA_IOC_ENUM_LINKS32:\n\t\tmutex_lock(&dev->graph_mutex);\n\t\tret = media_device_enum_links32(dev,\n\t\t\t\t(struct media_links_enum32 __user *)arg);\n\t\tmutex_unlock(&dev->graph_mutex);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_COMPAT */\n\nstatic const struct media_file_operations media_device_fops = {\n\t.owner = THIS_MODULE,\n\t.open = media_device_open,\n\t.ioctl = media_device_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = media_device_compat_ioctl,\n#endif /* CONFIG_COMPAT */\n\t.release = media_device_close,\n};\n\n/* -----------------------------------------------------------------------------\n * sysfs\n */\n\nstatic ssize_t show_model(struct device *cd,\n\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct media_device *mdev = to_media_device(to_media_devnode(cd));\n\n\treturn sprintf(buf, \"%.*s\\n\", (int)sizeof(mdev->model), mdev->model);\n}\n\nstatic DEVICE_ATTR(model, S_IRUGO, show_model, NULL);\n\n/* -----------------------------------------------------------------------------\n * Registration/unregistration\n */\n\nstatic void media_device_release(struct media_devnode *mdev)\n{\n}\n\n/**\n * media_device_register - register a media device\n * @mdev:\tThe media device\n *\n * The caller is responsible for initializing the media device before\n * registration. The following fields must be set:\n *\n * - dev must point to the parent device\n * - model must be filled with the device model name\n */\nint __must_check media_device_register(struct media_device *mdev)\n{\n\tint ret;\n\n\tif (WARN_ON(mdev->dev == NULL || mdev->model[0] == 0))\n\t\treturn -EINVAL;\n\n\tmdev->entity_id = 1;\n\tINIT_LIST_HEAD(&mdev->entities);\n\tspin_lock_init(&mdev->lock);\n\tmutex_init(&mdev->graph_mutex);\n\n\t/* Register the device node. */\n\tmdev->devnode.fops = &media_device_fops;\n\tmdev->devnode.parent = mdev->dev;\n\tmdev->devnode.release = media_device_release;\n\tret = media_devnode_register(&mdev->devnode);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = device_create_file(&mdev->devnode.dev, &dev_attr_model);\n\tif (ret < 0) {\n\t\tmedia_devnode_unregister(&mdev->devnode);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(media_device_register);\n\n/**\n * media_device_unregister - unregister a media device\n * @mdev:\tThe media device\n *\n */\nvoid media_device_unregister(struct media_device *mdev)\n{\n\tstruct media_entity *entity;\n\tstruct media_entity *next;\n\n\tlist_for_each_entry_safe(entity, next, &mdev->entities, list)\n\t\tmedia_device_unregister_entity(entity);\n\n\tdevice_remove_file(&mdev->devnode.dev, &dev_attr_model);\n\tmedia_devnode_unregister(&mdev->devnode);\n}\nEXPORT_SYMBOL_GPL(media_device_unregister);\n\n/**\n * media_device_register_entity - Register an entity with a media device\n * @mdev:\tThe media device\n * @entity:\tThe entity\n */\nint __must_check media_device_register_entity(struct media_device *mdev,\n\t\t\t\t\t      struct media_entity *entity)\n{\n\t/* Warn if we apparently re-register an entity */\n\tWARN_ON(entity->parent != NULL);\n\tentity->parent = mdev;\n\n\tspin_lock(&mdev->lock);\n\tif (entity->id == 0)\n\t\tentity->id = mdev->entity_id++;\n\telse\n\t\tmdev->entity_id = max(entity->id + 1, mdev->entity_id);\n\tlist_add_tail(&entity->list, &mdev->entities);\n\tspin_unlock(&mdev->lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(media_device_register_entity);\n\n/**\n * media_device_unregister_entity - Unregister an entity\n * @entity:\tThe entity\n *\n * If the entity has never been registered this function will return\n * immediately.\n */\nvoid media_device_unregister_entity(struct media_entity *entity)\n{\n\tstruct media_device *mdev = entity->parent;\n\n\tif (mdev == NULL)\n\t\treturn;\n\n\tspin_lock(&mdev->lock);\n\tlist_del(&entity->list);\n\tspin_unlock(&mdev->lock);\n\tentity->parent = NULL;\n}\nEXPORT_SYMBOL_GPL(media_device_unregister_entity);\n", "code_before": "/*\n * Media device\n *\n * Copyright (C) 2010 Nokia Corporation\n *\n * Contacts: Laurent Pinchart <laurent.pinchart@ideasonboard.com>\n *\t     Sakari Ailus <sakari.ailus@iki.fi>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n */\n\n#include <linux/compat.h>\n#include <linux/export.h>\n#include <linux/idr.h>\n#include <linux/ioctl.h>\n#include <linux/media.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/pci.h>\n#include <linux/usb.h>\n#include <linux/version.h>\n\n#include <media/media-device.h>\n#include <media/media-devnode.h>\n#include <media/media-entity.h>\n#include <media/media-request.h>\n\n#ifdef CONFIG_MEDIA_CONTROLLER\n\n/*\n * Legacy defines from linux/media.h. This is the only place we need this\n * so we just define it here. The media.h header doesn't expose it to the\n * kernel to prevent it from being used by drivers, but here (and only here!)\n * we need it to handle the legacy behavior.\n */\n#define MEDIA_ENT_SUBTYPE_MASK\t\t\t0x0000ffff\n#define MEDIA_ENT_T_DEVNODE_UNKNOWN\t\t(MEDIA_ENT_F_OLD_BASE | \\\n\t\t\t\t\t\t MEDIA_ENT_SUBTYPE_MASK)\n\n/* -----------------------------------------------------------------------------\n * Userspace API\n */\n\nstatic inline void __user *media_get_uptr(__u64 arg)\n{\n\treturn (void __user *)(uintptr_t)arg;\n}\n\nstatic int media_device_open(struct file *filp)\n{\n\treturn 0;\n}\n\nstatic int media_device_close(struct file *filp)\n{\n\treturn 0;\n}\n\nstatic long media_device_get_info(struct media_device *dev, void *arg)\n{\n\tstruct media_device_info *info = arg;\n\n\tmemset(info, 0, sizeof(*info));\n\n\tif (dev->driver_name[0])\n\t\tstrscpy(info->driver, dev->driver_name, sizeof(info->driver));\n\telse\n\t\tstrscpy(info->driver, dev->dev->driver->name,\n\t\t\tsizeof(info->driver));\n\n\tstrscpy(info->model, dev->model, sizeof(info->model));\n\tstrscpy(info->serial, dev->serial, sizeof(info->serial));\n\tstrscpy(info->bus_info, dev->bus_info, sizeof(info->bus_info));\n\n\tinfo->media_version = LINUX_VERSION_CODE;\n\tinfo->driver_version = info->media_version;\n\tinfo->hw_revision = dev->hw_revision;\n\n\treturn 0;\n}\n\nstatic struct media_entity *find_entity(struct media_device *mdev, u32 id)\n{\n\tstruct media_entity *entity;\n\tint next = id & MEDIA_ENT_ID_FLAG_NEXT;\n\n\tid &= ~MEDIA_ENT_ID_FLAG_NEXT;\n\n\tmedia_device_for_each_entity(entity, mdev) {\n\t\tif (((media_entity_id(entity) == id) && !next) ||\n\t\t    ((media_entity_id(entity) > id) && next)) {\n\t\t\treturn entity;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic long media_device_enum_entities(struct media_device *mdev, void *arg)\n{\n\tstruct media_entity_desc *entd = arg;\n\tstruct media_entity *ent;\n\n\tent = find_entity(mdev, entd->id);\n\tif (ent == NULL)\n\t\treturn -EINVAL;\n\n\tmemset(entd, 0, sizeof(*entd));\n\n\tentd->id = media_entity_id(ent);\n\tif (ent->name)\n\t\tstrscpy(entd->name, ent->name, sizeof(entd->name));\n\tentd->type = ent->function;\n\tentd->revision = 0;\t\t/* Unused */\n\tentd->flags = ent->flags;\n\tentd->group_id = 0;\t\t/* Unused */\n\tentd->pads = ent->num_pads;\n\tentd->links = ent->num_links - ent->num_backlinks;\n\n\t/*\n\t * Workaround for a bug at media-ctl <= v1.10 that makes it to\n\t * do the wrong thing if the entity function doesn't belong to\n\t * either MEDIA_ENT_F_OLD_BASE or MEDIA_ENT_F_OLD_SUBDEV_BASE\n\t * Ranges.\n\t *\n\t * Non-subdevices are expected to be at the MEDIA_ENT_F_OLD_BASE,\n\t * or, otherwise, will be silently ignored by media-ctl when\n\t * printing the graphviz diagram. So, map them into the devnode\n\t * old range.\n\t */\n\tif (ent->function < MEDIA_ENT_F_OLD_BASE ||\n\t    ent->function > MEDIA_ENT_F_TUNER) {\n\t\tif (is_media_entity_v4l2_subdev(ent))\n\t\t\tentd->type = MEDIA_ENT_F_V4L2_SUBDEV_UNKNOWN;\n\t\telse if (ent->function != MEDIA_ENT_F_IO_V4L)\n\t\t\tentd->type = MEDIA_ENT_T_DEVNODE_UNKNOWN;\n\t}\n\n\tmemcpy(&entd->raw, &ent->info, sizeof(ent->info));\n\n\treturn 0;\n}\n\nstatic void media_device_kpad_to_upad(const struct media_pad *kpad,\n\t\t\t\t      struct media_pad_desc *upad)\n{\n\tupad->entity = media_entity_id(kpad->entity);\n\tupad->index = kpad->index;\n\tupad->flags = kpad->flags;\n}\n\nstatic long media_device_enum_links(struct media_device *mdev, void *arg)\n{\n\tstruct media_links_enum *links = arg;\n\tstruct media_entity *entity;\n\n\tentity = find_entity(mdev, links->entity);\n\tif (entity == NULL)\n\t\treturn -EINVAL;\n\n\tif (links->pads) {\n\t\tunsigned int p;\n\n\t\tfor (p = 0; p < entity->num_pads; p++) {\n\t\t\tstruct media_pad_desc pad;\n\n\t\t\tmemset(&pad, 0, sizeof(pad));\n\t\t\tmedia_device_kpad_to_upad(&entity->pads[p], &pad);\n\t\t\tif (copy_to_user(&links->pads[p], &pad, sizeof(pad)))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (links->links) {\n\t\tstruct media_link *link;\n\t\tstruct media_link_desc __user *ulink_desc = links->links;\n\n\t\tlist_for_each_entry(link, &entity->links, list) {\n\t\t\tstruct media_link_desc klink_desc;\n\n\t\t\t/* Ignore backlinks. */\n\t\t\tif (link->source->entity != entity)\n\t\t\t\tcontinue;\n\t\t\tmemset(&klink_desc, 0, sizeof(klink_desc));\n\t\t\tmedia_device_kpad_to_upad(link->source,\n\t\t\t\t\t\t  &klink_desc.source);\n\t\t\tmedia_device_kpad_to_upad(link->sink,\n\t\t\t\t\t\t  &klink_desc.sink);\n\t\t\tklink_desc.flags = link->flags;\n\t\t\tif (copy_to_user(ulink_desc, &klink_desc,\n\t\t\t\t\t sizeof(*ulink_desc)))\n\t\t\t\treturn -EFAULT;\n\t\t\tulink_desc++;\n\t\t}\n\t}\n\tmemset(links->reserved, 0, sizeof(links->reserved));\n\n\treturn 0;\n}\n\nstatic long media_device_setup_link(struct media_device *mdev, void *arg)\n{\n\tstruct media_link_desc *linkd = arg;\n\tstruct media_link *link = NULL;\n\tstruct media_entity *source;\n\tstruct media_entity *sink;\n\n\t/* Find the source and sink entities and link.\n\t */\n\tsource = find_entity(mdev, linkd->source.entity);\n\tsink = find_entity(mdev, linkd->sink.entity);\n\n\tif (source == NULL || sink == NULL)\n\t\treturn -EINVAL;\n\n\tif (linkd->source.index >= source->num_pads ||\n\t    linkd->sink.index >= sink->num_pads)\n\t\treturn -EINVAL;\n\n\tlink = media_entity_find_link(&source->pads[linkd->source.index],\n\t\t\t\t      &sink->pads[linkd->sink.index]);\n\tif (link == NULL)\n\t\treturn -EINVAL;\n\n\tmemset(linkd->reserved, 0, sizeof(linkd->reserved));\n\n\t/* Setup the link on both entities. */\n\treturn __media_entity_setup_link(link, linkd->flags);\n}\n\nstatic long media_device_get_topology(struct media_device *mdev, void *arg)\n{\n\tstruct media_v2_topology *topo = arg;\n\tstruct media_entity *entity;\n\tstruct media_interface *intf;\n\tstruct media_pad *pad;\n\tstruct media_link *link;\n\tstruct media_v2_entity kentity, __user *uentity;\n\tstruct media_v2_interface kintf, __user *uintf;\n\tstruct media_v2_pad kpad, __user *upad;\n\tstruct media_v2_link klink, __user *ulink;\n\tunsigned int i;\n\tint ret = 0;\n\n\ttopo->topology_version = mdev->topology_version;\n\n\t/* Get entities and number of entities */\n\ti = 0;\n\tuentity = media_get_uptr(topo->ptr_entities);\n\tmedia_device_for_each_entity(entity, mdev) {\n\t\ti++;\n\t\tif (ret || !uentity)\n\t\t\tcontinue;\n\n\t\tif (i > topo->num_entities) {\n\t\t\tret = -ENOSPC;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Copy fields to userspace struct if not error */\n\t\tmemset(&kentity, 0, sizeof(kentity));\n\t\tkentity.id = entity->graph_obj.id;\n\t\tkentity.function = entity->function;\n\t\tkentity.flags = entity->flags;\n\t\tstrscpy(kentity.name, entity->name,\n\t\t\tsizeof(kentity.name));\n\n\t\tif (copy_to_user(uentity, &kentity, sizeof(kentity)))\n\t\t\tret = -EFAULT;\n\t\tuentity++;\n\t}\n\ttopo->num_entities = i;\n\ttopo->reserved1 = 0;\n\n\t/* Get interfaces and number of interfaces */\n\ti = 0;\n\tuintf = media_get_uptr(topo->ptr_interfaces);\n\tmedia_device_for_each_intf(intf, mdev) {\n\t\ti++;\n\t\tif (ret || !uintf)\n\t\t\tcontinue;\n\n\t\tif (i > topo->num_interfaces) {\n\t\t\tret = -ENOSPC;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmemset(&kintf, 0, sizeof(kintf));\n\n\t\t/* Copy intf fields to userspace struct */\n\t\tkintf.id = intf->graph_obj.id;\n\t\tkintf.intf_type = intf->type;\n\t\tkintf.flags = intf->flags;\n\n\t\tif (media_type(&intf->graph_obj) == MEDIA_GRAPH_INTF_DEVNODE) {\n\t\t\tstruct media_intf_devnode *devnode;\n\n\t\t\tdevnode = intf_to_devnode(intf);\n\n\t\t\tkintf.devnode.major = devnode->major;\n\t\t\tkintf.devnode.minor = devnode->minor;\n\t\t}\n\n\t\tif (copy_to_user(uintf, &kintf, sizeof(kintf)))\n\t\t\tret = -EFAULT;\n\t\tuintf++;\n\t}\n\ttopo->num_interfaces = i;\n\ttopo->reserved2 = 0;\n\n\t/* Get pads and number of pads */\n\ti = 0;\n\tupad = media_get_uptr(topo->ptr_pads);\n\tmedia_device_for_each_pad(pad, mdev) {\n\t\ti++;\n\t\tif (ret || !upad)\n\t\t\tcontinue;\n\n\t\tif (i > topo->num_pads) {\n\t\t\tret = -ENOSPC;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmemset(&kpad, 0, sizeof(kpad));\n\n\t\t/* Copy pad fields to userspace struct */\n\t\tkpad.id = pad->graph_obj.id;\n\t\tkpad.entity_id = pad->entity->graph_obj.id;\n\t\tkpad.flags = pad->flags;\n\t\tkpad.index = pad->index;\n\n\t\tif (copy_to_user(upad, &kpad, sizeof(kpad)))\n\t\t\tret = -EFAULT;\n\t\tupad++;\n\t}\n\ttopo->num_pads = i;\n\ttopo->reserved3 = 0;\n\n\t/* Get links and number of links */\n\ti = 0;\n\tulink = media_get_uptr(topo->ptr_links);\n\tmedia_device_for_each_link(link, mdev) {\n\t\tif (link->is_backlink)\n\t\t\tcontinue;\n\n\t\ti++;\n\n\t\tif (ret || !ulink)\n\t\t\tcontinue;\n\n\t\tif (i > topo->num_links) {\n\t\t\tret = -ENOSPC;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmemset(&klink, 0, sizeof(klink));\n\n\t\t/* Copy link fields to userspace struct */\n\t\tklink.id = link->graph_obj.id;\n\t\tklink.source_id = link->gobj0->id;\n\t\tklink.sink_id = link->gobj1->id;\n\t\tklink.flags = link->flags;\n\n\t\tif (copy_to_user(ulink, &klink, sizeof(klink)))\n\t\t\tret = -EFAULT;\n\t\tulink++;\n\t}\n\ttopo->num_links = i;\n\ttopo->reserved4 = 0;\n\n\treturn ret;\n}\n\nstatic long media_device_request_alloc(struct media_device *mdev,\n\t\t\t\t       int *alloc_fd)\n{\n#ifdef CONFIG_MEDIA_CONTROLLER_REQUEST_API\n\tif (!mdev->ops || !mdev->ops->req_validate || !mdev->ops->req_queue)\n\t\treturn -ENOTTY;\n\n\treturn media_request_alloc(mdev, alloc_fd);\n#else\n\treturn -ENOTTY;\n#endif\n}\n\nstatic long copy_arg_from_user(void *karg, void __user *uarg, unsigned int cmd)\n{\n\tif ((_IOC_DIR(cmd) & _IOC_WRITE) &&\n\t    copy_from_user(karg, uarg, _IOC_SIZE(cmd)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long copy_arg_to_user(void __user *uarg, void *karg, unsigned int cmd)\n{\n\tif ((_IOC_DIR(cmd) & _IOC_READ) &&\n\t    copy_to_user(uarg, karg, _IOC_SIZE(cmd)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n/* Do acquire the graph mutex */\n#define MEDIA_IOC_FL_GRAPH_MUTEX\tBIT(0)\n\n#define MEDIA_IOC_ARG(__cmd, func, fl, from_user, to_user)\t\t\\\n\t[_IOC_NR(MEDIA_IOC_##__cmd)] = {\t\t\t\t\\\n\t\t.cmd = MEDIA_IOC_##__cmd,\t\t\t\t\\\n\t\t.fn = (long (*)(struct media_device *, void *))func,\t\\\n\t\t.flags = fl,\t\t\t\t\t\t\\\n\t\t.arg_from_user = from_user,\t\t\t\t\\\n\t\t.arg_to_user = to_user,\t\t\t\t\t\\\n\t}\n\n#define MEDIA_IOC(__cmd, func, fl)\t\t\t\t\t\\\n\tMEDIA_IOC_ARG(__cmd, func, fl, copy_arg_from_user, copy_arg_to_user)\n\n/* the table is indexed by _IOC_NR(cmd) */\nstruct media_ioctl_info {\n\tunsigned int cmd;\n\tunsigned short flags;\n\tlong (*fn)(struct media_device *dev, void *arg);\n\tlong (*arg_from_user)(void *karg, void __user *uarg, unsigned int cmd);\n\tlong (*arg_to_user)(void __user *uarg, void *karg, unsigned int cmd);\n};\n\nstatic const struct media_ioctl_info ioctl_info[] = {\n\tMEDIA_IOC(DEVICE_INFO, media_device_get_info, MEDIA_IOC_FL_GRAPH_MUTEX),\n\tMEDIA_IOC(ENUM_ENTITIES, media_device_enum_entities, MEDIA_IOC_FL_GRAPH_MUTEX),\n\tMEDIA_IOC(ENUM_LINKS, media_device_enum_links, MEDIA_IOC_FL_GRAPH_MUTEX),\n\tMEDIA_IOC(SETUP_LINK, media_device_setup_link, MEDIA_IOC_FL_GRAPH_MUTEX),\n\tMEDIA_IOC(G_TOPOLOGY, media_device_get_topology, MEDIA_IOC_FL_GRAPH_MUTEX),\n\tMEDIA_IOC(REQUEST_ALLOC, media_device_request_alloc, 0),\n};\n\nstatic long media_device_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t       unsigned long __arg)\n{\n\tstruct media_devnode *devnode = media_devnode_data(filp);\n\tstruct media_device *dev = devnode->media_dev;\n\tconst struct media_ioctl_info *info;\n\tvoid __user *arg = (void __user *)__arg;\n\tchar __karg[256], *karg = __karg;\n\tlong ret;\n\n\tif (_IOC_NR(cmd) >= ARRAY_SIZE(ioctl_info)\n\t    || ioctl_info[_IOC_NR(cmd)].cmd != cmd)\n\t\treturn -ENOIOCTLCMD;\n\n\tinfo = &ioctl_info[_IOC_NR(cmd)];\n\n\tif (_IOC_SIZE(info->cmd) > sizeof(__karg)) {\n\t\tkarg = kmalloc(_IOC_SIZE(info->cmd), GFP_KERNEL);\n\t\tif (!karg)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (info->arg_from_user) {\n\t\tret = info->arg_from_user(karg, arg, cmd);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t}\n\n\tif (info->flags & MEDIA_IOC_FL_GRAPH_MUTEX)\n\t\tmutex_lock(&dev->graph_mutex);\n\n\tret = info->fn(dev, karg);\n\n\tif (info->flags & MEDIA_IOC_FL_GRAPH_MUTEX)\n\t\tmutex_unlock(&dev->graph_mutex);\n\n\tif (!ret && info->arg_to_user)\n\t\tret = info->arg_to_user(arg, karg, cmd);\n\nout_free:\n\tif (karg != __karg)\n\t\tkfree(karg);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\n\nstruct media_links_enum32 {\n\t__u32 entity;\n\tcompat_uptr_t pads; /* struct media_pad_desc * */\n\tcompat_uptr_t links; /* struct media_link_desc * */\n\t__u32 reserved[4];\n};\n\nstatic long media_device_enum_links32(struct media_device *mdev,\n\t\t\t\t      struct media_links_enum32 __user *ulinks)\n{\n\tstruct media_links_enum links;\n\tcompat_uptr_t pads_ptr, links_ptr;\n\tint ret;\n\n\tmemset(&links, 0, sizeof(links));\n\n\tif (get_user(links.entity, &ulinks->entity)\n\t    || get_user(pads_ptr, &ulinks->pads)\n\t    || get_user(links_ptr, &ulinks->links))\n\t\treturn -EFAULT;\n\n\tlinks.pads = compat_ptr(pads_ptr);\n\tlinks.links = compat_ptr(links_ptr);\n\n\tret = media_device_enum_links(mdev, &links);\n\tif (ret)\n\t\treturn ret;\n\n\tmemset(ulinks->reserved, 0, sizeof(ulinks->reserved));\n\n\treturn 0;\n}\n\n#define MEDIA_IOC_ENUM_LINKS32\t\t_IOWR('|', 0x02, struct media_links_enum32)\n\nstatic long media_device_compat_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t\t      unsigned long arg)\n{\n\tstruct media_devnode *devnode = media_devnode_data(filp);\n\tstruct media_device *dev = devnode->media_dev;\n\tlong ret;\n\n\tswitch (cmd) {\n\tcase MEDIA_IOC_ENUM_LINKS32:\n\t\tmutex_lock(&dev->graph_mutex);\n\t\tret = media_device_enum_links32(dev,\n\t\t\t\t(struct media_links_enum32 __user *)arg);\n\t\tmutex_unlock(&dev->graph_mutex);\n\t\tbreak;\n\n\tdefault:\n\t\treturn media_device_ioctl(filp, cmd, arg);\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_COMPAT */\n\nstatic const struct media_file_operations media_device_fops = {\n\t.owner = THIS_MODULE,\n\t.open = media_device_open,\n\t.ioctl = media_device_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = media_device_compat_ioctl,\n#endif /* CONFIG_COMPAT */\n\t.release = media_device_close,\n};\n\n/* -----------------------------------------------------------------------------\n * sysfs\n */\n\nstatic ssize_t show_model(struct device *cd,\n\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct media_devnode *devnode = to_media_devnode(cd);\n\tstruct media_device *mdev = devnode->media_dev;\n\n\treturn sprintf(buf, \"%.*s\\n\", (int)sizeof(mdev->model), mdev->model);\n}\n\nstatic DEVICE_ATTR(model, S_IRUGO, show_model, NULL);\n\n/* -----------------------------------------------------------------------------\n * Registration/unregistration\n */\n\nstatic void media_device_release(struct media_devnode *devnode)\n{\n\tdev_dbg(devnode->parent, \"Media device released\\n\");\n}\n\n/**\n * media_device_register_entity - Register an entity with a media device\n * @mdev:\tThe media device\n * @entity:\tThe entity\n */\nint __must_check media_device_register_entity(struct media_device *mdev,\n\t\t\t\t\t      struct media_entity *entity)\n{\n\tstruct media_entity_notify *notify, *next;\n\tunsigned int i;\n\tint ret;\n\n\tif (entity->function == MEDIA_ENT_F_V4L2_SUBDEV_UNKNOWN ||\n\t    entity->function == MEDIA_ENT_F_UNKNOWN)\n\t\tdev_warn(mdev->dev,\n\t\t\t \"Entity type for entity %s was not initialized!\\n\",\n\t\t\t entity->name);\n\n\t/* Warn if we apparently re-register an entity */\n\tWARN_ON(entity->graph_obj.mdev != NULL);\n\tentity->graph_obj.mdev = mdev;\n\tINIT_LIST_HEAD(&entity->links);\n\tentity->num_links = 0;\n\tentity->num_backlinks = 0;\n\n\tret = ida_alloc_min(&mdev->entity_internal_idx, 1, GFP_KERNEL);\n\tif (ret < 0)\n\t\treturn ret;\n\tentity->internal_idx = ret;\n\n\tmutex_lock(&mdev->graph_mutex);\n\tmdev->entity_internal_idx_max =\n\t\tmax(mdev->entity_internal_idx_max, entity->internal_idx);\n\n\t/* Initialize media_gobj embedded at the entity */\n\tmedia_gobj_create(mdev, MEDIA_GRAPH_ENTITY, &entity->graph_obj);\n\n\t/* Initialize objects at the pads */\n\tfor (i = 0; i < entity->num_pads; i++)\n\t\tmedia_gobj_create(mdev, MEDIA_GRAPH_PAD,\n\t\t\t       &entity->pads[i].graph_obj);\n\n\t/* invoke entity_notify callbacks */\n\tlist_for_each_entry_safe(notify, next, &mdev->entity_notify, list)\n\t\tnotify->notify(entity, notify->notify_data);\n\n\tif (mdev->entity_internal_idx_max\n\t    >= mdev->pm_count_walk.ent_enum.idx_max) {\n\t\tstruct media_graph new = { .top = 0 };\n\n\t\t/*\n\t\t * Initialise the new graph walk before cleaning up\n\t\t * the old one in order not to spoil the graph walk\n\t\t * object of the media device if graph walk init fails.\n\t\t */\n\t\tret = media_graph_walk_init(&new, mdev);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&mdev->graph_mutex);\n\t\t\treturn ret;\n\t\t}\n\t\tmedia_graph_walk_cleanup(&mdev->pm_count_walk);\n\t\tmdev->pm_count_walk = new;\n\t}\n\tmutex_unlock(&mdev->graph_mutex);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(media_device_register_entity);\n\nstatic void __media_device_unregister_entity(struct media_entity *entity)\n{\n\tstruct media_device *mdev = entity->graph_obj.mdev;\n\tstruct media_link *link, *tmp;\n\tstruct media_interface *intf;\n\tunsigned int i;\n\n\tida_free(&mdev->entity_internal_idx, entity->internal_idx);\n\n\t/* Remove all interface links pointing to this entity */\n\tlist_for_each_entry(intf, &mdev->interfaces, graph_obj.list) {\n\t\tlist_for_each_entry_safe(link, tmp, &intf->links, list) {\n\t\t\tif (link->entity == entity)\n\t\t\t\t__media_remove_intf_link(link);\n\t\t}\n\t}\n\n\t/* Remove all data links that belong to this entity */\n\t__media_entity_remove_links(entity);\n\n\t/* Remove all pads that belong to this entity */\n\tfor (i = 0; i < entity->num_pads; i++)\n\t\tmedia_gobj_destroy(&entity->pads[i].graph_obj);\n\n\t/* Remove the entity */\n\tmedia_gobj_destroy(&entity->graph_obj);\n\n\t/* invoke entity_notify callbacks to handle entity removal?? */\n\n\tentity->graph_obj.mdev = NULL;\n}\n\nvoid media_device_unregister_entity(struct media_entity *entity)\n{\n\tstruct media_device *mdev = entity->graph_obj.mdev;\n\n\tif (mdev == NULL)\n\t\treturn;\n\n\tmutex_lock(&mdev->graph_mutex);\n\t__media_device_unregister_entity(entity);\n\tmutex_unlock(&mdev->graph_mutex);\n}\nEXPORT_SYMBOL_GPL(media_device_unregister_entity);\n\n/**\n * media_device_init() - initialize a media device\n * @mdev:\tThe media device\n *\n * The caller is responsible for initializing the media device before\n * registration. The following fields must be set:\n *\n * - dev must point to the parent device\n * - model must be filled with the device model name\n */\nvoid media_device_init(struct media_device *mdev)\n{\n\tINIT_LIST_HEAD(&mdev->entities);\n\tINIT_LIST_HEAD(&mdev->interfaces);\n\tINIT_LIST_HEAD(&mdev->pads);\n\tINIT_LIST_HEAD(&mdev->links);\n\tINIT_LIST_HEAD(&mdev->entity_notify);\n\n\tmutex_init(&mdev->req_queue_mutex);\n\tmutex_init(&mdev->graph_mutex);\n\tida_init(&mdev->entity_internal_idx);\n\n\tatomic_set(&mdev->request_id, 0);\n\n\tdev_dbg(mdev->dev, \"Media device initialized\\n\");\n}\nEXPORT_SYMBOL_GPL(media_device_init);\n\nvoid media_device_cleanup(struct media_device *mdev)\n{\n\tida_destroy(&mdev->entity_internal_idx);\n\tmdev->entity_internal_idx_max = 0;\n\tmedia_graph_walk_cleanup(&mdev->pm_count_walk);\n\tmutex_destroy(&mdev->graph_mutex);\n\tmutex_destroy(&mdev->req_queue_mutex);\n}\nEXPORT_SYMBOL_GPL(media_device_cleanup);\n\nint __must_check __media_device_register(struct media_device *mdev,\n\t\t\t\t\t struct module *owner)\n{\n\tstruct media_devnode *devnode;\n\tint ret;\n\n\tdevnode = kzalloc(sizeof(*devnode), GFP_KERNEL);\n\tif (!devnode)\n\t\treturn -ENOMEM;\n\n\t/* Register the device node. */\n\tmdev->devnode = devnode;\n\tdevnode->fops = &media_device_fops;\n\tdevnode->parent = mdev->dev;\n\tdevnode->release = media_device_release;\n\n\t/* Set version 0 to indicate user-space that the graph is static */\n\tmdev->topology_version = 0;\n\n\tret = media_devnode_register(mdev, devnode, owner);\n\tif (ret < 0) {\n\t\t/* devnode free is handled in media_devnode_*() */\n\t\tmdev->devnode = NULL;\n\t\treturn ret;\n\t}\n\n\tret = device_create_file(&devnode->dev, &dev_attr_model);\n\tif (ret < 0) {\n\t\t/* devnode free is handled in media_devnode_*() */\n\t\tmdev->devnode = NULL;\n\t\tmedia_devnode_unregister_prepare(devnode);\n\t\tmedia_devnode_unregister(devnode);\n\t\treturn ret;\n\t}\n\n\tdev_dbg(mdev->dev, \"Media device registered\\n\");\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__media_device_register);\n\nint __must_check media_device_register_entity_notify(struct media_device *mdev,\n\t\t\t\t\tstruct media_entity_notify *nptr)\n{\n\tmutex_lock(&mdev->graph_mutex);\n\tlist_add_tail(&nptr->list, &mdev->entity_notify);\n\tmutex_unlock(&mdev->graph_mutex);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(media_device_register_entity_notify);\n\n/*\n * Note: Should be called with mdev->lock held.\n */\nstatic void __media_device_unregister_entity_notify(struct media_device *mdev,\n\t\t\t\t\tstruct media_entity_notify *nptr)\n{\n\tlist_del(&nptr->list);\n}\n\nvoid media_device_unregister_entity_notify(struct media_device *mdev,\n\t\t\t\t\tstruct media_entity_notify *nptr)\n{\n\tmutex_lock(&mdev->graph_mutex);\n\t__media_device_unregister_entity_notify(mdev, nptr);\n\tmutex_unlock(&mdev->graph_mutex);\n}\nEXPORT_SYMBOL_GPL(media_device_unregister_entity_notify);\n\nvoid media_device_unregister(struct media_device *mdev)\n{\n\tstruct media_entity *entity;\n\tstruct media_entity *next;\n\tstruct media_interface *intf, *tmp_intf;\n\tstruct media_entity_notify *notify, *nextp;\n\n\tif (mdev == NULL)\n\t\treturn;\n\n\tmutex_lock(&mdev->graph_mutex);\n\n\t/* Check if mdev was ever registered at all */\n\tif (!media_devnode_is_registered(mdev->devnode)) {\n\t\tmutex_unlock(&mdev->graph_mutex);\n\t\treturn;\n\t}\n\n\t/* Clear the devnode register bit to avoid races with media dev open */\n\tmedia_devnode_unregister_prepare(mdev->devnode);\n\n\t/* Remove all entities from the media device */\n\tlist_for_each_entry_safe(entity, next, &mdev->entities, graph_obj.list)\n\t\t__media_device_unregister_entity(entity);\n\n\t/* Remove all entity_notify callbacks from the media device */\n\tlist_for_each_entry_safe(notify, nextp, &mdev->entity_notify, list)\n\t\t__media_device_unregister_entity_notify(mdev, notify);\n\n\t/* Remove all interfaces from the media device */\n\tlist_for_each_entry_safe(intf, tmp_intf, &mdev->interfaces,\n\t\t\t\t graph_obj.list) {\n\t\t/*\n\t\t * Unlink the interface, but don't free it here; the\n\t\t * module which created it is responsible for freeing\n\t\t * it\n\t\t */\n\t\t__media_remove_intf_links(intf);\n\t\tmedia_gobj_destroy(&intf->graph_obj);\n\t}\n\n\tmutex_unlock(&mdev->graph_mutex);\n\n\tdev_dbg(mdev->dev, \"Media device unregistered\\n\");\n\n\tdevice_remove_file(&mdev->devnode->dev, &dev_attr_model);\n\tmedia_devnode_unregister(mdev->devnode);\n\t/* devnode free is handled in media_devnode_*() */\n\tmdev->devnode = NULL;\n}\nEXPORT_SYMBOL_GPL(media_device_unregister);\n\n#if IS_ENABLED(CONFIG_PCI)\nvoid media_device_pci_init(struct media_device *mdev,\n\t\t\t   struct pci_dev *pci_dev,\n\t\t\t   const char *name)\n{\n\tmdev->dev = &pci_dev->dev;\n\n\tif (name)\n\t\tstrscpy(mdev->model, name, sizeof(mdev->model));\n\telse\n\t\tstrscpy(mdev->model, pci_name(pci_dev), sizeof(mdev->model));\n\n\tsprintf(mdev->bus_info, \"PCI:%s\", pci_name(pci_dev));\n\n\tmdev->hw_revision = (pci_dev->subsystem_vendor << 16)\n\t\t\t    | pci_dev->subsystem_device;\n\n\tmedia_device_init(mdev);\n}\nEXPORT_SYMBOL_GPL(media_device_pci_init);\n#endif\n\n#if IS_ENABLED(CONFIG_USB)\nvoid __media_device_usb_init(struct media_device *mdev,\n\t\t\t     struct usb_device *udev,\n\t\t\t     const char *board_name,\n\t\t\t     const char *driver_name)\n{\n\tmdev->dev = &udev->dev;\n\n\tif (driver_name)\n\t\tstrscpy(mdev->driver_name, driver_name,\n\t\t\tsizeof(mdev->driver_name));\n\n\tif (board_name)\n\t\tstrscpy(mdev->model, board_name, sizeof(mdev->model));\n\telse if (udev->product)\n\t\tstrscpy(mdev->model, udev->product, sizeof(mdev->model));\n\telse\n\t\tstrscpy(mdev->model, \"unknown model\", sizeof(mdev->model));\n\tif (udev->serial)\n\t\tstrscpy(mdev->serial, udev->serial, sizeof(mdev->serial));\n\tusb_make_path(udev, mdev->bus_info, sizeof(mdev->bus_info));\n\tmdev->hw_revision = le16_to_cpu(udev->descriptor.bcdDevice);\n\n\tmedia_device_init(mdev);\n}\nEXPORT_SYMBOL_GPL(__media_device_usb_init);\n#endif\n\n\n#endif /* CONFIG_MEDIA_CONTROLLER */\n", "patch": "@@ -142,6 +142,8 @@ static long __media_device_enum_links(struct media_device *mdev,\n \n \t\tfor (p = 0; p < entity->num_pads; p++) {\n \t\t\tstruct media_pad_desc pad;\n+\n+\t\t\tmemset(&pad, 0, sizeof(pad));\n \t\t\tmedia_device_kpad_to_upad(&entity->pads[p], &pad);\n \t\t\tif (copy_to_user(&links->pads[p], &pad, sizeof(pad)))\n \t\t\t\treturn -EFAULT;\n@@ -159,6 +161,7 @@ static long __media_device_enum_links(struct media_device *mdev,\n \t\t\tif (entity->links[l].source->entity != entity)\n \t\t\t\tcontinue;\n \n+\t\t\tmemset(&link, 0, sizeof(link));\n \t\t\tmedia_device_kpad_to_upad(entity->links[l].source,\n \t\t\t\t\t\t  &link.source);\n \t\t\tmedia_device_kpad_to_upad(entity->links[l].sink,", "file_path": "files/2016_8\\109", "file_language": "c", "file_name": "drivers/media/media-device.c", "outdated_file_modify": null, "outdated_file_before": null, "outdated_file_after": null}], "outdated": 0}
